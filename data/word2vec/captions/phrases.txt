 Welcome to this course on compilers My name is Alex Aiken Im a professor here
 at Stanford University And were_going to be talking_about the implementation of
 programming_languages gtgt There_are two major approaches to implementing
 programming_languages compilers and interpreters Now this class is mostly
 about compilers But I do want to say a few words about interpreters here in the
 first lecture So what does an interpreter do Well Im gonna draw a
 picture here this box is the interpreter and it takes let_me label it with a big
 I it takes as input your_program That you wrote And whatever data that you
 want to run the program on And it produces the output directly Meaning that
 it doesnt do any processing of the program before it executes it on the
 input So you just write the program and you invoke the interpreter on the data
 and the program immediately begins running And so we can say that the
 interpreter is is online meaning it the work that it does is all part of running
 your_program Now a compiler is structured differently So we can draw a picture
 here Which well label with a big C for the compiler And the compiler takes as
 input just your_program And then it produces an executable And this
 executable is another program might be assembly_language it might be bytecode
 It could be in any number of different implementation languages But now this can
 be run separately on your data And that will produce the output Okay And so in
 this structure the compiler is offline Meaning that we preprocess the program
 first The compiler is essentially a preprocessing step that produces the
 executable and then we can run that same executable on many_many different inputs
 on many different data sets without having to recompile or do any_other processing of
 the program I_think its helpful to give a little_bit of history about how
 compilers and interpreters were first developed So the story begins in the
 1950s and in particular with a machine called the 704 built by IBM Thi s was
 their first commercially successful machine although there had been some
 earlier machines that they had tried out But anyway the interesting thing about the
 704 well once customers started buying it and using it is that they found that the
 software costs exceeded the hardware costs And not just by a little_bit but
 by a lot And This is important because these the hardware in these those days
 was extremely expensive And even then when hardware cost the most in absolute
 and relative terms more_than they would ever cost again already the software was
 the dominant expense in in making good use out of computers And this led a
 number of people to think_about how they could do a better job of writing software
 How they could make programming more productive Where the earliest efforts to
 improve the productivity of programming was called speed coding developed in 1953
 by John Backus Now speed coding is what we call today an early example of an
 interpreter And like all interpreters it had some advantages and disadvantages The
 primary advantage was that it was much faster to develop the programs So the
 in that sense the programmer was much_more productive But among its
 disadvantages code written speed code programs were ten to twenty times slower
 Then handwritten programs and thats also true of interpreted programs today So if
 you have an implementation that uses an interpreter theyre going to be much
 slower than either a compiler or writing code by hand And also the speed code
 interpreter took up 300 bytes of memory And that doesnt seem like very much In
 fact 300 bytes today would seem like an incredibly tiny program But in those
 days you have to keep in mind that this was 30 Percent Of the memory on the
 machine So this was 30 percent of the entire memory of the 704 And so the
 amount of space that the interpreter took up was itself a concern Now speed coding
 did not become popular but John Backus thought it was promising and it gave him
 the idea for another project The most_important applications in those days were
 scientific computations and programmers thought in terms of writing down formulas
 in a form that the machine could execute John thought that the problem with speed
 coding was that the formulas were in fact interpreted and he thought if first the
 formulas were translated in to a form that the machine could execute directly That
 the code would be faster And while still allowing the programmer to write the the
 the programs at a high_level and thus was the Formula Translation Project or FORTRAN
 Project born Now FORTRAN ran from 1954 To 1957 And interestingly they thought it
 would only take them one year to build the compiler but it would end up taking three
 So just like today they werent very good at predicting how long software projects
 would take But it was a very successful project By 1958 over 50 percent of all
 code was written in FORTRAN So 50 percent of programs were in FORTRAN And that is
 very rapid adoption of a new technology We would be happy with that kind of
 success today and of course at that time they were ecstatic And everybody thought
 that FORTRAN both raised the level of abstraction improved programmer
 productivity and allowed everyone to make much better use of these machines So
 FORTRAN one was the first successful high_level language and it had a huge impact on
 computer science In particular it led to an enormous body of theoretical work And
 one of the interesting things about programming_languages actually is the
 combination of theory And practice because its not really possible in
 programming_languages to do a good_job without having both a a very good grasp
 of fairly deep theory and also good engineering skills So theres a lot of
 very good systems building material in programming_languages and typically it
 involves a very subtle and fruitful interaction with theory And so and this
 is one of the things I_think thats most attractive about the areas the subject of
 studying computer science And the impact of FORTRAN was not just on computer
 science research of course but also on the development of practical compilers
 And in fact its influence was so profound that today auto compilers still
 preserve the outlines of FORTRAN one So what was the structure of FORTRAN one
 Well it consists five phases lexical_analysis and parsing which together take
 care of the syntactic aspects of the language semantic_analysis which of
 course takes care of more semantic aspects things_like types and scope
 rules Optimization Which is a collection of transformations on the program to
 either make it run_faster or use less memory And finally code_generation which
 actually does the translation to another generation And depending_on our goals
 that translation might be to machine codes It might be to a bite code for a
 virtual machine It might be to another high_level programming_language Well
 thats it for this lecture and next time well pick up here and talk_about these
 five phases in more_detail
 Welcome_back in this second half of the lecture well continue with our overview
 of the structure of a compiler Recall that a compiler has five major phases
 lexical_analysis parsing semantic_analysis optimization and code
 generation And now were_going to briefly talk_about each one and were_going to
 explain how a compiler understands these with an analogy to how humans understand
 English The first step at understanding a program both for a compiler and for a
 human is to understand the words Now humans can look_at this example sentence
 and immediately recognize that there are four words this is a and sentence And
 this is so automatic that you dont even think_about it but there is inaudible
 real computation going on here You have to recognize the separators namely the
 blanks And the punctuation things_like the periods and also clues like capital
 letters And these help you to divide up this group of letters into a bunch of
 words that you can understand And just to emphasize that this is not completely
 trivial lets_take a look_at this sentence And you can read this but it
 takes a little_bit of time Because Ive put the separators in in odd places So
 you can see the word is the word this the word a and the word sentence But
 again this isnt something that comes to you immediately You actually have to do
 some work to see where the divisions lie Because theyre not given to you in the
 way that youre used to The goal of lexical_analysis then is to divide the
 program_text into its words or what we call in compiler speak the tokens So
 heres an an example piece of program_text now instead of a piece of English
 text and lets walk_through this and identify the tokens So theres some
 obvious ones that are keywords like if and then gtgt And else that we want to
 identify And then there are variable_names things_like X and Y and Z
 Theres also constants things_like number one and the number two And then there
 are some operators double_equals is one and the assignment operator is another
 And heres already an interesting question How do we know that double
 equals is not two individual equals signs How do we know that we want this To be a
 double equal so we want and not two single equals Well we dont_know right
 now but well talk_about that gtgt In the lecture on how we implement Lexico
 analysis But were not done with all the tokens in this example either theres a
 few more The semi colons the punctuation are also tokens and then the separators
 are also tokens so heres a blank thats a token heres another blank thats
 another token and then there are lots of blanks here that serve to separate things
 like the keywords and the variable_names and other symbols from each other And
 those are the tokens of this example So for humans once the words are understood
 the next step is to understand the structure of the sentence and this is
 called parsing And as we all learned in elementary school this means diagramming
 sentences and these diagrams are trees and its a very_simple procedure Lets
 look_at this example This line is a longer sentence The first step in parsing
 is to identify the role of each word in the sentence So we have things_like nouns
 and verbs and adjectives But then the actual work of parsing is to group these
 words together into higher_level constructs So for example this
 particular sentence consists of a subject a verb and an object okay And that
 actually forms an entire sentence So right here we have the root of the tree
 called a sentence and thats broken down into constituent parts The high_level
 structure as we said is subject verb to object And then the subject is more
 complicated as is the object And this is an example of parsing an English sentence
 The analogy between parsing English text and parsing program_text is very strong
 In_fact theyre exactly the same_thing So heres our little example piece of code
 again so lets work through parsing it So clearly this is an if then else
 statement and so the root of our diagram of our parse_tree is gonna be if
 then else inaudible Nothing else consists of three parts Theres a
 predicate a then statement and an L statement And now lets look_at the
 predicate which consists of three pieces Theres a variable a comparison operator
 and another variable and together those form a relation So the comparison between
 two things is one of the things you can have as a valid predicate Similarly the
 then statement consists of an assignment where Z gets one and the else statement also
 has the form of an assignment Z gets two And to all together this is a parse_tree
 of the ifthenelse showing its structure breaking it up into its
 constituent pieces Now once_weve understood the sentence structure the
 next step is to try to understand the meaning of what has been written And
 this is hard So actually we dont_know how this works for humans still We_dont
 understand what_happens after lexical_analysis and parsing We do know that
 people do lexical_analysis and parsing in much the same way that compilers
 lexically analyze and parse programs But frankly understanding meaning is
 something that is simply too hard for compilers So the first important thing
 to understand about semantic_analysis is that compilers can only do very limited
 kinds of semantic_analysis And in particular the kinds of things that
 compilers generally do are try to catch inconsistencies So if the program is
 somehow self inconsistent inaudible compilers can often notice that and report
 errors But they dont really know what the program is supposed to do As an
 example of the kind of thing that we do in semantic_analysis again using an analogy
 in English lets_consider the following sentence So Jack said Jerry left his
 assignment at home And the question is what who does his refer to here It
 could be that his refers to Jerry in which case we would read Jack said Jerry
 left Jerrys assignment at home Or it could refer to Jack In which case we
 could read the sentence as Jack said Jerry left Jacks assignment at home And
 without more information we actually dont_know which one His is referring to
 whether its Jack or its Jerry And even worse lets_take a look_at this sentence
 down_here Jack said Jack left his assignment at home And the question is
 how many people are actually involved in this sentence It could be as many as
 three there could be two separate Jacks and his could even refer to somebody
 completely_different We_dont know without seeing the rest of the story That
 surrounds this sentence all the possibilities for his But it could also
 be as few as only a single person It could be that Jack and Jack and his are
 all the same person in this sentence And so this kind of ambiguity is a real
 problem in semantic_analysis And the analogy in programming_languages is
 variable bindings So we would have variables in this case a variable called
 Jack or maybe more_than one variable called Jack And a programming_language is
 going to have very strict rules to prevent the kind of ambiguities we had in the
 English sentences on the previous_slide So you_know in this example Question is
 what value is printed by this output statement and the answer is its going to
 print four because this use of the variable Jack binds to this definition
 here And the outer definition is hidden So the outer definition is not active in
 this scope because it is hidden by the inner definition and that is just a
 standard rule of a lot of lexically scoped programming_languages Now the pilots
 perform many semantic texts besides analyzing the variable bindings And so
 heres another example in English So Jack looked her homework at home And under
 the usual naming conventions assuming that Jack is male we know theres a type
 mismatch between Jack and her So we know that whatever her is it is not Jack
 And and therefore we known that this sentence is talking_about two different
 people And so this is analogous to type_checking The fourth compiler phase
 optimization doesnt have a very strong counterpart in everyday English usage but
 its a little_bit like editing And in fact its a lot like what professional
 editors do when they have to reduce the length of an article to get it down to
 some word budget So for example I have this phrase right here but a little_bit
 like ending and if I didnt like it if I thought it was too long I could replace
 the middle four words with two words Akin to So now it says but akin to
 editing and that means exactly the same_thing as the original phrase but uses
 fewer words And the goal in program optimization Is to modify the program so
 that it uses less of some resource Maybe we want to use less time we want the
 program to run_faster maybe we want it to use less space so that we can fit more
 data in memory For a handheld device we might be interested in reducing the amount
 of power that it uses If we have external communication we might be interested in
 reducing the number of network messages or the number of database accesses And
 theres any number of resources that we might want to improve other programs use
 of So heres a simple example of the kinds of optimizations a program might do
 We can have a rule in our compiler that says X equals Y times zero is the same as
 X equals zero And this seems like a real improvement because instead of doing the
 multiply we can just do an assignment So we save some computation by doing that
 Now unfortunately this is not a correct rule And this is one of the important
 things to know about compiling optimization is that its not always
 obvious when its legal to do certain optimizations or not Now it_turns out
 That this particular rule is valid for integers Okay so if X and Y are
 integers then multiplying by zero is always the same_thing as just signing
 zero But its invalid for floating_point And why is that well because you
 have to know some details of the IEEE floating_point standard but there is a
 special number in the IEEE standard called not a number and it_turns out that
 not a number called a NaN times zero is equal to not a number Any particular
 nonnumber times zero is not equal to zero If X and Y are plotting point_numbers you
 cant do this optimization In_fact if you did this optimization it would break
 certain very important algorithms that rely on the proper propagation of
 not a number Finally the last compiler phase is code_generation often
 referred to as Code Gen and Code Gen can produce assembly_code Thats the most
 common thing that a compiler would produce But in general its a
 translation into some other language And this is entirely analogous to human
 translation So just as a human translator might translate English into French a
 compiler will translate a high_level program into assembly_code To wrap_up
 almost every compiler has the five phases that we outlined However the proportions
 have changed a lot over the years and if we were to go_back to FORTRAN I and
 look inside of that compiler we would probably see a size and complexity that
 looks something_like this We have a fairly complex lexical_analysis phase an
 equally complicated parsing phase a very small semantic_analysis phase a A fairly
 involved optimization phase and another fairly involved code_generation phase And
 so we see a compiler where the complexity was sp spread fairly evenly throughout
 except for its semantic_analysis which is very weak in the early days And today
 if we look_at a modern compiler youll_see almost nothing in lengthening very little
 in parsing because we have extremely good tools to help us write those two phases
 We would see a fairly involved thematic analysis phase We would see a very large
 optimization phase and this is in fact the dominant component off all modern
 compilers and the a small codegeneration phase because again we understand that
 phase very very well Thats it for this lecture Future lectures well look_at
 each of these phases in detail
 Hello In this video_were going to talk_about something that Ive referred to as the economy
 of programming_languages
 So the idea_behind this video is that before we get into the
 details of how languages are implemented or designed I wanted to say something about
 how languages work in the real world and why certain languages are used and others are not
 And if you look around theres actually a few obvious questions that come up to anybody
 who thinks about programming_languages for more_than a few_minutes One question is
 why are there so many of these things We have hundreds if not thousands of programming
 languages in everyday use and why do all these things need to exist Why wouldnt one
 programming_language for example be enough
 A related question but slightly different is why are there new programming_languages
 That given that we have so many programming_languages already what is the need for new
 ones to be created And finally how do we know a good programming_language when we see
 it So what makes a good programming_language and what makes a bad programming_language
 I just want to spend this video talking_about these three questions And as well see I
 think the answers to these questions are largely independent of the technical aspects of language
 design and implementation But very interesting in their_own right
 So lets begin_with the question of why there are so many programming_languages And at
 least a partial answer to this question is not too hard to come by If you think for
 a few_minutes youd realize that the application_domains for programming have very distinctive
 and conflicting needs That is its very hard to design one language that would actually
 do everything in every situation for all programmers
 And lets just go_through some examples One domain that you might not think_about very
 much is scientific computing So these are all the big calculations that are done for
 engineering applications primarily but also big science and long running experiments
 simulation experiments
 And what are the needs for such computations Well typically you need very good floating
 point support Ill abbreviate that as FP You need good support for arrays and operations
 on arrays because the most_common data type in most scientific applications is larger
 arrays of floating_point numbers And then you also need parallelism
 Today to get sufficient
 performance you really have to exploit parallelism in these applications
 And its not every language actually supports all of these things well This is actually
 not an exhaustive list of the things you need but a few distinctive things that are needed
 But one language that has traditionally done a very good_job of supporting these things
 is Fortran And Fortran is still heavily used in the scientific community
 It was originally designed for scientific applications If you recall that the name
 means formula translation And it has evolved over time It_doesnt really look much like
 the original language anymore but its always retain this core constituency in scientific
 computing and remains one of the leading languages in that domain
 Now a completely_different kind of a domain is business applications And so what do you
 need here Well so here youre going to need things_like persistence You dont want to
 lose your data Businesses go to a lot of trouble to get the data and they need a way
 to hold onto it and they want that to be extremely reliable
 Youre going to need good report facilities Because typically you want to do something
 with the data So you need good facilities for report generation And also you want
 to be_able to exploit the data The datas actually in many modern businesses one of
 the most valuable assets and so you need good facilities for asking questions about your
 data Lets call it data analysis
 And again this is not an exhaustive list of things that you need but it is representative
 I would say And probably the most_common or one of the most_common used languages for
 this class of applications is SQL the database query language So relational databases and
 their associated programming_language languages I should_say but most notably SQL really
 dominate in this application_domain
 And then another domain lets do one more is systems_programming
 So by this I_mean things_like embedded systems things to control devices operating systems things_like that
 And what are the characteristics here Well we need very low level control of the resources
 The whole point of systems_programming is to do a good_job of managing resources and
 so we really want fine grained control over the resources And often theres a time aspect
 so you might have some real time constraints So you need to be_able to reason about time
 Because these are actually again devices and they need to be_able to react within certain
 amount of time if its a network device or something_like that you need to be responsive
 to the network Lots and lots of examples where timing is important And these are just
 two aspects and Im a little_bit Im running out of space here so Ill just stop with that
 But again these are representative of the kinds of things you need in systems_programming
 And probably today still the most widely_used systems_programming language or family systems
 of programming_languages is the C and to some extent C family of languages
 And as you can see the requirements in these different domains are just completely
 different from each other Whats important in one domain or most_important in one
 domain is not the same as in another domain And its easy I_think to imagine at_least
 that it would be difficult to integrate all of these into one system that would do a good
 job on all of these things
 That brings us to our second question Why are there new programming_languages
 There_are so many languages in existence why would we ever need to design a new one And Im
 going to begin the answer to this question with an observation that at first glance has
 nothing to do with the question at all So let_me just take a moment to explain it
 I claim that programmer_training is the dominant_cost for a programming_language And I_think
 this is really important so just going to emphasize the bit thats important here Its
 the programmer_training The cost of educating the programmers in the language So we think
 about a programming_language there are several things that have to happen for that language
 to get used Somebody has to design it But thats really not very expensive Thats just
 one or very few people typically Somebody has to build a compiler but that is also
 not actually all that expensive Maybe 10 to 20 people for a really large compiler project
 can build quite a good compiler
 The real cost is in all the users and educating them So if you have thousands or hundreds
 of thousands or millions of users of the language the time and money that it takes to teach
 them all the language is really the dominant_cost And here I dont mean just the actual
 dollar expense of buying textbooks and taking classes and things_like that
 Its also the fact that the programmers have to decide its_worth it for them to learn
 this language and many programmers learn on their_own time but thats a use of their
 time and the expense of their time is a real economic cost And so if you think_about the
 number of hours that it takes to teach a population of a million programmers a language thats
 really quite a significant economic investment
 Now from this observation we can make a couple of predictions pretty easily And again these
 are just predictions now that follow from this claim If you believe that its true
 So let_me erase it and fix it So first prediction is that widely_used languages will be slow
 to change And why should that be true Well if I make a change to a language of lots of
 people use I have to educate everybody in that community about the change And so even
 relatively minor language extensions small changes to syntax small new features even
 just simple changes in the interface of the compiler if you have a lot of users it takes
 a very long_time and is quite expensive to teach them all about that
 So as languages become widely_used the rate of change their rate of change will slow
 down And this predicts over time as the world of programming grows as we have more more
 programmers in the world we would expect the most popular languages which will have
 larger and larger user bases so larger and larger programmer basis to become more and
 more ossified To evolve more and more slowly And I_think actually what you see in practice
 is very consistent_with that prediction
 Now at the other end of the spectrum this same observation makes an almost what appears
 to be contradictory prediction which is that easy to start its easy to start a new language
 That in fact the cost of starting up a new language is very low And why is that Well
 because you start with zero users and so there is essentially zero training_cost at the beginning
 and then even when you have just a few users the cost of teaching them the changes in the
 language is not very high And its so new languages can evolve much_more quickly They
 can adapt much_more quickly to changing situations And its just not very costly to experiment
 with a new language at all And theres a tension between these two things
 When is a programmer going to choose between a widely_used existing language that perhaps
 doesnt_change very quickly and a brand new language Well theyre going to choose it
 if the productivity if their productivity exceeds the training_cost So if they perceive
 that by spending a little_bit of time and money to learn this new language theyre
 going to be much_more productive over a relatively short period of time then theyre going to
 make the switch
 So when is this likely to happen Well putting this all together languages are most likely
 to be adopted to fill a void And again this is a prediction that follows from the fact that programmer
 training is the main cost What do I_mean by this Well what I_mean is that programming
 languages exist for purpose people use them to get work done And because were still
 in the middle of the information revolution and there are new application_domains coming
 along all the time
 So there are new kinds of programming that emerge every few years or even more often
 than that So just in terms of recent history mobile applications are now something thats
 relatively new And theres a lot of new technology being built up to support mobile computing
 A few years ago it was the internet itself was a new programming platform and a bunch
 of new programming_languages like Java in particular got started during that time
 So a new programming niche is open up because the technology changes that what people want
 to do with software changes And this creates new opportunities for languages The old languages
 are slow to change and so they have some difficulty in adapting to fit these new domains And
 they arent really necessarily well suited to them for the reasons we talked_about on
 the previous_slide with the previous question because its hard to have one language that
 incorporates all the features you would want
 And so there are so these languages are not necessarily perfect for these application
 domains Theyre slow to adapt to the new situation And this tends to call forth new
 languages So when theres a new opportunity and some application_domain If there are
 enough programmers to support the language often a new language will arise
 Just want to point out another prediction that can be made from this one observation
 That programmer_training and Ill underline that is a dominant_cost per programming_language
 And that is that new languages tend to look like old languages That is that new languages are rarely if ever completely
 new They have a family resemblance to some predecessor language sometimes a number of
 predecessor languages
 And why is that Well partly that its hard to think of truly new things But also I_think
 if theres an economic benefit to this namely that it reduces the training_cost by having
 your new language look like an old language by leveraging off what people already know
 about the old language you make it easier for people to learn the language and make
 them learn it more_quickly And the most classic example of this is a Java versus C where
 Java was designed to look a lot like C And that was I_think very conscious to make
 it easy for all of the existing C programmers to start programming in Java
 Finally we can ask ourselves what is a good programming_language And here unfortunately
 the situation is much less clear I would just make one claim that there is no and Ill
 emphasize no universally_accepted metric for language design And what I_mean by that
 Well I guess the most_important part of this statement is the universally_accepted bit
 So I_mean that people dont agree on what makes a good language
 There_are lots of metrics out there and people have proposed lots of ways of measuring programming
 languages but most people dont believe that these are very good measures and there is
 certainly no consensus If you just look_at the world of programmers they cant agree
 on what the best language is and to convince yourself of this just go and take a look
 at any of the many news group posts where people get into a semi religious arguments
 about why one group of languages or particular language is better_than another language
 But even in the research community in the scientific community in among people who design
 languages I would say there is no universally_accepted consensus on what makes a good language
 And to just kind of illustrate the difficulties in trying to come up with such a metric let
 me discuss one that Ive heard people propose in all seriousness and that is that a good
 language is one people use And let_me put a question mark on that because I dont believe this
 statement
 And I_think a moments reflection with a moments reflection I can convince you that this isnt
 a great measure On the positive side I guess the argument for this is that its a very
 clear measure It measures the popularity of the language How many people are actually
 using it and presumably languages are more widely_used for a good reason In some sense
 perhaps they are better languages But this would imply if you believe this and follow
 it its logical conclusion that Visual_Basic is the best language above all other programming
 languages
 And Ive nothing against Visual_Basic Its a well designed system but I dont even think
 the designers of Visual_Basic would claim that it is in fact the worlds best programming
 language And as we saw in the discussion that we just had there are many_many other
 factors besides technical excellence that go into whether a programming_languages is
 widely_used or not
 And in fact technical excellence is probably not even the most_important reason that a
 language might be used It has much_more to do with whether it addresses a niche or application
 domain for which there isnt a better tool And then once its established and has lots
 of users of course theres inertia in history that aided in surviving And thats why we
 still have Fortran and Cobalt and lots of other languages from long long ago that we
 could if we were starting over today designed much better
 So to conclude this video on the economy of programming_languages I_think the two most
 important things to remember are that application_domains have conflicting needs and therefore
 it is difficult to design one system that incorporates everything that you would like
 to have So you cant get all the features that you would like into a single system in
 a coherent design at_least its very hard to do that and so it takes a lot of time to
 add new features to existing systems
 And the second point is that programmer_training is the dominant_cost for programming
 language And together these two things these two observations these really explain why
 we get new programming_languages because the old languages are difficult to change
 and when we have new opportunities its often easier and more direct to just design a new
 language for those rather_than trying to move the entire community of programmers and an
 existing systems to accommodate those new applications
 Hello in this and the next few_videos Im going to be giving a overview of COOL the
 programming_language in which youll be writing a compiler Cool is the Classroom
 Object Oriented Language and the acronym of course is COOL And the unique design
 requirement for COOL is that the compiler has to be_able to be written in a
 relatively short period of time We only have one quarter or in some cases a
 semester for students to write the compilers And so COOL has to be
 implementable quickly And actually since its used primarily for teaching
 compilers the number of COOL compilers in the world vastly exceeds the number of
 COOL programs So there many_many more compilers have been written thousands of
 compilers maybe tens of thousands of compilers have been written for COOL but
 probably only some dozens or hundreds COOL programs And so its probably the
 only language in existence for which this is true That that the number of
 compilers actually exceeds the number of programs but it does Tell you about the
 main design requirement Its much_more important in COOL that the compiler be
 easy to write then that it be easy to write programs in And so there are some
 quirks in the language Things that have been done specifically to make it easier
 to implement where that wouldnt take away from the the teaching value of the of
 the language But that would make it inconvenient to use the language on a
 daytoday basis as a working programmer So what is in the language Well its
 weve tried to design it so that it will give you a taste of modern notions of
 extraction static typing reuse through inheritance automatic_memory management
 And theres actually a few more things that well talk_about when we come to
 them But many things are left out Were not gonna be_able to put everything in the
 language and have it be implementable quickly Well be_able to cover some
 things in lectures but unfortunately therell even be some interesting language
 ideas that we wont be_able to get to in this class So the course project is to
 build a complete compiler And specifically youre going to compile COOL
 into MIPS assembly_language So MIPS is a real instruction set It was for a machine
 that was designed in the 1980s And there is a simulator for MIPS that runs on just
 about any kind of hardware And so this makes the the whole project very
 portable We can run your compiler or you can generate MIPS assembly_language and
 then that MIPS assembly_language can be simulated on just about whatever kind of
 machine you have access to The project is broken up into five assignments First
 youre gonna write a COOL program And that program itself will be an interpreter
 to give you a little_bit of experience with writing a simple interpreter And
 then the compiler itself will consist of the four the phases that we discussed
 lexical_analysis parsing semantic_analysis and code_generation And all of
 these phases I should emphasize are inaudible compatible Meaning that we
 have separate implementations separate reference implementations of each of
 these And so for example when you are working_on semantic_analysis you will be
 able to take the lexical_analysis parsing and code_generation components
 from the reference compiler and plug your semantic_analysis into that Framework
 and and test it against the reference components And so this way if you have
 trouble with one component or arent sure that your components is working very well
 you wont have a problem in working_on a different component because youll be_able
 to test that independently And finally theres_no required optimization
 assignment But we do have some suggestions for optimizations that you can
 do And many people have written optimizations for COOL And so this is an
 optional assignment if youre interested in learning something about program
 optimization So lets write the simplest possible COOL program And the first_thing
 to know is that COOL source files and in the extension dot CL for COOL and you can
 use whatever editor you like to write your programs I happen to use Emacs you can
 use some other editor if you like And every COOL program has to have a class
 called main And lets talk_about that business in a second So a class
 declaration in COOL begins_with the key_word class Followed by the name of the
 class So in this case main Followed by a pair of curly braces And inside the
 curly braces is where all the stuff that belongs to the class goes And every class
 declaration must be terminated by a semicolon So a program consists of a
 list of class declarations Each class declaration terminated by a semicolon So
 thats the structure of a class And now we need this class to actually do
 something so were_going to have a method in this class and lets call the method
 main In_fact the main_method of the main class must always exist This is the
 method thats run to start the program and furthermore this method must take no
 arguments So the empty argument list for the main_method is always empty And lets
 say the main_method its body always goes in a pair of curly braces So the main
 method always goes inside curly braces And a class consists of a list of such
 declarations And again those declarations must all be separated_by
 semicolons So in or terminated excuse_me by semicolons So in this case we
 only have one method in the class But still has to have its semicolon and now
 we can say what we want the method actually do so this is the place for the
 code for the method goes and lets just have the simplest possible method the one
 that just event evaluates to the number one Okay so inaudible an expression
 language which means that wherever a piece of code can go you can put an
 arbitrary expression any expression can go there theres_no explicit return
 statement for a method Its just a value of the method body is the value of the
 methods So in this case we just put the number one in there and that will be the
 value of this method when we run it So lets save that And now we can try
 compiling this simple program so how do we compile the compiler is called a COOL c
 for the COOL compiler and you just give the COOL compiler a list of COOL source
 files So in this case theres just one file 1CL hit enter and ooh we got a
 syntax_error so we have to come_back and fix that and the error said at or near the
 open curly brace on line three theres a mistake And I know what the mistake is
 because Im a competent COOL programmer at_least somewhat competent COOL
 programmer Cool methods must declare their return type So we need to put a
 type here And the syntax for the declaration is to put a colon after the
 name of the method and the argument list and then the name of a type And since
 were returning the number one for this program for sorry for this method we
 might as well say that the main_method is going to return an_integer So save that
 Go back to our compilation window and lets compile the program again And this
 time it compiles successfully And now if we look in our directory we see that there
 is a new file called 1s Thats the assembly_code for the program one And now
 we could try to run this code And the The the Mitch simulator is called spin
 and it just takes a assembly file to to simulate And so we just give it one
 inaudible hit enter and it will run A whole bunch of stuff is printed out But
 as you can see it says part way down that the COOL program successfully executed so
 thats good and then afterwards there are some statistics and things_like number of
 instructions executed a number of loads and stores a number of branches those
 things would be interesting if were worried about performance if we were to
 say working_on the optimization of the compiled code but were not doing that
 right now Were just running programs And we can see if this program works So
 the program ran It terminated successfully But it didnt actually
 produce any output And thats because we didnt ask it to produce any output If we
 want to have output We have to go_back and modify the program again So so what
 this program does currently is that it just returns its value but that but
 nothing is done with that value Its not printed out or anything like that If you
 wanted to have something printed out in a COOL program you have to do that
 explicitly So theres a special class built in a primitive class called IO And
 we can declare whats called a attribute of this class it will be a IO attribute
 and it will be called I okay and I will be a object that we use to do IO So now
 in our main_method Here we could add a call to outstring I dot outstring is
 how we invoke a method Okay so outstring is a method of the IO class so we use I to
 invoke that method and then we can pass it a string that we want printed out on the
 screen So for example we could say hello_world Okay And now we have to decide
 what to do with our with our number one there And let_me show you one more
 feature of COOL Lets leave the one there and lets make it part of a
 statement_block So a statement_block consists of a sequence of expressions
 separated_by semicolons And you can have any number of expressions and the
 semantics of a statement_block or an expression block is to just evaluate the
 expressions in order And the value of the block is the value of the last expression
 But now a statement or an expression block has to be included in its_own set of
 curly braces Okay so that now is a valid COOL program so let_me just read this for
 you so the body of the program is a block of expressions The first one executes A
 out string call to the object I which is going to print hello_world for us And
 then the second one evaluates to one which is the value of the entire of the
 entire method Okay actually I should_say its the value of the block okay and
 then because the block is the body of the method the value of the block becomes the
 value of the entire method So one will be returned from this method call So lets
 save this Go back over_here and lets compile this again So Looks like I
 failed to save it Lets compile this and we see we have a syntax_error And so it
 says on line four we have a syntax_error at or near our closing curly brace And
 the problem here is that a statement_block or expression block consists of a
 series or a sequence of expressions terminated by semicolons and we forgot
 to terminate the last expression in the sequence by its semicolon So we have to
 add that And now we should be_able to compile this and lo and behold it
 compiles correctly and then we can run it And now we see oh we got another
 mistake So we have an when the program ran it complained that we have a
 dispatched void So that on line four our dispatch was to an object that didnt
 exist And you can see the dispatch call right here to I and it doesnt exist
 because in fact we forgot to allocate an object for I So here we declare I to be
 of type IO but that doesnt actually create any objects That just says that it
 creates a variable_name I but I doesnt actually have a value So if you want I to
 actually have a value we have to initialize it to something So we can
 initialize it to a new IO object And new here is the way you allocate new objects
 in COOL and new always take a type argument so in this case were creating a
 new object in type IO and were assigning it To this object i And notice here that
 I is a is a is what would be called a field name in Java Its what we call an
 attribute in COOL So so these are the data el the data elements of the of the
 class And so the class can have both of names of things that are so attributes or
 fields that hold values as well as methods that can perform computation sound
 Lets save this and switch back And now well compile this again So and it still
 compiles And now we can run it And now it runs and low and behold as you can
 see down there third line from the the top it prints out hello_world And that
 looks a little_bit ugly because the the successful execution message is on the
 same line as our hello_world message So lets fix that Lets come_back over_here
 And in our string here we can add a new line Okay at the end of the string so
 backslash N is how you write a new line character in the string Save that come
 back over_here lets compile So if you dont_know Unix bang will repeat the
 previous expression the previous command that began_with the same prefix that you
 type after the bang So I want to run the last command that began_with C which is to
 compile and then I want to run the last command that began_with S which is to run
 spin And now we can see there it is all nice hello_world is on a line by_itself
 Lets continue now lets sound clear all this out sound So let_me just show
 you a few variations on the same program What Im going to do here is just rewrite
 it in a couple of different ways So I just illustrate a couple of features of
 COOL and get you more familiar_with the syntax and also just show some
 alternative ways to do the same_thing So you_know this this A block here of of
 expressions is kind of a clumsy way to to implement the Hello World program So
 lets get_rid of that Lets get_rid of the the block Lets get_rid of the one
 here at the end Okay lets just make the statement body a single expression again
 and and now the problem were_going to have is that the types wont match But
 just to illustrate that let_me show it to you so lets do COOL C of one dot CL and
 youll_see here that in complains that the inferred return type of the IO of the
 method main does not conform to the declared return type INT So coming back
 over_here the to the program The the compiler figured out that this expression
 I dot out string yields an object of type IO So it returns the i object as the
 results evaluating this expression And that does not match the type it And so
 naturally the compiler says hey somethings wrong with the types Well
 thats easily repaired We can just change the return type or the main_method to say
 it returns something of type IO So lets go_back over_here and see if that now
 works So we compile the program And then we run spin on the output and yes
 everything still works as expected Now We_dont have to be so specific about the
 type over_here since were not actually using the result of the method body for
 anything I_mean the program just exits once it prints the string We could have
 allowed ourselves more flexibility here We couldve just declared the result type
 of main to be of type Object So Object is the root of the class hierarchy in COOL
 Every other class is a subclass of Object So lets come_back over h lets save this
 first And then we can come_back over to our compilation window We can compile it
 And we can run it and it still works So now another thing we can do if we want is
 we could observe Here that this attribute that we declare this field I isnt really
 necessary Here we we allocate you_know we have a special name I when the main
 object is constructed to run the program a new inaudible object is allocated to I
 and then that gets used in the main_method We can actually just do all of
 that inside the main_method itself by just allocating a new inaudible object right
 here and then calling out string on that object Alright So this should also work
 And lets check it out So it compiles And lo and behold it rots Alright So
 coming back over_here lets illustrate one more or a couple more things that we
 could do So we could also say that inaudible inherits From IO So we have
 to have the IO functionality somewhere in order to call the out string method So we
 have been doing that by creating a separate object of type IO But now we can
 say well just the main object is itself And something that has all the
 capabilities of IO by inheriting from IO And if youve seen any inaudible
 language before this will be a familiar concept So main here gets all the
 attributes and methods of IO in addition to whatever attributes and methods of its
 own that it will have And now Instead of of having to allocate a new IO object in
 order to call out string we can just invoke it on self Which is the name of
 the current object when the main_method runs In other languages self is called
 this Okay and so lets we saved it so lets go over and compile this So it
 compiles it compiles and and it runs right So last example here we dont have
 to name self actually in this dispatch Theres a feature that allows_us to call a
 method without explicitly naming the object on which its dispatched and
 defaults to self so if no object is named in a dispatch then its just a dispatched
 self So this should also work sound And indeed it does So that concludes our
 first example In the next couple of videos well look_at some more_complex
 examples of COOL programming
 Welcome_back In this video_were going to look_at another example of cool
 programing This time lets move beyond the simple hello_world kind of examples
 and on to something more exciting say the ever popular factorial function So
 in order to write factorial well need to open a file which we can write some code
 Let_me start that And recall from last_time that every cool program has to have a
 main class and the main class is required to have a main_method And we dont care
 what the main_method return so well just have it return something a type object
 and then well just fill in a skeleton here on the file And so now were_ready
 to write some codes So what are we going to have the main_method do Well before
 we can actually write factorially before we can get to the guts of this program
 which is actually not very difficult We need to talk_about IO some more Because
 were_going to need to be_able to read and write numbers We need to be_able to
 read numbers from the the user whos running the program and print them back
 out So lets just review little_bit about IO also In order to invoke the IO
 functions we need an IO object And one of the IO functions is something that
 prints out a string So lets just write a program that we already know how to do
 just to confirm that we remember that And we can compile this program And it
 should just print one And lets see Indeed it does Okay So it prints out the
 number one And so now lets come_back here and lets talk_about how to do input
 So instead of just printing out the number one lets print_out a strain that
 the user types in So ins in here were_going to read a strain And in order
 to do that we need an IO object because there is another function another method
 called instrain Okay And so this will renew this string And return to string
 And to make_sure that we get the nice output lets concatenate on to that string
 a new line So this is just to When it prints this string back out it will be
 printed on i ts own line So lets try compiling this And steak It compiles
 And now we can run spin Remember the bang command in UNIX runs the previous command
 that began_with the same letters And now the program runs and it waits Cause its
 waiting for me to type something And if I type it type in one it prints back one and
 if I type in 42 it gives me back 42 And so now The next_thing we need to talk
 about is how to convert strings into integers because if were_going to do a
 factorial we want to work on integers and not strings And at the moment were just
 reading and writing strings So there is a library written in cool that does
 conversion between integers and strings And were_going to give the main class
 here the functionality of that class Which is called A2I for ASCII to integer
 And that defines a bunch of methods that can convert between strings and integers
 So lets add those commands in here So here heres our string That weve read
 in And what we want to do now is to convert this into an_integer So let_me
 just add a couple of parens here So theres our string okay And now were
 going to invoke On that the method Im_sorry were_going to call on that the
 function the method A to I Okay And lets just double check here that weve
 got friends in the right place So thats the argument to A2I Now we recall that
 when we have a dispatch to a method and its just sitting by_itself with not
 object its dispatched to the self_object And the self_object is the object
 of the current class that were in In this case the main object which has
 inherited the A2I methods And so the A2I functions should be defined in there Now
 we have an_integer And we can do something with that integer if we like
 So lets add some more inaudible here and lets just say we add just one to the
 integer Okay And then once were done with our integer Whatever operation it is
 that we want to do with the integer we need to convert it back to a string so
 that we can print it out An theres an inverse function I2A that will do So I
 dont_know if we have all the parens in the right places at this point So
 lets just check Yes That looks_like that should work So this will read in a
 string convert convert it to an_integer add one to it convert it back to a string
 concatenate on a new line and print it out Now lets see if all that actually
 works so lets run the compiler and we have a problem here It says that we have
 an undefined class A to I And the reason is we didnt supply the code for A2I So
 if we look in our directory here well see Ive already copied in the class file
 for A2I And I encourage you to go and look_at that code Its actually
 interesting code to see how the conversions are written in COOL
 But now we need to talk_about how to compile a program that uses a library And
 the way you do it is very_simple You just list all the class files on the command
 line when you call up the compiler And it will read them all in and treat them as a
 single program So in this case we compile compile fact together with A2I
 And that complies And then we can run it sound And now if I type in three it
 prints four And if I type in one it prints two And so the program seems to be
 working And now were almost ready to write our factorial function So what do
 we want to do in factorial Well we want to do something other than just adding
 one Instead we want to call our special function factorial So lets insert a call
 to factorial in here Okay and lets get_rid of the plus one And then lets check
 that we have all the parens that we need So we need to close off the the A2I call
 the factorial call The I2A call And then that last one should be the out string
 call And it is Okay So now we can add a method fact to this class And fact is
 gonna take an_integer argument So we need a parameter here And its type is in
 force And the whole_thing is gonna return an_integer And then we have body of our
 function And probably a good idea here just to make_sure that we got this much
 right to do something simple So lets just try to make a function that returns
 one more_than its argument So this will do exactly the same_thing that we had
 before and lets just confirm that that is working So EW compile with the A2I
 library and now we have a syntax_error And we see that I forgot the closing
 semicolon here for the method Remember the the class body is a list of methods
 and each method is terminated by a semicolon Must try compiling that again
 Now it compiles Lets run it We type in four gives a spec five Alright So looks
 right were_ready now to actually right the code for factorial And this is gonna
 be anti climactic because its actually a very_simple code if we write it
 recursively So lets do that So hows that going to work Well everybody knows
 the definition by Hardy Hope If I is equal to zero then the factorial of zero
 is one And we have a keyword there then one otherwise the factorial is going to
 be I times the factorial of I minus one Right and then if statements in
 inaudible always end in the keyword inaudible so its an if then else
 inaudible construct And that is actually the entire definition So now we
 should have a program that actually computes factorial it compiles so now
 lets run it So factorial of three is six And factorial of six is 720 and that
 looks right And if we try it one more time with a bigger number we get a we
 get a large number we think thats probably correct And so anyway our
 factorial function is working So now lets come_back here and just as an
 exercise lets rewrite this code iteratively So instead of using a
 recursive function lets write it using a loop And in order to that inaudible
 gonna get_rid of that code What are we going to need Well were_going to need
 an accumulator here Were_going to need a a local variable that we can use to
 accumulate the results of the factorial computation And the way you declare local
 variables in inaudible is with inaudible Statements or naudibl
 expressions So were_gonna have lets call this variable fact for the result
 of factorial And notice here that I can have a variable that has the same name as
 the function And the programming_language inaudible will not get
 confused about that Because variables and functions play different roles So well
 have the factorial fact excuse_me its of type Int and we do initialize it to
 one Alright so that multiplication will work I_think that the default for
 integers is to be initialized to zero and that would not be good if were_going to
 be multiplying up fact with other with other numbers Alright so then the a let
 has two parts It has the variable or variables that you are declaring This
 could actually be a list of variables Well only have one this time And then it
 has a body The the expression or the computation in which the fact variable is
 available And what do we want to do So I_think were_gonna need to have this be a
 statement_block cause were_gonna need to have more_than one statement in the
 sequence And well see why in just a minute But then we wanna have a loop And
 so what is our loop going to do Well were_gonna say while I is not equal to
 zero What do we and what do we need to do the opening for the loop_body the
 opening keyword is called loop sound And now I_think were_going to need
 another statement_block here So lets open up a block Were_gonna probably need
 to do more_than one thing The first_thing we want to do is we want to have fact Be
 fact times I so we know that I is not zero so we need to multiply the current
 value of I into fact to accumulate the result and then we want to subtract one
 from I and notice that the assignment statement in cool is this backwards arrow
 Thats how you do assignment Its also how you do initialization So
 initialization and assignment look the same Then we can close off our statement
 block Okay so the body of a while_loop is always a single expression In this
 case that expression is a block that consists of two statements sound And
 then we can close the loop And the closing for a loop is the pool key_word
 And then now were in a statement_block so this has to end with a semicolon
 Notice the statement_block up there from the let And now we want the result of the
 let block or the let expression to be factorial So whatever the whatever we
 got out of the while_loop whatever we computed in the while_loop we want that
 to be the result of the entire let expression sound so thats the last
 statement of our blog Remember the last statement of a statement blog is the
 value of the blog The body of the led is the the result of the led so fact will
 also be result of the whole led statements its just the result of the
 statement blog And since the body of the factorial method itself is just the led
 expression fact will be the result of the whole_thing And so this if weve
 written havent made any mistakes should be an iterative version of factorial So
 lets compile this And amazingly it complies on the first try And now lets
 run it sound And whoa It actually works So we got six And lets just do
 one more test to see that to convince ourselves that things are working
 reasonably well and they are Now let_me just point out one common mistake that
 you can easily make and that I make when I havent written cool programs for a
 little while If youre a C or programmer or a Java programmer you might think
 about writing assignments like this So I just use the equal sign to write
 assignment That looks completely fine if youre if youre familiar_with those
 languages or used to programming in those languages And now lets see what_happens
 when we try to compile this Oh it compiles just fine And then what_happens
 when we try to run it Well it runs inaudible input so lets give it input
 and Then we see that weve run out of heat And that looks_like an infinite
 loop So were_going around and around the loop And consuming memory for some reason
 And well we laugh get to that much later in the class Why why this loop
 actually ends up consuming memory But clearly we dont have enough memory in
 the loop and and eventually we run out And so so thats a sure sign Of an
 infinite look So what is going on here Well the thing is that equals Equals
 operator in cool is the comparison operators inaudible Well we compared I
 with O and that returns a boullion So these are perfectly valid cool
 expressions They just happen to be boullions So you dont ever actually I
 or factorial in this program Your just comparing fact with factoid body and I
 with I minus one and the program is perfectly happy to do that It just
 doesnt compute the factorial function And it never terminates because I never
 reaches zero So that concludes our factorial example And well do one more
 example next time of a more_complicated of a of a cool program with some
 nontrivial data_structures
 Hello again In this video_were gonna wrap_up our overview of cool with one more
 example of writing a cool program For our final example lets look_at a program
 that actually manipulates some interesting data_structure So well begin here by
 opening up a file And lets call our program list dot CL this time And as
 usual I will begin by writing our main routine and our main_method And once
 again lets lets make this inherit from IO so we can do the IO routines here
 And lets just begin_with something very_simple as always Lets just have
 something that prints out quotHello Worldquot but in a little_bit of an unusual way
 Lets were_going to end up writing a list a list abstraction And lets first
 build a list by hand or at_least build the elements of a list by hand and then
 well actually build the list abstraction and put them in a list So lets have
 some strings So we will have our string hello and this will also illustrate how
 you do multiple LET bindings simultaneously I shouldnt say
 simultaneously How you do multiple LET bindings in one LET expression So you do
 them by just listing them and notice that this uses commas as a separator rather
 than semicolons as a terminator So this left binding is going to define three
 names Hello world and new line all of which are strings And then Were_going to
 now print these out on the screen so we are going to want to be_able to do out
 string and since main inherits_from self we can do that without an object there
 because it just again dispatches to the self_object And we want to concatenate
 these strings together in the right order So well do hello dot and since hello is
 a string it can be concatenated to world and world is a string so it can be
 concatenated to newline And that should do the job And just like probably one more
 thing about this lead this these lead bindings here This notice if it the
 comma is the comma is the separator here meaning it doesnt come_after the last one
 on the list So it just separates that into the list so its not a terminator
 And now we can close up our main procedure Close up our class definition
 Save it and now see if it compiles Oh amazing First try And we run it and it
 prints hello_world as expected So now lets instead of introducing the three
 strings separately and then concatenating them together lets write an abstraction
 where we can build a list of strings And then that abstraction will have a function
 within it to do the to do the concatenation Alright so well have a
 class called list And every list needs to I_think to have two components So
 first its gonna have the item thats in the list and thatll be a string And
 then when you have a pointer to the next to the to the tail of the list to the
 rest of the list And so I have a next field that points or is another list is
 another list of strings Now we need a couple of methods in order to in order to
 use this list Well need to be_able to initialize a list in some way So the
 initialization function will take an item and the rest of the list the next part
 of the list And what is it going to do Well its gonna need to set the fields of
 the object And so this one has to be done as a series of assignment statements So
 well need a statement_block and we will set the item to be the I argument Well
 set the next attribute to be the N argument And now we actually want this
 initialized object here this this method here to return the object itself So
 that and thatll be convenient for chained together calls to inaudible So
 well have it return self Itll return the self_object And thats the end of out
 statement_block And then that is the end of our method And I made a mistake up
 here Weve gotta declare the return type of a knit And what its going to return
 of course it returns a a object of type list Ill need to put a list declaration
 there Alright so that takes care of of a knit And now we can use this to build
 build a list down_here So what should we do Lets actually have a new variable
 called list That well introduce here in this lead this series of lead bindings
 And lets just build a list out of these three objects So well say well have a
 new list and then well initialize to contain the string hello And What should
 the rest of the list be Well that should be another list which is initialized to
 have the string the world And what should be inside of that list file is another
 new list object which well initialize to have new line And now what do we put
 here actually theres a little_bit of problem here isnt there We need to put a
 list object here But we dont want to allocate a a new list object we want
 that to be really the equivalent of a of a null pointer And theres_no name for
 that in Cool actually you cant write_down the name of a null pointer Its
 called void in Cool Theres no theres_no no special symbol for that So well
 have to create a variable that is just not initialized And that will be as well
 uninitialized variable of type list will in fact be void Itll be a null pointer
 So lets call that nil And itll be a typed list and no initializer And so nil
 there will point to nothing or the the void point And then we can use nil to
 terminate our list here And then we have to close off all the params for all the
 nesting here And I_think thats it And so that will be our list Okay so we have
 a list of three strings And now what we want to do with that is to print it out
 And so what we would like to do is to have a list fall off the list and then a
 function thats gonna flatten that list and well just print it So that is the
 what the what the main program should do And now You have to write the flatten
 function So flatten takes no arguments And its going to return a string Its
 going to return a single string And flatten is a pretty simple function U h
 what do we have to do Well Theres really two cases One is if were at the
 end of the string and the other is if were not yet at the end of the string So
 lets test for that So how do we know if were at the end of the string Well if
 the next pointer is is void then there is no nothing more in the string And
 there actually is a special test for that in Cool Its called the isvoid function
 and its written like this So if isvoid of next okay So inaudible next field
 So if the next field is void then what are we going to return Keeps Well then
 the result here is just the item whatever the item was in this last element
 of the list And otherwise What we want to do Well otherwise we want to take the
 item and we want to concatenate onto it the result of flattening the rest of the
 list And that is our flattened method So lets see if that works So lets compile
 this And we got a couple of syntax errors here So lets go_back and see whats
 going on So we have a syntax_error here at the end of the the flatten method And
 we see that we left out the inaudible to close a conditional So a conditional has
 to be ended with with inaudible Alright and lets see if thats working
 now And we still have a syntax_error at line 29 And the mistake here is that we
 forgot to declare the type of this variable which is a list And then it
 gets initialized to this to this big expression that we wrote out Lemme just
 do the indentation a little more nicely here And notice something actually is
 worth_mentioning here that this definition here this definition of the variable
 list Depends on the definition of the previous variables in the let So each of
 so when a let binding is made the name of the variable thats bound is actually
 available in subsequent let expressions So in this case this variable list makes
 use of all of hello_world and newline Which were defining earlier in the same
 lead constr uct Lets save this and come over_here and compile it And we see we
 got another bug in the code So we come up here We see we Ive made a mistake
 here Ive used functional notation here calling flattened of next and what I
 actually wanted to do was to dispatch to next on the method flattened So that
 should be written like that All_right Probably getting close now Lets see if
 it works yet Well it compiles and now lets see if it runs And indeed it does
 Prints out hello_world Just as we expected Now lets go_back to our program
 and lets generalize this list abstraction in one way Lets_say that we can have an
 arbitrary list of objects not just strings And that will require us to
 change a few things so it can be initialized now with an object And now
 when it comes time to flatten this list we want to reduce a string We want to
 present produce a print representation But not everything in the in the list is
 necessarily a string And we need a way to traverse the list and do different things
 for different_kinds of things that might be in the list for different types of
 things that might be in the list And so theres a constructing cool for the type
 of and of an object at run_time and this is called the case construct So
 let_me first introduce a let expression here well let the string that were
 gonna construct Which is of type string And thats going to be initialized to
 something and now its going to be a case And what do we put a case on Well
 its going to depend_on the kind of thing the item is So the item in the list could
 be it could be different_kinds of types and we want to do a different operation
 depending_on what item actually is So the case item and then the key_word is of and
 now we have different branches of the case expression for different_kinds of things
 that could be in the list So lets_say that if its an inaudible Okay so what
 this does is this says that if the item is an INT then were_going to rename it
 to I were_going to bind I to that integer and then we can do something with
 I And what would we want to do with I Well wed probably want to convert it to
 a string So well do I to A quotIquot And what if in fact That item happened to be of
 type string The items in the list have to be of type string Well then we can just
 use the item itself as the string representation And we can do this for
 other kinds of types If we had other kinds of types in our system we could
 continue to list out other cases here and how to convert them into a string
 representation But lets just have a default case here Well say if its any
 other kind of type which would which would be covered by having a branch
 saying that if its of type object well call it O then we should just
 abort And so we should just call the abort function and quit And thats our
 case It needs to be terminated with a closing keyword called esag Again the
 reverse of of case And now we can use that inaudible we constructed in our
 little function here So if if the next field is void then were just gonna return
 the string Otherwise were_gonna return this string incatenated with the
 flattening out of the rest of the list Kay Now theres a couple of things we
 have to fix up We used the I to A method here which means that list needs to
 inherit From the conversion class A to I And theres another issue here I see
 And thats right here So if you notice The the The case statement needs to
 produce a string Okay and it_turns out that abort dose not return a string abort
 actually terminates the program but its type is that it returns an object And so
 here we have to convince the type_checker to convince to accept this piece of
 code and we need to get this branch here to type as a string So what we can do
 and this is ugly but its the way to do it is we put it in a block in a
 statement_block We call abort first and a gain that will just terminate the
 program And now we can put any string expression we want after that and thatll
 be the thatll give a type string to the entire block So we can just put the empty
 string here for example and that has to be terminated with a semicolon since this
 isnt a block And we can close that with a curly brace Okay So this is just
 something we have to do to make the type_checker happy And that may be everything
 we needed to do So lets try compiling this And we have to include the diversion
 library And we have one syntax_error So far And thats because we forgot to put
 the semicolon terminator on on each of our each of the each of the variables
 that we were introducing in the let Okay got to save that Lets try this again
 And oops I didnt actually manage to fix the syntax_error And thats because I put
 the semicolon in the wrong place Actually I I forgot Variables that are
 bond in a lent are separated_by by commas But the branches of the case have
 to be terminated by semicolons So what I said before was incorrect about using
 semicolons to terminate lent bindings Its just in case branches where we need
 it In this example Alright anyway coming back to this lets see if it
 compiles And it does And now lets run it And it works Now of course we havent
 actually exploited the ability to have different types of objects in the list So
 lets lets do that Lets add an_integer in here Type ints and lets give it a
 number 42 And we can insert it in here And now we can pass any object to a knit
 in the first_position So well just put in 42 right there And when we compile and
 run this it just print hello_world 42 If we if everything goes as expected And it
 does And that concludes our little tour of inaudible There_are a few features
 that we havent shown in these examples but you can look in the examples
 directory for lots more programs many more pro grams that will show you all the
 different ins and outs and details of the other language features as well as the
 ones weve covered here
 Welcome_back This is the first video in our long series of the
 implementation of compilers The call from last_time that a compiler has five phases
 Were_gonna begin by talking_about lexical_analysis and this will probably take us
 three to four videos to get through at_least and then well we will be moving on
 in order to the other phases Lets start by looking_at a small code fragment The
 goal of lexical_analysis is to divide this piece of code up Into lexical units so
 things_like the keyword if the variable_names i n j and the relational operator
 doubleequals and so on Now as a human being this is As we discussed last_time
 this is a very easy thing to do because there are all kinds of visual clues about
 where the units lie Where the boundaries between the different units lie but a
 program like lexical_analyzer It_doesnt have that kind of luxury In_fact what
 the what the likes of analyzer will see is something that looks more like this So
 here I overwritten the code out just as a string with all the white_space symbols
 included and is from from this representation this is a linear string
 you can think of this as bytes in the file that the lexical_analyzer has to work and
 its going to mark through placing dividers between the different units So
 it will recognize that theres a division there between the white_space and the
 keyword Then a division after the keyword and theres more a wide space the open
 paren the i another wide space double_equals and so on and it goes through
 drawing these lines diving up The the string into its lexical unit So I wont
 finish the whole_thing but you should get the idea Now it doesnt just place these
 dividers in the string however It_doesnt just recognize the substrings It also
 needs to classify the different elements of the string according to their role We
 call these token_classes Or sometimes Ill just call it the class of the token
 And in English these roles are things_like noun verb adjective Okay and there
 is ther e are many more or at_least or some more And in the programming
 language the classes the token_classes would be things_like identifiers
 Keywords I and then individual pieces of syntax like an open_paren or a close
 paren those are the classes by themselves A numbers And again there
 are more classes but theres a thick set of classes and each one of these
 corresponds to some set of strings that could appear in a program So token
 classes correspond to sets of strings And inaudible strings can be described
 relatively straightforwardly so for example The token_class of identifiers in
 most programming_languages might be something_like strings of letters or
 digits starting_with a letter So for example a variable_name or identifier
 could be something_like a1 or it could be f00 or it could be b17 all of those
 would be be valid identifiers and often often theyll be additional characters
 that allowed identifiers but thats the basic_idea Very very often The main
 restriction identifiers that they have to start with a letter An integer and
 typical definition of an_integer is a nonempty string of digits So something
 like zero or twelve Okay One followed_by two I should_say is actually a string of
 number in this case And and yeah it is actually whether admit some numbers you
 might not think of Things_like 001 would be a valid representation of a number or
 even 00 could be a valid integer according to this definition Keywords are typically
 just a fix set of reserved words and so here Ive listed a few else if begin
 and so on And then white_space as itself a token_class so we actually have to say
 in that string which is the representation of the program what every character in
 that string what token or what token_class its a part of What every substring
 is a part of and that includes the white_space So for example if we have a series
 of three_blanks if I say if and then an open_paren and I have three_blanks in
 here these three blank s would be grouped together as white_space So the goal of
 lexical_analysis is to classify substrings of the program according to their role
 This is the the token_class okay Is it a keyword a variable identifier And then
 to communicate these tokens to the parser So drawing a picture here lets
 switch_colors The lexical_analyzer communicates with the parser Okay and the
 functionality here is that the lexical_analyzer takes in a string Typically
 stored up also just a sequence of bytes and then when inaudible to the parser is
 sequence or pairs which are the token_class And substring which I would say
 string here that that of which is the sets of string which is a part of the
 input along with the class the role that it plays in the in the language and this
 pair together is called a token So for example if my string is that f00 42
 all right then that will go to the lexical_analyzer and that will come Ill
 write_down here three tokens And these would be identifier Who Operator say
 equals And Integer excuse_me 42 And here I just left these things as strings
 to to emphasize that these are strings So this is not the number 42 at this point
 in time its its the string 42 which is a plays an_integer role in the programming
 language And then these and when the price that takes this input is this
 sequence of pairs So the lexical_analyzer essentially runs over the input string and
 chunks it up into the sequence of pairs where each pair is a token_class and a
 substring of the original input As we turn to the example from the beginning of
 the video here it is written out as a string And our goal now is to lexically
 analyze this fragment of code We want to go_through and identify the substrings
 that are tokens and also their token_classes So to do this were_gonna need
 some token_classes So lets give ourselves some of those to work with
 Well need white_space And and so this is sequences of blanks new lines tab
 things_like that with the keywords And well need variables which well call
 identifiers And well need integers and now Ill call those numbers Here and then
 were_going to have some other operations some other classes things_like open_paren
 close_paren and semi colon and these are interesting These three ae interesting
 because theyre single_character token_classes that is is a set of strings but
 is only is only one string in the set so the open_paren corresponds to exact
 inaudible strings that contain only open_paren So often the punctuation marks of
 the language are in token_classes all by themselves Another piece of punctuation
 that well add here is is assignments That will be a token_class by_itself
 because its such an important operation But the double_equals will class as a
 relational operator with this class as an operator put it up here Alright So now
 what were_going to do is were_gonna go_through and tokenized this string and Im
 going to write_down for each substring What class it is You_know Im just gonna
 use the first letter here of the class Its indicated just to save time so I
 dont have to write everything up Hence we change colors so we can do this in a
 different color So the first token here is white_space token and then that
 followed_by the F keyword So okay And then we have a blank here which is another
 white_space and then the open_paren which is its_own token_class so Ill just leave
 it to identify itself there and then we have an identifier Okay White space and
 then an operator the doubleequals Another blank so thats white_space
 followed_by another identifier followed_by close parens Again a punctuation mark in
 a token_class by_itself And then we have three white_space characters so those are
 group together as a white_space token Followed by another identifier and more
 white_space and then another single_character token the assignment operator
 white_space and a number And then sem i colon again and punctuation mark and a
 token_class by_itself Two white_space characters can group together What
 follows in is a keyword so it gets classified as in the keyword token_class
 Another run of white_space characters and then another identifier Theres actually
 a blank there where we almost covered it up without marks The assignment operator
 by_itself in a token_class white_space a number and finally the semi colon by
 itself And there is our tokenization Weve identified the substrings and weve
 also labeled each one with its token_class
 Welcome_back In this video_were going to continue_our lecture on lexical
 analysis with some examples from past programming_languages where interesting
 lexical problems arose So weve_already talked a little_bit about Fortran and what
 are the interesting lexical rules in Fortran is the white_space is
 insignificant so white_space doesnt_matter and something_like VAR1 to which
 could be a variable_name VAR1 is exactly the same as VA R1 so these two program
 fragments have to mean exactly the same_thing sand the idea in Fortran is that you
 can take your_program and you could delete all the blanks from it and that shouldnt
 change what the program means at all Lets_take a look_at an example of how
 Fortrans white_space rule affects lexical_analysis Here are a couple of Fortran
 code fragments and I should_say that this example is taken from the dragon book and
 actually couple of the later examples were also taken from an older edition of the
 dragon book But anyway what we have here this is actually the header of a Fortran
 loop And you_know its a loop because it has the key_word do which is like four in
 modern C or C so Id say loop key_word And then we have out iteration variable I
 and we have a range that I will vary between So in this case I will go from
 one up to 25 And then this number five here this is a little_bit odd something
 you dont see in modern languages In the old days in Fortran you would have your do
 statement at the top of the loop and then the size of the loop or all the statements
 included in the loop Were named by a label they came right after the do
 statement So the loop will extend from the the header the do statement down to
 the label five So whatever statement was able with five all of the statements in
 between would be part of the loop And so the loop would execute those statements
 then well go_back around to the header and then we keep executing those until it
 had done so for every one of the values of the iteration variable in this case one
 to 25 Now heres a nother code fragment and as you can see this one is almost
 exactly the same as the one above The only difference is let_me just switch
 colors is here that this particular fragment has a comma in that position and
 this fragment has a period And it_turns out that this difference makes all the
 difference that these two fragment of code mean completely_different things So this
 fragment the first one is in fact a do loop as I said before so it has the
 keyword do the label five the variable I and the range one to 25 Now this fragment
 down_here this is actually a variable_name do 5I So far as writing without the
 blanks Remember the blanks dont matter This would be do 5I and this is an
 assignment equals the number 125 Okay And so you can see here these symbols the
 sequence the first sequence of symbols is interpreted completely differently
 depending_on whether theres a period or a comma further on And so lets just be a
 little more precise about that How do we know what do is So lets just focus_on
 the keyword here do and when were at this point when our focus is here right after
 the zero And keep in mind that that the way this is going to be implemented is by
 a left to right scan so were_going to be walking in this direction over the over
 the input looking_at each character successfully and when our focus reaches
 this point we can make a decision Is this a is this a keyword cause weve
 seen the entire keyword too And the problem is that we dont have information
 to make that decision We_dont know_whether this is do or whether its going
 to be eventually be part of a variable_name like do 5I And the only way to know
 is to look_ahead in the input to this position to see whether theres a comma or
 a period there So this is an example of lexical_analysis that requires look_ahead
 In order to understand the role of due as were_going left to right We have to pick
 ahead of the input to see some symbols that come later_on And we cant possibly
 disambiguate role of do until that poin t because up to this point the sequence and
 the symbols are exactly the same and so the only thing that distinguishes them is
 something thats much much further on And as you can imagine having lots of
 look_ahead complicates the implementation of lexical_analysis and so one of the
 goals in the design of lexical systems is to minimize the amount of the look_ahead
 or bound the amount of look_ahead that is required So you might wonder why Fortran
 has this funny rule about white_space It_turns out that on punch card machines it
 was easy to add extra blanks by accidents and as a result they added this rule to
 the language so the punch card operator wouldnt have to redo their work all the
 time Fortunately today we dont enter our programs anymore on punch cards But this
 example does help us understand better what were trying to do in lexical
 analysis so as I said the goal is to partition the string Were trying to buy
 the string up into the logically units of the language And this is implemented by
 reading left to right So were doing a left to right scan over the input
 recognizing one token at a time And because of that look_ahead may be
 required to decide where one token ends and the next token begins And again I
 want to stress that look_ahead is always needed but we would like to minimize the
 amount of look_ahead And in fact we like to bound it to some constant to this
 because it will simplify the implementation of lexical_analyzer quite a
 bit Now just to illustrate to look_ahead is something that we always have to worry
 about Lets consider this example which weve_looked at before and just notice
 that when were reading left to right lets look_at this keyword else here when
 we read the E We have to decide is that a variable_name or some symbol but itself or
 do we want to consider it together with the symbols that follow them And so
 theres a look_ahead issue here After we scanned E we have to decide does that sit
 by_itself or is it part of a larger lexical unit And you_know there a re
 single_character variable_names in this example like I J and Z and so its not
 unreasonable that E could also be one and another example is this doubleequals
 When we read a single equal sign how do we decide_whether thats a single equals
 like these other assignments or that its really a doubleequals Well in order to
 do that if our focus point is right here we have to look_ahead and see Theres
 another coming_up and thats how we know or how we will know That we wanted to
 combine it into a single symbol instead of considering this equals by_itself Another
 example from a a language from long ago PL_inaudible is a interesting language
 It was designed by IBM and it stands_for Programming Language One Alright It was
 designed to be the programming_language At least with an IBM that would be used by
 everybody and is supposed to encompass all the features that every programmer would
 ever need And as such it was supposed to be very very general and have very few
 restrictions And so one of the features of PL_inaudible is that Keywords are not
 reserved So in PL_inaudible you can use a keyword both as a keyword and also
 as a variable So you can use keywords and other roles other than keywords and that
 means you can write interesting interesting sentences or interesting
 programs like this And let_me just read this out loud because it sounds
 interesting if else then then else else then And the correct organization
 here of course is that this is a keyword this is a keyword and this is a keyword
 And the other things switch_colors here are all variables These_are all variable
 names And as you can imagine this mix a lexical_analysis somewhat difficult
 because when were just scanning left to right like when were coming through here
 when we say were at to this point you_know how do we decide_whether these things
 are going to be variable_names or keywords without seeing whats going on in the rest
 of the expression so lexical_analysis in PL_inaudible was quite challenging So
 heres another example from PL_inaudible Here we have a program
 fragment we have the word declare and then an open_paren and a close_paren
 encompassing a bunch of arguments so well point out the balance parens here and then
 just a list of n things inside the parens And it_turns out that the pending on the
 larger context in which this whole expressions sits this could be either a
 keyword Or it could be in array reference that mean when yeah that mean declare
 here could either be a keyword or it could be a name of an_array and this could be
 the end inaudible to the array And as it happens there is no way looking_at
 just this much that we can decide This fragment is valid is a valid declaration
 and its also a valid array reference So it would depend_on what came next It
 might depend_on for example whether there was an equal sign here in which cases
 would be interpreted as an assignment and and declare would be the name of an_array
 And the interesting thing about this example is that because the number of
 arguments in here is unbounded There could be n of them for any n This
 requires unbounded look_ahead Okay So to implement this properly as youre scanning
 left to right to decide_whether declare again is a keyword or rereference we
 would have to scan beyond this entire argument list to see what came next
 Fortren and PL_inaudible were designed in the 1950s and 1960s respectively and
 those experiences taught us a lot about what not to do in the lexical design of
 programming_languages So things are a lot better today but the problems have not
 gone away completely and Ill use an example from C that illustrate this So
 heres an example of C template syntax which you may be familiar_with or you may
 have seen the similar syntax in Java And C has another operator called Stream
 Input So this operator here reads from an input_stream and stores the results in a
 variable And the problem is here that theres a conflict with nested templates
 So for example if I have a template o peration that looks_like this Okay
 Notice what_happens here So my intention here is to have a nested application of
 templates but I wind_up with two great than signs together at the end and this
 looks just like the stream operator and the question is what should the lexical
 analyzer do Should it interpret this as two close brackets for template or should
 it interpret it as a two greater_than signs stuck together as a stream operator
 And it_turns out that for a very long_time I_think most C compilers have now
 fixed this The C compiler in this situation would regard this as a stream
 operator and you would get a syntax there And what do you think the solution was it
 turns_out that the only fix that you could really do to make this lexically analyzed
 the correct way was to insert a blank so you would have to write this and you would
 have to remember to put the blank in there so that the two greater_than signs were
 not together And you_know thats kind of ugly that we have to put in white_space to
 fix the lexical_analysis of the program So to summarize the goal of lexical
 analysis is to partition the input streams into lexemes okay So we have drop down
 dividing lines in the string to decide where the lexemes lie and we want to
 identify the token of each lexeme And because exactly because were doing a
 left to right scan sometimes we have to have look_ahead Sometimes we have to peek
 ahead in the input string to figure_out what the current string were looking_at
 what the current substring were looking_at what role it plays in the language
 In this video_were gonna talk_about regular_languages which are used to
 specify the lexical structure of programming_languages To briefly review
 the lexical structure of a programming_language is a set of token_classes And
 each one of the token_classes consists of some set of strings Now we need a way to
 specify which set of strings belongs to each token_class and the usual tool or
 doing that is to use regular_languages So in this video_were going to present like
 regular_languages and define what they are and then in subsequent videos were_going
 to look_at some examples using them in actual programming_languages To define
 the regular_languages we generally use something called regular_expressions And
 each regular_expression team now its a set There_are two basic regular
 expressions If I write the single_character C thats an expression and what
 at the notes is a language containing one string Which is the single_character C
 okay Thats one basic form so for any single_character I get a language with a
 one string language with just and then the only string is that character Another
 basic building block of regular_languages is the regular_expression epsilon which is
 the language That contains again just a single string this time the empty_string
 And one thing thats important to keep in mind is that epsilon is not the empty
 language okay So this is not correspond to the empty_string and the empty set of
 strings It is a language that has a single string namely the empty_string
 Besides the two base regular_expressions there are three compound regular
 expressions and well just go_through them here in order The first is a b which
 corresponds to the union of the languages a and b So this would be the set a such
 that a is in the language of big A little a is in the language of big A union
 little b such that b is in the language of little b so just the union of the two sets
 of strings Concatenation is like string concatenation So if I have two languages
 a and b or two regula r expressions a and b then the concatenation of a and b
 Is equal to all of the strings Little a concatenate with little b where a is drawn
 from the language big A and little b is drawn from the language big B And so this
 is cross sporadic operation Choose a string from a Choose a string from
 capital B and then combine put them together with the string from a first and
 choosing strings at all possible ways from all possible combined strings and thats
 the language a concatenated_with b And finally theres a kind of looping
 inaudible This is pronounced a star or is called the Kleene iteration and or the
 Kleene closure And a star is equal to the union For i greater_than zero of a to
 the i a to the ith power Whats that mean Well a to the ith power is just a
 to concatenated_with itself By times So this is inaudible And note that
 because i can be zero one of the possibilities here is a to the zero so a
 concatenate with itself zero times and what is that well thats the language
 epsilon So thats the language contain the empty_string So the empty_string is
 always an element of a star To_summarize the last_couple of slides the regular
 expressions over some alphabet sigma The smallest of that expressions that include
 the following So lets define it so the regular_expression r are equal to epsilon
 is always a regular_expression Or another_possibility is the single
 character c where c is an element of our alphabet okay So this is important the
 regular_expressions define with respect to some alphabet So we have to pick a family
 of characters that will form the base cases of the regular_expression and here
 you_know We have one base regular_expression for each character in the
 alphabet And then we have the compound expressions So another_possibility Is
 that a regular_expression is the union of two regular_expressions Another one is
 that the concatenation of two regular_expressions And the last one is that it
 could be the iteration of a regular expre ssion So these five cases are the set of
 regular_expressions over a given alphabet Now this syntax here for describing the
 regular_expressions with these vertical bars and these different cases on the
 right_hand side in this recursive definition of r If you havent_seen this
 before this is called the grammar And thats not important for this lecture
 Its not what this this lecture is about but were talking_about grammars when we
 get to parsing Next Id like to do a few examples of actually building regular
 languages writing the mountain and thinking_about what they mean And as we
 said whenever were talking_about a regular language we first have to say
 what the alphabet is And so for these examples lets just use the alphabet zero
 and one So these are going to be languages which consists of strings of 0s
 and 1s And lets start with a very_simple example Lets think_about the language
 one star And what language that to note So well we know the definition of star
 If you remember that was the union over i greater_than zero of one to the i Okay
 And what is that equal to Well thats just one Repeated i thats what the
 concatenation of one to the i means okay It means one concatenated_with itself i
 and so this is going to be the empty_string Thats one concatenated_with
 itself zero followed_by one followed_by eleven followed_by one concatenated_with
 itself three followed_by one concatenated_with itself four followed_by one
 concatenated_with itself any number of times Okay And this and so we can see
 that this is just equal to all strings Of 1s All_right Now lets do a second
 example lets think_about the language one Plus zero concatenated_with the
 language one okay And remember how concatenation works is across products we
 take every string in the first expression and combining with every string in the
 second expression So this is going to be equal to the strings a b where a is drawn
 from one zero and b is drawn from one All_right And what can that be when
 theres two traces for a A could be one or zero and b could be one so in fact this
 is equal to the set one one and the strings one one the second inaudible
 of the strings one one and one zero All_right Lets do another examples slightly
 more_complex Lets build up here to having two iterations in a union so have
 zero one and think of about whats that equal to but weve_already know what
 one is equal to Thats equal to all strings of ones and so by analogy zero
 must be all strings of zeroes then we take the union of those two things so this is
 actually really easy to write_out Lets write them out in this notation so we have
 zero to the i for i again equal to zero okay Thats zero union with One to the
 i or greater_than zero Thats the strings of all one So theres a set at
 this expression to nodes And for our last example lets think_about zero one
 Now that iterated Okay So we put the star around the union of the two
 individual character instead of having the star on each character individually in
 taking the union of the two things So what is the what is this expression equal
 to Well lets work with the definition of star So we know That this is the
 union over i greater_than or equal to zero of zero one to the i And what does that
 look like well that looks_like first of all theres the empty_string right And
 then another string in this language is is Excuse me is drawn from zero one
 and so this I shouldnt say another string but another set of strings is the
 language zero one And then zero one concatenated_with itself okay And in
 general is going to be zero one concatenated by_itself i times Now what
 does that mean That_means that every position if we have a string of length i
 at every position we could pick a zero or a one to plug in and this works for any
 length string This is gonna be true of strings of every length and so in fact
 this language is just going to be all strings Of 0s and 1s In_fact what that
 means is this is the cycle effect on our alphabet Our alphabet that consists of
 zero and one and so this is the set of all strings that you can form over the entire
 alphabet And that has a special name when that happens_when you have a regular
 expression that denotes the set of all strings you can form out of the alphabet
 we write that as sigma star okay So just meaning that all the strings of the
 alphabet integrated as many times as you like One last point I wanna make on this
 before we go on here is that there are actually lots of ways to write each of
 these different languages Theres not a unique way to write these So for example
 lets just take this language here The second one that we did and let_me switch
 colors Another alternative way to write this since we know the meaning of it is
 these two strings one one and one zero I could have written it as one one one
 zero and that would mean exactly the same_thing We used two expressions denote
 exactly the same set similarly with one star I could write this as one ltigt oneltigt
 And cuz this wouldnt change anything Adding in the single string one wouldnt
 change anything since one is already included in oneltigt This might be kind of
 altigt silly way to write that set but it doesnt_matter it has a meaning and it means exactly
 the same things as oneltigt The point again is that there is more_than one wayltigt
 to write_down the same set to write to write you can write multiple regular_expressions
 that denote the same set
 Welcome_back In this video_were going to take a little digression and talk_about
 formal_languages A formal language has played a big role in theoretical computer
 science but theyre also very important in compilers because inside of the compiler
 we typically have several different formal_languages that were manipulating A
 regular_expressions are one example of a formal language but its actually helpful
 I_think in understanding regular_languages and all the formal_languages well see
 later_on in later videos to talk_about what the formal_languages in general So
 lets begin_with the definition A formal language has an alphabet So some set of
 letter sigma And then a language over that alphabet is just a set of strings of
 the characters drawn from the alphabet So in the case or regular_languages we had
 certain ways of building up sets of strings of characters but other kinds of
 languages would have different sets of strings And in general a formal language
 is just any set of strings over some alphabet An example of a language that
 youre familiar_with is a form from the alphabet of English characters and it is
 just the set of English sentences Now This is not quite a formal language and
 that we might disagree in which string of English characters are in fact valid
 English sentences but one could imagine that we could define some rules that we
 would say the certain strings are English sentences and others arent And if we
 could come to this on agreement this would be a fully formal language Now a much
 more rigorous formal language would be something_like the following we could
 pick our alphabet to be the asking character set and the language to be the
 set of all Valid C program So this is definitely a very well defined language
 This is exactly the set of inputs that C compilers will accept And the the
 important contrast I want to draw here is that the alphabet is actually interesting
 So different formal_languages you_know Have a very very different alphabets and
 we cant really talk a bout what the formal language is or what sort of strings
 were_interested in unless to find that alphabet Another important concept for
 many formal_languages is a meaning function Typically we have one of the
 strings in our language and lets call that some expression e and the expression
 e by_itself is just a piece of syntax Its a program in some sense or it
 represents something else that were Which is the thing were actually
 interested in And so we have a Function L that maps the strings in the language to
 their meanings And so for example in the case of the regular_expressions this
 would be a regular_expression And that would be map to a set of strings The
 regular language that that regular_expression to notes and we saw an example
 where we wrote out the meeting function for regular_expression last_time so lets
 use regular_expressions as an example and Im gonna first write_down the meaning of
 the regular_expressions The way I wrote it down in the last_video so if you recall
 we had a regular_expression epsilon and that denoted a set Which contain just one
 string namely the empty_string And then we had a regular_expression C for every
 character in the alphabet which also do need a socketing just one string namely
 the single_character C And then we had a bunch of compound expressions So for
 example A B That was equal to the union of the sets A and B and we had the
 concatenation so I could I could inaudible A and B and that was equal to
 a cross product where I selected a string from each set in order and concatenated
 them together And finally there was iteration so I could write a star and that
 was the union over I Greater than zero of all the sets A to the I I ends An
 interesting thing about this definition is you can see that they were mapping over
 we have expressions and let_me switch_colors here over_here we have expressions
 and over_here we have the sets But theres something kind of odd about the
 way this is written and not quite right cuz you can see here we clearly we have
 an expression We have a piece of syntax A B and then somehow on the other side
 this this A this A and this B have magically turned into sets that were
 taking the union of and similarly down_here were choosing an element from this
 set but this set is also an expression and what does that mean Somehow were
 conflating the sets in the expressions and this is what The meaning function is
 intended to fix and this what they or or or intended to make clear So we what
 we really wanted to say is that theres some mapping That the language L epsilon
 is the set so the so L maps from expressions into sets of strings Okay
 Its a function that maps one to the other and it you havent_seen this notation
 before this is a standard notation for describing functions It does says that L
 is a function from things in the domain in this domain to this range okay And
 similarly the language of this expression is the set and it becomes really useful
 for the compound expressions cuz here we say the language of this expression Is
 equal to the language of a union with the language of B and now you can see the
 recursion First we interpret A and B using L and we take the union of the
 result Okay so now its clear whats asset and whats an expression and
 similarly here the language of a concatenated_with B we are going to
 select elements from the language of these two expressions and then were_going to
 form another set from those two sets And finally for iteration The language of a
 star is equal to the union over the meaning of a bunch of expressions A to
 the I is an expression This is a a piece of syntax and we have to convert it to A
 set N order to take the union And so about this is The proper definition of
 the meaning of regular_expressions where weve made the meaning function L explicit
 and weve shown exactly how recursively we apply L to decompose the compound
 expressions into several expressions that we compute the meaning of and then
 computed the sets from those from those separate smaller s ets So theres other
 reasons for using a meeting function We just saw one of them which is to make
 clear What is syntax and what is semantics in our definitions Some parts
 of the definition are expression and some parts are the the meanings or the sets
 and the using L makes it clear that the arguments to L are the the programs or
 the expressions and the results Are the the sets The outputs are the sets But
 there are a couple of other reasons for separating syntax and semantics One is
 that it allows_us to consider notation as a separate issue That is if we have
 syntax and semantics being different then we can vary the syntax while we keep the
 semantics the same and we might discover That there that some kinds of syntax are
 better_than others for the problems that were_interested in for the languages
 that were_interested in And another reason for separating the two is because
 of expressions and meanings because syntax and semantics are not in one to one
 correspondents And I actually illustrated this with regular_expressions in the
 previous_video but I want to iterate here that that there are generally many more
 expressions than there are meanings so that means there maybe multiple way To
 write an expression that means the same_thing Id like to take a moment to
 illustrate why separating syntax from semantics is beneficial for a notation
 So everybodys familiar_with the the r number system so I can write numbers like
 zero one 42 and 107 and there are very nice algorithms for describing how you add
 and subtract and multiply such numbers but there are older systems of notation for
 numbers Things_like the Roman numerals I could have the number one I could have
 the number four the number ten and say the number 40 I_think is written like that
 and And an issue with this number system first of all let_me stress that these two
 have the same meaning So the the meanings of expressions in this language
 are Are the integers and its exactly the same in this language So the idea the
 mean ing of these two systems are just the numbers but the notation is extremely
 different The number written in Roman numerals was completely_different from a
 number written in Arabic numerals And the fact is that the Roman numerals are really
 painfully to do addition and subtraction and multiplication and in fact Back in
 ancient times when this was a common system was not very well known how to do
 it and very few people were actually good at doing arithmetic with with the system
 because of because the algorithms were kind of complicated And when we moved to
 the the Arabic system later That it was a big improvement because people it was
 easier for people to learn how to do basic arithmetic with these kinds of numbers
 And the only thing that changed between one system and the other was the system of
 notation And so notation is extremely important because it governs how you think
 and it governs the kinds of things you can say and the sort of procedures that you
 will use So dont underestimate the importance of notation and this is one
 reason for separating syntax from semantics because we can leave the idea of
 what were trying to do than numbers alone And play with with different ways
 of representing them and we might discover that some ways are better_than others The
 third reason I gave for separating syntax and semantics is that in many interesting
 languages multiple expressions multiple pieces of syntax will have the same
 semantics Now going back again to regular_expressions lets_consider the regular
 expression zeroltigt Now there are many ways to write the same language which is theltigt
 language of all strings of zeroes so string of zeroes of any length So for
 example I could also write that as zero zeroltigt Another way to write it is asltigt
 epsilon zero zero and here you can see that that this expression is all the
 strings of 0s of at_least link one and then we get the empty_string for epsilon
 so that is zero and then just you_know Any combination of these things
 would also amount to an eq uivalent language for example that one and so on
 So theres actually an unbounded unlimited number of way I could write this
 language but all of these mean exactly the same_thing and if you think_about it What
 this means is that in general if I draw the two domains differently I_think about
 different expressions over_here and different distinct meanings over_here and
 the function L that maps between them The function L is many to one So there are
 Yeah There_are points in the space that where many different expressions or pieces
 of syntax map to the same meaning And this is just a general characteristic of
 Interesting formal_languages and this is actually extremely important in compilers
 because this is the basis of optimization The fact that there are many different
 programs that are actually functionally equivalent thats what allows_us to
 substitute one program that runs faster than another thats what allows_us to
 replace one program with another if it runs faster and does exactly the same
 thing So we couldnt do optimization and you_know the reason we can do optimization
 as precisely because the meaning function is many to one So meaning is many to one
 and keep in mind important point here its never one to many We_dont want the
 opposite situation If we have the opposite situation Where L could map a
 single point to two different meanings Well first of all this would no_longer be
 a function but but also it would mean that the meaning of certain expressions
 say in our programming_language was not well defined Thats that when you wrote a
 program was actually ambiguous whether it meant this or it meant that and thats a
 situation we dont like So we expect meaning functions to be many to one for
 nontrivial languages and we dont want them ever to be one too many And that
 concludes todays video Next time Going to go_back and continue with our
 discussion of lexical_analysis
 Welcome_back In this video Im going to show how to use regular_expressions to
 specify different aspects of programming_languages Must begin_with the keywords
 and this serves a relatively simple case and Ill just do it for three keywords
 Ill write a regular_expression for if else or then and it would be obvious how
 to do it for more So lets write a regular_expression for if and that would
 be under regular_expression for i And followed_by the regular_expression for f
 and were taking the concatenation of these two And then were_going to union
 that with the regular_expression for else and what is that Well else consist of
 four individual characters so we have to write_out a concatenation of those four
 characters And as you can see this is a little_bit verbose with all of these
 quotes and kind of messy to read So in fact theres a short hand thats normally
 used and let_me switch over to that right now So if I want to write the regular
 expression for a sequence of single_character expressions I could just
 inaudible around the outermost characters in the sequence So for example
 most of the tools that let you write this I put a quote at the beginning right IF
 and then I close quote and this means exactly The same_thing as this This is
 the concatenation of two single_character regular_expression and similarly for else
 And similarly for them And if I have more keywords Ill just write them out and
 union them altogether Now lets_consider a slightly_more complicated example Lets
 think_about how to specify the integers who want to be the nonempty strings of
 digits So the first problem here is to write_out what a digit is and thats
 pretty straight_forward A digit is just any of the individual characters zero
 through nine and we already know how to write_out single_character regular
 expressions And its just a union of ten of those to specify this and itll take me
 just a moment to finish Here we go So thats a regular_expression for the set of
 strings correspondin g to all the single digits And because we want to refer to
 this from time to time and also because as a very common thing to watch too most
 tools have a facility for naming regular_expressions So for example I can name
 this one to be digit so a single digital is anything that is generated or is in the
 set denoted by this regular_expression And now what we want to do is to have
 multiple digits Well we know a way to do that We can just iterate a single digit
 as many times as we like And so here we get all strings all possible strings of
 digits and this is very very close to what we wanted except that the string that
 we want has to be nonempty We_dont want to count the empty_string as an_integer
 And theres an easy way to do that We just say that the whole sequence has to
 begin_with a single digit and then its followed_by zero or more additional digits
 so just Is just to reiterate that we see there has to be at_least one digit and is
 followed_by zero more additional digits And this pattern actually for a given
 language is extremely common So if I wanted to say that I have at_least one a I
 write that as a a because this has zero or more a the second part is zero more
 as and the first part says there has to be at_least one a And because this is so
 common theres a short hand for it I_think is supported by every regular
 expression processor and that is to write a plus An a means is just is just a
 short hand for a star And so we can actually simplify this regular_expression
 a bit and write simply digit plus Now lets look_at yet another example even
 more sophisticated than the the previous two Lets think_about how to define the
 identifiers which are strings of letters or digits that begin_with a letter And so
 we already know how to do digits so lets focus_on the letters for a moment So how
 we write_out a regular_expression for the letters while were_gonna name it So
 well say that the letters or actually a single letter And now we have to write a
 regula r expression for all the individual letters and thats you_know
 straightforward but tedious We have to say little a lower_case b lower_case c
 lower_case d And well as you can see this is gonna be rather a large regular
 expression Were_going to have 26 lower_case letters and 26 upper case letters and
 the whole_thing is going to be rather tedious as to write_down so lets actually
 not do that Instead let_me mention a shorthand A tool support to make it
 easier to write_out exactly this kind of regular_expression which is called the
 character range So inside your square brackets I can write a range of
 characters So how do I do that Well I have the starting character and an ending
 character and I separate them by a hyphen And what this means is the union of all
 the single_character regular_expressions beginning_with the first character and
 ending with the second character so everything in between So this is exactly
 the regular_expression for all the lower_case letters and then I can have another
 character range and so at the same square brackets for all the upper case letters
 So A through Z Okay And this regular_expression here on the right defines
 exactly the big union that I didnt wanna write_out okay And that gives_us a
 definition of the single letter and now were in great shape We are we already
 have definition for a digit we already now we have definition for letter and so
 that we can write_out the rest of this definition So we want the whole Regular
 expression to always begin_with a letter Okay so identify always begins_with a
 letter and after that is allowed to be a string of letters or digits okay So they
 are suggest that theres going to be a union So After the first letter we can
 have either a letter or a digit and then we can have an_arbitrary string of those
 things So we put a start on the whole_thing and that is the definition of
 identifier Begins with a single letter and its followed_by zero or more letters
 and digits So Because were doing a complete lexical_specification we also
 have to deal_with even the parts of the string that were not really interested
 in We have to have at_least specification of them so then we can recognize and throw
 them away In particular we have to be_able to recognize the white_space and
 Were just gonna take white_space to be a nonempty sequence of blanks new lines
 and tabs even_though there are other kinds of white_space characters Things_like
 maybe like rubout Depending on your keyboard there might be others but these
 three will suffice to illustrate all the important points So you_know blank is
 relatively easy to write_down Thats just a single quote around the blank space but
 theres a problem with new line in tab Because a new line that carries return in
 the file has special meaning typically You_know and on the line you end
 whatever command youre working_on in this regular_expression tools like SQL tools
 And you_know tab also is not an easy thing to write_down and it doesnt look
 much different from a blank in a lot of cases So what tools do is they provide
 separate name for these and its and typically its done by having some kind of
 escape character and a backslash Is the most_common one thats used and then
 followed_by a name for the character So back slash n is typically used for new
 line and back slash t is typically used for tab And I just want to stress I_mean
 the reason for doing this example is to illustrate this that We have to have a
 way of naming some characters that dont really have a very nice print
 representation There_are other characters that that dont even have really any kind
 of print representation and we still need a way to talk_about them in our regular
 expressions because them might be embedded in a file that we have to lexically
 analyze at some point And so anyway the way this is done is by providing a
 separate Naming scheme for such on principal characters and typically as the
 one that escape sequence So something beginning_with special character like back
 slash followed_by the name of the character So n for new line and this
 case nt for tab And so to finish off our definition this gives_us You_know One
 character white_space and then we want a non empty sequence or such things so we
 could wrap the whole union there in parenthesis and put a plus on it and that
 Get us what we want Lets pause for a moment in discussing programming_languages
 and look_at another example of using regular_expressions from a different
 domain Here I have an email address and as it_turns out email addresses form a
 regular language and every email processing device in the world So your
 mailer and the mail servers that you use all do regular_expression processing to
 understand what the email address is telling them in the email messages that
 they go by And And so we can think of an email address is being is consisting of
 four different strings separated_by punctuation Theres a username and then
 three parts to the domain Okay Lets just assume for simplicity that the
 strings only consist of letters and practice they can consist of other kinds
 of characters too but lets just keep things simple and we can write_out the
 more_complicated things using the regular_expressions but the structure would be the
 same as if we just consider them to be made of letters And then these four
 strings are separated_by punctuations so theres the sign and to decimal points
 thats Form the separators of the strings so these is a relatively straightforward
 thing to write a regular_expression for given what we know so_far so the user name
 would be the nonempty sequence of letters and then that would be followed_by an
 sign And then the first part of the domain will also be on empty sequence of
 letters followed_by a dot and then the rest is just the same Hey so here were
 quite concisely we had specified large family of email addresses As I said in
 reality the email addresses are slightly_more complicated but they can be written
 out with just a slightly_more complicated regular_expression Finally for our last
 example lets look_at a fragment of the lexical_specification of a real
 programming_language and this case that language is Pascal which is in the
 inaudible family of languages Pascal is an early example of a type language and
 its in the same general family as Fortran and C And in this particular fragment of
 pascal deals with the definition of numbers and so lets_take a look here
 Ill start at the bottom and look_at what the overall definition of a number is So
 a number consist of three things some digits and Ill just read out this
 abbreviation and optional fraction And an optional exponent so were dealing here
 with floating_point numbers and so a floating_point number have a bunch of
 digits and then it can be followed possibly by a fraction or not and that
 could be followed possibly by an exponent or not and And the idea although we
 cant see it just for this particular definition is that either of action or the
 exponent can be present independent of the other So now lets work briefly from the
 bottom_up lets just check the digits on what we expect So a single digit is in
 fact the union of all the common digits just as we would hope And then a
 nonempty sequence of digit is a digit plus so this is what weve_already seen
 And now the interesting bid is to look_at how the optional fraction and the optional
 exponent are defined and the optional fraction looks a little less scary so
 lets do that one first So whats going on inside the fraction well if we have a
 decimal fraction there is gonna be a decimal point and thats gonna be followed
 by strong of digits so this is just The fractional parts of the floating_point
 number its just of it comes_after the decimal point And whats this plus
 epsilon doing out here Well remember the plus is union and epsilon stands_for the
 empty_string So what this is saying is that either the fractional portion of the
 number is present or its completely absent So this is how you say something
 is optional You write_out the regular_expression for the thing and then you do
 plus epsilon at the end and that means well either everything I said before it
 can be there or nothing is there Okay And the optional exponent is structured
 similarly but somewhat more_complex So you can see the whole exponent is optional
 because theres some regular_expression here Thats union with epsilon And so
 either Something is there and this is the optional or this exponent part or is not
 present at all And now lets look inside how the exponents define if its there
 So an exponent always begins_with e So this is exponent you_know standard
 exponent notation and it always has a nonempty string of digits So theres e
 followed_by some digits and in between theres an optional sign We know the sign
 is optional because epsilon is one of the possibilities The whole the whole sign
 might be absent And one of the possibilities for the sign well it could
 be negative or it could be positive So either theres a positive or negative sign
 or no sign In which case presumably its interpreted to be positive Now this
 idiom where we have epsilon indicate that something is optional is also
 extremely common and so theres another short hand that many tools provide so
 another way of writing this thats common is to say that Thats my fractional
 component and then it might be absent So the question mark after a regular
 expression just means exactly this construction that we take that regular
 expression and we or with epsilon And so this one this regular_expression is a is
 a little more_complicated Theres two optional components so lets just write
 out what that would look like so we would have the exponent begins_with e and then
 we have a sign Which is either or and thats optional so we put a question mark
 after it followed_by a nonempty string or digits and then this whole_thing is
 optional The whole exponent is optional S o this is an alternative and more
 compact way to write this expression To wrap_up I I always convinced you in this
 video that regular_expressions can describe many useful languages Weve seen
 some fragments from programming_languages but also we saw that email addresses
 could be specified this way Other things that are regular_languages are things_like
 phone numbers and file names are also regular And there are many_many examples
 in everyday life where regular_languages are used to describe some simple set of
 strings And I also want to emphasize that so_far weve used regular_languages as a
 language specification We use it to define the set of strings were_interested
 in But we havent_said anything about how to actually implement lexical
 analysis We still need an implementation And thats what well talk_about in future
 videos In particular in particular were_going to look_at the problem of
 given a string as an irregular expression Or how do we know_whether that string is
 in the language of the regular_expression are
 Welcome_back In this video_were going to talk_about finite_automata
 which well see in future_videos are a good implementation model for regular
 expressions So in the last few_videos weve_been talking_about regular
 expressions which we use as the specification language for lexical
 analysis And in this video_were gonna start something new Were_gonna talking
 about Finite_Automata which are the For a convenience as an implementation mechanism
 for regular_expressions And so regular_expressions and finite_automaton are very
 close related It_turns out that they can specify exactly the same languages called
 the regular_languages We wont prove that in this course but well certainly make
 use of that fact So moving right along What is a finite_automaton Well here is
 a typical definition as you might see in a automaton theory textbook Finite
 automaton consists of an input alphabet So its a set of characters that it can
 read It has this finite set of states We should probably emphasize that This is
 what makes it a finite_automaton is that it has some set of states that it can be
 in One of those states is special and its designated as the start_state Some
 subset of the states are accepting states so these are the states that But well
 well just find that more in a minute but intuitively if the automaton terminates
 after reading some input on one of these takes that it accepts the input
 Otherwise it rejects the input and finally the automaton has some set of
 state transitions that is in one state they can read some input and go to another
 state So lets look_at that little more_detail so a transition in a finite
 automaton If Im in in this case Ive written out one particular transition
 here Were in state one and we read the input A then the automaton can move to
 state two okay And there could be lots of different transitions for the automaton
 from different states and different inputs and its read the following way If were
 in state one on input A we would go to state two And if the automaton ends in
 an_accepting state when it gets to the end of the input that is going to do whats
 called accepting the string Meaning that it will say yes That string was in the
 language of this machine So intuitively the automaton starts in the start date and
 it repeatedly reads inputs one input character at a time makes a transition So
 itll see what kind of transition it can make out of its current state based on
 that input to another state and if thats done ringing the input its in one of the
 final_states that it will accept Otherwise is going to reject the input
 Now one of the situations in which it rejects well if it terminates In a state
 S thats no one of the final or accepting states okay So that ends in any_other
 state besides one of the accepting states and its going to reject If the machine
 gets stuck Meaning it finds itself in a state and theres_no transition of that
 state on the input So in particular lets_say that in some state as a news and
 the input is A and theres_no transition Theres no transition specified per state
 as an input A so the machine cant move anywhere and it get_stuck and thats also
 a rejecting state And so in these two situations if if you either get to the
 end of the input and its not in a final_state or If it never reaches the end of
 the input because it can stuck and both of those cases it rejects the string That
 string is not in the language of the finite_automaton Now theres an
 alternative notation for Finite_Automata that I_think is more intuitive for
 examples and so were_going to emphasize that way of writing the mount In this
 notation a state is represented as a known graph which just draws a big circle The
 start_state is represented as a node that has an edge or an arrow into it with no
 source So this is a transition into the node but no source node that it comes from
 and that indicates the unique start_state An accepting_state is drawn as a node wit
 h just double circles like this And finally a transition is drawn as an edge
 between two nodes of the graph So with this as the time in this state that Im
 circling in blue and I read the input a well then I can move to this state at at
 the tail of the arrow So now lets do a simple example Lets try to write up the
 automaton that accepts only the single digit one So all we need is start_state
 And will probably want an_accepting state as well and now the questions is what do
 we put in between the two Well there would be some kind of transition here and
 its a good guess that we should take that transition if the machine reads the one
 Now let_me take a moment here to talk_about how the machine executes so lets
 label these states Lets call this state a and lets call this state b okay So
 the machine will have some input Okay and we can write that input out will be
 here So lets just say we have the single_character one and it begins in some
 state namely the start_state And so one configuration of the machine is the state
 that it is in And the input And typically we would indicate where it is in the input
 by just a pointers saying what position it is in the input And the important thing
 to know about input in inaudible the input pointer always advances So when
 we or it only advances so when we read a character input the input pointer moves
 to the right and it never moves back Alright So from state a we have a rule
 We can see that were in state a The next_input character is a one and that allows
 us to take a transition to state b and so now where b in state b and where as our
 input point well its beyond the end of the input indicating we are at the end of
 the input And so now this is We are in an_accepting state and we pass the end of
 the input and so we accept Okay So lets do another execution So we start
 in state a and lets_take as our input the string zero Okay And Id like to draw
 the pointer Actually I should have drawn it before the input so well al ways put
 the pointer between two input elements In this case its a merely to the left of the
 one were about to read So in this case were about read zero so in state a Our
 input is zero We look_at our machine We see that there is no transition on zero
 All_right And so the machine stays stuck It_doesnt make any move at all and this
 is our final configuration And we could see that were not at the end of the input
 and so this is a reject Okay so in this case the machine rejects that string as
 not being in the language of the machine Lets do one more example Lets_say that
 were in state well were always beginning in state a and the start_state
 and lets_say our input this time is the string ten okay And our input pointer is
 there All_right So again were in state a The input is a one and so well move to
 state b And now the input doesnt_change Just the input point changes but Ill just
 copy the input over to show the difference Now the input pointer has
 advanced cuz weve read one character of input and now were in another state And
 now we can see that were in state b Our next_input is zero and there is no
 transition on zero from state b and so even_though were in an_accepting state b
 as a final_state its one of the accept state and we havent consumed the entire
 input And so this The machine also rejects this string so this is also a
 reject And in general we can talk_about the language Of a finite_automata that is
 equal to the set ofaccepted strings Okay So the language of a finite
 automaton when Im talking_about the language of a finite_automaton I_mean the
 set of strings that the automaton accepts So now lets do a more_complex example
 Lets try to write_out an automaton that accepts any number of one followed_by a
 single zero So once_again well need a start_state and well also need a final
 state and now lets start by thinking_about whats the shortest string is thats
 in the language of this machine So in this case we know it has to end in a
 singl e zero So a zero definitely has to be a zero transition has to be the last
 move and before that zero can come any number of what In a particular there
 could be no 1s So one transition in this machine is that from start_state on input
 zero we can definitely go to the final_state because the single string consisting
 of a single zero isnt the language of this machine And now the only question is
 how do we encode the fact that any number of 1s can proceed to zero Well there is
 an easy way to do that We can just add a inaudible by the start_state And take
 that transition if we read at one And what does this mean This means that well
 stay in the state state as longer are were reading 1s and as_soon as we read
 zero well move to the final_state because that has to be the end of the
 string if the machine is going to accept it So lets do a couple of examples to
 convince ourselves that this works Let_me label this states again So this is state
 a and thats stat b So Lets write_out here states and input So well begin in
 state a and lets_take as input 110 okay So lets do an_accepting case first All
 right So our input pointer begins to the left of the first character So were in
 state a in start_state Were reading a one and that says we should take a
 transition that puts us back in state a And so we advance the input pointer And
 now we consume the first one and and again were in state a and the next_input
 is a one so well make another transition to state a And the input cleaner will
 advance So now were in state a and the next_input is a zero and so well take the
 transition to b and now in this configuration so the input pointer has
 reached the end of the input were in an_accepting state and so the machine
 accepts 110 is in the language of this machine So now lets do an example where
 we will reject the input And what configuration do we begin in and again a
 configuration for a finite_automaton that just means you_know a point in the
 execution it alwa ys consist of a state and a position of the the input pointer
 So our initial state is a and now lets just choose the string I dont_know
 lets_take 100 and lets confirm that this is not in the language of the machine All
 right So we begin in state a and our input pointer is there Now we read a one
 and that means well you_know So its from state a transition of one We stay in
 state a and the input pointer advances And now we see a zero So from state a and
 input zero we make a transition to state b And now the input point is here so now
 were in state b and we have an input of zero but there is no transition the b and
 zero there are no transitions out of b at all and so the machine gets stuck it
 cant get to the en of the input and again even_though were in an_accepting
 state we havent read the entire input yet and so that means the machine will reject
 And so 100 is not in the language of this machine
 inaudible Welcome_back At this video_were going to talk_about how inaudible
 expressions are used to construct a full lexical_specification on the programming
 language Before we get started I want to quickly summarize the notation for regular
 expressions One of the shorthands we talked_about last_time is a which means a
 sequence of at_least 1a or the language aaltigt Sometimes youll_see a vertical barltigt
 used instead of unions or a b Can also be written a vertical_bar b and optional a
 is abbreviation for the regular_expression a epsilon And then we have character
 ranges which allows_us to do a big union a bunch of characters in order And then
 one more thats used thats thats actually fairly important but we didnt
 discussed last_time is the compliment of the character range So this expression
 here means any character except the characters a through z So the last
 lecture we talked_about a specification for the following predicate Given a
 string s is it in the language as the function l of a regular_expression As we
 we define the language of regular_expressions and talked_about their
 semantics in terms of sets of strings And so for any given regular_expression we
 could reason out by hand whether a given string was in that language or not and
 this turns_out not to be enough for what we wanted to do So just to review what
 is it we wanted to do when were given an input which is a bunch of characters so
 heres a string of characters And it can be much longer than just setting
 characters and But we actually wanted to do is to partition the string We want to
 drop lines in the strings divide up into the words of language Now of course each
 one of these words are to be The language at some regular_expression But just
 having a a a definition or a yes no answers not quite the same_thing as
 having a method for partitioning a string into its constituting parts And so were
 gonna have to adapt regular_expressions to this problem and and this requires
 some small extensions and thats what this video is all about So lets talk_about
 how to do this The first_thing were_going to do when we want to design the
 lexical_specification of the language is we have to write the regular_expression
 for the lexing to be to the inaudible classes and we we talked_about how to do
 this last_time So for the numbers we might use digit plus desire as our regular
 expression and we might have a category of keywords which is just the list of all
 the keywords in the language We would have some category perhaps of identifiers
 There is the definitely we talked_about it last_time Sequences of letters or
 digits that begin_with with the letter and then were having a bunch of Bunch of
 punctuations things_like open parens close parens etc So we write_down a
 whole set of regular_expressions One for each syntactic category in the language
 and thats the starting point for our lexical_specification The second step
 what were_going to do is were_going to construct a gigantic regular_expression
 which just matches all the lexings for all the tokens and this is just the union of
 all the regular_expressions that we wrote out on the previous slides So well just
 take the union of all those regular_expressions and that forms the lexical
 specification of the language And well just write this out we dont really care
 what these regular_expressions are but theyre just some set r1 r2 and so on
 and the whole_thing were_going to call r And now heres the heart of how we
 actually use this bicycles specification to perform lexical_analysis So lets
 consider an input We input the string x1 up to xn And now for every prefix of that
 input okay Were_going to check_whether its in the language of the regular
 expression So were_gonna look_at some prefix trying with the first character and
 were_gonna ask ourselves is it in the language of that big regular_expression
 And if it is if it is in the language well then we know in particular that that
 prefix is in the language of one in the constituen t regular_expressions cuz
 remember that r The sum of all the different talking classes of our language
 okay So we know that this prefix x1 through xi is in the language of sum rj
 Okay And so that we know that thats a word In our language is one of Is in one
 of the talking classes that were_interested in And so what we do is we
 simply delete that prefix from the input and then we go_back to three and we
 repeat And in this way we keep biting off prefixes of the input and well do this
 until the string is empty and then we have inaudible analyzed the entire program
 Now this algorithm has a couple of ambiguities or a couple of things that are
 under specified and those are actually interesting So lets_take a moment and
 talk_about those The first question is how much input is actually used So lets
 consider the following situation Lets_say that we have the x1 to xi is in the
 language of our lexical_specification And lets_say theres a different prefix
 thats also in the language of our lexical_specification and of course your I is is
 not equal to J What does that look like Well it would look like the following
 kind of situation we would have our input string And we have two different prefixes
 of the input that are both valid talking classes and the question is which one of
 these do we want And you_know just be kind of inaudible here to have a
 concrete example lets_consider What happens_when a is at the is at
 the beginning of the input After we chopped off a bunch of other input and
 perhaps we have this substring or this prefix of the input that were looking_at
 and the question is you_know should this be regarded as a single which would be
 an assignment operator in most languages or would it be regards to which in
 some language is a comparison operator And and this is an example weve_looked
 at before and discussed and theres actually a well defined answer to this
 question And it is that we should always take the longer one and this has a
 name thats c alled the maximal munch So the rule is that when faced with a choice
 of two different prefixes of the input either which would be a valid token we
 should always choose the longer one And the reason for this is thats just the way
 humans themselves read things so when we see we dont see two different
 equal operators we see and if I Look at you_know that the sentence that I
 wrote up here you_know when we look_at HOW we dont see three letters We gather
 that altogether in one long token We go as far as we can until we see a separator
 and so because this is the way humans work we make the tools work the same way
 and this normally or almost always does the right thing Second question is which
 token should be used if more_than one token matches So what do I_mean by that
 Well again we have our prefix of the input and its in the language of our
 lexical_specification and just remember that the lexical_specification itself
 again is made up as the union a bunch of regular_expressions for token_classes
 Now since that since this prefix is in the language of the lexical lexical
 specification that means that it again it must be in the language of some
 particular token_class rj But nothing says that it isnt also in the language of
 a completely_different token_class Meaning at the same string could be
 interpreted as a as one of two different tokens and the question is if this
 happens which one should we pick So for example just to make this concrete Recall
 that we could have a lexical_specification for key words which would be things_like
 if and else and so on and also for identifiers And then the identifier was
 the letter Followed by a letter or a digit Repeat it okay And if you look_at
 these two specifications youll_see that the string if IF is both of them So IF
 is in the language of keywords And its also in the language of the identifiers
 And so should we treat it as a keyword or an identifier Now the normal rule in most
 languages is that if its a keyword then i ts always a keyword and you_know the
 identifier is actually dont include the keywords And but actually its a real
 pain to write_out the identifiers in such a way that you explicitly exclude the key
 words This is a much_more natural definition Ive written here for the
 identifiers And so the way this gets resolved is by a priority ordering and
 typically the rule is to choose the one Listed first Okay So when there is a
 choice when there is more_than one token_class which the string might be long the
 one that is listed first is given higher priority So in our file defining our
 lexical_specification we would put the key words before the identifiers just as we
 have done here The final question is what to do if no rule matches What if I have
 the prefix of the input That is not in the language Of my lexical_specification
 Now this can actually arise Certainly there are lots and lots of strings that
 are not gonna be in the language of the lexical_specification of most languages
 And the question is how to handle that situation So its very important for
 compilers to do good error_handling They cant simply crash They need to be_able
 to give the user the programmer a feedback about where the error is and what
 kind of error it is so we do need to handle this gracefully And the best
 solution for lexical_analysis is to not do this so dont let this ever happen And so
 what we wanted to do instead is to write a category of arrow strings So all of the
 strings Not in the lexical_specification of the language So we write_out a regular
 expression Again this is another regular_expression here For all the possible
 error strings all the possible erroneous strings that could occur as you_know
 invalid lexical input and then we put it last Put it last in priority So that it
 will match after everything else matches and and the reason for putting it last
 Is that this actually allows_us to be a little_bit sloppy in in how we define the
 error strings It can actually overlap with earlier regular expressi ons We can
 include things in the error strings that are in fact not errors But if we put it
 last in priority then it will only match if no earlier regular_expression match and
 so in fact they will only catch the error strings Then the action that well take
 when we match the error string will be the prints in the error message and give
 device a feedback like where it is in the file and such To wrap_up this video
 regular_expressions are very nice and concise notation for string patterns but
 to use them in lexical_analysis requires a couple of small extensions Some
 particulars a couple of ambiguities we have to resolve we want our matches to be
 as long as possible So we take As much input at a time as we can and we also want
 to choose the highest Priority match So the regular_expressions are given a
 priority The different token_classes have priorities and when theres tie when the
 same prefix of the input could match more_than one we pick the one that has the
 highest priority Typically this has done just by listing them in order in a file
 and the ones listed first have higher priority over the ones listed later I
 just wanna warn you that when you go to right lexical specifications when you go
 to actually implement lexor for a language the interaction of these two
 rules that we take longest possible matches and we break ties and favor of the
 highest priority rules That this lead to some tricky situations and its not always
 obvious that youre getting exactly what you want You have to think carefully
 about the Ordering of the rules and its actually how you write the rules so that
 you get the behavior that you desire And finally to handle errors we typically
 write_out Catch all regular_expression that soaks up all the possible erroneous
 strings and give it the lowest priority so that it only triggers if no valid token
 class matches some piece of the input If I leave we havent discussed these yet
 but they are very good algorithm to know for implementing all of these and in fact
 well be_able to do it in only single pass over the input and with very few
 operations per character Just a few Just a simple table look up and this would be
 the subject of our future_videos
 Welcome_back In this video_were going to talk_about converting regular_expressions
 into nondeterministic_finite automata Before we get started I wanna give you an
 overview of the plan for the next few_videos We have a lexical_specification
 that we want to implement and the first step is for someone to write that down as
 a set of regular_expressions Now that bites all the courses not implementation
 thats just specification So we have to translate that into a program that can
 actually do lexical_analysis and this actually happens in several steps The
 first part is a translate Those regular_expressions into nondeterministic_finite
 automata that recognize the same exactly the same_thing And then those
 nondeterministic automata are translated into deterministic automata and finally
 those deterministic automata are implemented as a set of Lookup tables and
 a little_bit of code for traversing those tables So in previous videos we talked
 about this piece and weve also defined this piece And so now were_ready to put
 the whole_thing together and in this particular video_were going to focus_on
 this component right here The translation of regular_expressions to
 nondeterministic_finite automata So the plan is that for each kind of regular
 expression were_going to find an equivalent Nondeterministic Automata
 automaton that accepts exactly the same language as the language or a regular
 expression And heres a little_bit of notation were_gonna use Well define
 these automaton for regular_expressions and usually what were_going to be doing
 is needing to modify their start states and their final_states so well just
 indicate the start_state with the l and the final_state With the double circle
 and we wont worry too much about the overall structure of the machine as long
 as we have a handle on the start_state and the final_state I should_say that in the
 machines well build here there will only be one final_state Okay so lets begin
 So for the epsilon regular_expression once the machine that accepts that well
 this is a very_simple machine We can just have a start_state and a final_state and
 epsilon transition between them so this machine accepts exactly the empty_string
 Certainly for a single_character A we can define a one transition two state machine
 that accepts that one character So from the start_state we can move to the final
 state if it only if we read that particular character okay So those are
 out two simple regular_expressions and now we have to do the compound regular
 expressions And these are little inaudible involved So lets talk_about
 concatenation first And so because were_gonna build these machines up from
 smaller Regular expressions to larger ones we can assume that weve_already
 converted A and B separately in two machines So I have the machine for A And
 to have a machine for B and now all I have to do is say how Im going to paste
 together these two machines to form a machine a compound_machine that
 recognizes the same language as a concatenated_with B And heres the
 construction the start_state for the compound_machine will be the start_state
 for A so well just keep that start_state for A the same and then we modify the
 final_state of A So we make the final_state of A no_longer a final_state and
 Ive done that here by removing the double circle on the final_state of A andthe
 epsilon transition to the start_state of B Now if we think_about it that does
 exactly the right thing but that says is that first you recognize some portion of
 the input that belongs to the language of A and when we get to that what would
 been the final_state of A we can jump to the start_state of B without consuming any
 input and then try to read the rest of the string as part of the language as as a
 string in the language of B And for union we have a similar way of phasing together
 the machines Although the the structure is somewhat different so we at a new
 start_state for the compound_machine and What does A B mean It means either the
 input is in the language of A or its in the language of B And epsilon transitions
 are really good for capturing this because we just make a decision right from the
 start_state is the string going to be in language of A or is it going to be in the
 language of B So we make a nondeterministic choice and then we read
 the string as using that the automaton that we chose and if we get to the final
 state Either those machines we can make the epsilon transition to the new final
 state for the compound_machine Now remember what the notion is of acceptance
 for Nondeterministic Automata you_know They make these guesses but if theres any
 guess that works then we say that its in the language of the machine So if in
 fact the string is in the union of A or B then either choosing A or choosing B will
 work and so the machine will accept the string And finally the most complicated
 case for iteration is star we have the following construction So heres the
 machine for A Embedded in here Weve added a new start_state and a new final
 state And now lets talk_about how this works So one possibility if we remember
 that epsilon is always in the language of Altigt and so we have this transition hereltigt
 We can go straight from the start_state to the final_state and accept the empty
 string And so that just guarantees that the empty_string is in the language
 Otherwise what do we do Otherwise we can make a transition an epsilon
 transition to the start_state of A And then we can from the final_state of A if
 we reach it we can go_back to the start_state of the whole machine and we can do
 this as many times as we like Okay So theres the iteration of A Surround this
 loop right here And when we reach the final_state of A we can also decide to
 just make a transition to the final_state of the machine we conclude that the last
 time And so this machine recognizes zero or more strings in the language of A So
 now lets do an example So heres a regular_expression and we want to build a
 equivalent nondeterministic machine that recognizes the sa me language and were
 gonna follow our construction Which works by induction on the structure of the
 regular_expressions starting_with the simple regular_expressions and building up
 to the compound one so what do we have here So we have a machine for accepting
 one okay So we need a machine that accepts one and if we call it had two
 states and it just you_know made a transition between the two on the number
 one Similarly a machine for accepting zero Okay And now we need to put them
 together in a machine that accepts either one or zero And the way we did that is we
 made a choice from A from a start_state for the compound_machine where you can
 either move to the machine for accepting one or the machine for accepting zero And
 then we have at the end also epsilon_moves back to the final_state of the
 compound_machine Okay And now we need to iterate this so we need to be_able to
 accept zero or more of ones or zeros and so were_going to take this entire Block
 here and paste it into the pattern that we had for iteration so how do we do that
 Well we have a new start_state and a new final_state okay And theres an epsilon
 move from the start_state to the new final_state to guarantee that we accept the
 empty_string And then we can just iterate this inner machine as many times as we
 like We can make an epsilon move to the start_state We could execute the machine
 ones and if we decide we want to do it again well we can do that Okay Go back
 around for another time Or from the final_state we can decide that weve_seen
 enough and we can just move to the final_state of the compound_machine So this
 machine then accepts the language one zeroltigt And now we have a little_bit moreltigt
 to do We have to accept we have other machine that accepts just one so we build
 another machine that accepts The digit one and now we need to compose two of these
 things to concatenate them and that was very_simple We just have an epsilon move
 from the final_state of the first machine to the start_state of the second mac hine
 and then these are all the states of the final machine And we just need to now
 label our final final_state or the end of the state that were actually gonna use in
 the end of the final_state of the entire machine which should be that one and the
 start_state Which is this state over_here And thats the entire construction
 for the nondeterministic automata or a nondeterministic automata that recognizes
 this language
 Welcome_back In this video_were going to talk_about converting nondeterministic
 finite_automata into deterministic finite_automata Here again is our little diagram
 of the pipeline of a lexical_analyzer how one is constructed So beginning_with the
 lexical_specification we write our regular_expressions Last time we talked
 about the step the conversion of regular_expressions and the nondeterministic
 finite_automata and this time were_going to talk_about this step And as you might
 guess in the final video in the series well talk_about the final step which is
 the implementation of DFAs So heres the Nondeterministic Finite_Automata and we
 finished up with last_time And the first_thing were_gonna discuss today is an
 important idea called the Epsilon Closure of a state And the basic_idea of the
 epsilon culture is that I pick the states And it could a set of states but well
 just do it for a single state And then I look_at all the states that I can reach by
 following only epsilon_moves And so b is the state that were starting_with so b
 would be included in the set and then theres an epsilon move to c So c would
 be included in the set and theres another epsilon move to d so d would be included
 in the set So we would say the epsilon_closure of b is the set b c d And lets
 do one more as an example Want to take the epsilon_closure of g And when we
 switch_colors up to this one Ill erase that and to this one in pink Our
 purpleish pink So the epsilon_closure of g we always have to follow all the
 epsilon transitions out of g So h would be in the epsilon_closure of g but its
 not just single epsilon move This is recursive So any number of epsilon_moves
 that I can take all of those states are included in the epsilon_closure of g So
 in fact i would also be included A would be included and b and c and d will also be
 included And now if I look_at all of these states that have been colored in the
 light purple color I can see that I cant reach any new states from those states
 using only epsilon_moves and so the epsilon_closure of g would be equal to and
 inaudible out here its a b c d Ghi Okay So that is the epsilon_closure of a
 state Recall from the last_video that an NFA maybe in many states any given point
 in time that is because of the choices it can make for a given input and NFA may
 reach multiple different states And the question we want to address now is how
 many different states can it be in Well if a nondeterministic_automaton has n
 states And it winds_up in some subset of those states as how big can that subset b
 will clearly the cardinality of that said has to be less_than or equal to n So the
 NFA can get into a most and different states Now instead I want to know the
 number of different subsets well how many different subsets are there of any things
 Well that means there are two to the n one possible subsets of n states And
 theres something very interesting about this number First of all its a very big
 number so clearly the NFA can get into lots of different configurations
 particularly one it has a lot of different states but the important thing is that
 this is a finite set of possible configurations And this is going to give
 us the seed of the idea For converting an NFA into a DFA or Deterministic Automata
 because all we have to be_able to do to convert a Nondeterministic Automata into
 Deterministic Automata is come up with a way for the Deterministic Automata to
 simulate for the inaudible of the Nondeterministic Automata and the fact
 that the Nondeterministic Automata can only get into a finite set of
 configurations even that configurations is very large is exactly what we will
 exploit in the construction Now were_ready to give the construction showing how
 to map an_arbitrary nondeterministic_finite automaton to an equivalent
 deterministic finite_automaton So lets begin by saying whats in our NFA So
 well have a set of states Which well call s and these are the states of the
 Nondeterministic machine Theres a star t state a little s which is one of the
 states and there is a set of final_states F And then we also have to give the
 transition_function and I want to write_out the state transition_function I want
 to use the state transition_function to define a a operator that were_going to
 find handy for defining our DFA So Id say that a applied to a set of states so x
 here is a set of states and a is a character in the input language So a and
 x is those states y such that there is some x little x here single state in the
 set of states such that theres a transition from x to y on input a Okay
 So this is just a way of saying Ive given the transition_function at this set level
 It says for a given set of state x show me all the states that you can reach on
 input a Alright So now were_ready to define our DFA So what will the DFA be
 Well its gonna have to have all of these things Its gonna have to have perhaps
 where the states are What are the start_state is Whats the final_states are and
 whats the transition_function is So lets begin_with the set of states The
 states will be the subsets Of s So the states of the DFA will be all possible
 subsets of the states of the NFA so there will be one state of DFA for each subset
 of possible each possible subset of states of the NFA And of course this is
 potentially a very big number but its still finite and so we can use that set
 of of subsets of states as the states based of the Deterministic machine So now
 whats the start_state of the DFA Well thats going
 to
 be the epsilon_closure Now one of the set of final_states Well
 so the final_states will be consist of those state x and every member of the
 states of the DFA are sets of states of the NFA So that x is a set and is can be
 every x such that x intersected with the set of final_states of the NFA is not
 empty And finally we need to define the transition_function And do we do that
 Well we we need to say that for a given state x and another state y when is there
 a transition between them on some input a Well that there will be such a transition
 under that conditions and well lets write them out So remember were in state x
 And what do we need to know Well we need to know the set of states that we can
 reach on input A and well be justifying that thats A of X and then once_weve
 gotten to where these once_weve seen where we can go from the set of states X
 of input A Theres also a possibility of making inaudible after that so
 furthermore we have to take the inaudible closer of that set of states
 okay And So well say that theres a transition from x to y if y is equal to
 this set of states Alright And notice that theres only one such set of states
 for any x and that guarantees of this is a deterministic_machine Each machine each
 state will only have one possible move on each input so We can just now it goes to
 our check list and see if we have a deterministic_machine We have a finite
 set of states We have a start_state and we have a set of final_states and we have
 a transition_function with only one more per input and no epsilon_moves And so
 that is in fact a deterministic_machine and the property that it maintain is that
 each_step of computation The state of the DFA records the set of possible states
 that the NFA could have gotten into the same input So lets work to an example of
 constructing a Deterministic machine from a Nondeterministic machine Heres the
 Nondeterministic Finite_Automata that we built in the last_video and again this is
 the one that I used at the beginning of the video to define epsilon enclosure So
 were_gonna do the example slightly differently than the construction I gave
 on the previous_slide If we actually have to write_out all the subsets of this many
 states it will take us a very very long_time And it_turns out that not all of the
 subsets were actually used by the DFA So were just going to enumerate the states
 that we actually need and well do that by beginning_with the start sta te of DFA and
 then working out which additional states are required So how do we do that Well
 we begin_with the start_state of the NFA which is just this state a And then
 recall at the start of the DFA is the epsilon_closure of that state so that
 corresponds to this purple set here Alright So the first state of the DFA
 the start_state is the subset of states a b c d h i And now we have to work out
 from this particular state from the start_state what_happens on each of the
 impossible input values So the alphabet of this machine is one and zero so you
 would have to have two transitions out of the state one for an input of one and one
 for an input of zero So lets do input zero first And we can see looking_at the
 purple set that theres only one possible transition and thats from the state D to
 the state F So certainly the state s is included in the set of states if the NFA
 can reach but then once we get the state f theres a lot of epsilon_moves that we can
 take and so in fact the second state of the DFA corresponds to a much larger set
 Its all the its the epsilon_closure of f and that is this set of states f g h
 i a b c and d okay So these are the set of possible states that the NFA could
 be in after reading a single zero Next lets_consider what_happens from the start
 state on an input of one Which possible states can the NFA reach And if we look
 at the transition_function we see there are two possible moves that the NFA could
 take It could be in state c In which case it would move to state e or it could
 have been state i thats also part of the purple set in which case it would move to
 state j So there are two possible states that the NFA can get into as a result of
 reading a one and then after that theres a bunch of epsilon_moves that can take
 place and in fact it_turns out that after reading a one the machine could be in any
 state except for state f And thats this set of states and youll notice that this
 particular set of states the read set includes the final_state of the NFA so
 this is also a final_state indicating that after reading one the NFA could be in an
 accepting_state So this would be an_accepting state of the DFA Well we still
 have to fill in for both of the two states that weve added here The two states on
 the right of the machine what they do on input zero what they do on input one So
 lets figure that out So beginning_with the red state on input zero what can
 happen Well look the red state includes state d and it can move to state f but
 weve_already computed what_happens on the epsilon what the epsilon_closure that is
 just the green state And so if Im in the red state and I read zero I move to the
 green state If Im in the red state and I read a one youll_see that both states
 NFA states c and i are in the red set And so it just takes us back to the red set
 And similarly for the green state if I read a one I move to the red state And
 if I read a zero I stay in the green state And so this then is our
 deterministic_machine down_here This is the deterministic_machine and again it
 simulates the NFA So every move at the deterministic_machine it records the set
 of possible states that the NFA could be in and it will accept a string infinitely
 if the NFA could accept the string
 Welcome_back In this video_were going to wrap_up our presentations on lexical
 analysis with the discussion of how we implement Finite_Automata using a variety
 of different techniques Just to review heres our little flow chart of how
 lexical analyzers are constructed And today were_going to be focusing on this
 last step The implementation of DFAs and actually I should_say that this chart is
 not quite completely accurate because sometimes we dont go all the way to
 DFAs We stop with NFAs and we implement them directly and so well talk a little
 bit about that What if we didnt want to build a DFA and instead wanted to base our
 lexical analyzers directly on an NFA So beginning now with DFAs Its very_simple
 to implement a deterministic finite_automaton as an_array Theres dimensional
 array and one of the dimensions will be the state So we might have states here
 and the other dimension will be the input symbols And so I might have a state i and
 then input A and I simply look up in that position and there will be the next state
 k to which were_going to move So the table stores at every particular input
 symbol and state the next state that the machine will move to So lets do an
 example of converting a deterministic_automaton into a table driven
 implementation so here is the automaton that we built last_time and recall that
 several_videos ago We started with a regular_expression which we convert into a
 nondeterministic_finite automaton and then we converted it to a deterministic
 automaton just in the last_video And here it is again and now lets talk_about how
 to realize it as a table So draw 2dimensional table and there are three
 states so we will need three rows And just label these rows S T and U and then
 there are two inputs zero and one and now lets fill in the entries in the table
 So in state S on input zero where do we go We go to state T So the entry in the
 S0 entry will be T And some really from state S input one will wind_up in state U
 So on so the S1 entry will be U okay And then certainly for the other the other
 rows of the table lets do the T row next on one we go to U and on zero we stay in
 T So this this row is also TU And finally for U what_happens well on zero
 we go to T and on one we stay in U so this row is also TU and theres our table That
 describes the transition relation of the automaton And now if we would think_about
 how we would use this transition relation in a program you can easily imagine We
 would start out say with our input index Porting to the beginning of the input and
 lets call that zero and we can have to have a current state and we start at the
 start_state lets just say that thats row zero so in this case that would be row
 S And then while what we wanted to do we wanted to walk over the input And check
 whether and checking on it you_know and make the transitions for each element of
 the input one at a time and we want to stop when the input is empty So while
 there is still as input lets_say we have an_array of characters that is our that
 is our input and while the entry in that array is not null lets do the following
 Were_gonna update the state At each_step and what are we gonna update it to lets
 give this array a name Lets call this array A So were_gonna look up in our
 transition relation A and what are we going to use Well were_gonna look up
 the current state And were_going to look up the input And in that entry I_think
 you_know Using the the current state and the current input were_gonna transition
 to a new state and we also wanted to increment the input pointer So well do
 that at the same time And there is our loop that processes an input according to
 the transition table A And as you can see this is a very compact very efficient
 process Just really just a little_bit of index arithmetic and one two table look
 ups one for the input and one for the state transition table per character of
 input So thats really hard to imagine having a process thats much faster or
 more_compact than this Now that was one strategy for implementing a deterministic
 finite_automaton and you may have noticed that one disadvantage of that particular
 approach was that there were a lot of duplicate rows in the table In_fact all
 the rows of the table were exactly the same and we could save some space by using
 a slightly different representation So instead of having a 2dimensional table
 we can just have a 1dimensional table and this table again Would be one entry for
 each state so S T and U And what this table would contain is a pointer to a
 vector of moves for that particular state So there will be a pointer here and it
 would point to another Table another one dimensional table that would say what we
 should do zero and what we should do on one So in the case of S we wanted to
 move to state T if it was a zero and to state U if it was a one And now when we
 go to fill in the entry for T we see we dont need to duplicate this row We can
 actually just share this row and similarly for U And so this table this
 representation is actually much_more compact which just share the rows that are
 duplicated in the automaton And it_turns out that in the kinds of automata that we
 look_at for lexical_analysis its very very common to have duplicated rows And
 so this can actually resolve the significant compaction of the table
 particularly when you consider a number of possible states Remember there could be
 To the N one states in a DFA For an NFA with end states And while the blow up is
 often not the worst case it can be very substantial So the two dimensional table
 we had on the in the previous_slide could actually be quite quite large and we
 keep we can sometimes have a much_more compact representation by little tricks
 like this Now in this advantage of this particular representation is this extra
 interaction right here I_mean these pointers actually take time to the
 reference and so now in our loop will be a little_bit slower We we we have to do
 table look up the reference Pointer do another tab le look up and then we can
 make the move Finally its also possible that we might not want to convert to a DFA
 at all It might be that the particular specification we gave is very expensive to
 turn into a DFA The table has just become truly huge and we might be better off just
 using the NFA directly And so one can imagine an implementation of an NFA as
 well We can also implement that via a table In this case we would have to have
 a row for each state in the NFA And we wont do them all here But we could have
 Rows for every state of the NFA and then you_know where were_going to go if the
 input is zero or if the input is one And so in this case And I almost forgot we
 would also need a transition in the most naive or the most straight_forward
 implementation of where we would go if if an epsilon And and now remember
 because these are NFAs in general this will be a set of states because we might
 have more_than one epsilon transition or more_than one transition on zero and one
 And so in this case an epsilon A can go to B So this would be the set of states B
 and B can go To C or D And C can only go to E and on one alright And D can go to
 F on and input of zero and so on We fill in the rest of the table and this table is
 guaranteed to be relatively small because the number of states is limited by the
 number of states in the NFA and the size of the input alphabet Once again we
 could do a sharing of common rows and and other tricks like that to compress the
 table if we like But now the inner_loop for simulating this automaton is gonna be
 much_more expensive because we have to deal_with sets of states rather_than
 single states So in every point in time where we can be tracking a set of states
 and when were_going to do a move we have to look up for every state in the set
 where it can potentially go including things_like the epsilon_moves and carry
 out all possible epsilon_moves so we always have an accurate assessment of the
 complete set of states if the NFA could reach So while this sav es a lot of space
 in terms of the tables in terms of the size of the tables it can be much slower
 to execute than deterministic_automaton Summarize a key idea in the
 implementation of lexical specifications is the conversion of nondeterministic
 finite_automaton to deterministic finite_automaton This is what takes a general
 high_level specification use of regular_expressions and confer to them to
 something as completely deterministic and only uses a few operations per input
 character Now in practice tools provide tradeoffs between speed and space So so
 DFAs are faster And less compact so the tables can be very large and and at times
 thats a practical problem and NFAs are slower to to implement but more concise
 And the tools give you generally a series of options often in the form of
 configuration files or command lines which is that allow you to to choose whether
 you want to be closer to a full DFA something thats faster and perhaps bigger
 or to a pure NFAs something thats slower but consumes less space
 In this video_were going to transition from lexical_analysis to parsing and talk
 a little_bit about the relationship between those two compiler phases Weve
 already_talked about regular_languages and its_worth mentioning that these are the
 weakest formal_languages that are widely_used But they have of course many
 applications some of which we saw in previous videos The difficulty with
 regular_languages is that a lot of languages are simply not regular And
 theres some pretty important languages that cant be expressed using regular
 expressions or finite_automata So lets_consider this language which is the set of
 all balanced parentheses So some elements of this language would be at the string
 one openparen one closeparen two openparens two closeparens three
 openparens three closeparens and so on And you can imagine that this is actually
 something thats fairly representative of lots of programming_language construct So
 for example any kind of nested arithmetic expression would fit into this class but
 also things_like nested if and elses will have this category this characteristic
 And here with the nested inaudible its just the f statement the functions like
 an openparen Not every languages like cool which has the explicit closing fee as
 well but theyre implicit in many languages and so there are lots of nesting
 structure in programming_languages constructs and those cannot be handled by
 regular_expressions So this raises the question of what the regular_languages can
 express And why they arent sufficient for recognizing arbitrary nesting
 structure So we can illustrate the limitations of regular_languages and
 Finite Automaton by looking a simple two state machine So lets_consider this
 machine We have one we have start_state and then the other state is the accepting
 state And well have this machine Just be a machine that weve_already seen
 actually and itll recognize strings with odd numbers of 1s So if we see a one and
 were in the start_state we move We now see an odd number of 1s We move to the
 accepting_state and we stay there until we see another one In which case weve_seen
 even number of 1s and then were in the start_state So whenever we see an odd
 number of 1s were in the final_state Whenever we see an even number of 1s
 were in the start_state And if we feed this a fairly long string of 1s lets
 lets select only seven 1s in it Then whats it going to do is going to go_back
 and forth and back and forth between these states Its gonna wind_up in the final
 state when it gets to the last one so itll accept but notice that it doesnt
 know how many times its been to that final_state It_doesnt remember the
 length of the string it doesnt have any way of counting how many characters the
 string had in it And in fact all I can count here is the parity So in general
 Finite_Automata can really only express things where you can count modulus on k
 So they can count mod k for some k where k is the number of states in the machine
 And so you_know if I have pretest the machine I can keep_track of whether the
 string length is divisible by three or some other similar property but I cant do
 things_like count to an_arbitrary i so if I need to recognize a language that
 requires counting arbitrarily high like recognizing all strings of balance
 parentheses we cant do that with the finite set of states So what does a
 parser do it takes the sequence of tokens as input from the lexer and it produces a
 parse_tree of the program And for example in cool heres an input expression that
 is input to the lexical_analyzer The lexical_analyzer produces this sequence of
 tokens as its output Thats the input to the parser Then the parser produces this
 parse_tree where the nesting structure has been made explicit So we have the if
 and else and then the three components the predicate the then branch and the
 else branch of the if To_summarize the lexer takes a string of character as input
 and produces a string of tokens as output That string of tok ens is the input to the
 parser which takes a string of tokens and produces a Parse_Tree of the program And
 its_worth mentioning a couple of thing here First of all sometimes the Parse
 Tree is only implicit So the a compiler may never actually build the full Parse
 Tree Well talk more about that later Many compilers do build an explicit parse
 tree but many do not The other thing thats worth_mentioning is that there are
 compilers that do combine these two phases into one where everything is done by the
 parser So the parsing technology is generally powerful enough to express
 lexical_analysis in addition to parsing But most compilers still divide up the
 work this way because regular_expressions are such a good match for lexical_analysis
 and then the parsing is handled separately
 In this video_were gonna begin our_discussion of parsing technology with
 contextfree grammars Now as we know not all strings of tokens are actually valid
 programs and the parser has to tell the difference It has to know which strings
 of tokens are valid and which ones are invalid and give error messages for the
 invalid ones So we need some way of describing the valid strings of tokens and
 then some kind of algorithm for distinguishing the valid and invalid
 strings of tokens from each other Now weve also discussed that programming
 languages have a natural recursive structure So for example in Cool an
 expression That can be anyone of a very large number of things So two of the
 things that can be are an if expression and a while expression but these
 expressions are themselves recursively composed of other expressions So for
 example the predicate of an if is a a inaudible expression as is the then
 branch and the else branch and in a while_loop the termination test is an expression
 and so is the loop_body And contextfree grammars are in natural notation for
 describing such recursive structures So within a contextfree_grammar so formally
 it consist a set of terminals t a set of nonterminals n a start_symbol s and s is
 one of the nonterminals and a set of productions and whats a production A
 production is a symbol followed_by an arrow followed_by a list of symbols And
 these symbols there are certain rules about them so the x thing on the left_hand
 side of the arrow has to be a nonterminal Thats what it means to be on the left
 hand_side so the set of things on the left_hand side of productions are exactly the
 nonterminals And then the right_hand side every yi on the right_hand side can be
 either a nonterminal or it can be a terminal or it can be the special symbol
 epsilon So lets do a simple example of a Contextfree Grammar Strings of balanced
 parenthesis which we discussed in an earlier video can be expressed as follows
 So we have our start_symbol and One_possibility for a string o f balanced
 parentheses is that it consists of an open_paren on another string of balanced
 parentheses and a close_paren And the other_possibility for a string of balanced
 parentheses that is empty because the empty_string is also a string of balanced
 parentheses So there are two productions for this grammar and just to go over the
 to to relate this example to the formal definition we gave on the previous_slide
 what is our set of nine terminals its just The singles nonterminal s what our
 terminal_symbols in this contextfree_grammar is just open and close_paren No
 other symbols Whats the start_symbol Well its s Its the only nonterminal so
 it has to be the start_symbol but generally we will when we give grammars
 the first production will name a start_symbol so rather_than name and explicitly
 whichever production occurs first the symbol on the left_hand side will be the
 nonterminal for that particular contextfree_grammar And then finally
 what are the productions with the we said there could be a set of productions and
 here are the two productions for this particular ContextFree Grammar Now
 productions can be read as rules So lets write_down one of our productions
 from the from the example grammar and what does this mean This means wherever we see
 an s we can replace it by the string of symbols on the right_hand side So
 Wherever I see an s I can substitute and I can take the s out If that important I
 remove the s that appears on the left side and I replace it by the string of symbols
 on the right_hand side so productions can be read as replacement rule so right_hand
 side replaces the left_hand side So heres a little more formal description of
 that process We begin_with the string that has only the start_symbol s so we
 always start with just the start_symbol And now we look_at our string initially
 its just a start_symbol but it changes overtime and we can replace any
 nonterminal in the string by the right_hand side side of some production for
 that nonterminal So for exam ple I can replace a nonterminal x by the right_hand
 side of some production for x X in this case x goes to y1 through yn And then we
 just repeat step two over and over again until there are no nonterminals left
 until the string consist of only terminals And at that point were done
 So to write this out slightly_more formally a single step here consist of a
 state which is a which is a string of symbols so this can be terminals and
 nonterminals And somewhere in the string is a nonterminal x and there is a
 production for x in our grammar So this is part of the grammar okay This is a
 production And we can use now production to take a step from to a new state Where
 we have replaced X by the right_hand side of the production Okay So this is one
 step of a contextfree derivation So now if you wanted to do multiple steps we
 could have a bunch of steps alpha zero goes to alpha one goes to alpha two and
 these are strings now Alpha is are all strings and as we go along we eventually
 get to some strong alpha n alright And then we say that alpha zero rewrites in
 zero or more steps to alpha n so this means n zero greater_than or equal to
 zero steps Okay So this is just a short hand for saying there is some sequence of
 individual productions Individual rules being applied to a string that gets us
 from the string alpha string zero to the string alpha n and remember that in
 general we start with just the start_symbol and so we have a whole bunch of
 sequence of steps like this that get us from start_symbol to some other string So
 finally we can define the language of a ContextFree Grammar So inaudible
 contextfree_grammar has a start_symbol s so then the language of the contextfree
 grammar is gonna be the string of symbols alpha one through alpha n such that for
 all i Alpha i is an element of the terminals of g okay So t here is the set
 of terminals of g and s goes the start_symbol s goes in zero or more steps to
 alpha one Im_sorry a1 to an okay And so were just saying this is just saying
 that all the strings of terminals that I can derive beginning_with just the start
 symbol those are the strings in the language So the name terminal comes from
 the fact that once terminals are included in the string there is no rule of
 replacing them That is once the terminal is generated its a permanent feature of
 the string and in applications to programming_languages and contextfree
 grammars the terminals are to be the tokens of the language that we are
 modeling with our contextfree_grammar With that in mind lets try the
 contextfree_grammar for a fragment of inaudible So inaudible expressions
 we talked_about these earlier but one possibility for a inaudible expression
 is that its an if statement or an if expression And we call that inaudible
 if statements have three parts And they end with the keyword inaudible which is
 a little_bit unusual And so looking_at this looking_at this particular rule we
 can see some conventions that way that are pretty standard and that well use so
 that nonterminals are in all caps Okay so in this case was just inaudible well
 try that in caps and then the terminal_symbols are in in lower_case all right
 And another_possibility Is that it could be a while expression And finally the
 last possibility Is that it could be identifier id and there actually many
 many more possibilities and lots of other cases for expressions and let_me just show
 you one bit of notation to make things look a little_bit nicer So we have many
 we have many productions for the same nonterminal We usually group those
 together in the grammar and we only write a nonterminal on the right_hand side once
 and then we write explicit alternative So this is actually Completely the same as
 writing out expert arrow two more times but we here we just is this is just a way
 of grouping these three productions together and saying that expr is the
 nonterminal for all three right_hand sides Lets_take a look_at some of the
 strings on the language of this Context Free Grammar So a valid Kuhl expression
 is just a single identifier and thats easy to see because EXPR is our start
 symbol Ill call it EXPR And so the production it does says it goes to id So
 I can take the start_symbol directly to a string of terminals a single variable
 name is a valid Kuhl expression Another example is an eth expression where eth
 of the subexpressions is just a variable_name So this is perfectly fine structure
 for a Kuhl expression Similarly I can do the same_thing with the while expression
 I can take the structure of a while and then replace each of the subexpressions
 just with a single variable_name and that would be a syntactically valid cool while
 loop There_are more_complicated expressions so for example here we have a
 why loop as the predicate of an if expression Thats something you might
 normally think or writing but perfectly well form and tactically Similarly I
 could have an if statement or an if expression as the predicate of and if its
 inside of an if expression So so nested if expressions like this one are also
 syntactically valid Lets do another grammar this time for simple arithmetic
 expressions So well have our start_symbol and only nonterminal for this
 grammar be called e and one of the possibilities while e could be the sum of
 expressions Or and remember this is an alternative notation for e arrow Its
 just a way of saying Im going to use the nonterminal for another production We can
 have a sum of two expressions or we could have the Multiplication of two
 expressions And then we could have expressions that appear inside the
 parentheses so parenthesized expressions And just to keep things simple we could
 just have as our base only base case simple identifier so variable_names And
 heres a small grammar over plus and times to see and in parentheses and variable
 names inaudible a few elements of this language So for example a single
 variable_name is a perfectly good element of the language id_id is also in this
 language Which s is id_id id and we could also use parens to group things so
 we could say id_id id thats also something you can generate using these
 rules and so on and so forth There_are many_many more strings in this language
 In this video_were gonna continue_our discussion of parsing with the idea of a
 derivation So a derivation is a sequence of productions so beginning_with the start
 symbol we can apply productions one at a time In sequence and that produces a
 derivation And a derivation can be drawn in a different way instead of as a linear
 sequence of replacements we can draw it as a tree So for example if I have a
 nonterminal x it appears in a derivation then when I replace x I can represent
 that by making the children Of x the left_hand side of the rule that I used to
 replace x So I applied production x goes to y1 to yn I add the y1 to yn is
 children of x in the tree that Im building up Lets do an example Here is
 our simple grammar of arithmetic expressions and lets_consider this
 particular string id_id id So what were_going to do now is were_going to
 parse this string and were_going to show how to produce a derivation for the string
 and also at the same time build the tree And here it is Over here there is
 derivation beginning in e and ending in the string that were_interested in with
 one production applied each_step along the way and here is the corresponding tree and
 this is called a parse_tree This is a parse_tree of this expression or this
 input string So lets walk_through this derivation in detail The right side in
 red were_going to have the tree that were building up And on the left side in
 blue were_going to have the steps in the derivation that weve taken so_far So
 initially our derivation consists of just the start_symbol e and our tree consists
 of just the root which is also the start_symbol So the first step is that we have
 a production e goes to e e and what that means is over on the tree we take the
 root of the tree and we make we give it three children e ne So now we replace
 the first t by e z We use the production e goes to e z and that means
 we take the first e in the tree and we give it to three children e
 inaudible Continuing along we take the fi rst e here that remains in this
 expression and we replace it by id which means we make id a child of the left most
 e in the tree that were building And then we replace the second e by id using
 the production e goes to id and finally we use the same_thing with the third e and
 now we have completed our Parse_Tree So here again from the start_symbol to the
 string we were_interested in parsing and in the process we built up this Parse_Tree
 of the expression Now there are lots of interesting things to say about parse
 trees So first of all parse_trees have terminals at the leaves and nonterminals
 at the interior_nodes and furthermore inorder traversal of the leaves is the
 original input So lets back up and look_at our example and confirm all this If we
 look_at the leaves we can see that they are all terminals Okay And the interior
 nodes are all nonterminals In this case its only one nonterminal in our language
 all the interior_nodes are e and the leaves are the terminals of the string
 And then we can see if we do an inward reversal of the leaves we get exactly
 this input string that we started with Furthermore the Parse_Tree shows the
 association of the operations and the input string does not So you may notice
 here that the way that this Parse_Tree is constructed the times binds more tightly
 than the plus because the times is a subtree Of the tree containing plus And
 so this means that we would do the e e first before we would add e and some some
 of you may have wondered well how did I know To pick this Parse_Tree because
 actually if you think_about it theres another derivation Actually there are
 several derivations that will give me a different Parse_Tree where the plus where
 the times is towards the root and the plus is nested inside the times So lets not
 worry_about that for right now and lets just say that somehow we knew that this
 was the Parse_Tree we wanted and I gave you a derivation that produces that Parse
 Tree Continuing on the previous derivation I_showed you is actually a very
 special derivation Its whats called a leftmost_derivation where each_step will
 replace the leftmost_nonterminal in our string of terminals and nonterminals And
 theres a natural and equivalent notion of a rightmost_derivation and here it is
 Here is a rightmost_derivation for the same string Again beginning_with the
 start_symbol ending with a string were_interested in And notice that at each
 step were replacing the rightmost nonterminal So here we replace the only
 nonterminal e and we get e c And then in the second step we replace the second
 nonterminal e with id and so on for the rest of the string So lets just
 illustrate this entirely with our little picture here of the tree and the
 derivation simultaneously so once_again over_here is our tree and this is the
 root the start_symbol e and and in blue is our derivation so we begin by replacing
 e by e e Thats the only nonterminal so its the rightmost one and then working
 from the right side of the tree we replace the right e by id and then the
 left id gets replaced_by e z And now the right most e that remains is replaced
 by id and finally the only e that remains is also replaced_by id Now I want to
 point out that the rightmost and leftmost derivations I_showed you have exactly the
 same Parse_Tree And this was not an accident Every Parse_Tree has a rightmost
 and a leftmost_derivation Its just about the order in which the branches are added
 So for example if I have the first production e goes to e e now I have a
 choice on how to build my tree I can either work on This subtree or I can
 work on that subtree And if I build this one first that will be a rightmost
 derivation If I continue to always work on the rightmost nonterminal of course
 And if I work on this one first I can use that to do a leftmost_derivation Now its
 important also to realize that there are many derivations besides rightmost and
 leftmost I could I could choose nonterminals in some random order to do
 my replacements But th_e rightmost and leftmost ones are the ones that were most
 concerned with
 In this video_were going to talk_about ambiguous context_free grammars in
 programming_languages and what to do about them Well begin by looking_at our
 favorite grammar for expressions over and and identifiers and well just look
 at the string id_id id Now it_turns out that this particular string and lets
 write it down one more time id_id id This string has two parse_tree using this
 grammar Lets do the Parse tree on the left first We begin_with the start_symbol
 e and the first production in this derivation that gives_us this Parse tree
 must be that e goes to e e E e And then we replace the left most e by e e
 We use the production e goes to e e and we still have the plus e left over and at
 this point you can see that were_going to get this parse_tree Weve done with those
 two productions We have done this much The construction of the parse_tree and the
 rest of productions are just generating these ids So thats a three more
 productions and we can see that you_know if we do those well get id_id id no
 problem alright So now lets switch and do the derivation on the right or excuse
 me the parse_tree on the right so this begins also with e But this time we use
 the production e goes to e e first all right And now were_gonna replace the
 right most e By e goes to e e so we have e e e and now weve with those
 two productions weve done this portion of the parse_tree and once_again with three
 more productions we can get to id_id id so there you can see weve_got two
 derivations That produced two distinct Parse trees And just to be completely
 clear about this in this case were getting two different Parse trees Each of
 these derivation each of these Parse trees has many derivations Each Parse
 tree has a left most derivation a right most derivation and many other
 derivations This situation is something different Here we have two derivations
 that yield completely_different Parse trees and that is the sign or the
 definition of an ambiguous grammar So a grammar is ambiguous if it has more_than
 one Parse tree for some string And another way of saying the same_thing is
 that there is more_than one right most or left most derivation for some string So
 if some string has two right most derivations or more or two left
 derivations or more then the that string will have two distinct parse_trees and
 that grammar will be ambiguous Ambiguity is bad If you have multiple parse_trees
 for some program then that essentially means that youre leaving it up to the
 compiler to pick which of those two possible interpretations of the program
 you want it to generate_code for and thats not a good idea We_dont like to
 have ambiguity in our programming_languages and leave decisions about what
 the program means up to the compiler
 In this video_were going to take a rest a little_bit and talk_about how compilers
 handle errors and in particular what kind of error_handling functionality is
 available in parsers Compiler has two relatively distinct jobs The first is to
 translate valid programs That is if it gets a program from a programmer that is
 correct is a valid program it needs to produce correct code for that program
 Now distinct from that task is the job of giving good feedback For erroneous
 program and even just detecting the invalid programs we dont want to compile
 any program that isnt a valid program in the programming_language And programming
 languages have many different_kinds of errors Heres just a few So for example
 we might have lexical errors thats for using characters that dont even appear in
 any balanced symbol in the language and these would be detected by lexical
 analysis phase We could have syntax errors and this would be the parsing
 errors when all the individual lexical units are correct but theyre assembled in
 some way that doesnt make_sense and we dont_know how to compile it There could
 be semantic errors for example when types mismatch Here Ive declared excess in
 integer and use it as a function and that would be the job of type_checker to catch
 those And then Actually there may be many errors in your_program that are not
 errors of the programming_language The program you wrote is actually a valid
 program but it doesnt do what you intended Youre likely bugging your
 program and so while the compiler can detect many kinds of errors it doesnt
 detect all of them and you_know once we get past what the compiler can do then
 its up to testers and users to find the rest of the problems in the program So
 what are the requirements for a good error_handling Well we want the inaudible
 report errors accurately and clearly so that we can identify what the problem is
 quickly and fix it The compiler itself should recover from the error error
 quickly So when it hits an error it shouldn t take a long_time to make a
 inaudible on what to do before proceeding And finally we dont want
 error_handling to slow down the compilation of valid code That is we
 shouldnt pay a price for the error_handling if were not really using it Im
 going to talk_about three different_kinds of error_handling Panic mode and error
 productions are the two that are used in current compiler So these are actually
 things that people use today Automatic local or global correction is an idea that
 was pursued excessively in the past And I_think its historically quite interesting
 particularly as a contrast what people do today and also why people try to do it a
 long ago inaudible is the simplest and most popular method of error_recovery
 thats widely_used and the basic ideas that when an error is detected the parser
 is going to begin discarding tokens until one that has a clear role in the language
 is found and thats going to try to restart itself and continue from that
 point on And these tokens the ones that its looking for are called the
 Synchronizing Tokens And these are just tokens that have a wellknown role in the
 language and so that we can reliably identify where we are So a typical
 strategy might be to try to skip to the end of a statement or to the end of a
 function if an error is found in a statement or function and then begin
 parsing either the next statement or the next function So lets look_at a simple
 hypothetical example of panic mode error_recovery So heres an expression Clearly
 it has a problem We shouldnt have two plus signs in a row so something has gone
 wrong here at the second plus and whats going to happen is the parser is going to
 come_along The parser is going to be proceeding from left to right Its gonna
 see the openparen its gonna see the number one its gonna see the plus
 everything is good and then its gonna see the second plus and its not gonna know
 what to do Its going to realize That its stuck and that theres_no expression
 in the language that has two p lus signs in a row and it needs to do something to
 recover Its encountered a parsing error and it has to take some error action at
 this point So in panic mode recovery what its going to do is its going to hit
 the panic button So right at this point its going to say I give up Im not
 parsing normally anymore It goes into a different mode Where is simply throwing
 away input until it finds something that it recognizes and for example we could
 say that the policy in this particular for this particular kind of error is to
 skip ahead to the next integer and then try to continue So well just throw away
 the plus in this case and then it would restart here at the two expecting to see
 another integer Try to finish off this expression and it would treat that as one
 two and then the parenthesis would match and the rest of the expression would parse
 just fine Now in tools such Bison which is widely_used parser generator and one
 that you may use for the project there is a special terminal_symbol called error to
 describe how much input to skip and the productions that are given in Bison look
 like this So you would say at the possibilities for e are that e could be an
 integer e could be an The sum of the two es two expressions it could be a
 parenthesized expression or if none of this work okay So these are the normal
 productions Alright If none of those work then we could try some of these
 productions that have error in them And error is a special symbol for Bison and it
 says well these are the alternatives to try if these things over_here didnt work
 So if you find an error Lets focus_on this one right here so if this says that
 if you find an error while youre trying to parse an e Okay we havent actually
 said how that works yet Well see that in the future_videos but conceptually the
 parser is trying to recognize one of these kinds of expressions here Its in a state
 where it thinks it wanted to see an_integer or a or a parenthesized
 expression and if that isnt working out i f it get_stuck well then hit the panic
 button and you can declare that its in error state and it can throw away all the
 input This error will match all the input up to the next integer And then this
 whole_thing could be counted as an e As one of these things and then we will try
 to continue the parsing Similarly if we encounter an error somewhere inside a pair
 of match parenthesis well we could just throw away the whole_thing and just reset
 at the parenthesis boundaries and they continue parsing So these are two
 possible error_recovery strategies if we find an error for this particular kind of
 symbol in the grammar And you can have these error These productions that
 involved the error token for for as many different_kinds of symbols in the language
 as you like Another common strategy is to use what are known as error productions
 and this specify known common mistakes that programmers make adjust as
 alternative productions in the grammar So heres a simple example lets_say we were
 working_on a programming_language or compiler for a programming_language that
 was used by a lot of mathematicians and instead of writing Five x like computer
 scientists do these guys always wanted to write five x to just juxtapose the five
 and the x to look more like normal mathematical notation And they complain
 that this is always giving them parser errors If the parser is just complaining
 over and over again then this is not a well formed expression Well we could
 just go in to our grammar and add a production that made of of well form We
 could just say well now its legal if I have that one kind of expression is just
 to have two expressions that inaudible opposed next to each other with no
 intervening operator And this has a disadvantage obviously of complicating the
 grammar If we do this a lot our grammar is gonna get a lot harder to understand
 Its gonna be a lot harder to maintain and essentially all this is doing is promoting
 common mistakes to alternative syntax but this is used in practice Peo ple do this
 sort of thing and you will see for example when you use TCC and other production C
 compilers they will often warn you about things youre not supposed to do but
 theyll accept them anyway and this is essentially the mechanism by which they do
 that Last strategy I want to talk_about a little_bit is error correction So so_far
 weve just talked_about strategies for detecting errors but we could also try to
 fix errors that is if the program has mistakes in it the compiler could try to
 help the program out and say oh you obviously didnt mean to write that Let
 me try to find a program for you that that works And these kind of corrections
 in some sense we wanted to find programs that are nearby programs that arent too
 different from the programs at the that the programmer supplied but we couldnt
 compile correctly And theres a few different things that you can do to the
 things that people have tried are things_like token insertions and deletions So
 here youd like to minimize the edit distance That would be the metric that
 you would use to determine whether a program was close to the original program
 that the programmer supplied You could also do exhaustive search within some
 bounce to try all possible programs that are close to the program that was
 supplied And a couple of disadvantages to this actually number of disadvantages
 You can imagine that this is hard to implement It its actually quite
 complex This will slow down the parsing of correct programs because we need to
 keep enough state around that we can manage the search or or the editing in
 the case of that way actually doing counter and error and of course nearby is
 not really is not really that clear what that means and various notions of nearby
 may or may not actually take us to a program of the the programmer would
 actually be happy with The best one example of error correction is the
 compiler PL crosstalk This is PL_inaudible compiler thats the PL part
 and the C stands_for either correction or Cornell which is where the compiler was
 built and PL crosstalk is wellknown for being willing to compile absolutely
 anything You could you could give it the phone book You can and people did give it
 things_like speech from Hamlet soliloquy and it would print_out a lot of error
 messages Sometimes these error messages would be very funny to read And it would
 in the end do correction and produce always a valid running PL_inaudible
 program And you might ask why do people bother with that It_doesnt seem but that
 may not seem very compelling To us today And have to realize that when this work
 was done back in the 1970s people live in a very different world There was a very
 slow recompilation cycle It could take a whole day To get your_program To compile
 and run you would essentially submit your_program in the morning and you might not
 get results back until the afternoon And with that kind of turnaround cycle Even
 one syntax_error in your_program was devastating You can lose a whole day
 because you mistype the keyword and having the compiler try to take a stab at finding
 a working program for you if the correction was small and you save an
 entire day you_know to think it can fix that one small mistake you made and give
 you a valid run that was actually a useful thing to do And so the goal then
 was to find as many errors in one cycle as possible They would try they would try
 to find as many errors to try to recover Find as many errors as possible Give you
 as good feedback as possible so you could fix as many errors avoid as many retry
 cycles as possible And and even possibly automatically correct the program So that
 you could see if the correction were right and and then possibly the the results
 you got back were useful on the inaudible to do even more debugging
 before the next round Now today were in a completely_different situation We were
 very fast almost interactive recompilation cycle for many projects and
 as a result users generally arent interested in finding many errors They
 ten d to correct only one error per cycle Compilers still report many errors Ill
 give you lots and lots of errors but my observation certainly might have it
 Personally what I see many other people do is they only fix the first one because
 its the most reliable and the one that definitely needs to be fixed before
 before you can try to compile again If the compilation is fast enough thats
 probably the most proactive thing to do And as result complex error_recovery
 today is just less compelling than it was a few decades ago
 In this video_were gonna talk_about the core data_structure used in compilers the
 abstract_syntax tree To briefly review a parser_traces the derivation of a sequence
 of tokens but this by_itself Is not all that useful to the compiler because the
 rest of the compiler needs some representation of the program It needs an
 actual data_structure that tells it what the operations are in the program and how
 theyre put together Well we know one such data_structure is called a Parse_Tree
 but it_turns out that a Parse_Tree really isnt what we wanted to work on Instead
 we wanted to work on something called an Abstract Syntax Tree And the Abstract
 Syntax Tree is really just the Parse_Tree but with some details ignored We have
 abstracted a way From some of the details of the Parse_Tree And heres an
 abbreviation that youll_see ASTs stand for Abstract Syntax Tree So lets look
 at the grammar Heres the grammar for plus expressions over the integers and we
 also parenthesize expressions And heres a string and after lexical_analysis what
 do we have Well weve_got a sequence of tokens again with their associated lexemes
 telling us what the actual strings were And that gets past into the parser and
 then we build a parse_tree And heres a parse_tree for that expression Now if
 its expressed that this representation the parse_tree is actually perfectly
 adequate for compilation We could do our compiler using the parse_tree This is a
 faithful representation of the program The problem is that it would be quite
 inconvenient to do that and to see this it only point out some features of the
 parse_tree First of all you can see if the parse_tree is quite robust so for
 example we have here a node e and it has only one child So when theres only one
 successor of the of the node what is that really doing for us Well we dont
 really need the e at all we could just put the The five right here and and make
 the tree smaller and similarly for the other single successor nodes Furthermore
 these parentheses h ere well these are very important in parsing because they
 show the association of of this of the arguments with respect to these two plus
 operations It shows that this plus is nested this plus down_here is nested
 inside Of this plus up here But once_weve done the parsing the tree structure
 shows us the same_thing We_dont need to know that these were inside a parenthesis
 that the fact that these two expressions or the argument of this plus already tells
 us all we need to know And so you_know All of these nodes in here are also in a
 sense redundant We_dont really need that information anymore And so we prefer to
 do is to use something called an Abstract Syntax Tree that just compresses out all
 the junk in the Parse_Tree So here is a Abstract syntax_tree or a hypothetical
 abstract_syntax tree that would represent the same_thing as the parse_tree on the
 previous_slide and you can see here weve really just cut it down to the essential
 items We have the two nodes We have the three Arguments and the association is
 just shown by which plus node is nested inside the other We_dont have any of the
 extraneous nonterminals We_dont have the parenthesis Everything is much simpler
 and you can imagine that itll be easier to write algorithms that walk over a
 structure like this rather_than the the rather elaborate structure we had on the
 previous_slide Of course again is called an abstract_syntax tree because it
 abstracts away from the concrete syntax We suppress details of the concrete syntax
 and just keep enough information to be_able to faithfully represent the program
 and compile it
 In this video_were going to talk_about our first parsing_algorithm recursive
 descent parsing So Recursive_Descent is what is called a topdown parsing
 algorithm and you might suspect that there are also bottomup algorithms and they are
 indeed such things but we will be talking_about them later but in a topdown parsing
 algorithm the parse_tree is constructed from the top so starting_with the root
 node and from left to right And so the terminals then will be seen in the order
 that they appear in the token string So for example if I have this token string
 here this is a hypothetical parse_tree that I could construct and the numbers
 here correspond to the order in which the nodes of this parse_tree are constructed
 So we have to begin at the roots thats the first_thing that happens and then if
 T2 is a Belongs here in the parse_tree That would be next_thing that happened but
 then if we have a nonterminal of the next position that will be number three and
 then if it has children well the left most one should be going left to right
 will be the fourth thing to be generated And then lets_say the two children of
 number four are both terminals that would be the next two terminals in the input and
 so on The next_thing thatll happen is the second child of number three and then
 the last two terminals appearing in left to right order So lets_consider this
 grammar for integer expressions and lets look_at a particular input a very_simple
 one just open_paren five close_paren And now what were_going to do is were
 going to parse this using a recursive_descent strategy Im not gonna actually
 show you any pseudocode or anything like that Im just going to walk_through how
 this how this input string would be parsed But using this grammar and the
 Recursive_Descent Algorithm and the basic_idea is that we begin_with a nonterminal
 we begin_with the root node and we always try the rules for nonterminal in order So
 we will begin by starting_with e goes to t and if that doesnt work well try e goes
 to t e So this is gonna be a top_down algorithm beginning at the root Were
 gonna work from left to right we try the productions in order and when the
 productions fail we may have to do some back tracking in order to try alternative
 productions There_are three parts Theres the grammar that were using
 There is the parse_tree that were building and initially thats just the
 root of the parse_tree 3e and finally theres the input that were processing
 and well indicate our position in the input how much of the input we have read
 by this big fat red arrow and it always points to the next terminal_symbol to be
 read The next token to be read So in this case were starting_with an open
 paren Okay And also in the grammar you can see the highlighting here the brighter
 red color indicates which production were_going to try So were_going to begin to
 build our Parse_Tree by trying production e goes to t and what does that mean
 Well that means we make t the child of e and then we continue trying to build the
 Parse_Tree Well so remember were_going left to right and topdown so now t is an
 unexpanded nonterminal is the only unexpanded nonterminal so we have to work
 on it And what are we going to do well were_going to try a production for t and
 since we havent tried any yet well just try the first one t goes to it So the
 next step is to make nth a child with t and thats what our parse_tree looks_like
 And now we actually have something that we can check We can check_whether were
 making progress So observe that as long as were generating nonterminals we dont
 really know_whether were on the right track or not We have no way to check
 whether the nonterminals that were generating are gonna produce the the
 input string But once we generate a terminal_symbol then we can compare that
 with the next_input token to see if theyre the same and in this case
 unfortunately theyre not So the nth that we generated here doesnt match the
 open_paren in the input and so clearly this parse th is parsing strategy or
 this Parse_Tree that were building isnt going to work out So what were_going to
 have to do is were_gonna have to back track That_means were_gonna undo one or
 more of our decisions Were_gonna go_back to our last decision point and see if
 theres another alternative to try So whats the last decision we made well we
 decide to use t goes to nth so we can undo that and then we could try the next
 production for t And that happens to be t goes to n t so expand t using that
 production and now once_again we generated a terminal in the left most
 position and so now were able to compare that with the input and once_again
 unfortunately the nth token does not match the open_paren so we have to back
 track again So we undo that decision And this takes us back to trying alternatives
 for t Theres one more possibility and thats the t goes to (e) So we expand t
 using that production And now we can compare the token open_paren With is
 this open_paren With the open_paren in the input and they match And so thats
 good That_means that were we might be on the right track And since they match
 anything that we do in the future is going to have to match the different input and
 so well advance the input pointer So now where were_gonna work on next Well
 we have to expand this nonterminal e and were_gonna do the same_thing we did
 before Were just gonna start with the first production So we have e goes to t
 and then we have to work on t so were_gonna pick the first production for t and
 we have t goes to int So now we can compare Is int matching int in the input
 And if it does and so we advance the input pointer again And now were here and
 whats left well we progressed to this point Were looking_at that open_paren
 and that also matches So that matches the input and now weve matched everything in
 the parse_tree and our input pointer is at the end of the string and so this is
 actually a successful_parse of the input of the input string And so that means th
 at we accept and the parser terminates successfully
 In this video Im going to cover a limitation of the Recursive_Descent
 Algorithm that I presented last_time Heres the grammar from our last
 presentation and heres its implementation again as a set of
 mutually recursive function that together implement this simple recursive
 descent strategy And now lets think_about what_happens
 When we go to parse the input int simplest possible input strength
 Well lets work through it So remember we start with the function
 that implements all the productions for the nonterminal e
 And so what were_going to do here were_going to call e
 And that will try calling E1 All_right
 And what is E1 going to do E1 is going to call T
 Because of course the first production is E goes to T
 So lets_take a look_at what T does T is going to try out the production of
 T1 all right And what does T1 do
 Well T1 recognizes an int Okay so thats good
 And it will match it and return okay and then E will return and we will
 succeed in parsing And I forgot to mention it also the
 process the input point will be moved across the int and so when were done
 you will return and we will have succeeded in parsing the string int
 because E return_true the production for E return_true and we consumed all of the
 input All_right
 So now lets_consider a slightly_more complicated example okay
 So lets try the input string Int_times int
 All_right So again we start with the production E
 Okay And the first_thing well do is well
 try the production E1 Same thing we did last_time
 E1 is going to call the function T And T is going to try the first
 production for T Which again is the production int
 Okay And the input pointer of course is
 here and then it will try to match that against an int
 Okay If I match the first token in the input
 stream against the the terminal int And it will succeed
 Okay So the input pointer will be moved over
 So T1 will return_true All_right
 And as a result This right_hand side here of the function
 T will also succeed because T1 returns_true so T will return_true
 Okay Therefore E1 will return_true and E E1
 returning true will cause E to return_true
 And in fact that will be the end of the execution of the program will terminate
 E will return_true and the input player will only have advanced as far as int
 and so we will reject the parse This is actually ends up getting
 rejected And the question of course is what
 happened All_right
 Why didnt we succeed in parsing this input
 Which is clearly in the language of this grammar
 Well the story here is actually a little_bit interesting
 What happened is down_here when we discovered that Int matched the first
 production for T we said that T was done
 Okay T had succeeded had matched its input
 And then when E ultimately returns and the whole parse fails because we havent
 consumed the input we dont have a way to back track and try another alternative
 for T If we were_going to succeed we would have
 to say oh well even_though we found a production for T that matched part of the
 input Since the overall parts fail that must
 not have been the right production to choose for T
 Maybe we should try some other productions for T
 And in fact if wed tried the second production for T T2
 We would have matched Int_times T and then we probably would of succeeded
 We would have been able to manage int times int
 Okay And so the problem here is that even
 though there is backtracking within a production while were trying to find a
 production that works for a given nonterminals
 So while there is backtracking For a nonterminal during the time that were
 trying to find a production that works for that nonterminal but there is no
 backtracking once we have found a production that succeeds for
 nonterminals So once a nonterminal commits and
 returns and says I have found a way to parse part of the input using one of my
 productions Theres no way in this particular
 structure this particular algorithm to go_back and revisit that decision and try
 a different production All_right
 So the problem is that if a production for nonterminal x succeeds theres_no
 way to backtrack to try a different production for x later
 So once x once the function for x has returned
 And were really committed to that production
 Now that means that the particularly Recursive_Descent Algorithm that I should
 in the last_video is not completely general and Recursive_Descent is a
 general technique There_are algorithms for Resursive
 Descent parsing that can parse any grammar
 That can implement the full language of any grammar
 And they have more sophisticated backtracking than what I_showed in the
 algorithm that I presented last_time Now the reason for showing this
 particular algorithm is that its easy to implement by hand
 So this is actually an algorithm or approach to Recursive_Descent that while
 it has this limitation as you can see its very mechanical and very
 straightforward to design a parser for a given for a given grammar
 And it will work for a rather large class class of grammar
 So in particular itll work for any grammar where for any nonterminal at
 most one production can succeed So if you_know from the way that youve
 built your grammar that in any situation that the grammar can get into
 or the Recursive_Descent Algorithm can get into during parsing that at most one
 production can succeed Then it this this parsing is gradually
 will be sufficient because there will never be once you find a production that
 succeeds there will never be a need to go_back and revisit that decision
 because it must be the case that none of the other productions could have
 succeeded And it_turns out that the example grammar
 that were working with in the last_couple of videos could actually be
 written to work with this algorithm All_right
 And we would have to left factor the grammar
 Well actually theres more_than one way to rewrite the grammar to work with this
 Recursive Decent Algorithm but one way to do it Is to left factor it
 Im not going to say any more about left factoring in this video because thats
 going to be a topic of a video thats coming_up shortly
 Welcome_back In this video Im going to outline a general algorithm for recursive
 descent parsing Before I dive into the details of the recursive_descent parsing
 algorithm let_me justify a couple of small things that were_going to use
 throughout this video Token is going to be a type and were_gonna be writing codes
 and so token would be the type of all the tokens And the particular tokens that
 well use in the example are things_like int openpare closeparen and and
 so token is a type and these things are instances values of that type And then
 were_going to need a global variable called next that points to the next token
 in the input string And if you recall from the previous_video we used a big
 arrow to point into the input to indicate our current position The global variable
 next is going to play the same role in our code So lets begin The first_thing
 were_going to do is define a number of Boolean functions and one function we have
 to define is one that matches the given token in the input So how does this work
 Well it takes this argument a token okay this is a type token again And and
 then it just checks whether that matches whats currently pointed to in the input
 string so is t okay equal to the thing pointed by next and notice theres a side
 effect we increment the next pointer And whats returned then is a Boolean This is
 either true or false So yes the token that we passed in matches the input or no
 it doesnt And again just to stress this those at the next pointer is
 incremented regardless of whether the match succeeded or failed Now another
 thing we need to check for a matchup is the int production of asset This is a
 particular production of a particular nonterminal s And well denote that by a
 function that returns a Boolean and is written as s sub n So this is this is a
 function that only checks for the success of one production of s And when that I
 wont write_out the code for that now well see that in a minute And then were
 gonna need another func tion that tries all the productions of s so this one is
 going to be called just s with no subscript no subscript and so with this
 one well succeed if any production of s can match the input alright So were
 going to have two classes of functions for each nonterminal One class that where
 theres one function per production and it checks it checks just whether that
 production can match the input and then one that combines all the productions for
 that particular nonterminal together and checks whether any of them can match the
 input Okay thats the general plan Now lets see how this works for some specific
 productions and well just use the same grammar that we used in the last_video
 The first production of that grammar is e goes to t and now we wanted to do is we
 want to write the functions that are needed to decide_whether this production
 matches some input And this one happens to be simplicity itself and its easy to
 see why So were first of all were writing the function e1 this is the
 function that deals with the first production for e and succeeds returns_true
 only if this production succeeds in matching some input Well How would this
 production match any input Well it can only match some input if some production
 of t matches the input and we have name for that function thats the function t
 which tries all the different productions for t So e1 succeeds returns_true exactly
 when t succeeds returns_true and thats all there is to this first production For
 the second production we have a little more work to do Now e will succeed if t
 e can match some of the input and how does that work Well first t has to match
 some of the input so some production of t has to match a portion of the input and
 after that we have to find a in the input following whatever t matched and if
 matches then some production for e has to match some portion of the input And
 notice the use of the short circuiting double end here So this is actually
 important where youre exploiting the semantics of do uble end and C and C
 which evaluates The arguments to the double end in left to right orders So
 first t will execute and notice that t has embedded within its side_effects on
 the pointer into the input So its incrementing the next pointer and
 incrementing exactly however far t makes it So whatever t manages to match the
 next pointer will advance that far When this function returns its left pointing
 to the next terminal that t did not match and that needs to be a plus And the call
 of term will increment the next pointer again which is exactly where e should pick
 up and whatever e can match it will increment the next pointer just beyond
 that So that the rest of the grammar outside of this particular call can match
 it And then notice that this particular function is called e2 because this is the
 function for the second production for e Well we have one more thing to deal_with
 for e and that is the function e itself We need to write the function that will
 match any alternative for e and since its only these two productions it just has to
 match one of these two productions And that this is where the backtracking is
 dealt with Now the only bit of state that we have to worry_about in the backtracking
 Is this next pointer so that needs to be restored if we ever have to undo our
 decisions And so the way we accomplish that is we just have a local variable to
 this function called save that records the position of the next pointer before we do
 anything So before we try to match any input we just remember where the next
 point started when this function was called Okay And now to do to to do the
 alternative matching we first try e1 And we see if it succeeds and if it if it
 doesnt succeed actually lets do the succeeds case first If this succeeds if
 this returns_true then The semantics of double or here it means we dont evaluate
 e2 so this will not be evaluated The second component here will not be
 evaluated if e1 if e1 returns_true Itll short circuit cuz it knows that its going
 be tru e no matter what and itll just stop there And notice that whatever side
 effects e1 has on the next pointer will be retained and will remember and when we
 return_true the next pointer will be left pointing to the next piece of unconsumed
 input Now lets_consider what_happens if e1 returns false Well if e1 returns
 false well then the only way this or can be true is if the second component is
 true And whats the first_thing we do The first_thing we do is restore the next
 pointer Okay before we try e2 And if each returns_true then the whole_thing
 returns_true and and the e function succeeds If the e function fails well
 they were out of alternatives for e and the failure is gonna be returned to the
 next higher_level production in our derivation and it will have to backtrack
 and try another alternative Now finally what about this particular statement next
 save here Well this is not strictly needed Notice that here we save the next
 pointer in the save variable and then the first_thing the very first_thing we do if
 we copy it back over the next again This is just for uniformity to make all the
 productions look the same but since this is the very first production we actually
 dont need this assignment statement if we dont want to have it So lets turn our
 attention to the nonterminal t There_are three productions The first one is the t
 goes to int And thats a simple one to write We just have to match the terminal
 int so the next_thing in the input has to be an_integer and if it is then t1
 succeeds T2 is slightly_more complex Thats the production int t t goes to
 int t so we have to match an int in the input followed_by a followed_by
 something that matches any production of t The third production is t goes to (e)
 So what has to happen We have to match an openparen first and then Something
 that matches one of the productions of e we call the function e there and then
 finally a closeparen And then putting all three of these together in the
 function t that tries all three alternatives we just have exactly the
 same structure we had for e So we saved the current input pointer and then we try
 the alternatives t1 t2 and t3 in order and each_step we restore the input point
 before we try the next alternative Start the parser up we have to initialize the
 next pointer to point to the first token in the input_stream and we have to invoke
 the function that matches anything derivable from the start symbols So in
 this case thats just the function e And recursive_descent parsers are really easy
 to implement by hand In_fact people often do implement them by hand and just
 following the discipline that I_showed the previous slides To wrap_up this video
 lets work through a complete example So heres our grammar and here is all the
 code for the recursive_descent parser for this grammar and here is the input that
 well be looking_at and were_gonna just mark the next pointer pointing to the
 initial token of the input all right And Ill also draw the Parse_Tree that were
 constructing at the same time So well begin by invoking the start_symbol so
 were_gonna be trying to derive something from e And the first_thing well do is
 well try the first production So well try e1 and what does e1 do E1 is going
 to try t Its gonna try to derive something from t So our possible Parse
 Tree looks_like this And so we invoke t and what is t going to do is were_going
 to try all three productions for t in order and so I was gonna call t1 and well
 see that t1 is going to fail because its going try it an int so I wont put it in
 the parse_tree since it isnt going to work but the int is not going to match the
 openparen So thats going to return false which will cause us to backtrack It
 will reset The the input pointer okay And to the beginning of the string and
 then itll try t2 And t2 is also going to ask well is the input pointer an int
 And recall that the term function here always increments the input pointer So in
 fact this pointer is going to move over one one tok en but this is going to
 return false because int doesnt match openparen So well come_back here The
 input point will be restored back to the beginning of the string and then its
 gonna try the alternative t3 Now when we finally get the t3 something good is
 going to happen First thing its going to do is going to ask is the first_thing in
 input an openparen And in fact it is And so the input pointer will advance to
 point to the int And then its going to try to match something derivable from e so
 now we have our first recursive call to e Were back here at e and its going to try
 e1 first and then e2 And so it calls e1 and e1 will only match something if it can
 match t Okay so this is were down_here inside of e now and now were_going to
 call t And whats t going to do was going to try all three productions for t in
 order The first one of which happens to be the single token int and that is going
 to match Its going to call term int t1 is calling term int so that matches the
 next token in the input_stream So were happy about that The input pointer
 advances again And now we return through all these levels of calls T1 succeeds
 which means that t succeeds which means that e succeeds Okay And now were back
 here in the production for t3 and were_going to ask well is the next_thing that
 we see in the input a closeparen And it did it is and so a closeparen well be
 recorded And now t3 will succeed which means that t succeeds this t succeeds and
 finally well return to the root call e and that returns_true which means that the
 Parse succeeded That plus the fact that we are now at the end of the input there
 is no more input to consume and we have returned from the start_symbol with true
 and so we have successfully parse the input string
 In this video_were gonna talk_about the main difficulty with Recursive_Descent
 Parsing a problem known as Left Recursion Lets consider a very_simple
 grammar that consist of only one production s goes to s followed_by a So
 the Recursive_Descent Algorithm for this production is the following So we just
 have a function called s1 for the first production of s And its going to succeed
 if the function s succeeds and then after that succeeds we see a terminal a in the
 input_stream And then we have to write a function for the symbol s itself and since
 theres only one alternative theres only one production for s we dont need to
 worry_about backtracking or anything So as well succeed exactly when as one
 succeed Theres only one possibility in this case and now I_think you can see the
 problem whats going to happen Well when we go to parse an input string were
 going to call the function s which is going to call the function s1 And then
 what the function as one gonna do well the very first_thing its going to do is
 to call the function s again And as a result the function s is going to go into
 an infinite loop and were never going to succeed in parsing any input This will
 always go into an infinite loop So The reason that this this grammar doesnt
 behave very well is because it is left recursive And a left recursive grammar is
 any grammar that has a nonterminal where if you start with that nonterminal and
 you do some nonempty sequence of rewrites Notice the plus there You have
 to do more_than one rewrite So if youre actually doing a sequence of
 replacements you get back to a situation_where you have the same symbol still in
 the left most position And you can see this is not going to be good for parsing
 So in the case of this grammar up here what_happens while we get s goes to sa it
 goes to saa goes to saaa And so on and we can always get to a situation_where we
 have a long string of as and an s on the left end of the string And if we always
 have an s on the left end of the string we can never manage any input because the
 only way we manage input is if the first_thing we generate is a terminal_symbol
 But if the first_thing is always a nonterminal we will never make any
 progress And it just doesnt work I_mean Recursive_Descent does not work with
 LeftRecursive Grammars Well this seems like a major problem with recursive to
 same parsing It is a problem but as well see shortly its really not so major So
 lets_consider a leftrecursive grammar that slightly_more general form So here
 we have two productions now for s s goes to s followed_by something alpha or it
 goes to something else that doesnt mention s and lets call that Beta And if
 you think_about the language that this generates its gonna join all strings
 that start with a beta and then follow and followed_by any number of alphas And
 but it does it in a very particular way So if I write_out some a derivation here
 where I used a few where I used the first production a few times You can see whats
 going on So again s goes to s followed_by alpha And then s goes to s followed_by
 alpha alpha and then s goes to s followed_by alpha alpha alpha and if I repeat
 this I get S followed_by any number of alphas and then in one more step I can
 Put in beta and I get beta followed_by any number of alpha So thats the proof that
 it generates that language That language that begins_with a beta and has some
 sequence of alphas but you can see that it does it right to left it produces the
 right under the string first and in fact the very last thing it produces if the
 first_thing that appears in the input and thats why It_doesnt work with Recursive
 Descent Parsing because Recursive_Descent Parsing wants to see the first part of the
 input first and then work left to right And this grammar is built to produce the
 string right to left And therein lies the idea that allow us to fix the problem so
 we can generate exactly the same language producing the strings from left to right
 instead of right to left and th_e way we do that is to replace leftrecursion by
 rightrecursion And this requires us to add one more symbol in this case to the
 grammar so instead of having s go to something involving s on the left well
 have s go to beta so the first_thing notice in the very first_position and then
 it goes to s_prime and what does s_prime do well s_prime produce what you would
 expect a sequence of alphas and it could be the empty sequence And if you write
 out some you_know Example derivation here well have s goes to beta s_prime Which
 goes to now using the rules for s_prime goes to beta alpha s_prime Goes to beta
 alpha Alpha s_prime goes to and after any number of sequent any number of rewrites
 we get beta followed_by sub sequence of alphas followed_by s_prime And then in
 one more step we use the Epsilon Rule here and we wind_up with beta followed_by some
 number of alphas And so you can see it generates exactly the same string as the
 first grammar but it does so in a rightrecursive way instead of a
 leftrecursive way So in general we may have many productions some of which are
 leftrecursive and some of which are not And the language produced by this
 particular form of grammar here is gonna be all the strings They are derived_from
 asst start with one of the betas So one of the things here that doesnt involve s
 and it continues with zero or more instances of the alphas And we can do
 exactly the same trick This is just generalizing the idea that we had before
 where we only have one beta and one alpha to many betas and many alphas and so the
 general form of rewriting this leftrecursive grammar in using
 rightrecursion is given here So here each of the betas appears as an
 alternative in the first_position We only need one additional symbol s_prime and
 then the s_prime rules is take care of generating any sequence of the alpha i
 Now it_turns out that that isnt the most general form of left recursion There_are
 even other ways to encode left recursion in a grammar and heres another way thats
 important So we may have a grammar that where nothing is obviously leftrecursive
 So if you look here you see that the s doesnt even appear on the right_hand side
 here And if you look_at this production here a doesnt appear anywhere on the
 right_hand side so theres_no whats called Immediate LeftRecursion in this
 grammar But on the other hand there is leftrecursion because s goes to a alpha
 and then a can go to s beta And so there we have in in two steps produce another
 string with s at the left end and so this is still a LeftRecursive Grammar We just
 delayed it by inserting other nonterminals at the left most position
 before we got back to s So this left recursion can also be eliminated In_fact
 this can be eliminated automatically it doesnt even require human intervention
 And if you look_at any of the text pretty quickly in the Dragon Book youll find
 algorithms were doing that
 In this video_were going to continue_our discussion of topdown parsing algorithms
 with another strategy called predictive_parsing So predictive_parsing is a lot
 like recursive_descent Its still a topdown parser But the parser is able to
 predict which production to use And its never wrong inaudible parser is always
 able to guess correctly which production will yield to will lead to a successful
 parse if any production Well it lead to a successful_parse And it does have some
 two ways first of all it looks at the next few tokens so it uses lookahead to
 try to figure_out which production should be used So based on whats coming_up in
 the input string but also it restricts the grammars So this this is only works
 for a restricted form of grammars And theres the advantage is that theres_no
 back tracking involved and so the parser is completely deterministic if you were to
 try alternatives The predictive parsers accept what are called the LLK grammars
 And this is a really cryptic name and so let_me explain it The first L stands_for
 lefttoright scan So that means were starting at the left end of the input and
 reading left to right And in fact thats what we always do so all the techniques
 that we looked_at look_at will have an L in the first_position The second L stands
 for a leftmost_derivation So we are constructing a leftmost_derivation That
 means were always working_on the leftmost_nonterminal in the parse_tree And K
 here stands_for K tokens of look_ahead And in practice while the theory is
 developed for arbitrary k in practice k is always equal to one And so in
 fact well only discuss the ks k equals to one in these videos To_review
 in recursive_descent parsing in each_step there may be many choices of production to
 use and so we need to use backtracking to undo bad choices In an LL1 parser in
 every step theres only going to be one choice of productions of possible
 production to use And and what does that mean Well it means that if I have an
 input string if I have a configuration of the parser where I have some terminal
 symbols omega and a non_terminal a you_know possibly now followed_by some other
 stuff there could be terminals and nonterminals but again a here is the
 leftmost_nonterminal And the next_input Is a token T Well then there is exactly
 one production A goes to alpha on input T Okay theres only one possible production
 that we can use And any_other production is guaranteed to be incorrect Now it can
 be that that even A goes to Alpha wont succeed It could be that we will be in a
 situation_where theres_no production we could use But in inaudible parser
 there will always be at most one that we could use So in this case we would chose
 to rewrite the string to Omega Alpha Beta Lets_take a look_at our_favorite grammar
 the one weve_been using for the last_couple of videos We can see an issue here
 with using this grammar for a predictive parser Take a look_at the first two
 productions for T They both begin_with Ns And so if I tell you that the next
 terminal in the input_stream as were parsing along is an_integer that doesnt
 really help you in trying to distinguish between these two productions in deciding
 deciding which one to use So in fact with only one token of look_ahead I cant
 choose between these two productions And that is not the only problem actually so
 we have a problem with T but the same problem exist with E We can see that here
 both production for E begin_with the nonterminal T and it is really clear
 what were to make of that because a T against a nonterminal terminal so how we
 even do the prediction but the fact that they begin_with the same_thing suggest
 that its not going to be easy for us to predict which production to use based of
 only a single token of look_ahead So what we need to do here is we need to change
 the grammar This grammar is actually unacceptable for predictive_parsing or at
 least for LL1_parsing And we need to do something thats called left factoring the
 grammar So the idea_behind left factoring is to eliminate the common prefixes of
 multiple productions for one non_terminal So thats a mouthful Lets do an example
 Lets begin_with the productions for E And we can see again that E that both
 productions for E begin_with the same the same prefix What were_going to do is
 just factor out that common_prefix into a single production So were_going to have
 one production where E goes to T And then were_going to have multiple suffixes So
 lets introduce a new non_terminal X that will handle the rest So here we have E
 goes to TX So it says that everything that E produces begins_with T and thats
 consistent_with these two productions And now we have to write another production
 for X that handles the rest And what would that be Well one possibility is if
 were in this production we need to have a Plus E and then in this production
 theres nothing So thats easy to handle right One_possibility for X as it goes to
 Plus E and the other_possibility as it goes to Epsilon And now you can see the
 general idea We factor other common_prefix we have one production that deals
 with the prefix and then we write and then we introduce a non_terminal or the
 different suffixes And then we just have multiple productions one for each
 possible suffix And you can see what this is going to do This is effectively going
 to delay the decision about which production were using So instead of
 having to decide immediately which production were_going to use for E Here
 in this grammar we wait until weve_already seen the T whatever is derived
 from the T And then we have to decide_whether the rest of the production is a
 plus E or the empty_string Lets do the other set of productions So we have tea
 goes to and now the common_prefix is int that we want to eliminate So were_going
 to have just one production that begins_with int and then well have a new a
 nonterminal to stand for the various possible suffixes And now we also have
 another production that doesnt h ave anything to do with int and so well just
 leave that one alone that production just stays here Because it already begins_with
 something different we wont have any trouble predicting between these two
 possible productions these two possible productions And now we have to write The
 productions for Y And again we just take the suffixes of the productions that we
 left_factored and write them down as alternatives So one is empty and the
 other one is times T So we wind_up with times T or epsilon
 In the next few_videos were_gonna talk_about how to construct LL1_parsing tables
 And in this particular video_were gonna begin by looking_at how be build something
 called first sets Before we get into the main topic of this video which is
 something called First Sets we need to say a little_bit about how were_going to
 construct parsing_tables or what the conditions are for constructing LL1
 parsing_tables And so what were_interested in knowing were building a
 were building a parsing_table And we want to know for a given non_terminal A
 Kay this is the leftmost_non terminal And a given input symbol the next_input
 symbol T cough We want to know what con under what conditions we will make
 the move A goes to alpha Well replace A the non_terminal by the right_hand side
 alpha Alright and that means that the entry in th the AT entry in the table
 would be Alpha and there are two situations in which we would like to do
 this Alright So the first one is if alpha can derive T in the first_position
 That_means that beginning_with alpha there is some derivation some sequence of
 moves could be zero or more moves that will result in a T appearing in the first
 position of the string thats derived And if there is such a derivation then using
 the move A goes to alpha at this point when T is the next_input symbol would be
 a good idea Because then we would be_able to match the T Eventually alpha could
 generate the T and then wed be_able to match the T and then continue with our
 parsing of the rest of the input Alright so in this situation when alpha can
 generate a T in the first_position we say that T is an element of the first of
 alpha T is one of the things there may be more things But T is at_least one of
 the things that alpha can produce in the very first_position One of the terminals
 I should_say that alpha can produce in the very first_position Now theres
 another situation a slightly_more complicated situation in which we might
 want to make the move or we wou ld want to make the move That if we see A as the
 leftmost_non terminal and T as the next_input that wed like to replace A by A
 goes to alpha Alright And the situation here that were_going to consider is what
 if alpha cannot derive T So alpha cannot in any sequence of moves derive
 T So in fact what does that mean That_means T is not Gonna be in the first of
 alpha okay So and our next_input symbol is T Were still looking_at the situation
 where we have A as the leftmost_non terminal and T as the next_input symbol
 Now This doesnt sound very promising Because we have an input symbol T that we
 want to match And the leftmost_non terminal that weve_got up next that we
 have to do a derivation for cant generate the T And so but it_turns out
 that this that its not hopeless That we actually may still be_able to parse the
 string even in that situation provided that alpha can go to epsilon So if alpha
 can derive epsilon if alpha can go away completely and we can basically erase the
 alpha then it could be that some other part of the grammar can come in and match
 the T Alright and so in what situation would that be Well here are the
 conditions So if A goes to Alpha as a production and alpha can go to epsilon via
 zero or more moves Alright so alpha can eventually be completely wiped out
 Alright and If T can come immediately_after A in the grammar so there has to be
 a derivation for this to make_sense there should be a derivation where we are using
 the A okay With the A as an important piece of the derivation you_know from the
 start_symbol And what comes immediately_after the A is the next_input symbol that
 we are expecting So in this situation if we could get_rid of the A Then by having
 a go at the epsilon then well still be on track cuz potentially some other piece of
 grammar could come in and match the T Alright So in that case we would say
 what what do we have to test for What under what conditions can we do it Well
 we want to be_able to do this if T can come_after A in the grammar and we say
 that T is in the follow of A T is one of the things that can come_after A in the
 grammar Now this is an important point and a place_where people sometimes get
 confused and so I want to to emphasize this notice that We are not talking
 about A deriving T A does not produce T T appears in a derivation After A okay
 So the A and the T here it it doesnt have anything to do with what A produces
 This has to do with where A can appear in derivations alright So if the T can come
 after the A in a derivation then we say the T is in the follow of A Right So in
 this video_were gonna focus_on only this first part the first sets In the next
 video well look_at the follow_sets and then the video after that well talk
 about how to put it all together to build this parcing table sound All_right
 lets focus now on our main topic for this video the computation of first sets So
 here first of all we have to have a definition of what a first set is And so
 were_going to say for an_arbitrary string This is actually x here is a
 string Could be a ter could be a single terminal it could be a single
 nonterminal or it could be a string of grammar symbol All_right and if that If
 that X can derive T in the first_position through some sequence of moves then we
 say that T T is a terminal here is in the first of X okay So all the possible
 terminals that can be derived in the first_position will be in the first of X Now
 For technical reasons that will become clear in a minute we also need to keep
 track of whether x can produce epsilon Now so even_though epsilon is not a
 terminal_symbol if x can go to epsilon be a zero or more steps then well say that
 epsilon is in the first of x and this turns_out to be needed We need to keep
 track of whether x whether things can produce epsilon in order to compute all
 the terminals that are in the first set of a given grammar symbol Alright so now
 heres a sketch of the algorithm So first of all for any terminal symb ol the only
 thing the terminals can produce are themselves So every terminal_symbol in
 here I should just say T is the terminal So for every terminal_symbol it is in
 its first set just consists of a the site containing only that terminal All_right
 so now lets_consider a non_terminal X okay so here X is a non_terminal and what
 it would be in the conditions when epsilon is in the first of X well if theres a
 epsilon production if X goes immediately to epsilon then obviously X can produce
 epsilon epsilon should be in the first of X But also if X can produce any_other
 right_hand side Alright Where everything on the right_hand side can go to epsilon
 Well then the whole right_hand side can go to epsilon So in that case also
 epsilon is in the first of X I noticed that this will only happen if this it can
 only it can only potentially happen if all the EIS here are nonterminal symbols
 themselves Obviously if theres any terminal_symbols on the right_hand side
 then that production can never go completely to the empty_string Okay It
 will always produce at_least that that terminal But if every nonterminal On the
 righthand_side can produce epsilon Meaning epsilons in the first of all
 those nonterminals And there are no terminals on the right_hand side Then
 Then epsilon will be in the first of X Alright theres one other situation and
 heres where we make use of the fact that we are keeping track of where epsilon can
 be produced alright So lets_say that we have a production like this okay and lets
 say the first N symbols A1 through AN here can all go to epsilon So this can all
 disappear and can be replaced_by the empty_string What does that mean so if we have
 derivation like this Okay were to some number of moves it goes to Alpha well
 that means that X can through a bunch of moves here to derive Alpha itself okay
 So X will go to Alpha by wiping out all of the AIs and I forgot to put the alpha
 here on the end there should be an Alpha after As have been there Okay And wh at
 does this mean Well this means that anything that is in the first of alpha is
 going to also be in the first of X All_right So if any prefix of the righthand
 side can disappear then the remaining suffix the alpha it doesnt_matter what
 the alpha is is left Then the first of alpha will be a subset of the nonterminal
 on the right left_hand side of X in this case All_right Okay Alright So that is
 a definition of a first sets and how you compute them Okay And we have to we
 have to compute them for the terminals and for the nonterminals alright Thats
 what these these second two rolls here cover the nonterminals I just noticed
 as I mentioned here at the beginning that this is well defined for any_other
 Grammar sequence as well I_mean excuse_me any_other string in the grammar as
 well It_doesnt if I if I know how to compute it for terminals I know how to
 compute it for non terminals Then I can compute it for arbitrary strings in the
 grammar as well Lets analyze do an example Lets_take a look_at this grammar
 and lets see if we can compute the first sets Lets start with the easy stuff
 Lets do the terminal_symbols Alright So for the terminals its really you_know
 extremely straightforward The first of plus is plus The first of times Is just
 times every terminal is in a has its first set the first set of every terminal
 is just the second term in that terminal and so on for the others and this is not
 worth writing out So itll be the first of open_paren will just be open_paren the
 first of close_paren will be just close_paren and I_think that is all Now we have
 to do ants as well okay Alright so these are the first sets for the terminal
 symbols And now lets look_at something more interesting lets talk_about the
 first of the non_terminal symbols So What about the first of E Well if we look_at
 the production for E lets remember our rules So we know that anything thats in
 the first of T will also be in the first of E So the first of T Is a subset of th
 e first of E Okay so in order to know what the first of E is we have to know
 what the first of T is At least to know part of the first of E we have to know the
 first of T So lets move on then to first computing The first of T Lets lets try
 to get that set Now the first of T is actually pretty easy because if we look_at
 the productions for T we can see that they produce terminals in the first_position
 All_right So the only possibility in the the only possibilities in the first of T
 are open per en and int And since there are only two productions for T and both of
 them have a terminal in the very first_position theres_no other terminal
 symbols that could be produced in the first_position by T So we can just read
 off the first of T directly from the grammar And its the open_paren in int
 Okay Now lets return to thinking_about the first of E So remember there was
 another case that we need to keep_track of Or sorry that we have to consider
 So it could be or clearly everything in the first of T is in the first of E and
 weve_already noted that down But if T can go to epsilon then things that are in
 the first of X Could also be in the first of E And now weve computed the first of
 T and we see that epsilon is not in there The first of T always generates at_least
 one terminal_symbol and so therell never be a situation in which X can contribute
 to the first of E because T is always guaranteed to generate at_least one
 terminal So in fact this subset that we wrote up here is not a subset at all its
 an inequality The first of T and the first of E are equal So the first of E is
 also open per rand and Nth All_right So now lets_take a look_at the first of X
 Okay So the first effects well clearly pluses in the first of X because one
 production per X plus immediately in the first positions so we must add plus to
 the first of X And then X has an epsilon production so it can also go to epsilon so
 that means epsilon is also in the first of X And what about the first of Y Well the
 fir st of Y its a similar structure to the productions request we see we have
 one production here in the inaudible of the terminal in the first_position and
 thats times So the first of y has times in it And then y also has an epsilon
 production Y can go directly to epsilon so epsilon is also in the first of y
 Alright And thats actually it for this grammar These_are the complete first sets
 for all of the symbols of the grammar The terminals just have themselves in their
 first sets and then the nonterminals we computed have these sets So that
 concludes our_discussion of first sets and in the next video_were going to take a
 look_at computing follow_sets
 In this video_were going to continue_our discussion of the construction of
 laugh(1) parsing_tables by taking a look_at how we build the follow_sets So heres
 the definition of follow of x and recall that the follow set for a given symbol in
 the grammar isnt really about what that symbol can generate really doesnt
 depend necessarily at all on what the symbol can generate It depends_on where
 that symbol can appear where that symbol is used in the grammar And we say that t
 is in the follow of x if there is some place in the grammar some derivation
 where that terminal t can appear immediately_after the symbol x Okay so
 for all such E They make up the follow set of x And heres some intuition about
 how we would compute follow_sets Lets_say we have a situation_where X goes to
 two symbols A B right And then anything that B can produce in the first_position
 will clearly be in the follow of A So if we have X goes to AB And then through
 some more steps we can get something_like this A goes B goes to T beta then
 we have a situation_where the T comes immediately_after the A and so clearly
 something that was in the first of B is in the follow of A So so the basic rule
 is that you have two symbols that are adjunct somewhere the first of the second
 one is in the follow of the first one Hm now Another interesting effect here is
 that if we have a symbol at the end of a production Lets_take a look_at the B
 here for a moment okay And a claim here that anything thats in the follow of the
 left_hand side is gonna be in the follow of B In this case that the follow of X
 is a subset of the follow of B And lets_take a look_at that Lets_say that we
 have a situation_where we have a derivation from the start_symbol okay We
 wind_up with X followed_by T It can it can be other stuff around the inaudible
 lets ignore that for the moment lets just focus_on the XT Then we can use this
 production X goes to AB and in one step we can get to ABT And then we see T was in
 the follow of X a nd also T is in the follow of B as a result Okay So
 anything in the follow of X would also be in the follow of B And we can generalize
 this observation about what occurs at the end of a production So anything that
 occurs at the end of the production it its follow set will include the follow set
 of the symbol on the left_hand side of the production Well what is the end of the
 production If if B can go to epsilon if B can disappear then A will appear at the
 end of the production Okay so if B can go to epsilon then it will also happen
 that the follow of X would be in the follow of A And following up here in our
 example so we Or up here we start with the start_symbol We got to XT In one
 step we got to ABT and so T was in the follow of B But now B can go into
 epsilon and so we can also get to AT and therefore T is also in the follow of A
 And finally theres one special case Remember that we have our special symbol
 marking the end of the input and what can that follow Well the end of the input is
 in the follow of the start_symbol alright And this is just a way again of
 keeping track of what were_going to do when we run out of input And well see
 how thats used when we built the parsing_tables but we always add as an initial
 condition that dollar_sign is in the follow of the start_symbol So now lets
 take a look_at and sketch of the algorithm for computing follows So thats as we
 just said the dollar_sign is in the follow of the start_symbol And now we
 take a look_at each production Okay A goes to alpha X beta were were
 focusing here on the X Okay if we look_at every production and we look_at every
 symbol on the right_hand side of that production And the first of beta okay
 the thing that can follow x in this in this production the first of that will be
 in the follow of x and also we just subtract out epsilon if it was in the
 first subbeta Were not interested anymore in the epsilons for the purposes
 of follow sites epsilon never appears in follow sites so follow sites are always
 just sets of terminals And now the second part of the algorithm is it if we have
 some suffix of a production beta that can go to Epsilon so Epsilon is in the first
 beta Alright this suffix of the production can completely disappear then
 as we saw on the previous_slide the follow of left_hand side symbol will be in the
 follow of X And thats it in terms of the rules for computing follow_sets So now
 lets work through an example So heres our grammar again And were_going to
 compute the follow_sets for each of the symbols of the grammar So lets begin
 with the with the start_symbol Well start with the follow of E And by
 definition we know that dollar is in the follow of E So we get that one easily
 And now the question is what else could be in the follow of E Alright So in order
 to figure that out we have to look_at where E is used in the grammar Alright
 So remember always at follow_sets are about where the symbol is used Not what
 it produces Alright So here Is a place_where E is used and we can see that it is
 merely followed_by a terminal_symbol so certainly close_paren is in the follow of
 E right And theres one other place_where E is used and thats over_here And
 it appears that the right end of the production and so then we know that
 anything thats in the follow of X is also gonna be in the follow of E And thats a
 constraint and so well right that down over_here coz this is just a property of
 the relationship That the follow_sets will have when were done computing them
 This doesnt immediately tell_us anything new thats in the follow of E But we know
 that as we go along and we learn about things that are in the follow of X well
 have to add them to the follow of E And let_me just divide up The slide here so
 we will put our properties that we know about relationships between fallocates
 over on the left_hand side and well put the actual fallocates over_here on the
 right side So now to make thats the only two places those are the only two pl
 aces where E is used in the grammar and to make further progress we need to know
 something about the follow of X Okay if we want to make further progress on the
 follow of E then we need to figure_out whats in the follow of X So lets focus
 on that for a minute So wheres X used in the grammar well its used in only one
 place and thats here Okay Where it appears at the right end of a production
 And what and so therefore the symbol on the left_hand side will be a subset of the
 follow set of X So were_gonna know that the follow of E is a subset of the follow
 of X Alright And what does that mean Well so follow of X is a subset of follow
 of E And follow of E is a subset of the follow of X So that really means that
 these two sets are equal The follow of X and the follow of E whatever they wind_up
 being are gonna have to be the same set And now weve_looked at all the places
 where E is used in the grammar Weve looked_at all the places_where X is used
 in the grammar We cant learn anything more about what is in the sets the follow
 sets of E and X Were not forced to add anything else to either set and so were
 done And so we can close off this set And we know the follow of E consists of
 dollar_sign and closed_paren And we also know that X Has the same set the same
 follow set Alright so now lets move on to the follow of T All_right So whats
 going to be in the follow of T Well we again we have to look_at where T is used
 in the grammar So T is used in two places The first one is here in the first
 production And so whats going to be in the follow of T Well it could be anything
 that is in the first of X Okay Cuz X comes immediately_after T And if you
 recall from the previous_video there were only two things in the first of X One was
 plus So this plus is definitely in the follow of T and lets just review cough
 Excuse me How that can happen so we can go from E To TX okay Im using the
 first production And now we see the X comes_after the T And then in one more
 step we can go to T plus E And now we have a derivation where plus follows T
 And thats how we thats thats Y pluses in the follow of T Alright So the other
 thing that was in the first of X was epsilon because theres an epsilon
 production for X over_here But remember that were not interested in we dont
 include epsilon in follow_sets And so X doesnt contribute anything else to the
 to the follow of T But since X can go to epsilon remember what that means That
 means that over_here looking back at this first use of T again in the grammar this
 X can disappear Right and that means that anything it is in the follow of E is also
 in the follow of T Now we already know the follow of E So we can just add those
 things in Okay And let_me write that down over_here just so that we dont
 forget it So to follow Of of E is a subset of the follow of T We wont really
 need this fact again but useful to write it down perhaps Alright and now we are
 done with this use of x Weve included everything implied by this production that
 we can in the follow of T and so now have to look_at the other place_where T is used
 and thats over_here Okay and so here were_going to see that T is in the right
 end of a production so anything that is in the follow Y can also be in the follow
 of T alright So the follow of Y Is going to be a subset of the follow of T
 alright So now we can go off and work on the follow of Y We have to in order to
 figure_out what the follow of T is going to be were_gonna have to know the follow
 of Y So where is Y used in the grammar Well there is only one place and thats
 over_here And also Y appears in the right_hand of production which means that the
 left_hand side symbol its follow set will be included in the follow of Y gtgt And so
 the follow of t will be a subset of the follow of y All_right And now again we
 have two follow_sets that are subsets of each other Follow of y is a subset of
 follow of t and follow of t is a subset of follow of y And so these two sets we
 know are going to have to be equal Okay So we can write_down here At the follow
 of Y includes plus dollar enclose parette Just like the follow of T And
 now were done Weve weve follow of T and follow of Y Weve followed all the
 implications of how the follow of T gets things into what can be included in the
 follow of T Weve worked out all the places_where Y is used in the grammar and
 added all the things that we can based on its context And theres nothing more
 that were forced to add either said So we can go ahead and close these sets off
 Theyre finished Alright So now Weve done the follow of E X T and Y So
 weve taken care of all the terminal_symbols But Im_sorry All the
 nonterminal symbols But we still need to compute the follow_sets for the terminal
 symbols And unlike the case with first sets the follow_sets for terminal_symbols
 can actually be interesting So lets_take a look_at the follow of open_paren Okay
 what can follow an open_paren in a derivation Well open_paren is only used
 in one places Its here Okay And so what can follow in open parens is whatever
 is in the first of E And remember that the first of E was the same as the first
 of T because T always produces something in the first_position And the first of T
 was what It was open_paren An int kay And if you think_about this for a minute
 this makes complete sense What can come_after an at any valid at any valid string
 in this grammar while its going to be another nested parenthize expression or
 is it going to be an_integer In particular you couldnt have a times or a
 plus immediately_after an and you couldnt absolutely have the end of the input you
 couldnt have the input stop after an and have a valid string So now lets_take a
 look_at the follow of Okay So whats in that set Again we look_at where the
 symbol is used Its only used here in this one production So and because it
 appears at the right end of the production we know that whatever is in
 the follow of T is going to be in the follow of ) all right And so what was in
 the follow of T cough that was ) Okay So now lets move on and take a
 look_at the operators Lets look_at the follow of plus So wheres plus used
 Well its only used here So whatevers in the first of E is going to be in the
 follow of plus And we already know what the first of E was That was an open
 inaudible An int Okay And remember also that E cannot go to Epsilon So E
 cannot ever disappear completely because T always produces at_least one terminal
 Therefore only the things that are in the first of year in the follow of plus
 Because E cant go to Epsilon we only have to include the things that are in the
 first of E in the follow of plus Again if you think_about it for a minute this
 makes complete sense What could come_after a plus Well it could be an
 integer the second_argument to an addition or it could be the beginning of
 another nested expression And it couldnt be a times It certainly couldnt be the
 end of the input cuz you always have to have an argument after the plus And gtgt
 And I_think thats it I_think thats all the other possibilities gtgt Okay Alright
 Now lets_take a look_at the follow of times What can come_after a times Where
 is times used its used here So things that are in the first of t Are gonna be
 in the follow of times again alright inaudible we already know what that is
 Thats the same as the first of E Thats open_paren and ints And again this makes
 complete sense What can come_after a times Its either the beginning of
 another inaudible expression or an_integer Its certainly not a plus or the
 end of the input okay And again T cannot go to epsilon and so thats the
 only thing Those are the only things that can be in the follow of times And now we
 just have one more symbol to go We have to look_at the follow of an_integer of an
 int Okay where is that used in the grammar Well its right here Alright
 So the whats gonna be in the follow inaudible what s going to include
 everything in the first of Y Okay Whats in the first of Y well times was in the
 first of Y and epsilon was in the first of Y but remember we dont include
 epsilons in follow_sets So Y contributes times to the follow of int But now
 because Y could go to the epsilon and epsilon could inaudible that means that
 this int could wind_up being at the right end of this production Okay it could
 the Y could disappear and then whatever could follow the T could also follow the
 int Right so we have to include the things in the follow of T In the follow
 of it and what was in the follow of T where that was a plus It was a dollar
 And it was a close_paren okay And what does that tell_us Well it tells_us okay
 for most anything to follow an int but as an open_paren cannot follow an int So you
 cant have another nesit expression with a begin right after an int without an
 intervening operator Alright And that completes the computation of the follow
 sets for this example
 In this video_were gonna put together what weve learned about first and follow
 sets to construct LL1_parsing tables Our goal is to construct a parsing_table T for
 a context_free grammar G And this is done by production So were_gonna do this one
 production at at time And were_going to in turn consider each production A goes
 to alpha in the grammar G And so the first case Is if we are trying to figure
 out whether we can use A goes to alpha and T happens to be in the first of alpha
 Alright so if we know that some terminal T is in the first of the right_hand side
 then If we were in a situation_where A was the leftmost_non terminal and T was
 the next token of input then then expanding by A goes to alpha would be a
 good move because the alpha could potentially through more productions
 match the T And so well add to the parsing_table at the A T entry The right
 hand_side alpha Alright the other situation that were_interested in is
 what if we need to get_rid of the A okay So if the A cannot possibly match
 the T alright So lets_say the T is not in the first of alpha or we have some
 other situation_where we want to erase the A Well then it would be okay to use
 production A goes to alpha provided that Alpha can actually go to Epsilon so alpha
 can go away completely Alright So we can eliminate all trace of the A And T
 follows A in the grammar So T is able to come_after excuse_me T is able to come
 after A in some derivation So if T is in the follow of A and the right_hand side
 of the inaudible code epsilon then we add the move that when A is the leftmost
 non_terminal and T is the next_input we can expand A by A goes to alpha And
 finally a special case for dollar because dollars technically not a
 terminal_symbol If were at the end of the input okay so we have some stuff
 left on the stack particularly we have if nonterminal a is still our leftmost
 nonterminal but weve run out of input well then our only hope is to get_rid of
 the a completely And so we want to pick a production for a that can go to epsilon
 so we look for a production a goes to alpha where epsilon is in the first of
 alpha and dollar can follow a in a derivation So that is the procedure or
 those are the rules for constructing a parsing_table And now lets work through
 an example So heres our_favorite grammar that weve_been looking_at for the last
 few_videos And now lets_take a look_at what the parsing_table will look like
 alright And the parsing_table will consist of columns that are labeled by
 the terminal_symbols of the grammar All_right so here well have open_paren
 closed_paren plus times and inch And then the rose will be labeled by the
 terminal_symbol so well have E T X and Y All_right and now were just going
 to take each production and apply our rules and see what entries in the table
 we create All_right so when would we use E goes to TX Alright well so the first
 thing to observe about this production is that it cannot produce epsilon on the
 right_hand side So TX always produces at_least one terminal And so the second
 case where were_interested in whether the production can go to zero as it could
 go to epsilon excuse_me is not going to apply All_right So we just have to
 consider what it can generate in the first_position So the only things that this
 inaudible can generate in the first_position are things that are in the first
 of T which are open_paren and int So there are two situations in which we will
 use the production E goes to TX that is if E is the leftmost_nonterminal and the
 next_input is an open_paren Okay And the other one is that if the next_input is a
 inaudible alright Okay so now lets_take a look_at this production Right So
 when are we going to use T goes to open_paren E closed_paren Well if T is the
 leftmost_nonterminal alright Thats this one on the left_hand side and an open
 paren is the next symbol in the input kay Thats the only thing in the first of
 this right_hand side Then it would be a good move to expa nd by open_paren E
 closed_paren So theres only one situation Where we use that production
 Alright And for the other production the other T production were_going to use
 that when T is the leftmost_nonterminal and theres an INT in the input So over
 here well have inaudible And I forgot one column over_here for dollar So well
 stick dollar in there at the very end Okay So now weve covered these first
 three productions Lets_take a look_at this production So when would we use X
 goes to plus E Well clearly the only thing on the first of the right_hand side
 is plus and the terminal_symbol on the right_hand side is X so the X plus entry
 we would want to expand by X goes to plus E And similarly for Y the production
 the first production involving Y when Y is the terminal non_terminal were trying
 to expand and theres a times in the input we would use the production Y goes
 to times T Okay And now we just have the two epsilon productions left And these
 are the only productions actually that can go to epsilon And so when would we
 use when would we use X goes to epsilon or Y goes to epsilon Alright so recall
 that we need to know What is in the follow of X in order to know when to use X
 goes to epsilon And we computed that in the last lecture But lets just write it
 down again here Okay And so what was in the follow of x well we had to look_at
 where x was used in the grammar x was used there I it appears at the right
 hand_side of the production So it would be things that were in the follow of e
 What was in the follow of e well e is the start_symbol so inaudible is in the
 follow of e close_paren is in the follow of e Alright And then what was in the
 follow of y Thats the other one where well need to know the follow set Again
 we have to look_at where Y is used so Y is used there That_means everything thats
 in the follow of T is in the follow of Y The follow of y will therefore include the
 first of x because x can come_after t So plus will be in the follow of y Alright
 but then x can go to epsilon and so everything is in the follow of E will be
 in the follow of t and therefore also in the follow of y So the other two things
 in the follow of y were the dollar_sign and close_paren Alright and so this is
 saying okay is that if we are in a situation Where we have an X Okay
 Lets just focus_on the X goes to epsilon production for a moment Lets_say that we
 have X on the stack okay on top of the stack and is our next_input Well what
 can we do At the end of the input we have to get_rid of the X so obviously we want
 to use the X goes to epsilon move okay so that makes_sense And the other
 situation that follows it tell_us to use X goes to epsilon as if there is a) on the
 stack because the X cannot generate a) by_itself But hopefully some other symbol
 thats on the stack will be_able to generate once we get_rid of the x okay so
 we also use x goes to epsilon In this situation And then inaudible for follow
 of Y or for Y goes to epsilon that production There_are three things in the
 follow three terminals in the follow of Y And we should use Y goes to epsilon if
 they are the next_thing in the input So so if we see a plus and were trying to
 expand a Y well use Y goes to epsilon If we see a closed_paren and we see and
 were trying to expand a Y well use Y goes to epsilon And finally if were
 completely out of input and we still have a Y left over well use Y goes to
 epsilon And that is the complete parsing_table all right And now you can see How
 this will work in every situation Okay For our leftmost on terminal and for
 every possible input or lack of input we have a production that we can use And now
 there are a lot of blank entries in this table And what do those correspond to
 Lets_say that we were trying to expand x And the next_input symbol was a open
 paren Well theres_no entry here Okay so thats an error Thats a parsing
 error So whenever you encounter a blank entry in a table you try to view the
 blank entry when youre parsing thats when you inaudible a parsing error
 because what this tells_us the fact that theres a blank entry it tells_us that
 there is no valid move There is no way that we could parse that string And we
 discovered that at the point where we tried to access an error or blank entry in
 the table So now lets_consider what_happens when we try to build an LL1
 parsing_table for a grammar that is not LL1 And lets_take a look_at the simple
 left recursive grammar that we have looked_at before So S_goes to SA is one
 production and S_goes to B is the other production And to build a parsing_table
 for this well need to know the first and follow_sets So lets_take a look_at the
 first of S Alright So what can S produce in the first_position Well it can
 clearly produce a B And theres_no epsilon Theres no possibility of
 generating epsilon from S As a matter of fact thats the only thing thats going to
 be in the first of S And what about the follow of S well what can follow an S
 Well thats the start_symbol so clearly dollar isnt the follow of S And then the
 subterminal the terminal A appears right after S in the first production so A is
 also in the follow of S And now were_ready to build our table And its going
 to be a very small table because we only have one nonterminal And then we have
 two terminals A and B And we have the end of input symbol So its just three
 entries potentially in this table Alright And so now lets_take each
 production and see where we should put it So lets just take a look_at the
 second production first Cuz thats inaudible for no particular reason So
 if S_goes to B when should we use that Well clearly if we see a B in the input
 this would be a good one to use Cuz cause the because that the first of the
 right_hand side includes B alright So so S_goes to B Would be used if we see a
 B in the input And now what about S_goes to SA Well here again this cant
 generate epsilons so were only interested in what it can produc e in the first
 position And once_again the first of S is just B And so we also have the move
 in the SB entry we would have the move as goes to SA And now we see the problem
 right Here we have an entry that has multiple moves This is a multiply defined
 entry Okay and what does that mean Well that means if we see an s in if we have an
 s and we want to expand okay were trying to if s is our leftmost_non terminal so
 its at the top of the stack and b is our next_input symbol alright this table
 doesnt tell_us exactly what move to make Its not deterministic It says theres
 two possible moves That we can make And so this is how you_know a grammar is not
 laugh(1) because if it winds_up that you build the laugh(1) parsing_table and you
 have more_than one entry More than one possible move in some position in the
 table some entry excuse_me I used the word entry incorrectly So if you wind_up
 let_me say that again If you wind if you build the table and some entry in the
 table has more_than one move in it then theres not an unique move For every
 situation for the parser And that grammar is not laugh(1) So we just said if any
 entry is multiply defined in the parsing_table then the grammar is not LO1 And in
 fact this is the definition of an LO1 grammar so the only way to be sure that
 the grammar is LO1 or the mechanical way to check that the grammar is LO1 is to
 build the LO1 parsing_table and see if all the entries in the table is unique Now
 that we do know however that there are certain classes of grammars that are
 guaranteed not to be L1 not to be L1 And what are some of those Well any grammar
 that is not left_factored Will not be lo1 okay Any grammar that is left
 recursive will not be lo1 Okay any grammar that is ambiguous Is also
 guaranteed to not be L1 But this is not an exhaustive list Other grammars are not
 L1 too So in particular If the grammar required more_than one token of look
 ahead it would not be all one But even that isnt a complete list So e ven
 grammar is beyond that that are not going to be all one So what this amounts to
 these three things here amount to quick checks that you can do To test whether a
 grammar is guaranteed not to be L1 But if just because a grammar is left
 factored and it is not left recursive and is ambig unambiguous that doesnt
 guarantee that its L1 And the only way to know for sure is to construct the
 parsing_table and see if all of the entries in it are unique And
 unfortunately it_turns out That most programming_languages theyre context
 free grammar So the grammars that describe most programming_languages are
 not LL1 And the L1 grammars are too weak to actually capture all of the interesting
 and important constructs in commonly using programming_languages And there are more
 powerful Formalism for describing grammars and or practical grammars and
 were_going to be looking_at those in future_videos It_turns out that they
 build on everything that weve learned here over the last few_videos for Elmer
 grammar so none of that will be wasted but they assembled those ideas in a more
 sophisticated way to build more powerful parts
 This is the first of what will be a considerable sequence of videos on bottom
 up_parsing The first_thing to know is that bottom_up parsing is more general
 than deterministic top_down parsing So recall we talked_about recursive_descent
 which is a completely general parsing_algorithm but requires backtracking And
 now were focused on deterministic techniques and we talked_about LL one or
 predictive_parsing last_time And now were_gonna switch shift gears and talk
 about bottom_up parsing And it_turns out though even the bottom_up parsing is more
 general its just as efficient and it uses all of the ideas that we learned in
 top_down parsing And in fact bottom_up Parsing is the preferred method thats
 using most of the parser generator tools So one good thing about bottom_up
 parcers is they dont need left_factored grammars so we can revert to the natural
 grammar for our example and natural here is in quotes because we still have to
 encode the precedence of plus and times so bottom_up parcers arent going to deal
 with ambiguous grammars And lets just as an example consider how a bottom_up
 parcer would work on the following typical input string So the first_thing
 to know about bottom_up parsing is that it reduces what we call reduces a string
 into the start_symbol by inverting productions by running productions
 backwards So heres an example On the left_hand side is the sequence of states
 of the string On the right_hand side are the productions that were used And the
 thing to observe lets just look_at the very first step Is that we began_with the
 entire string We began_with the the the string of terminals And we picked some of
 those terminals In this case just one this particular Int right here And we ran
 a production backwards We replaced the Int here by the left side of the
 production We began_with we matched the right side of the production Int and we
 replaced it by the left side So Int went backwards here to T And then in the next
 step we took Int_times T this substr ing of The string that were working_on And
 we replace it by the lefthand_side of this production N times T was replaced_by
 T and so on At each_step here were matching some portion of the string And
 Im underlining the portion thats being replaced at each_step And were running
 and that matches the right_hand side of sum production And then were replacing
 that substring by the left_hand side And finally this entire string here is
 replaced_by E And we wind_up at the start_symbol So we began_with an input string
 This is our input string up here Alright put string of tokens and we end with the
 start_symbol down_here And if you read the moves in this direction If you start at
 the bottom and read towards the top Well these are just productions And in fact
 this whole_thing is a derivation This is just a normal derivation going from bottom
 to top But in this direction when we run it backwards beginning_with the string
 towards the start_symbol we call these reductions And I havent told you exactly
 how we decided what reductions to do and you might wonder well how I knew to do
 this particular sequence of reductions Well heres another interesting property
 of bottomup_parsing So if you read the productions_backwards they trace a
 rightmost_derivation so if we begin here with e so were gonnaso remember the
 parser is actually going in this direction so this is the direction of
 parsing here But now were_gonna look_at the steps the parser took in reverse and
 were_going to see that it was in fact a rightmost_derivation So here E went to
 TE Well E was the only non_terminal But then E here is the one thats
 expanded its the rightmost non_terminal And then this T is expanded its also the
 rightmost non_terminal to get int And now this T is the rightmost tom non
 terminal Its expanded to get Int_times T And then this is the only end right
 most non_terminal and so we wind_up with the whole input string Int_times Int plus
 int And this leads us to The first important f act about bottom_up parsing
 which is that a bottom_up parser_traces a rightmost_derivation in reverse all
 right So if youre ever having trouble with bottom_up parsing its always
 helpful to go_back to this basic fact Bottom up parser_traces a rightmost
 derivation but it does so in reverse by using reductions instead of productions
 So heres the series of reductions again Shown on the left And here is the parse
 tree that is constructed from those reductions And this is actually I_think
 a very helpful picture if we animate it to see the sequence of steps and to see
 what a bottom_up parser is really doing So here we begin_with the input string
 Over here And we show the same input string here And now were just going to
 walk_through the sequence of steps that the bottom_up parser takes A series of
 reductions And show how it builds an entire parse_tree And the basic_idea is
 that in each_step were performing a reduction And remember when we do a
 reduction we replace the children of the right_hand side of sum production by its
 left_hand side And just like when we were doing top_down parson well we will do the
 same_thing here In the input and then we make T its parent And now you can see
 whats going to happen A top_down parser begins_with the start_symbol and produces
 the tree incrementally by expanding some non_terminal at the frontier At the
 current at a current leaf of the partially constructed parse_tree The
 bottom_up parsers is going to begin_with all the leaves of the eventual pars tree
 The entire input And its going to build little trees on top of those And its
 going to be pasting together all the subtrees that its put together so_far to
 build the complete tree Lets walk a few more steps and see how that happens So in
 the next step we go from Int_times T to T so Int_times and the sub tree rooted at
 the other T become children of this non_terminal T and you can see weve taken
 these three sub trees here and pasted them together into a larger tree So as we
 go throug h the parcer bigger and bigger portions of the original input are gonna
 be pasted together into larger and larger trees And the next reduction takes the
 Int to the far into the input and reduces it to T And that gets reduced to E and
 then At the very end the three remaining sub trees are all pasted together into one
 parse_tree for the whole_thing with a start_symbol as the root
 In this video_were gonna continue_our discussion of bottomup_parsing with the
 main strategy used by all bottomup parsers socalled shiftreduce parsing
 Here is a quick review of the most_important thing that we learned last_time
 that a bottom_up parser_traces a right most innervations in reverse Now this
 particular fact has an important consequence So lets think_about a state
 of a shift_reduced parse where we have string alpha_beta and omega and lets
 assume the next reduction is going to be the replaced beta by X Okay so remember
 were running productions_backwards Then I claim that Omega has to be a string of
 terminals And why is that Well if you think_about it if this is a rightmost
 innervations in reverse then when X is replaced when we take this if we look_at
 the forward step is the the backward step So remember the parser is running
 this way replacing data by X But if we think_about the rightmost innervations in
 the other direction then X has to be the rightmost nonterminal which means there
 are no nonterminals to the right of X and so all the Character all the tokens
 whatever it is in this string have to be terminal_symbols Now it_turns out that
 those terminal_symbols to the right of the right most nonterminal are exactly the
 unexamined input in bottom of parsley implementations That is if I have alpha
 X omega and Im and X is my right most to nonterminal then this is the input
 that we havent read yet This is unexamined Input And its gonna be useful
 to mark where we are in the parse where our where our input focus is And were
 gonna do that by using a vertical_bar So were_gonna just place drop a vertical
 bar Between the place_where we read everything to the left and weve actually
 been working_on this So this stuff to the left here can be terminals and
 nonterminals and we the parts that weve_seen all of that stuff And the stuff to
 the right is after the parts hasnt seen Now we dont_know whats out there
 although we do know its all terminal_symbols An d the vertical_bar is just
 gonna mark the dividing line between the two substrings To implement bottom_up
 parsing it_turns out we only needs two kinds of actions Shift moves and reduce
 moves And weve_already talked somewhat about reduce_moves and so we have to
 introduce shift_moves So lets do that now So a shift move reads one token of
 input And we can explain that or represent that by moving the vertical_bar
 one token to the right So if our input focus is here and we want to read one
 more token of input then we just move the vertical_bar over And this signifies that
 now the parser knows about that next terminal_symbol And now we can start
 working_on it It can do things We can match against it for the purposes of
 performing reductions Again the stuff out here to the right of the vertical_bar the
 parser hasnt seen yet And then a reduce move is to apply an inverse production at
 the right end of the left string So if in production a goes to xy and we have x and
 y here immediately to the left of the vertical_bar So this is our focus point
 okay and x and y the right_hand side of the reduction is right there Then we can
 do a reduction we can replace that right_hand side by the left_hand side and this
 is a reduce move Here is the example from the last_video and this is exactly the
 example just showing the reduced_moves now with the vertical_bar also shown So
 this shows where the input focus was at the point where each of the reductions was
 performed And whats missing of course now we know is the sequence of shift
 moves So here is the sequence of shift_moves and reduce_moves that take the
 initial input string to the start_symbol So lets walk_through this in more_detail
 So were_going to go step by step And were_going to show each shift and each
 reduce move And now in addition to our input string down_here we also have a
 pointer showing where the where in the input we are So initially we havent_seen
 any of the input and our input pointer is to the left of the entire str ing So the
 first move is to do a shift And then we do another shift and then we do another
 shift And now just look_at the example from before if you look back at that at
 that example you_know the next_thing we need to do is to reduce So remember were
 only allowed to reduce to the left of the arrows So we can only reduce over on
 this side of the arrow So we always have to read enough of the input before we can
 perform a reduced move And then we perform another reduce move okay And
 then it_turns out the next_thing to do is two shift_moves and we havent explained
 yet how we know_whether to shift or reduce were_going to get there Im just showing
 that there exists a sequence of shift and reduce_moves that succeed in parsing this
 example Now weve shifted the entire input onto this sorry weve weve We
 shifted over the entire input so theres_no more input to read And now all we can
 do is reduce_moves But fortunately there is a sequence of reduce_moves from this
 point that we can we can perform So here we reduce int and then we reduce T
 plus T Oh forgot we first reduce T to E and then we reduce T plus E back to the
 start_symbol
 In this video_were going to introduce another important concept in bottomup
 parsing the notion of a handle To_review bottom_up parsing is these two
 kinds of actions we have shift_moves which just read one token of input and
 move the vertical_bar one to the right And reduced_moves which replace the right
 hand_side of a production inaudible to the left of the vertical_bar by a
 production left_hand side So in this case the production must have been A goes
 to XY And also reviewing what we did in the last_video the left string can be
 implemented by a stack where the top of the stack is marked by the vertical_bar
 So shift pushes the terminal on to the stack and reduce pops zero or more symbols
 of the stack and thats gonna be the right_hand stack of some production And
 then its going to push one nonterminal on to the stack which is the left_hand
 side of that same production And the key question in bottom of parsing and the one
 we havent addressed at all yet is how do we decide when to shift and when to
 reduce So lets_take a look_at this example grammar And lets think_about a
 step of a parse where weve shifted one token onto the stack We have Nth on the
 stack and then we have times N plus N still to go that we havent_seen yet Now
 at this point we could decide to reduce by T goes to N because we have the production
 T goes to Nth right here And so we could then get into this particul potential
 state or this particular state where we have T on the stack and then the rest of
 the input that looks_like that A but you can see that this would be a mistake
 There is no production in the grammar that begins Hence T times Theres no
 production up here that looks_like T times And therefore if we were to to to
 make this move we would get_stuck We could continue to do reductions to
 rummage around in the string But we would never be_able to get back to the start
 symbol Because there is no way to deal a sub string that has t times something in
 it So what that shows us is that we dont always want to reduce just because we have
 the righthand_side of a production on top of the stack To repeat that even if
 theres the righthand_side of some production sitting right there on top of
 the stack it might be a mistake to do a reduction We might want to wait and do
 our reduction someplace else And the idea about how we decide is that we only want
 to reduce if the result can still be reduced to the start_symbol So lets_take
 a look_at a right most innervations So beginning_with the start_symbol we get to
 some state after after some number of steps where that means just an_arbitrary
 number of steps We get to some state X is the right most nonterminal and then the
 next step is to replace X with by the right_hand side of some production And
 remember again with bottom_up parsing the parsers are actually going in this
 direction okay So this is the reduction direction The derivation direction the
 production direction Because thats the easiest way to talk_about what strings are
 derived We wanna begin_with a start_symbol But the inaudible but the
 parsers actually going against the flow of these arrows Anyway if this is a
 rightmost_derivation Then we say that alpha_beta is a handle of alpha_beta
 omega And that just means that yes it would be okay in this situation to reduce
 beta to X And we could replace beta by X because its not a mistake We can still
 by some sequence of moves get back to the start_symbol You_know by by doing more
 reductions So handles formulize the intuition about where it is okay to do a
 reduction A handle is just a reduction that also allows further reduction back to
 the start_symbol And we clearly only want to do reduction at handles If we do a
 reduction at a place that is not a handle even_though it looks_like its the right
 hand_side or maybe actually be the right_hand side of some production that does
 not mean That its actually a handle and we might if we could reduce there we may
 get_stuck So all we said so_far is what a handle is Weve defined a handle We
 havent_said anything about how to find the handles And actually how we find the
 handles is gonna consume much of the rest of our_discussion of parsing
 Welcome_back in this video_were gonna talk_about the key ideas behind techniques
 for recognizing handles There is good news and bad news when it comes to
 recognizing handles The bad news is that there is no known efficient algorithm that
 recognizes handles in general So for an_arbitrary grammar we dont have a fast
 way to find the handles when were parsing The good news is that there are
 heuristics for guessing handles and for some context_free grammars for some
 fairly large classes of context_free grammars these heuristics always identify
 the handles correctly We can illustrate the situation with a Venn diagram If we
 start with a set of all context_free grammars then the unambiguous context
 free_grammars are a subset of those and then an even smaller set are called the
 LR(k) grammars And here just to remind you l stands_for left to right scan
 r stands_for rightmost variation and k stands_for the number of tokens of
 look_ahead Now the LRK grammars are one of the most general deterministic families
 of deterministic grammars that we know of But those arent the ones that are
 actually used in practice Most of the bottom_up tools that are practical use
 what are called the LALRK grammars which are a subset of the LRK grammars And then
 what were_gonna talk mostly about is a simplification of those called the simple
 LR grammars or the SLRK context_free grammars And these containment
 relationships or inaudible that is there are grammars that are inaudible R
 k but not s l r k for every k and similarly there are grammars that are l r
 k for every k that are not l a l r k As weve_already said its not obvious how
 to detect handles So what does the parser know Well it sees the stack At
 each_step it knows the stack that it has already constructed And so lets see how
 much progress we can make just thinking_about what information we can get from
 the stack So heres a definition Were_going to say that alpha is a viable
 prefix If there is some omega such that alpha bar omega is a configuration a
 valid configuration of a shift_reduce parse Now keep in mind that the alpha
 here This is the stack And the omega here is the rest of the input And what
 does that means That_means the parser knows this part The parser knows alpha
 it doesnt know much of omega It can do some lookahead it can look_at a small
 prefix of omega usually just one token but it certainly doesnt know the whole
 thing So what does a viable_prefix mean Well a viable_prefix is a string that
 does not extend past the right end of the handle And the reason we call it a viable
 prefix is because it is a prefix of the handle So as long as the parser has
 viable_prefixes on the stack no parsing error has been detected And really the
 definition is just giving a name to something its not anything very deep
 the fact that alpha bar omega is is viable thats just saying we havent
 encountered an error That this is some state of a shift_reduce parse It hasnt
 said yet how were_going to identity it or anything like that its just saying that
 these are the valid states of shift_reduced parse Now the definition is
 useful in one way if it bring us to the last important fact important fact number
 three about bottom_up parsing In this effort any grammar the set of viable
 prefixes is a regular language and this is really an amazing fact and one thats
 going to take us a little while to demonstrate but this is the key to bottom
 up_parsing At least all the bottom_up parsing tools are based on this fact that
 the set of viable_prefix can be recognized by a finite_automaton So were_going to
 show how to compute this automaton that accepts the viable_prefixes but first
 were_going to need a number of additional definitions The first definition we need
 is the idea of an item Now an item is a production that just has a dot somewhere
 on the right_hand side So heres an example Lets_take the production T goes
 to open_paren E closed_paren What were_going to do is were just gonna put the
 dot in eve ry possible position on the right_hand side So well have one item
 where the dot is all the way at the left end Well have one where the dot is all
 the way at the right end And then well have items where the dot is between every
 pair of consecutive symbols So in this case there are four items for the
 production One special case is what do we do with epsilon productions Well for
 an epsilon production there is no there are no symbols on the right_hand side
 Well just say there is one item X goes to dot And these items youll_see them
 referred to if you if you look in help pages and in the literature as the LR
 zero items Now were_ready to discuss how we recognize viable_prefixes And the
 problem is that the stack has only bits and pieces of the right_hand side of
 productions In general most of the time we dont have a complete right_hand side
 on top of the stack Most of the time we only have a part of the right_hand side
 And It_turns out that what is on the stack is actually not just random its
 its it actually has a very special structure In in these bits and pieces
 are always prefixes of right_hand sides of productions That is in any successful
 parse what is on the stack always has to be a prefix of the right_hand side of some
 production or productions Lets_take a look_at an example Lets consider the
 input open_paren inaudible closed_paren And heres one of our_favorite
 grammars Now this configurations where I have open_paren E inaudible on the
 stack Remember that this is our stack And we have the close inaudible in the
 input This is actually a state or a valid state of a shift inaudible And you can
 see here that open_paren E is a prefix of the production T goes to open_paren E
 close_paren And after we shift the remaining close_paren onto the stack then
 well have the complete right_hand side and it will be ready to reduce So this is
 where the items come in The item T goes to open_paren E Dot closed_paren This
 describes this state of affairs I t says that so_far we have seen open_paren E of
 this production And were hoping in the future to see the closed_paren So another
 way of thinking_about it is that this item records the fact that were working_on
 this production And then so_far weve_seen this much Everything to the left of
 the dot is what weve_already seen and is what is on the stack and What is to the
 right of the dot is what were waiting to see before we could possibly reduce And
 we may or may not see that remember the parser doesnt know the input In this
 case of course its the very next next symbol and so it can see in the
 lookahead but you_know at this point in time the parser doesnt know for sure
 whats coming_up and you_know and and if this dot were further to the left there
 might be many_many more symbols that we had to go before we could perform the
 reduction So anyway whats to the left of that records what weve_already seen
 And what is to the right of the dot says that what we are waiting to see on the
 stack before we can perform a reduction And now we could talk_about the structure
 of the stack So you see its not just arbitrary collections of symbols In_fact
 it has this very particular structure So the stack is actually a stack of prefixes
 of right_hand sides So the stack always has this organization where theres a
 bunch of prefixes stacked up literally stacked up on the stack And whats going
 to happen is that the ice prefix if you were to pick a prefix out of this stack of
 prefixes While that must be the prefix of some production Okay The right_hand side
 of sum production And what that means is that that prefix that inaudible prefix
 on the stack will eventually reduce to the left_hand side of that production So
 it will eventually reduce to XI in this case And then that XI has to be Part of
 the missing suffix of the prefix that is below it on the stack So if I look_at the
 previous prefix the one thats right below prefix inaudible on the stack
 Then when I perform this reducti on that XI needs to extend that prefix to be
 closer to a complete right_hand side of that particular reduction Okay so in
 particular theres going to be some production That is going to already have
 a portion of its right_hand side on the stack So prefix of I minus one And X I
 is going to extend that prefix and then theres gonna be some more stuff possibly
 that were waiting to see even after the X I is put there And recursively all the
 prefixes above prefix K eventually have to reduce to the missing part of the right
 hand_side of prefix K the alpha K that goes on the right_hand side inaudible
 This image you have a stack of prefixes were always working_on the topmost
 prefix on the stack so you will be always working here on the right and shifting and
 reducing but every time we perform a reduction That has to extend the prefix
 immediately below it on the stack And when these when a bunch of prefixes have
 been removed from the stack through reductions then we when we get to work
 on the prefixes that are lower in the stack So lets illustrate this idea with
 an example So here is another input string and were_gonna use the same
 grammar You can you can rewind if you want to see the grammar again But lets
 consider this state where we have open_paren inaudible star on the stack And
 we have int close_paren remaining in the input kay And so what items would
 record what is the what is the stack structure here and how do the items record
 it Well lets start here at the bottom lets actually work from the bottom_up So
 we have in start the top of our stack so we this is the right_hand side that were
 currently working_on and that would be a prefix to this production T goes to int
 star T Okay So what this says is that were looking you_know we weve_seen in
 stars so_far and were waiting to see inaudible Im not showing the items
 but Im just showing the productions that this is eventually going to use Now the
 one thats below it here the the prefix thats below it o n the stack is right
 here in between the open_paren and the int This ones an interesting case Its
 actually epsilon So theres nothing there now on the stack But eventually once the
 int star has reduced to T Okay Then that T is going to reduce to E And currently
 of course theres not a T there at all So weve only seen epsilon Weve seen
 none of the prefix of this production on the stack And then for the last
 production the one deepest in the stack were currently weve currently seen an
 open_paren And were w and we think were working_on this production T goes
 to open_paren E closed_paren alright So when this E is produced that will extend
 this right_hand side And now we can record all of this with the stack of
 items T goes to open_paren dot E E goes to dot T and T goes to N star dot T
 Okay and we just record what we said on the previous_slide that so_far we see
 the open_paren of this production Weve seen nothing out of the right_hand side of
 this production and weve_seen N star so_far of this production And just notice
 how the left_hand side of each of these productions is going to eventually become
 part of the right_hand side of the Of the right part of the right_hand side of the
 production we are working_on just below it in the stack So when weve reduced this
 instar T to T that will extend this production when it reaches E that will
 extend this production To_summarize this video we can say a little more precisely
 how we go about recognizing viable_prefixes The crux of the problem is going
 to be to recognize a sequence of partial right had sides of production Where each
 of those partial right_hand sides can eventually reduce to part of the missing
 suffix of its predecessor Next time in the next video_were going to actually
 give the algorithm for implementing this idea
 In this video_were finally gonna come to the technical highlight of bottom_up
 parsings After all the definitions of the previous videos now were actually gonna
 be_able to give the algorithm for recognizing viable_prefixes So lets dive
 straight into the algorithm The first step is really just a very technical point
 and its not not that important But were_going to do it anyway because it
 makes things simpler Is to add a dummy production as prime_goes to S to our
 grammar of interest G So just to set the stage we are trying to compute the viable
 prefixes of G Were trying to come up with a algorithm for recognizing the
 viable_prefixes of G If S is the start_symbol were_going to make up a new start
 symbol as prime so as prime would be the new start_symbol of augmented grammar and
 its just one production for as prime as prime_goes to S Right So this just allow
 us to know exactly where our start_symbol is used in particular our new start
 symbol as prime is only used in one place and Left hand_side of this one
 production and that just makes things a little_bit simpler Now recall what we
 are trying to do We claim that the set of viable_prefixes for a given grammar is
 regular and so what were_going to do is were_going to construct a
 nondeterministic final automaton that recognizes the viable_prefixes okay And
 the states of this NFA are going to be the items of the grammar Now the input to the
 NFA is the stack So the NFA reads the stack okay And then it So lets
 indicate this so the NFA is gonna take the stack as an argument and its either
 gonna say yes thats a viable_prefix or no And its gonna read the stack from
 bottom to top So its gonna start at the bottom of the stack and read towards the
 top of the stack And our goal here is to write a non entromystic finite_automaton
 that recognizes the valid stacks of the purser So that is how well know That
 our parser hasnt really counted any parse errors Because this automaton that were
 going to construct will always output e ither yes this stack is okay meaning it
 could wind_up parsing the input Or no what weve_got on the stack now doesnt
 resemble any valid stack for any possible parse of any input string for this
 grammar Okay so lets think_about what we what we need the moves of this machine
 to be So lets_say that were in the state E arrow alpha dot X beta Now
 what does that say So that says that so_far weve_seen alpha on the stack Okay
 so remember the machine is reading the stack from bottom to top This records the
 fact that the machine has already_seen quotalphaquot on the stack So what would be an
 okay thing to see next on the stack Well if this is a valid stack if having
 quotalphaquot on the stack at this point is valid well then certainly it would be
 okay if the next_thing on the stack was an quotXquot So we have a transition that if
 were in this state Where we are working_on this production and weve_seen alpha
 on the stack If the next_thing is an X on input X then we can go to this state
 Where now we record the fact that weve_seen alpha X on the stack and were
 waiting to see the remaining portion beta of that production Okay so this is
 one kind of move that the non triamistic phymotine can make and again we do we
 add this kind of a move for every item So for every item in the grammar I if it
 if the dot is not all the way at the right end then there will be a move like this
 where the dot moves over for whatever symbol happens to come to the right of the
 dot The other class of transitions are the following And these are the the more
 interesting ones So lets_say that were in this configuration here Where again
 weve_seen alpha And then the next_thing on the stack is X And here X is a non
 terminal sound And I should have said that in the previous case X was either a
 terminal or a non_terminal So this X here is any grammar symbol not just a non
 terminal But this four here the the moves here in part four are specifically
 for non terminals Okay so anyway if X is not on the stack Okay lets_assume
 that weve_seen alpha and then the next_thing on the stack is not S Well is it
 possible that there could be a valid configuration of the parser where we saw
 alpha but then X didnt appear next And the answer is yes because as we said
 before the stack is a sequence of partial right_hand sides So it could be that all
 thats on the stack right now for this production Is alpha and if the next_thing
 on the stack is eventually going to reduce to X It might not be X itself it might
 be something that will eventually reduce to X Well what does that mean Well
 that means that whatever is there on the stack has to be derived_from X it has to
 be something that can be Generated by using a sequence of X productions cause
 eventually its going to reduce the X So for every item that looks_like this and
 for every production for X now were_going to add the following move were_going to
 say that if theres_no X on the stack well then we can make an epsilon move we
 can just shift to a state where we try to recognize the right_hand side plus
 something_derived from X And these are the only two kinds of moves Either the
 items eith sorry either the grammar symbols were looking for are there on the
 stack and we extend gtgt The prefix of a right_hand side So this rule here extends
 a prefix So as we see more of that production on the stack or it tries to
 guess or tries to discover where the ends of the prefixes are So if if alphas as
 much of the production that is on the stack currently well then this must be
 this x here must this this point here must mark the start of another right_hand
 side in our stack of right_hand sides So we would expect to see something_derived
 from some production for x Two more rules Every state in this automaton is
 going to be an_accepting state That_means that if the automaton manages to
 successfully consume the entire stack then the then the stack is viable And
 just notice that not every state is going to have transition on every possibl e
 symbol So there will be plenty of possible stacks that that are rejected
 simply because the automaton gets stuck And finally the start_state of this
 automaton is the item as prime_goes to dot S So remember the states of the machine
 are the items of the grammar And this is why we added this dummy production is
 just so that we could conveniently name the start_state So now lets_consider
 one of our grammars weve_been using a lot so this is the grammar And now were
 going to augment it with the extra production as prime_goes to E And lets
 take a look_at the automaton for that recognizes the viable_prefixes of this
 grammar And here it is and as you can see its rather large it has a lot of
 states and a lot of transitions and I just want to show it to you here before we
 describe how we calculated it just so you can get an idea that these inaudible for
 recognizing viable_prefixes for grammars are actually quite elaborate But now
 lets break this down and see how it was produced So lets begin_with the start
 state of this machine so we have S_prime goes to dot E And remember what this
 says is we want to be_able to reduced to the start_symbol to the new start_symbol
 And so were reading the stack and were hoping to see an E on the stack and if we
 dont then were happy to see something_derived from E So what transition we
 make from the state One_possibility is that we do in fact see an eon a stack and
 in that case the dot simply moves over saying yes weve read the first item on
 the stack or the weve read the E on the stack and so weve_seen the full right
 hand_side of this production Now that would indicate that we were probably done
 with parsing This is the state that you would reach have youd read the entire
 input and successfully parsed it you would have reduced the old start_symbol
 and be about to reduce to the augmented the the new start_symbol But if youre
 not so fortunate as to see an E on the stack then you need to hope that youll
 see something derive fro m E And there are a couple of possibilities there One
 is that we could see something that would eventually use this production E goes to
 T And since we havent_seen any of it yet we put the dot all the way at the
 left indicating that were hoping to see a T which could then reduce to E and
 which could then reduce to S_prime Now if we dont see a T on the stack by_itself
 the other_possibility is that we could be working_on this production E goes to T
 plus E And again we havent_seen any of it so the dot Goes on the left_hand side
 And then notice how were crucially using the power of nondeterministic automata So
 here we dont_know which production is going to which right_hand side of a
 production is going to appear on the stack And in fact I notice that these
 productions are not even left_factored so we dont_know whether its going to be
 just a T there or a T plus E but We just use the guessing power of the inaudible
 chromatin you chose which one to to use Remember the inaudible sepse is any
 possible choice except So you can always guess correctly So intuitively you can
 You will be_able to pick the right one Now of course we could compile this down
 to a deterministic_machine that wont have to make any guesses But at this level
 were writing inaudible its extremely useful not to have to figure_out which of
 these two productions to use We can just try both and see what_happens Now lets
 focus_on this state E goes to dot T What are the possibilities there Well
 one possibility is that we see a T on the stack And then we see in a complete right
 hand_side And notice how when the dot was all the way to the right_hand side that
 is going to indicate that were_ready to do a reduce So well talk_about that a
 little_bit later but essentially thats how were_going to recognize handles When
 we finally reach a state where the dot is all the way to the right_hand side thats
 going to say this could be a handle that you might want to reduce Now if we dont
 see a T on the stack then we just see something_derived from T and theres a
 couple of possibilities a few possibilities there One_possibility is
 that its going to be the production T goes to int so since were just starting
 on this production again we just put the dot all the way at the left Another
 possibility were working_on T goes to (E) And the third possibility that were
 working_on T goes to int x T And each of the case here notice that the dots are
 all the way at the left indicating that were just getting started we have not
 actually seen any of the right_hand side yet Now lets shift our focus to this
 production E goes to dot T plus E This item excuse_me One_possibility is that we
 see an E on the see a T on the stack okay in which case the dot just moves
 over And the other_possibility is that we see something_derived from T in which
 case we will go to one of the states that begins a T production And notice here
 that we already have all three of those items in our automaton Were just going
 to it states that we went to from the item E goes to dot T So this this item E
 goes to dot T plus E could also move to those three states Now lets focus_on
 this item here T goes to dot openparen E closedparen Well theres
 only one possible move here so this is only a a terminal its not its not a
 nonterminal so theres not going to be any possibility of having something
 derived_from openparen We just have to see the openparen in the input So
 theres only one possible transition here which is that we see the openparen
 excuse_me on the stack and the dot just moves over Now from this state once
 again we got is just next to a or just to the left of a nonterminal so we might
 see that nonterminal on the stack or we might see something_derived from that
 nonterminal Well if we see that nonterminal on the stack your dot just
 moves over and we get T(E) indicating that weve_seen an both an ( and E on the
 stack and were still waiting to see the ) Well we might also see somethin g
 derived_from e okay So we add these two transitions to the two items that begin
 productions for e sound Alright now lets focus_on this state T goes to open
 paren_E dot closed_paren Again cause its a terminal that the dot its next to
 is only one possible move We have to see that open_paren if we see anything at all
 And well wind_up with the item T goes to open_paren E closed_paren dot And now
 weve recognized the entire right_hand side of that production on the stack
 Lets_take a look_at this item So were here because a terminal_symbol the only
 possibility is to read that terminal_symbol on the stack So this would be the
 next item E goes to T plus dot E sound Focusing on that item again
 inaudible we could possible see an E on the stack Right in which case we would
 have recognized the entire writing inside of this production We have E goes TE Or
 we can see something_derived from here which case we make a transition back to
 one of those two states Now where we got productions left to go or items left to
 go Here we haw T goes toint So we would have to see it next on to stack and
 that would be the full right_hand side of that production Down here we still have
 T goes to dot inaudible times T So again theres a terminal_symbol here
 inaudible And so that would be the next_thing we would need to see on the stack
 for this production to remain viable And once_weve seen the inaudible we would
 like to see the times So we wind_up in this state and now weve_got dot next to
 T So again one possibility is that we see the T on the stack and then weve_seen
 the full righthand_side of this production But we might only see
 something derive from T The might the the T might not be there yet It might be
 in a state where were still waiting for the T to appear through some sequence of
 reductions But then we would need to see something derive from T And in this case
 we would make a transition to one of the three states that begin the productions
 for T And that s the full automaton That is those are all the states and all
 the transitions for the automaton that recognizes the viable_prefixes of this grammar
 In this video_were going to use our example automaton for recognizing viable
 prefixes to introduce one more idea The idea of a valid item To refresh your
 memory heres where we left off last_time This is the complete
 nondeterministic_automaton for recognizing the viable_prefixes of the example grammar
 sound And Using the standard subset of states construction we can build a
 deterministic_automaton that is equivalent to the nondeterministic_automaton So
 heres the deterministic_automaton that recognizes exactly the same language This
 automa this deterministic_automaton notices the viable_prefixes of our
 example grammar But now notice that each state is a set of items So theres a set
 of nondeterministic_automaton states in each of these states And recall that what
 that means is that the nondeterministic_automaton could be in any one of these
 states And in particular this state here is the start_state because it has the item
 S_prime goes to dot E The states of this deterministic_automaton are called
 variously cananugal collections of items or the cananugal collections of LR zero
 items If you look in the dragon book it gives another way of constructing the LR
 zero items than the one that I gave Mine is somewhat simplified but I_think also a
 little easier to understand if you are seeing this for the first time Now we
 need another definition Well say that a given item is valid for a viable_prefix
 alpha_beta If the following is true that beginning from the start_symbol this is
 our extra start_symbol and by a series of rightmost_derivation steps we can get to
 a configuration alphaxomega and then in one step x can go to betagamma And
 what this says is after parsing alpha and beta after seeing Alpha and beta on the
 stack the valid items are the possible tops of the stack of items That that we
 could that this item could be the determination state of the
 nondeterministic_automaton A simpler way of explaining the same idea is that for a
 given viable_prefix alpha the items that are valid in that prefix are exactly the
 items that are in the final_state of the DFA after it reads that prefix So these
 are the items that describe the state after youve seen the stack alpha Now an
 item is often valid for many_many prefixes So for example the item T goes
 to open_paren e closed_paren is valid for all sequences of open parens And to see
 that We can just look_at our automaton and confirm that if we see an open_paren
 remember this is the start_state So if we see an open_paren we take this
 transition we wind_up in this state here And then every open_paren we see we just
 go round and round in this state So if I have a sequence of five open parens as my
 input then Ill have transitions one two three four five all looping in
 this state And notice that this item Is in is one of the items in that state And
 that just says that this item is valid for any prefix or for excuse_me any
 sequence of open parens
 In this video_were finally going to give an actual bottom_up parsing_algorithm In
 particular well talk_about SLR or simple LR parsing which will build on the
 ideas of valid items and viable_prefixes that weve_been discussing in our recent
 videos The first_thing were_going to do is to define a very weak bottom_up parsing
 algorithm called LR0 parsing And the basic_idea here is that were_going to
 assume a stack contains a contents alpha and that the next_input is token T And
 that the DFA this is the DFA that recognizes the viable_prefixes On input
 alpha that is when it reads the stack_contents it terminates in some state S
 sound And Theres only gonna be two things that this that this parsing
 algorithm needs to do So if S if the final_state of the DFA contains the item X
 goes to beta dot Well what does that say That says weve_seen the complete
 right_hand side of X goes to beta on the top of the stack and that furthermore
 everything thats below the stack still says that x goes to beta dot is a valid
 or a viable sorry is a valid item for this state Meaning its okay to reduce by
 X goes to beta So if we see a complete production dot all the way in the right
 hand_side in the final_state of the DFA then were just going to reduce by that
 production The other possible move is a shift If we wind_up in a state where X
 goes to beta t and then some other stuff is a valid item what does that say That
 says that it would be okay at this point to add a T to the stack And if T is our
 input well then we should do a shift move sound Now when does LR0 parsing
 get into trouble Well there are two possible problems it could have It might
 not be_able to decide Between two possible reduced_moves So if any state
 of DFA has two possible reductions meaning it seem two complete productions
 and it could reduce by either one then theres not enough information to decide
 which reduction to perform and the parts wont be completely deterministic and
 this is called a reduced reduced co nflict So again this happens if a
 particular state has two separate items indicating two separate reductions The
 other_possibility is that the final_state of the DFA after reading the stack
 contents might have An item that says to reduce and another item that says to
 shift And this is called a shiftreduce conflict So in this case there would
 only be a conflict in a state where T was the next item in the input But in that
 situation we wouldnt know_whether to shift T onto the stack or to reduce by X
 goes to beta sound Lets_take a look_at the DFA for recognizing viable_prefixes
 that weve_been using for the last_couple of ideas and in fact this particular DFA
 does have some conflicts So lets_take a look_at this state right here here we
 could either reduced by E goes to T you are in this state or if the next_input is
 a plus we could do a shift and In so in this particular situation if the next
 input is plus we could either shift and use this item or we can reduce and use
 that item So this particular state has a shift_reduced conflict Now thats not
 the only conflict in this in this grammar though In this state here we have a very
 similar problem Here we could shift if the next_input is a times Or we could
 reduce by T goes to inaudible And so this state also has a shift_reduce
 conflict It_turns out that its not difficult to improve on LR0 parsing and
 well present one such improvement in this video called SLR or simple LR parsing And
 this is going to improve on LR0 by adding some heuristics that will refine when we
 shift and when we reduce so that fewer states have conflicts The modification to
 LR0 parsing that gives_us SLR_parsing is really quite small We just add one new
 condition to the reduction case So before if we saw it X goes to beta dot
 in the final_state of our DFA recall what that means That_means beta is on the top
 of the stack and it is viable And so its fine to reduce Now We do have a little
 bit more information So so notice that the automaton her e doesnt take any
 advantage of whats coming_up in the input This is based entirely this
 decision here is based entirely on the stack_contents But it might be that it
 doesnt make_sense to reduce based on what the next_input symbol is And how can we
 take advantage of that Well if you think_about it whats going to happen We have
 our stack_contents And it ends in a beta and now were_going to make a move
 where were_going to replace that by X Okay And if the next_input symbol is t
 so remember we have a vertical_bar here and a t following what does that mean
 Well that means that x has to come before t in the derivation Or in another words
 t is gonna follow x And if t cant follow x if t is a terminal_symbol that cant
 come_after the nonterminal x than it makes no sense to do this reduction So we
 only do the reduction if t is in the follow of x We just add that restriction
 and that is the only change to the parsing_algorithm So if there are any conflicts
 under these rules either shift_reduce or reduce reduce then the grammar is not an
 slr grammar Just notice that these rules amount to a heuristic for detecting the
 handles So we take into account two pieces of information The contents of the
 stack thats that the DFA does for us and it tells_us what items are possible
 when we get to the top of the stack and also whats coming_up in input and we can
 use that to define our reduction decisions And for those grammars where
 there are no conflicts meaning there is a there is a unique move in every
 possible state under those rules Then this heuristic is exact you_know for
 for those grammars And we just define those grammars to be the SLR grammars
 Lets consider how things have changed for our running example The deterministic
 automaton for recognizing the viable_prefixes of the grammar weve_been looking
 at for several_videos now Recall that we had shift_reduced conflicts under LR zero
 rules in two states So now lets look_at this state first the upper state So
 here we re going to shift if theres a plus in the input Thats what this item
 tells_us to do It tells_us theres if theres a plus then the right move is to
 shift And so Now the question is when are we going to reduce Well were only
 going to reduce if the input is in the follow of E And what is the follow of E
 We computed that a long_time ago but just to remind you remember that E here is the
 original start_symbol of the grammar so certainly dollar_sign will wind_up in the
 follow of E And the other_possibility for the follow of E is close_paren because
 here at this point in the grammar close_paren comes_after E And thats the only
 two possibilities So what that says now what that means is that in this
 particular state we are going to reduce if either were out of input Or if the
 next I the next token in the input is a closed_paren and will shift if the next
 token in the input is a plus And in any_other situation we will report a parsing
 error And so theres_no longer any shift_reduced conflict in this state and
 theres always a unique move for every possible input The situation is
 similarly similarly improved for the other state So here were_going to shift
 in theres a times in the input and were_going to reduce if the input is in the
 follow of T And what is the follow of T sound Recall We computed this again a
 long_time ago and I just happen to know what it is And so Ill just tell you
 Well it included everything in the follow of e So a dollar_sign in close_paren are
 in the follow of T But also a plus is in the follow of T because of this usage over
 here in the grammar where plus appears really after T But those are the only
 things in the follow of T And so now were_going to reduce only if were out
 of input or if the next_input item is a close_paren or a plus and theres also a
 no shift_reduce no_longer any shift_reduce conflict in this state And so this
 grammar is an SLR1 grammar Now many grammars are not SLR To emphasize that
 SLR is an improvement on LR0 but it s still not a really very general class of
 grammars So All ambiguous grammars for example are not SLR We can improve a
 little_bit on the SLR situation We can make SLR parsers even more grammarous by
 using precedence declarations to tell it how to resolve conflicts So lets revert
 to the most natural and also most ambiguous grammar for plus and times over
 the integers and weve_looked at this grammar before If you build the DFA for
 this grammar if you go_through and build the DFA for the viable_prefix of this
 grammar you will discover that there is a state That has the following two items in
 it one says that if we see E times E that we have seen E times E on a stack and
 that we can now reduce by ecos E times E The other one will say that if theres a
 plus coming_up in the input we should shift And notice that this is exactly the
 question Of whether times has higher precedence than plus When youre in this
 situation should you Reduce thereby grouping the two Es together here
 Grouping the multiplication operation first Or should you shift the plus in
 which case youll be working_on that for a sentence at the top of the stack So in
 this situation the declaration times has higher precedence than plus resolves the
 conflict in favor of the reduction So we would not do the shift and we would wind
 up with no shiftreduce conflict Note that the term precedence declaration is
 actually quite misleading These declarations dont define precedence They
 dont Do that directly at all What they really define are conflict resolution
 They say make this move instead of that move It happens that in this particular
 case Because were dealing with a national grammar simple grammar for plus
 and times that the conflict resolution has exactly the effect of enforcing the
 precedence declaration that we want But in more_complicated grammars where there
 are more interactions between various pieces of the grammar these declarations
 might not do what you expect in terms of enforcing precedence fortuna tely you
 can always print_out the automaton The tools provide Usually a way for you to
 inspect the parsing_automaton And then you can see exactly how the conflicts are
 being resolved and whether those are the resolutions that you had intended And I
 recommend when youre building parsers especially if its a a fairly complex
 parser that you do examine the parsing_automaton to make_sure that its doing
 what you expect So now were_ready to give the algorithm for SLR_parsing So
 inaudible automaton our parsing_automaton that recognizes viable_prefixes
 The initial configuration is going to be with the vertical_bar all the way to the
 left so the stack is empty This is our full input and we inaudible dollar to
 indicate the end of the input And now were_going to repeat until the
 configuration has just the start_symbol on the stack and dollar in the input
 Meaning all the input is gone and weve reduced the entire input to the start
 symbol So An inaudible configuration will be written as alphaomega where
 alpha is the contents of the stack and omega is the remaining input and what
 were_going to do is were_going to run M run the machine on the current stack alpha
 and if M rejects alpha if M says that alpha is not a viable_prefix then were
 going to report a parsing error Were_gonna stop right there Now if M accepts
 alpha and it accepts it in a state if it ends in a state with items I then were
 gonna look_at the next_input call that A and what are we going to do Well were
 going to shift Yes theres an item In I that says it would be okay to see the
 terminal A Next Okay So thats just our shift move And then were_going to reduce
 if theres a reduction item in the in the set of valid items And the next_input can
 follow the nonterminal on the left_hand side So these are just the two rules that
 we discussed before And then well report a parsing error if neither of these
 applies Okay now one interesting thing about this algorithm if you read it
 carefully and you th ink about it for awhile Youll realize that this step is
 actually not needed that we dont need to check here For whether M accepts the
 stack or not Because this staff down_here where we report a parsing error if
 neither of these steps applies this already implies that we will never form an
 invalid stack That our their stacks will always be viable The parsing errors will
 be caught at this line and we wont pollute the stack with symbols that cant
 possibly result in viable_prefixes So in fact this error check here is not
 needed M is always going to accept the stack If there are any conflicts in the
 last step meaning its not clear whether to shift or reduce in some state for some
 input symbol then the grammar is not SLRK And K again is the amount of look
 ahead In practice we just use one token of look_ahead So typically just looking
 at the next token in the input_stream
 Welcome_back In this video_were going to do an extended example of SLR_parsing
 To_review here is the parsing_automaton for the grammar that weve_been looking_at
 in the last_couple of videos And this is just the deterministic version of the non
 deterministic_automaton we built last_time And Ive just gone through and
 numbered all of the states So lets_take a look_at what_happens when we parse the
 input inaudible times inaudible And just to review weve appended dollar_sign
 here to the end to indicate where the end of the input occurs Thats just an end of
 input marker And because this is the beginning of the parse we havent_seen any
 input yet And so the vertical_bar is all the way at the left_hand side of the
 input So the machine begins in state one and theres nothing on the stack The
 vertical_bar is all the way to the left again so the stack is empty So it just
 terminates in state one And these are the possible items that are valid for the
 initial state of the parser So among those items we see that there are two
 that tell_us that its okay to shift an_integer in this state And of course the
 first input is an_integer and so there are no reduced_moves All the other items
 in here also have their jobs all the way at the left side of the item so theres
 no possible reduced move in this state The only thing we could possibly do is
 shift and its okay to shift an_integer So to summarize on the initial
 configuration of the parser the DFA halts in state one it never even gets out of
 state one so it starts there and ends there without even reading any input
 because the stack is empty and the action that that state tells_us to do is to
 shift So that leads us in the following state theres an int on the stack and we
 have a times coming_up on the input So what_happens in that situation Well we
 begin The automaton is going to read the stack So starting from the bottom of the
 stack were in the start_state And then we read an int theres an int on the
 stack and we win d up in this state And what does this state tell_us we can do
 Well it tells_us one possibility is to reduce by T goes to int But again we
 will only do that if the following input is in the follow of T And times which is
 the next_input item is not in the follow of T So times is not in the follow Of T
 and so reducing here is not a possibility That leaves only the other item to
 consider and here we see that this item says we can the time So if the times the
 next_thing in input which it is its okay to shift So the DFA halts in state
 three and because theres a times in the input the move is to shift And that puts
 us into this configuration where we have int and times on the stack Times is at
 the top of the stack int is below it and we have an int coming_up in the input So
 what_happens now again the DFA is going to read the entire stack So beginning at
 the bottom of the stack the first_thing it sees is an int and it moves to that
 state And then it sees a times and so it moves to this state And now in this
 particular state what are the possibilities Well we can see first of
 all that there are no reduced_moves There_are no items with the dot all the
 way at the right end So the only possibility is a is a shift And we could
 shift if the upcoming inputs a open_paren which its not More usefully we
 could shift if the upcoming input is an inaudible which is exactly what we see
 So the DFA terminates in state eleven and the move in that state is to shift
 And that puts us into this state where we have int times int on the stack and we
 are out of input We are at the end of the input So lets see what_happens on the
 stack int times int The automaton reads it int times int and it winds_up back in
 state three Sa3 tells_us that we can shift if the next_input item is a times
 and which it is not Or we can reduce if whatever the next Is in the next_input is
 in the follow of T And in fact dollar is in the follow of T So in the end of
 the input come_after a T on the stack And that means its fine to reduce by T goes
 to int So once we do that once we do the reduction T goes to int we wind_up in
 the state times T Thats our stack_contents and of course were still at the
 end of the input So once_again the DFA is going to read the entire stack_contents
 from the bottom to the top First it reads the int at the bottom of the stack then
 it sees the times And then it finally reads the t at the top of the stack And
 it winds_up in a new state state four And the interesting thing about this
 particular step is that the DFA took a different path through the state graph
 than it did the previous time And thats because the stack_contents changed We
 didnt just add stuff to the stack and so we didnt extend the previous path We
 actually replaced some symbols or a symbol on the stack with a new symbol in this
 case the nonterminal T and that caused the DFA to take a different path Now what
 does this item in state four tell_us to do Well it says that we can reduce by T
 goes to N times T if whatever Follows in the input is in the follow of T And once
 again dollar is in the follow of T And so well do that reduction and now were
 left with the static contents just consisting of T And of course were
 still at the end of the input And lets see what_happens now So now of course the
 contents of the stack have changed even more radically and so the DFA just goes
 off in a completely_different direction It reads T winds_up in this state and this
 state says we can either shift a plus if theres a plus in the input And again
 theres_no more input Or we can reduce by E goes to T if dollar if the end of the
 input is in the follow of E Which it is And so the reduction will be the one that
 we do And now we have this stack_contents consisting only of E Lets see
 what_happens in that situation Now we make a transition to this state state
 two And we only have one item S_prime goes to E dot And so this is a reduced
 move And again dollar is in the follow of S_prime cause that is the start
 symbol And since that is the start_symbol we accept at this point So once
 we get to that item as our reduce move we know that the input has been
 successfully parsed
 In this video_were going to wrap_up our_discussion of SLR_parsing were_going to
 give the full SLR_parsing algorithm and also talk_about some important
 improvements The SLR_parsing algorithm we discussed in the last_video has one major
 inefficiency And that is that most of the work that the automation does when it
 when it reads the stack is actually redundant And to see this think_about
 the stack So we have our stack and this is the bottom over_here And this is the
 top of the stack over_here And what is going on in each_step In each_step we
 might shift something onto the stacks we might add one symbol or we might pop some
 symbols and and push one symbol onto the stack But basically theres going to be
 some small number of symbols that change at the top of the stack at each_step But
 most of the stack stays the same And then we rerun the automaton on the entire
 stack And so this work is all repeated Everything that stayed the same From the
 previous stack is repeated work and then we do a little_bit of new work just at
 the very top of the stack And clearly if we could avoid this we could make the
 algorithm run much much_more quickly The way to exploit the observation that most
 of the work of the automaton is repeated at each_step is to simply remember the
 state of the automaton on each stack prefix So were_going to change the
 representation of the stack were_going to change what goes in the stack so
 before we just had symbols on the stack but now were_going to have pairs Each
 element of the stack will be a pair of a symbol and a DFA state Thus the stack
 now is going to be a stack of pairs and whereas before a stack would have
 consisted just of the symbols sym1 up to sym n now were_going to have the same
 symbols but each one of them is going to be paired with a DFA state and that DFA
 state is going to be the result of running the DFA and all the symbols to its left
 So all the symbols below it in the stack So if I_think about my stack and if I draw
 a little picture of the stack as a line then the DFA state here Lets call this
 state I will be the result of running the DFA on the entire stack_contents to the
 left of that point And again if I look_at some other point in the stack at the
 state stack state thats stored there That would be running the results of
 running the DFA on the entire stack context contents up to that point And
 one small detail here is that the bottom of the stack we have to get started We
 need to have the start_state stored at the bottom of the stack And we just store
 that with any dummy symbol It_doesnt matter what symbol we pick So now were
 ready to actually give the details of the parsing_algorithm And the first step is
 to define a table go to And this maps a state and a symbol to another state And
 this is just the transition_function of the DFA This is the graph of the DFA
 written out as an_array Our SLR_parsing algorithm will have four possible moves A
 shift X move will push a pair on the stack X is a DFA state so thats named
 in the shift move now And then the other element of the pair is the current input
 And then well also have reduce_moves which are just as before So to recall a
 reduce move will pop the a number of elements from the stack equal to the
 length of the right_hand side And then it will push the left_hand side onto the
 stack And then finally accept an error moves for when weve successfully parsed
 the input and for when the parser gets stuck The second parsing_table is the
 action table which tells_us which kind of move to make in every possible state The
 action tables indexed by a state of the automaton and the next_input symbol And
 then the possible moves are things_like shift_reduce accept or error So lets
 consider if we do shifts if the final_state of the automaton at the top of the
 stack has an item that says it would be okay to shift an A And go to that is from
 this state we can go to state J on input A Then the move in state I on input A
 will be to shift AJ onto the stack And th ink about what that means for a second
 What that says is that we have a stack And then the next_input is A And then at
 this point its okay to shift an A onto the stack And furthermore that the state
 of the automaton at this point is SI Okay So the state of Irarta inaudible
 the top of the stack is SI The next_input is A Remember that the go to table is a
 transition_function of the machine So if we move the vertical_bar over if we shift
 that A on to the stack well now we dont just put A on the stack we have to
 put a pair on the stack And the question is what machine state should go there
 Well its going to be state that we would reach from state I from state SI on input
 A which The go to table tells_us in this case is state SJ And for that
 reason the action when we terminate in state I and the next_input is A is to
 shift the pair A J onto the stack The other three moves that go into the action
 table are things weve_already seen So if the final_state of the automaton at the
 top of the stack has an item that says that we can reduce and the follow up
 condition requirement is satisfied Mainly that the next_input can follow
 the left_hand side non_terminal of the production Then in the entry I for
 inaudible if were in state SI and we have input a we can reduce by the
 production x goes to alpha And theres one exception here were not going to do
 that reduction if the lefthand_side is the special start_symbol the new start
 symbol that we add to the grammar is prime Because in that case if the item
 that were_going to reduce by is sprime goes to sdot and were at the end of the
 input then we want to accept And any_other Situation is an error So in any
 other situation if were in state I and we have the next the next_input is A
 well we dont_know whether to shift_reduce or accept And so that is an
 error state Finally here is the full SLR_parsing algorithm And Im just going to
 walk you through it so that we can see how all of the ideas weve_been di
 scussing and all the various pieces fit together Lets let our initial input be
 called I And well just give it a name and its gonna be treated as an_array that
 we can index The index will be called J and initially its zero so that were
 pointing to the first token in the input string Well just assume that the first
 state of the DFA is called state one And that means our initial stack is going to
 have state one for the state of the automaton and some other dummy symbol that
 we dont care about In the in the first_position So the stack is just a pair
 with inaudible in the start_state of the DFA And now were_going to repeat the
 following loop until weve either successfully pars the input or we detect
 an error And at its steps what were_going to do Well were_going to look_at
 the next_input symbol and were_going to look_at the final_state of the automaton
 on the stack_contents and thats always the state of the pair thats on the top of
 the stack and were_gonna look those two things up in the action table and thats
 gonna tell_us what kind of move to make So lets just go_through the moves in
 order Lets consider the shift move first So what_happens If were if it
 says were supposed to shift and going to state K then what were_going to do is
 were_going to shift the input that means were_going to take the next_input symbol
 and or the current input symbol excuse_me and were_going to push that on to the
 stack together with state K of the inaudible That pair goes on to the
 stack and we also bumb the input pointer so that were looking_at the next
 character of input Now Let_me erase that so you can continue to read it Now what
 about the reduce_moves So this ones a little_bit interesting First thing were
 going to do is were_going to pop a number of pairs off the off the stack thats
 equal to the length of the righthand_side So we pop a number of items off the
 stack thats going to the right thats equal to the righthand_side of the
 production and then what do w e push on to the stack Well were_gonna push the
 nonterminal on the lefthand_side of the stack And now the question is what state
 goes on to the stack What DFA state Well With that weve popped the stack We
 can look_at the new top state of the stack So the DFA state was now the top
 state After weve done the pops well tell_us what the final_state of the DFA
 was and what is left of the stack And then now that were pushing X under the
 stack we want to know what state the DFA would go into on the transition labeled X
 And so we use the Go To table to look that up The current top state of the stack On
 symbol X where does the FA go That is the state that gets pushed onto the stack
 And then finally if if the move is accept we halt normally And if the move
 is error we halt and report an error or execute our error_recovery procedure One
 interesting fact about this algorithm is that it only uses the DFA state and the
 input The stack symbols are not used in really interesting way And so we could
 actually get_rid of the stack symbols and just do parsings with the DFA states on
 the stack But that of course would be throwing away the program and we still
 actually need to program for the later stages of the compiler And so to do the
 type_checking and cogeneration we need to keep the symbols around Now simple LR
 parsing is called simple for a reason And in fact in practice its a bit too
 simple The widely_used bottomup_parsing algorithms are based on a more powerful
 class of grammars called the LR grammars And the basic difference between the LR
 grammars and the SLR grammars is that look_ahead is built into the items So what
 does that mean Well a LR1 item is going to be a pair which consists of an item
 Just like we saw before And this means exactly the same_thing as before And a
 lookahead In case of an LR1 item theres just one token of lookahead If this was
 an LR2 item there could be two tokens of lookahead in there And the meaning of
 this pair is that if we ever get aroun d to state where we have seen all of this
 production all the righthand_side of this production Then its going to be
 okay to reduce if the lookahead at that point is Dollar thats the end of the
 input And of course there could be any_other token in there any_other terminal
 symbol in there besides dollar And this turns_out to be more accurate than just
 using follow sense recall that the point where a reduction decision is made in SLR
 parsing we just look_at the entire follow set for the symbol on the left_hand side
 of the production And this mechanism of encoding the lookahead in to the items
 allow us to track and find the inaudible which lookaheads are actually possible in
 particular production sequences And if you look_at the automaton for your parser
 actually its not an LR1 automaton Its an LALR1 automaton which is something
 very close to an LR automaton its a little_bit of an optimization over an LR
 a pure LR automaton but anyway it uses exactly the same kinds of items with this
 pair of a of a standard LR0 item in a look_ahead If you look_at that automaton you
 will see items that look like this and that will help you in reading the
 automaton and figuring out what it is doing
 In this video_were gonna to work through a couple of SLR_parsing examples So lets
 do a very_simple example Lets consider the grammar S_goes to SA or S_goes to B
 And what does this grammar do It produces strings of As followed_by a B So any
 number of As followed_by a single B And notice that the grammar is left recursive
 and recall that thats not a problem for a bottom_up parser Slr parsers LR parsers
 are perfectly happy with left recursive grammars So lets begin by working out
 what the automaton for this grammar should be what the parsing_automaton
 should be And recall that the first step is to add a new production to the grammar
 We have to add a new start_symbol That all it does it has one production that
 goes to the old start_symbol And thats again just for technical reasons Now
 the start_symbol or sorry the start_state of the NFA of the parsing_automaton
 is this item S_prime our new start_symbol goes to dot S our old start
 symbol And rather_than build the NFA and then do the subset of states construction
 Lets just go ahead and work out what items must be in the first state of the
 DFA So remember that all the epsilon_moves in the in the DF in the NFA are
 due to moves that happen because we dont see a non_terminal on the stack But
 instead see something_derived from that non_terminal So if we have a dot Right
 next to a non_terminal That_means that theres an epsilon move in the NFA to all
 the items that have for all the productions all the all the first items
 for the productions of that non_terminal What do I_mean by that I_mean that this
 state I_mean epsilon production to S_goes to dot SA So this is the first item in
 recognizing this production So the dots all the way at the left And there would
 also be an item for the other production for S S_goes to dot B Alright so thats
 the epsilon_closure in the NFA of this start item So thisll be the first state
 These three things these three items would be the first state of the DFA And
 now we have to consider what would happen on each of the possible transitions for
 each of the symbols that we might see on the stack So lets think_about what
 happens if we see a B So if we see a B on the stack then the only item thats going
 to be in that state is S_goes to B dot okay So itll be fine to see a B and this
 would be the only item that was valid for the stack_contents Now another
 possibility is that well see an S So if we see an S on the stack what will
 happen Well were_going to go to a state that has two items S_prime goes to S dot
 so that weve_seen S on the stack and were_ready to reduce by by this
 production possibly And also S_goes to S A And now Clearly in this state lets
 talk_about his state down_here There_are no more transitions possible In all there
 is only one item in the state dots all the way at the right_hand side so that state
 is completely done In this state the one over_here on the right side While one of
 these items is complete the dots all the way at the right But the other item still
 has an A so there could be one more transition out of this state To the item
 S_goes to SA dot Alright And now if we look_at this we see that for the most
 part these states are in pretty good shape So these two states this one down
 here and this one over_here they only have a single item and so theres_no
 possibility of a shift_reduce conflict in those states Theres only one item
 theres only one thing to do The only possibility here in both of these states
 is to reduce This state the initial start_state has no reduce_moves So its
 only shift_moves here so there cant be a shift_reduce conflict because there are
 no reduce items No possible reduce actions And there is to reduce reduce
 conflicts for the same reason The only state of interest really for the point of
 view for what who the grammar is SLR1 is this middle state And here we can either
 reduce by s_prime goes to s dot or we could shift and A onto the stack And the
 question is what is in the follow of S_prime So what can follow S_prime in the
 grammar And if we look back up at our grammar well see that nothing can follow
 S_prime S_prime is the start_symbol and so in fact the only thing in the follow
 of S_prime is the And to the input And so what that tells_us is that well reduce
 by s_prime goes to s if if were out of input And otherwise if there is an A on
 the stack sorry if theres an a in the input then well shift it onto the stack
 And so this grammar is SLR1 There_are no shift_reduce or reduce reduce conflicts
 implied by this parsing_automaton Lets do another example slightly_more complex
 In_fact lets just extend the previous grammar Well have a a production S
 goes to SAS okay So now we have the non_terminal twice with an A in between Or S
 can go to B just like before And now lets work out the parsing_automaton for
 this grammar And once_again Well need to add a dummy start_symbol To the grammar
 And it will go Its only production will be to generate the old start_symbol And
 now lets begin working out whats in the parsing_automaton for this particular
 grammar And and just like before were not going to go_through the effort of
 constructing the NFA That would be a systematic way to do it One way to it is
 is the way we sketched Was just to construct the NFA first and then do the
 subset of states construction But this grammar is small enough And simple enough
 that we can work out directly what is in what are in the states what items are in
 the states of DFA So just like before because the dart here is immediately next
 to the S we know that we can without consuming any input at all make an epsilon
 transition in the interface to the items that start the productions for S So these
 will be in the also in the DFA state And thats it We cant add any_other
 productions here So S is the only non_terminal And weve added all the first
 items initial items for S And so that is the complete state Okay So just like
 before one possibility is that well see a B on the stack And so that would give
 us the item S_goes to B dot And thats the only item valid for that state
 Another_possibility is that well see an S on the stack Okay In which case well
 make a transition to the state S_prime goes to S dot And S_goes to S dot AS
 alright So we saw that same state before in the in the other automaton Now we
 could also see an A Now what state would that take us to And this is going to be a
 little different In this state we could have the item or will have the item SA
 dot S and I notice that the dot is right next to S so instead of seeing an S on
 the set we could also see something_derived from S in the next position on
 this stack And so we have to throw in all the productions for S Theres only two of
 them But that means we could have the item S_goes to dot SAS and S_goes to dot
 P Alright and then out of this state now there are a couple of different
 possible transitions we could see an S or we could see a B Well if we see a B
 then we wind_up in this state over_here And if we see an S Well whats going to
 happen If we see an S then well wind_up in another new state Where we have S
 goes to SAS dot Weve seen the complete right_hand side of that production Or S
 goes to SAS Actually that dots in the wrong place so lets erase that and
 lets put it in the right place Its right here Before the A not after the A
 Alright and now we have to think_about what_happens in this state So in this
 state the only possible input is an A and if it isnt A whats we going to have
 were_going to have S_goes to SAS and then were_gonna have to add the initial
 productions for S again And so that would just take us back to this state and like
 other transition labels too we go to this state on an S and we come_back to that
 state the bottom state here for the top state on an A And I_think if we hadnt
 made any mistakes that that is the complete transition system and all the
 states for this DFA Now the question is is this is this parsing_automaton is it
 this is is this the parsing_automaton of a a solar one grammar And in order to
 answer that question we have to look for possible reduce reduce and shift_reduce
 conflict Well a quick scan of all the states here will show you or convince you
 that there are not Any states where there are two possible reducemoves So
 there cant be any reduce reuse conflicts in this in this automaton We can ignore
 states that only have a single item or states that have no possible reducemoves
 at all Because those are states in which there cannot be a shiftreduce conflict
 and that means we can ignore these two states The two states over_here at the
 extreme left So now were left with these three states to think_about Alright so
 we look_at this state last_time As before the follow of S_prime Is just
 equal to the dollar_sign And so theres_no shift_reduce conflict in this state
 Because on on input A we can only shift We cant reduce by S_prime goes to S All
 right and now were down looking_at these two states And lets just consider this
 bottom state first Alright so what does this state say to do Well this state
 says that well first of all observe That the only transitions out of this
 state are on B and S and there are no reduced_moves in this state at all so
 theres_no possibility of a shift_reduce conflict in this state either That leaves
 us with just this state to think_about Now this state does have a reduced move
 the first item here is a is a reduction and that says that we should reduce by S
 goes to S A S if whatever comes next is in the follow of S so were_gonna need to
 know whats in the follow of S Well from S_prime goes to S we know that anything
 thats in the follow of S_prime is in the follow of S So clearly dollar is in the
 follow of S And then from this part of the grammar here we can see that A is in
 the follow of S And then from this occurrence here of S we know that since
 it occurs at the the far right side of the production that an ything in the
 follow of the right_hand side the left_hand side non_terminal is also gonna be
 in follow of S Well in this case theyre the same It just says that the follow of
 S is a subset of the follow of S which is trivially always true and so it doesnt
 add anything new And so we wind_up with just the follow of S being just two
 things dollar_sign and A But that poses a problem because this says that if we
 see an A in the input we should reduce And this move here says that if we see an
 A in the input we should shift And so this state does have a shiftreduce
 conflict Alright and so this grammar is not SLR what
 Welcome_back In this video_were going to give a very brief introduction an
 overview of what were_going to be talking_about in semantic_analysis Lets
 take a moment to review where we are in our_discussion with compilers So we
 talked_about lexical_analysis and from the point of view of enforcing the
 language definition the main job that lexical analyses does is detect input
 inaudible preemptive symbols that arent part of our language The next step is
 parsing We finished talking_about that too And again from the point of view of
 trying to determine whether a program is wellformed or not or whether its a
 valid program the job of parsing is to detect all the sentences in the language
 that are illformed or that dont have a parse string And finally What were_going
 to talk_about now whats going to occupy us for a while is semantic_analysis And
 this is the last of what are called the front end phases So if you think of
 lexical_analysis parsing and semantic_analysis as filters that progressively
 reject more and more input strings until finally youre left after all three phases
 have run with only valid programs to compile well semantic_analysis is the
 last line of defense Its the last one in that pipeline and its job is to catch all
 potential remaining errors in a program Now you might ask yourself why do we even
 need a separate semantic_analysis phase And the answer to thats very_simple
 There_are there are some features of programming_languages some kinds of
 mistakes you can make that parsings simply cant catch Parsing well use in context
 free_grammars is not expressive enough to describe everything that were_interested
 in in a language definition So some of these language constructs are not context
 free And the situation here is very very_similar to what it was when we switched
 from lexical_analysis to parsing Just like not everything could be done with a
 finite inaudible And we wanted to have something more Our context_free grammar
 to describe additional features of our programming_languages inaudible Grammars
 by themselves are also not enough and there some additional features beyond
 those that cant be easily expressed using context_free constructs So what does
 semantic_analysis actually do In a case of cool C it does checks of many
 different_kinds and thats pretty typical So heres a list of six classes
 of checks that are done by Cool C and lets just run through them quickly First
 we want to check that all identifiers are declared and we also have to check that
 any scope restrictions on those identifiers are observed Cool C compiler
 has to do type_checking and this is actually a major function of the semantic
 analyzer in Cool There_are a number of restrictions that come from the object
 oriented nature of Cool We have to check that the inheritance relationships between
 classes make_sense We_dont want classes to be redefined we only want one class
 definition per class Similarly methods should only be defined once within a
 class Cool has a number of reserved identifiers and we have to be careful that
 those arent misused And this is pretty typical lots of languages have some
 reserved identifiers with special rules that have to be followed for those
 identifiers And actually this list is not even complete There_are a number of
 other restrictions And well be talking_about all of those in future_videos The
 main message here is that its medic analyzer needs to do quite a few different
 kinds of checks These checks will vary with the language The kinds of checks
 that cool C does are pretty typical of statically typed checked object_oriented
 languages But other families of languages will have different_kinds of checks
 Welcome_back In this video_were gonna begin our_discussion of semantic_analysis
 with the topic of scope The motivating problem for talking_about scope is that we
 want to be_able to match identifier declarations with the uses of those
 identifiers We need to know which variable were talking_about when we see
 variable X if variable X might have more_than one definition in the program And
 this is an important aesthetic analysis step in most programming_languages
 including inaudible So here are a couple of examples taken from cool This
 definition Y this declaration Y that its a string will be matched with this used
 and so well know at this point here that Y is supposed to be a string and youll
 get some kind of an air for a compiler because youre trying to add a string and
 a number In the second example Heres a declaration of Y And then in the body of
 the inaudible we we dont see any use of Y And that by_itself is not an error
 Its perfectly fine to declare a variable that you dont use Although you could
 imagine generating a warning for that that doesnt actually cause the program to
 behave badly But instead what we see here is a use of X and theres_no matching
 definitions So the question is where is the definition of X We cant see it And
 if there is no outer definition of X then well get and undefined or undeclared
 variable error here at this point So these two examples illustrate the idea of
 scope The scope of an identifier is that portion of a program in which the
 identifier is accessible And just know that the same identifier may refer to
 different things and different parts of the program And different scopes for the
 same name cant overlap So whenever the variable x for example means it can
 only refer to one thing in an given part of the program And identifiers can have
 restricted scope There_are lots of examples Im sure youre familiar_with
 them of identifiers whose scope is less_than the entire program Most programming
 languages today have what is called static s cope And cool is an example of a
 statically scoped language The characteristic of static_scoping is that
 the scope of the variable depends only on the program_text not on any kind of
 runtime behavior So what the program actually does at runtime doesnt_matter
 The scope is defined purely syntactically from the way you wrote the program Now it
 may come as a surprise that there is any alternative to static_scoping In_fact
 probably every language that you have used up to now has had static_scoping But there
 are a few languages that are what are called dynamically scoped And for a long
 time actually there was an argument about whether static_scoping was better_than
 dynamic scoping Although today I_think it is pretty clear that static_scoping
 camp has has won this discussion But historically at_least LISP was an
 example of a dynamically scoped language And it has switched in the meantime This
 is actually a long_time ago now that it changed to static_scoping Another
 language which is now mostly of historical interest it isnt really used anymore
 called Snowball also had dynamic scoping And the characteristic of dynamic
 scoping is that the scope of a variable depends_on the execution behavior of the
 program So lets_take a look_at an example of inaudible So here we have
 some inaudible code and a couple of different declarations of X and also
 some different uses of X Let_me erase these inaudible underline so I can use
 the color to indicate binding So lets_take a look_at this definition The
 question is which of these uses of x we have three uses of x actually refer to
 that definition So it is in fact these two the ones that are outside of the
 inner let These actually refer to this definition So here if you refer to x
 you get the value zero But this other definition here The inner definition of
 x is is used by this use of x So this use of x gets this va this meaning of
 x which in this case returns the value one And whats going on here is that
 were using the most closely whats called the most closely nested rule So a
 variable binds to the definition that is most closely enclosing it of the same
 name So this x the closest enclosing definition of x is this one but for these
 two xs the closest and only enclosing definition of x is this outer one So in
 dynamically scoped language a variable would refer to the closest binding in the
 execution of the program meaning the most recent binding of the variables so heres
 an example lets_say we have a function G and G defines a variable A and heres
 it initialized say to four and then it calls another function Another function
 that isnt in the same syntactic scope So here Ive written F right next to G but
 actually F could be in some completely other part of the code and F refers to A
 And the question is what is the value of A here Well If its if we dynamically
 scoped then its going to be the value that was defined in G and here F of X will
 actually return four that will be the result of this call because this reference
 to A Well refer to this binding or this definition of A and G And we cant say
 much_more about how dynamics how dynamics scope works until we talk in a little more
 detail about how languages are implemented So well talk_about dynamic
 scope again a little later_on in the course In Cool identifier bindings are
 introduced by a variety of mechanisms Now there are class declarations which
 introduce class names Method definitions which introduce method names And then
 there is several different ways to introduce object object identifiers And
 these are the inaudible expressions inaudible parameters of functions
 attribute definitions in classes and finally in the branches of case
 expressions Its important to understand that not all identifiers follow the most
 closely nested rule that we outlined before So for example a rather rather
 large exception to this rule is class definitions in Cool So class definitions
 cannot be nested And in fact they are globally visibl e throughout the program
 And what does that mean That_means that a class name is defined everywhere If its
 defined anywhere in the program that class name is available for use anywhere
 in the program or everywhere in the program And in particular a class name
 can be used before it is defined So as an example take a look_at this fragment of
 cool code here And here we see that in class foo we declare y to be of type var
 and then later_on we declare class var This is perfectly fine cool code The fact
 that var is used before it is defined has no effect on whether the program is
 correct This is a completely legal cool code Similarly with attribute names
 Attribute names are global within the class in which they are defined so I
 that means they can be used again before they are defined So for example I can
 define a class foo and I can define a method that uses attribute a and then
 later_on only later_on do I define what attribute a is and that is perfectly
 legal So normally The list attribute definitions before method definitions but
 thats not required A actually the method and attributory definitions can come in
 any order we like within a class and inparticular an attribute can be used
 before it is defined Finally method names have quite complex rules For example a
 method doesnt have to be defined in the class in which it is used It could just
 be defined in some pairing class And also methods can be redefined So its possible
 to whats called overwriting of a method and give a method a new definition Even
 though it has been defined before We_dont actually have the language yet to
 talk_about these rules with any precision but well be going into this in future_videos
 In this video_were going to talk_about simple tables an important data_structure
 in many filers before we talk_about what a simple table is I want to talk_about a
 generic algorithm that were_going to be seeing instances of over and over again
 for the rest of the course So a lot of semantic_analysis and in fact a lot of
 code_generation can be expressed as a recursive_descent of an abstract_syntax
 tree And the basic_idea is that in each_step we do the following three things
 were always processing a particular node in the tree so if I draw a picture of the
 abstract_syntax tree it might have a node and some subtrees hanging off of it And
 we may do some processing of the node before we do anything else We arrive at
 the node say from the parents we come to here for the parent we do some processing
 in the node and Im just indicating that by coloring it blue to indicate that we
 did something here And then we go off and we process the children Okay And after
 we process the children after we come_back to the node we do something else We
 may do some post processing of the node and then we return And of course at the
 same time when weve gone off and processed the children then were
 processing all their nodes in the same pre imposed fashion so theyre getting the
 same treatment with some stuff being done before each node is touched and some
 stuff being done after all their children have been processed Okay And There_are
 many_many examples of this kind of an algorithm This is called a recursive
 descent traversal of a tree There_are some instances in which well only process
 each note before we process the children Some where we only process each note after
 we process all the children Some where we do both as illustrated here in this little
 diagram And returning to the main topic of this particular video When were
 performing semantic_analysis on a portion of the abstract_syntax tree were_going
 to know need to know which identifiers are defined Which identifiers are in
 scope An exam ple of this kind of recursive_descent strategy is how we can
 process let bindings to track the set of variables that are in the scope So we
 have our let node in the fx syntax_tree and in one sub tree we have the
 initialization and in the other sub tree we have e the body of the let and then
 this is a let for some particular variable and lets just write that
 variable inside the parent node here And so when we begin processing of this O
 just imagine that were coming from above So were doing this Were processing the
 abstract_syntax tree recursively And so we reach this point from some parent and
 Theres going to be a set of symbols that are currently in scope That thats some
 data_structure that lives off to the side And in fact thats going to be our symbol
 table And what is going to happen here Well the first were_going to have to do
 is were_going to have to process the initializer Were_going to need to know
 whether thats what whatever function were doing on this like type_checking or
 whatever We might get on and process that first And well pass the symbol table in
 Okay And then were_going to process the body of the let But when we do that were
 going to pass in a set of symbols that are in scope But now also X is now going to
 be in scope So X is going to be added before we process E to the set of symbols
 And then when we return from some expression E its going to be removed So
 itll restore the symbol table to its previous state So that after we leave
 this sub tree of the abstract_syntax tree we only have the same set of symbols to
 find that we had before we entered it So in the terminology of the three part
 algorithm For recursive_descent that we had on the first slide What are we doing
 here Well before we process E we are going to add the definition of X to our
 list of current definitions Already any_other definition of X that might have been
 visible outside of that expression Then we are going to recurse we going to
 process all Of the abstract_syntax tree no des in the body of the inaudible
 inside of E and after we finish processing E we are gonna remove the
 definition of X and restore whatever old definition of X we had And a symbol table
 is just a data_structure that accomplishes these things It tracks the current
 bindings of identifiers at each point in the abstract_syntax tree For a very
 simple simple table we could just use a stack and it would have just say the
 following three operations we could add a symbol To the symbol table and that will
 just push the symbol push the variable onto the stack and whatever other
 information we want like its type Well have a find symbol operation that will
 look up the current definition for a symbol And that can be done by simply
 searching the the stack Starting from the top for the first occurrence of the
 variable_name And this will automatically take care of the hiding of all
 definitions So for example If we have a stack lets_say has X Y and Z on it and
 then we come into a scope that introduces a new Y Y on top and now if we search the
 stack we find this y first effectively hiding the old definition of y and then
 When we leave a scope we can remove a symbol simple popping a stack Well just
 pop the current variable off of this stack That will get_rid of the most
 recent definition And and leave the stack leave the set of definitions in the
 same state it was before we entered the node at all So this example if we left
 the scope where the Y is defined and that was popped off the stack So that was
 gone Now when we search for Y well find the outer definition The one that was
 defined outside of that inner scope So this simple symbol table works well for
 let because the symbols rate at one at a time and because declarations are
 perfectly nested And in fact the fact that declarations were perfectly nested is
 really the whole reason that we can use a stack So take a look_at this little
 example lets_say we have three nested lets and here Im not showing the
 initializers in the less sub trees and they they dont matter for what I want
 to illustrate So if you think_about it as we walk from the root here down to the
 inner bindings were pushing things on the stack well push things on the stack
 in the order X Y and then Z And then as we leave after weve processed this sub
 tree and were leaving it walking back out were_going to encounter these left
 scopes in exactly the reverse order And popping them off the stack is exactly the
 order in which we want to remove them and thats why a stack works well So
 Structure works fine for lets but for some other constructs its not quite as
 good as it could be so for example consider the following piece of code
 Illegal piece of code I should add Lets_say were declaring a method and it has
 two arguments named X Now thats not legal but in order to detect that its not
 legal you Why is it not legal Its not legal cause theyre both defined in the
 same scope So Functions or methods have the property that they introduce
 multiple names at once into the same scope And its not quite so easy to use
 a stack where we only add one thing at a time or one name at a time to model
 simultaneous definition in a scope So this problem is easily solved with just a
 slightly fancier simple table Here is the revised interface now with five methods
 instead of three The biggest change is that now we have explicit enter and exit
 scope functions and so these functions start in the nested scope and exit the
 current scope And the way you think_about this is that our new structure is a stack
 of scopes so inaudible is the entire scope and the inner scope Is are all the
 variables that are defined at the same level within that single scope So just
 like before we have a find symbol operation that will look up a variable
 name and it will return the current definition or null if there is no
 definition in any scope thats currently available Well have an add symbol
 Operation that adds a new symbol to the table and adds it in the curren t scope
 so whatever scope is at the top of our scope stack And then one more new
 operation check scope Will return_true if X is already defined in the current
 scope So this just to be clear what this does this returns_true if X is defined in
 exactly the top scope It_doesnt return_true unless X is defined in the scope at
 the very very top of the stack And this allows you to check for double
 definitions So for example in the code that I had before on the previous_slide
 if we had two declarations of X How would we check this Well we would add X to the
 symbol table in the current scope And then we would ask well is X already
 defined in this scope for the second one And this interface would be return_true
 and we would know to raise an error saying that X had been multiply defined
 Finally let_me just add that this is the simple table interface or something very
 close to this is the simple table interface that is supplied with the cool
 project And theres already implementation of this interface provided
 if you dont want to write your own So lets wrap_up this video by talking a
 little_bit about class names which behave quite differently from the variables
 introduced in let bindings and in function parameters In particular class names can
 be used before they are defined as we discussed a few_videos ago And what that
 means is that we cant check class names in a single pass We cant just walk over
 the program once And check that every class that is used is defined because we
 dont_know that weve_seen all the definitions of the classes until we reach
 the very end of the program And so there is a solution to this we have to make two
 passes over the program In the first pass we gather all the class definitions we go
 through and we find every place_where a class is defined record all of those
 names And in the second pass we go_through and look_at the bodies of the
 classes and make_sure they only use classes that were defined And the lesson
 here this is actually not complicated to implement I_think its quite clear Should
 be quite clear how this will work But the message here is that semantic_analysis is
 going to require multiple passes and probably more_than two And in fact you
 should not be afraid when structuring your compiler to add lots and lots of simple
 passes if that makes your life easier so its better to break something up into
 three or four simple passes rather_than to have one very very complicated pass where
 all the code is entangled I_think youll find it much easier to debug your
 compilers if youre willing to make multiple passes over the input
 Welcome_back in this video_were going to have an introductions to types So a very
 basic question to ask is what is a type anyway And this question is worth asking
 because the notion of type what a type is does vary from programming_language to
 programming_language Now roughly speaking the consensus is that a type is a set of
 values and also perhaps more importantly a set of operations that are
 unique to those values a set of operations that are defined on those
 values So for example if I looked_at the type of integers there are some
 operations that you can do on integers You can do things_like you can add And
 you can subtract integers and you can compare integers whether they are greater
 than or equal or less_than these operations are you_know about numbers and
 then there are operations on strings And strings are a different type They have
 Operations like incantation and testing whether a string is is an empty_string or
 not And the other various variety of functions that are defined on strings And
 the important thing that these operations are different from the operations defined
 on integers and we dont want to mix them up It would be bad if we started doing
 string operations on integers for example We would just get nonsense So in modern
 programming_languages types are expressed in a number of different ways In object
 orient languages Often we see classes being the notion of type So inaudible
 in cool the class names are the types theyre all the with one exception
 called self_type The class names are exactly the types And I just wanted to
 point out that this need not be the case It happens that its often convenient in
 inaudible Victorian languages to equate classes and types But there are other
 designs that where the classes are not the only kinds of types or whether
 theyre And in some languages inaudible where theres_no notion of
 class the types are completely_different things So classes and types are really
 two different things that happen to be identif ied in a lot of object_oriented
 designs And I just want you to be aware that thats not necessarily the only way
 to do it So consider the assembly_language fragment add R1 R2 R3 and what
 does this actually do Well it takes the contents of register R2 and the contents
 of register R3 it adds them together and it puts the results in register R1 And
 the question is what are the types of R1 R2 and R3 And you might hope that theyre
 integers but in fact this is a this is a trick question Because at the assembly
 language level I cant tell Theres nothing that prevents R1 R2 and R3 from
 having arbitrary types They could be they could be representatives of any kind
 of type and because theyre just a bunch of registers with zero and 1s in them the
 add operation will be happy to take them and add them up even if it doesnt make
 sense And produce a bit_pattern that then stores into R1 So to make this a little
 clear perhaps its useful to think_about a a certain operations that are legal for
 values of each type So for example it make perfect example to add two integers
 if I have two bit patterns that represent integers then when I sum them up I would
 get a bit_pattern that represents the sum of those two integers But on the other
 hand if I take a function pointer and integer and I add them together I really
 dont get anything Okay this is another the function pointer is a bit_pattern The
 imaginer s a bit_pattern I can take those two bit patterns I could run them through
 and I do get out a new set of bits But theres_no useful interpretation of that
 results The resulting things I get doesnt mean anything but the problem is
 that these both have the same assembly_language implementation Okay nothing at
 the assembly_language level these two operations look exactly the same So I
 cant tell at the assembly_language level which one of these Im doing If I want
 there to be types if I want to make_sure that I only do operations on the correct
 that I only do certain operations on on their correc t types then I need some
 sort of type description some sort of type_system to enforce those distinctions
 Perhaps Im belaboring this point but I_think its important so one more time A
 languages type_system specifies which operations are valid for which types Then
 the goal for type_checking is to ensure that operations are used only only with
 the correct types And by doing this type_checking enforces the intended
 interpretation of values because nothing else is going to check Once we get to the
 machine code level its all just a lot of 0s and 1s and the machine will be happy
 to do whatever operations we tell it to on those 0s and 1s whether or not those
 operations make_sense So the purpose of type systems is to enforce the intended
 interpretations of those bit patterns and make_sure that if I have a bit_pattern for
 integers that I dont do any noninteger operations on that and get something that
 is meaningless Today programming_languages fall into three different
 categories with respect to how they treat types There_are the statically types
 languages where all or almost all of the checking of types is done as part of
 compilation and Cool is one of these And other languages that youve probably seen
 like C and Java are also statically typed Then there are the dynamically
 typed languages where almost all of the checking of types is done as part of
 program execution And the Lisp family of languages like Scheme and Lisp itself
 are in this category as are languages like Python And pearl So youve probably
 used or heard of at_least some of those languages And finally there are the
 untyped languages where no type_checking is done at all either at compile_time or
 at run_time And this is basically what machine code does So machine code has no
 notion of types and it forces no extraction boundaries when it executes
 For decades there has been debate about the relative merits of static versus
 dynamic typing and without taking sides let_me lay out for you what the various
 proponents on each side say So the people who believe in static typing say
 that static checking catches many programming errors in compile_time and it
 also avoids the overhead of runtime type checks If I hooked on all the type
 checking and compiled time well I dont have to check the types at runtime I
 dont have to check when I go do an operation that the arguments are of the
 correct type because I already that check once and for all in compile_time And
 these things are both definitely true These_are the two big advantages of static
 checking First of all Proves that some errors can never happen those are caught
 at compile_time so I never have to worry_about those errors at run_time and its
 faster Dynamic typing proponents counter that aesthetic type systems are
 restrictive So essentially aesthetic type_system has to prove that the program
 is well typed that all the types makes_sense And it does this by restricting what
 kinds of programs you can write There_are some programs that are more difficult to
 write in an aesthetic type language because the compiler has a hard time
 proving them correct And theres also a belief that I I see commonly stated that
 rapid prototyping is more difficult with ecstatic type_system I_think the idea
 here is that if youre prototyping something if youre exploring some idea
 you may not actually know exactly what all the types are at that point in time And
 having to commit to something that is going to work in all cases You_know to
 having a type correct program when youre just trying to fiddle around and figure
 out what it is youre trying to do Thats very constraining and makes the work go
 quite a bit slower So whats the actual situation and practice today Well an
 awful lot of code is written in inaudible type languages And the
 practical inaudible type languages that people use a lot have always have some
 kind of escape mechanism So in C in Java in C you have some notion of
 unsafe cast In C an unsafe cast can just results in a runtime crash In Java it r
 esults in an inaudible runtime when you have an unsafe or failed downcast But
 the the effect is that you can get run_time errors for type reasons sound Now
 on the dynamic typing site the people who programming dynamic languages they always
 end up or seemed end up record fitting static typing to these dynamically typed
 languages So typically if a dynamically typed language because popular enough than
 people trying to write optimizing compilers for them and the first_thing
 that people want to have on an optimizing compiler Is some insta type information
 because it helps to generate much better code And so people wind_up going back and
 trying to figure_out how to get as many types as they can from these dynamically
 types languages as_soon as they start trying to build serious tools to improve
 the programs written in these languages And in my opinion its really debatable
 whether either compromise because both of these are compromises on the either strict
 static or strict dynamic point of view But if either one of these represents the
 best or the worst of both worlds But this is certainly where we are today in
 practice Now Cool is a statically typed language and the types that are available
 in Cool are the class names so every time you define a class you define a new type
 and the special reserve symbol SELFltugtTYPE which well be talking_about in a separateltugt
 video all of its_own And the way cool works is that the user declares the types
 for identifiers For every identifier you have to say what its type is But then the
 compiler does the rest of the work The compiler refers the type for expressions
 And in particular the compiler assigns a type to every single expression in in the
 program So it will go_through the entire abstract_syntax string and using the
 declared types for identifiers it will calculate a type for every expression and
 subexpression To wrap_up its_worth mentioning that theres a couple of
 different terms people use for the process of computer types and that they mean
 slight ly different things So the simpler problem is what is known as type_checking
 Here we have a fully typed program meaning we have an abstract_syntax free
 with all the types filled in on every node and our only job is to check That
 the types are correct so we can just look_at each note and its neighbors and confirm
 that the types are correct in that part of the tree And we can do this for every
 part of the tree and check that the Program is type correct Type inference
 on the other hand is the process of filling in missing type information So
 here the view is that we have an abstract_syntax tree with no types on it or
 perhaps just a few types in key locations on say on the declared variables and
 then we want to fill in missing types We have some nodes in there there with
 absolutely no type information at all and its not just a question of confirming or
 checking that the types are correct we actually have to fill in the missing type
 information And these two things are different Actually there are many
 languages that are actually very very different but people often use the terms
 interchangeably and will not be particularly careful in my videos about
 which term I am using either
 In this video_were going to talk_about type_checking in cool Thus far weve
 seen two examples of formal notation used to specify parts of a compiler Regular
 expressions were used in lexical_analysis and context_free grammars which we used in
 parsing It_turns out that theres another formalism which has gained widespread
 acceptance in type_checking and thats logical rules of inference Ifelse rules
 are logical statements that have the form if some hypothesis is true then some
 conclusion is true So ifelse rules are implication statements that some
 hypothesis implies some conclusion And in the particular case of type_checking an
 example or typical kind of reasoning that we see in type_checking is that if a
 couple of expressions have certain types then some other expression is guaranteed
 to have a certain type And so clearly that the type_checking statement here is
 an example of an inference rule An inference rule notation is just a compact
 way of encoding these kinds of if then statements Now if you havent_seen this
 notation before it will be unfamiliar but actually its quite easy to read with
 practice And well start with a very_simple system and gradually add features
 So well use a logical conjunction for the English word and and implication for the
 English word if and then And now one special thing the string x colon t is
 read that x has type t So this is logical assertion saying that x has a particular
 type So now consider the following very_simple type rule If E one has type int
 and E two has type int then E one plus E two also has type int And we could just
 take the definitions we gave on the previous_slide and just gradually reduce
 this to a mathematical statement So for example we can replace the if then with
 an implication And we can replace the word and with a conjunction And now we
 just have these has type statements alright And we had a notation for that
 and we wind_up with this purely mathematical statement that which says
 exactly the same_thing That if E1 has type int and E2 has type int that implies
 that E1 E2 has type int And notice that that statement that we just wrote
 out is a special case of an inference rule Its a bunch of hypothesis conjoined
 together and implies some conclusion The traditional notation for inference rules
 is given here The hypotheses are written about the horizontal line and the
 conclusion is written below And it means exactly the same_thing as what we had on
 the previous_slide Mainly that if all the things above the horizontal line are true
 These_are all the hypotheses then the thing below The horizontal line can be
 concluded to be true And theres one piece of new notation here This is the
 turnstiles that are used for the hypotheses and the conclusion And the
 turnstile is read it is provable that And what this means is that were just
 going to say explicitly That something is provable in the system of rules that were
 defining So the way you would read this is that if its provable that all these
 hypotheses are true So if its provable the first hypothesis is true all the
 middle hypotheses and if its improvable if its provable the last hypothesis is
 true Then it is provable that the conclusion is true And cool type rules are
 going to have The following kinds of hypothesis and conclusions were_going to
 prove within the system that some expression has a particular type So with
 those definitions out of the way we actually have enough to write at_least a
 few simple type rules So if I integer literally if its an_integer class
 interfering in my program then this rules says it is provable that I has type ENT
 So every integer constant has type ENT And heres the rule for add written out
 now in the inaudible rule notation If its provable then that E1 has type int
 and is provable that E2 has type ENT Then it is provable that E1 plus_E2 has type
 ENT So notice that these rules give templates for describing how to type
 integers and expressions The rule for integer constants just use a generic
 integer i It didnt give a separate rule for every possible integer and the rule
 for plus used expressions e one and e two It didnt tell you what particular
 expressions they were It just said give me any expression e one any expressions e
 one and e two that have type int And so we can plug any expressions we want in
 that satisfy the hypotheses and then we can produce a complete typing for actual
 expressions So as a concrete example lets show that one plus two has type ent
 So we want to type the expression one plus two and since we know the rule for add
 that means we need to construct a proof of the type of the number one and a proof of
 the type of the number two And we have a rule for dealing with integer classes
 mainly we can prove because one is an_integer class that has type ent and we
 can prove that two is type ent and then now we have the two hypothesis we need for
 the sum expression and we can prove that one plus two has type ent So an
 important_property of any reasonable type_system is that it be sound And sound is
 here is a correctness condition What we want is whatever the type_system can prove
 that some expression has the type systems has a particular type T The if I actually
 run that program If I take E and I execute it on the computer the value that
 it returns the value that comes_after running E in fact has the type predicted
 by the type_system So if the type_system It is able to give types of things that
 actually reflect what kind of value you get when you run the program then we say
 that the type_system is sound Now clearly we only want sound rules but some sound
 rules are actually quite a bit better_than others so for example If I have an
 integer literal And I want to give it a type while we we I_showed you the best
 possible rule before where we said that inaudible has type inaudible But it
 would also be correct just not very precise to say that has inaudible has
 type object Certainly if I evaluate an_integer I will get back an object because
 every integer in is also an object But this isnt all that useful because now I
 cant do any of the integer operations And so there are lots of different sound
 rules theres not just one unique rule for any given inaudible expression that
 will be sound but some of them are better_than others and in the case of integer
 literals the one we really want Literal has type it because that is the most
 specific type that we can give to that type of program
 In this video_were gonna continue_our development of Cool type_checking with a
 discussion of type environments Lets begin by doing some more type rules So
 heres one for the constant false So its provable that the constant false has the
 type inaudible and thats not surprising If I have a string literal S
 then its provable that that has type string And thats also not very
 surprising The expression new T produces an object of type T And the type rule for
 that is very straightforward New T has type T And were just going to ignore
 self_type for now As I mentioned in an earlier video well deal_with self_type
 later in a video all on its_own Here are a couple of more rules If its provable
 that an expression e has type bool then an e Boolean complement of e not e also has
 type bool And finally perhaps our most complex rule so_far the rule for a while
 loop and we call that the e1 here is the predicate of the loop this is what
 determines if we keep executing the loop or not and e2 is the body of the loop
 And so type one is required to have type bool It needs to be provable that e one
 had type bool and we allow e two the body of the loop to have an_arbitrary
 type It can have any type t It has to have some type so it has to be type able
 under some Rules but we dont care what the type is because the type of the entire
 expression is just object We_dont actually return the this expression
 doesnt return an interesting value doesnt produce an interesting value and
 to discourage people from trying to rely on it we just type the whole_thing as
 object And this is a little_bit of a design decision Now we could have
 designed a language for example where the type of a while_loop is was type t
 And that you would get the last value of the loop that was that was executed but
 the problems is that if E one the protocol loop is false and reaches the loop the
 first time Then you never evaluate e two and no value is produced and in that case
 you would get a a void value Which if so mebody tried to dereference it would
 result in a run_time error Thats so to discourage programmers from lying On the
 loop producing a meaningful value We could just type it as object So far
 its been pretty straight_forward to define reasonable type rules for every
 construct that weve_looked at But now we actually come to a problem Lets_say we
 have an expression which consists just of a single variable_name and thats a
 perfectly valid cool expression and the question is what is the type of that
 variable call it X And as you can see When were just looking_at X by_itself we
 dont have enough information to give X a type This local structural rule does not
 carry any information about the type of X And stepping back one level inference
 rules have the property that all the information needs to be local Everything
 we need to know To carry out the function of the rule has to be present in the rule
 itself There_are no external data_structures Theres nothing were passing
 around here thats on the side Everything has to be encoded in this rule and so_far
 at_least we just dont_know Enough to say what the type of a variable should be So
 the solution to this problem is just to put more information in the rules and
 thats what were_going to do so a type environment gives types for free
 variables So what is a free variable a variable is free in an expression if it is
 not defined within that expression So for example in the expression X X is free
 In the expression x plus y (xy) well here this expression uses both x and y
 and theres_no definition of either x or y in that expression so x and y are free
 And that expression If I have let Y So Im declaring a variable Y in X Y
 Well whats free in this expression well this expression uses X and Y but the use
 of Y is governed by a definition of Y that occurs within the expression itself So we
 say here that Y is bound Y is a bound variable in this expression but X is still
 free so only X is free in that expression And the ide a here is that if
 I have an expression with three variables and you want me to type_check it you have
 to tell me what the types of those variables are So I can type_check X if
 you tell me what the type of X is I can type_check X plus Y if you tell me the
 types of X and Y And I can type_check this expression this line expression if
 you tell me the type of its one free variable X the type of Y We will be
 given a declaration by the let but we still have to tell me what the type X is
 So the free_variables are just those variables where you have to give me the
 information and then I can carry out the type_checking The type environment
 encodes this information so a type environment is a function from object
 identifiers from variable_names to types So let O be a type environment One of
 these functions from object identifier names types And now were_going to extend
 the kinds of logical statements that we prove to look like this And the way that
 this going to be read is that under the assumptions that variables have the types
 given by O So the assumptions go over_here on the left side of the turnstile
 These_are the assumptions that were making about the free_variables in E So
 the assumption that excuse_me three variables Have the types given by o is
 provable thats this turn style here that the expression e has type t And so
 this notation very nicely separates what were assuming This is input to our
 process of figuring out what the type is from what were proving So if you tell me
 the types of the free_variables as given by o then I can tell you the type e The
 type environment has to be added to all the rules that weve gone through so_far
 So for example for intergal literals if I have some set of assumptions of all the
 types of variables that doesnt really change it doesnt in fact it doesnt
 change what the type is an intergal literal Any intergal literal will still
 have type int And so in this case for this particular kind of expression I we
 dont use any of our assumptio ns about the types of variables Now its a little
 bit different with the case of sum expressions So if I have the expression E
 one plus E two and I have some assumptions zero about the types of
 variables well then I want to prove that E one has type int and Im gonna do that
 using the types of the variables given by zero so E one might contain free
 variables and Ill need to look in zero to figure_out what the types of those
 variables are And similarly for E two I will type E two under the same set of
 assumptions And if E1 has type int under the assumptions O and E2 has type int
 under the assumptions O Well then I can conclude that E1 plus_E2 has type int
 under the same set of assumptions O And we can also write new rules so now our
 big problem with free_variables becomes a very easy problem If I want to know what
 the type of X is and theres a missing O here if I want to know what the type of X
 is I simply look it up in my object environment So under the assumption that
 the variables have the types given by O what is the type of x Well I look up in
 O what the type of X is assumed to be and I then can prove that X has that type T
 So now lets_take a look_at a rule that actually does something interesting with
 the variables from the point of view of the environments So here is a inaudible
 expression And lets remind ourselves what this does This is a inaudible
 expression that has no initialization So it says that X is going to be a new
 variable Its going to have type_T0 and that variable is going to be visible in
 the sub expression E1 And so now how am I going to type_check that Well Im
 going to type_check E1 in some kind of environment And this is a new notation
 here so let_me define what it means So remember O is a function it maps a
 variable_names to types and OTX this notation here is also a function And what
 this is is the function O modified at the single point X to return T So in
 particular a this function this whole_thing here is one function this wh ole
 thing Im underlining here is a function that applied to X is Returns t So that
 says that this sort of assumptions says that x has type t and for any_other
 variable So I apply it to some other variable y where x is different from y
 Well then I just get whatever type y has in inaudible Okay So what this rule
 then says is that Im going to type_check E1 in the same environment O except that
 at point X its going to have the type_T0 So were_gonna change just the type of
 X that have type_T0 because thats the type of the new identifier thats bound at
 E1 And all the other types will be the same And using those assumptions Ill
 try to prove that E1 has some type I will get a type for E1 And then that will be
 the type of the entire let expression Now notice something about the type
 environment What this says is that before we type_check E1 we need to modify Our
 set of assumption Modify our type environment to include a new assumption
 about x then we type_check e one and then of course when we leave type
 checking e one were_going to remove that assumption about x that new assumption
 because outside of the let we just have the original set of assumptions though
 And so I hope that that terminology and that description reminds you of something
 that we talked_about earlier because this type environment is really implemented by
 the simple table So in our rules The type environment that carries around the
 information that will be stored or is typically stored in the symbol table of a
 compiler
 In this video_were going to talk_about sub_typing another important idea in cool
 and other object_oriented languages Lets begin by taking a look_at the typing rule
 for let with initialization So last_time we looked_at the let rule but didnt have
 the initializers Lets just see how adding the initializer right here changes
 things So whats going to happen here Well first of all notice that the body of
 the rule is almost the same So We type_check E1 in an environment where X has
 type_T0 The type is declared to have in the let And all the other variables have
 whatever types O gives them And we can add some type T1 and thatll be the type of
 the whole_thing So this piece Right here is exactly the same as before So whats
 new is this line of where we type_check the initializer And so how does that
 work Well first of all under the assumptions o we type_check e zero we get
 some type t zero And this is really an aside from the main point but notice that
 we use the environment o in particular x The new definition of X is not available
 in E0 so if E0 uses the name X that means it uses the name of some other X thats
 defined outside Of the lead because we didnt include a this definition of X in
 the environment for type_checking E0 All_right now but the main point a thing I
 want to point oh Im a sly is that easier or here has type zero which is exactly
 same type as X and thats a requirement of this rule it says that E0 has to have
 the same type as X and thats actually fairly weak of because its really a no
 problem if E0 has a type which is a subtype of T0 a T zero can hold any
 subtype of T zero that would be absolutely fine But here weve limited
 ourselves to only allowing initializers that exactly match the type of X So we
 can do better if we introduce the sub_typing relation on classes And the most
 obvious form of sub_typing is that if X is a class and inherits directly from from
 Y meaning theres a line in the code that says X inherits_from Y Then x should be a
 sub_type o f Y And furthermore this relationship is transitive So if x is a
 sub_type of y and y is a sub_type of z then x is a sub_type of z And finally as
 you might expect its also reflexive so every class is a sub_type of itself And
 using sub_typing we can write_out a better version of the let rule with
 initialization So once_again the body the the part of the rule that deals with
 the body of the let is exactly the same as before so lets not look_at that and
 Now what were_going to do is were_gonna type_check E0 and we get some type_T0 out
 and then T0 now is only required to be a subtype of T so this here is another
 hypothesis And it just says that T zero has to be a sub_type of T and what is T
 well T is now the type that X is declared to be So this allows E zero to have a
 type thats different from the type of X and the only issue here is that more
 programs will type_check with this rule in the previous one The previous rule that
 we had was certainly correct any program that compiled with that rule would run
 correctly but this is a more permissive and still correct rule More programs will
 compile and type_check correctly using this rule Subtyping shows up in a number
 of places in the cool type_system Heres the rule for assignment which is in many
 ways similar to the rule for let So how does an assignment work well on the left
 hand_side is a variable and the right_hand side is an expression were_gonna
 evaluate the expression and assign whatever value we get back To the
 variable on the lefthand_side And so what how is this typechecked Well first of
 all we have to look up the type of X in the environment and we discovered it has
 some type_T0 And then we typecheck E1 in the same environment So the set of
 variables here is not changing And so we typecheck E1 environment O and we get
 some type T1 And now what has to be true for this assignment to be correct Well it
 has to possible for X to hold the value of type T1 So Xs type_T0 has to be a super
 type has to be bigger_than the type of T1 So if this inaudible Is satisfied
 then the assignment is correct Another example that uses subtyping is the rule
 for attribute initialization which except for the scope for identifiers is very
 very_similar to the rule for normal assignments So recall what a class looks
 like you can declare a class in Cool and it has at the top level some set of
 attributes and methods And what does an attribute Definition look like Well it
 looks_like one of these things Its a variable declared to some type and you can
 have an initializer on the right_hand side And so in what environment then is
 this initializer type checked Well its type checked in this special environment O
 sub c which just consists of the types of all the attributes that are declared in
 class c So this mean we have to make a pass over the class definition pull out
 all the attribute definitions all the names of the variables and their types
 build an environment That inaudible all that information and then we can type
 check the initialize rs because remember the initializer for an attribute can refer
 to any of the initialize rs for the class So lets_take a look_at how this works
 First we look up the type of X in the environment Thats sum type T O Now we
 type_check E1 in the same environment Thats sum type T1 And then just as with
 assignment T1 needs to be a subset or a subtype of the type T O Now we come to
 another interesting example how we type_check If and Else And the important thing
 about If and Else is that when were doing type_checking we dont_know which branch
 is going to be taken we dont_know whether the program is going to execute E1
 or E2 and in general actually this statement may or If this expression may
 execute multiple times doing a run of the program in sometimes it may execute only
 one other times it may execute it two And so what that means that the resulting
 type of am If and Else is either the type of E1 or the type E2 and we dont Know a
 compile_time which one is going to be So the best we can do Is to say the type
 of entire if then else is the smallest super type larger than either e1 or e2 The
 need to compute an upper_bound over two or more types comes up often enough that
 were_going to give the operation a special name Well call it the LUB or
 least_upper bound of X and Y And the least_upper bound of X and Y is going to
 be Z if Z is an upper_bound so meaning its bigger_than both X and Y and also if
 it is the least among all possible upper bounds So what this line here says is
 that there is some other Z prime thats bigger_than X and Y Well then z has to be
 smaller than z prime So z is the least if z smallest of all the possible upper
 bounds of x and y And in Cool and in most object_oriented languages the least
 upper_bound of two types is just their least common ancestor in the inheritance
 tree So typically the inheritance tree is rooted at object or some similarly
 named class that incorporates that includes all possible classes of the
 program And then theres some kind of a hierarchy which is a tree That descends
 from object and and if I want to find the least_upper bound of two types say
 this type and this type I just have to walk back through the tree until I find
 their least common ancestor And so in this case if I pick these two types out
 of my tree this would be the least_upper bound of those two types
 In this video_were going to continue_our discussion of type_checking and cool with
 the rules for type_checking methods and method calls So heres the situation we
 want to type_check a method call lets_say that we have a dispatch on some
 expression easier and were calling some method named F and we have some arguments
 E one through E N Well so clearly were_gonna type_check E zero its gonna have
 some type E zero and similarly were_gonna type_check all of the arguments and
 theyre gonna have some types and then the question is what is the return type
 of this method call what value what kind of value do we get back after we call this
 method And as you can probably see were in a very_similar situation here that we
 were in before when we were typing check the variable reference We have this name
 F and we dont_know anything about what it does we dont_know the behavior of F is
 unless we have some information about Fs behavior we cant really say what kind of
 value it is going to return An added wrinkle in cool is that method object
 identifiers live in different name spaces That is it is possible in the same scope
 to have a method called foo and also an object called foo and we wont get them
 confused They are different enough and used differently enough in the language
 that we can always tell when were talking_about the object foo and when were
 talking_about the method foo But what this means in effect is that theres two
 different environments One for objects and one for methods and so in the type
 rules this is going to be reflected by having a separate mapping a separate
 method environment thats going to record the signature of each of the methods And
 a signature as is a standard name that youll probably hear used in other
 contexts but the signature of a function is just its input and output types And so
 this table m is gonna take the name of a class Its gonna take the name of a
 method in that class and is just gonna tell_us what are the argument types of the
 methods So all but the last type in the list here is one of the arguing types of
 the method and then the last type is the result type Thats the type of the return
 value So the way we are going to write the method signature is just as a tutor or
 a list of types the first all but the last one taken together are the are the
 types of the arguments in order And then the very last one is the type of the
 result And so an entry like this in our method environment just means that f has
 a signature that looks_like this It takes in arguments with the respective types
 and it returns something of type t n plus one So with the method environment added
 to our rules now we can write a rule for dispatch So notice first of all that we
 have these two mappings one for object identifiers and one for method names on
 the left_hand side of the turnstile We have to propagate that method environment
 through all the typing for the sub expressions and for the case of method
 dispatch we just type The type of the expression were dispatching to e zero
 and all of the arguments and get types t one through t n and then we look up the
 type of f in the class t zero So what class are we dispatching to Well thats
 gonna be to the class of e zero And so where do we look up m in our environment
 Where there better be a method called F to find in class T0 and it must have some
 signature with the right number of arguments And then the actual arguments
 that were passing the E1 through EN theyre types have to be subtypes of the
 declared formal_parameter So here the signature of F Says that for example
 the first argument of f has type t one prime and so were_going to require that
 the type of e one be some type t one such that t one is a sub_type of t one prime
 And similarly for all the other arguments of the method call And if all of that
 checks out if that has a signature like this and all the sub_type requirements on
 the actual arguments and the formal arguments match then were_going to say
 that the entire_expression inaudible Return something of type t n plus one the
 return type of the method The typing rule for static dispatch is very_similar to the
 rule for regular dispatch So recall that syntactically the only thing thats
 different is that the programmer writes the name of the class at which they wish
 to run the the method So instead of running the method F as defined in the
 class E0 whatever that class happens to be were_going to run whatever that
 method F happens to be in some ancestor class of the class of E0 And how is that
 expressed in the type rules Well once_again we type E0 and all of the
 arguments And now we require that whatever the type was we discovered for
 E0 it has to be a subtype of T So T has to be an ancestor type in the class
 hierarchy of the type of E0 And moreover that class T had better have a method
 called F That has the right number of our units with the right kind of types such
 that all the type constraints work out that the actual argument types are sub
 types of the corresponding formal argument types and then if all of that is true
 well be_able to conclude that the entire dispatch expression has a type t n plus
 one which is the return type of the method
 In this video_were going to talk_about how one takes the type_checking rules and
 translates them into an implementation The high_level overview of cool type
 checking is that it can be implemented in a single traversal over the abstract
 syntax_tree And theres actually two phases here Theres the top_down phase
 in which the type environment is passed down the tree And theres a bottom_up
 phase in which the types are passed back up So we start at the root of the tree
 with an initial type environment this type of environment is passed down
 recursively through the various nodes of the abstract_syntax tree until we hit the
 leaves And starting at the leaves we use the environment to compute the types of
 each subexpression working our way back up the tree to the root Lets start our
 discussion of the implementation of cool type_checking with one of the simpler
 rules in the type_system the rule for addition And lets just briefly review
 what this rule says It says that the type_check E one plus E two we first have to
 type_check E one and then we have to type_check the sub expression E two And both
 of those sub expressions have to have type end And if they do then we can conclude
 that the overall_expression the sum of the two sub expressions also has type A
 And furthermore this type_checking is carried out in some environment In this
 case the environment is the same for the entire_expression and both sub
 expressions Just just to remind you theres always an object environment for
 the object names and scope a method environment for the methods of the various
 classes and we always need to know the current class Now how will we implement
 this Well we will have a recursive function called type_check It takes two
 arguments it takes an inaudible environment and this will be a record Im
 not specifying exactly how this record is declared but it is essentially going to be
 three parts m o and c And it also takes an expression and so here we are just
 doing the case here where the expression is E1 E2 And what should the code look
 like Well we can pretty much just read the rule and translate directly into code
 and this is one of the nice things about the notation for type systems is that it
 really tells you very very clearly how to write the implementation from the
 description So whats the first_thing we have to do Well we have to typecheck the
 sub expression E1 And we can see from the rule that the environment in which E1 is
 type checked is exactly the same as the environment of E1 plus_E2 So we just pass
 whatever our original environment argument was for E1 plus_E2 We pass an an
 argument on to a recursive call of the type_check to type_check the sub
 expression E1 And that typechecking will run and it will return some type T1 and
 we dont_know that T1 is an_integer at this point Were_gonna have to check
 that so we just remember what the type of E1 is And furthermore we type_check E2
 okay And that also happens in the same environment we can see that here in the
 rule And again well get back some type for E2 so type T2 And then we confirm
 that both T1 and T2 are type integer And we could have done a the track that T1
 is is int a right away right after we had the type_check T1 that would be a fine
 thing to do Here just to save space on the slide I have to clip the checks for T1
 and T2 a on one line And if that check succeeds If it doesnt succeed
 presumably there should be some code in here to print_out an error message But
 if that if both T1 and T2 are in fact integers than the type of the whole
 expression is also an_integer So thats whats returned by this call by the
 outermost call here to the type_check function So now lets_take a look_at a
 somewhat more_complex type_checking rule and its implementation Heres the rule
 for a net with initialization So were declaring a variable x of type t And
 thats going to be visible in the expression E1 But before we execute E1
 were_going to initialize X to the value of E0 And then after weve evaluated the
 entire let expression we expect to get back something of type T1 And now for all
 of that to work out A few things have to be satisfied and those are listed as
 premises here of the rule First of all E0 has to have some type_T0 which is a
 subtype of T And thats to guarantee that this initialization is correct that X can
 actually hold something of E0s type And for the entire_expression to have type T1
 well then E1 has to have type T1 But that type_checking is carried out in an
 environment thats extended with the declaration for X So we so we also know
 within E1 that X has type T So now lets write the typechecking case for this So
 the function type_check is again is gonna take an environment as argument and now
 were doing a case for a led with initialization So just reading of the
 the rules and what the conditions are that we have to check we can see that one of
 the first things we have to do or one of the things we have to do is to check that
 E zero has some type T zero So we just have a recursive call to type_check here
 This is carried out in the same environment as the overall_expression So
 we just pass the environment on to the recursive call And now were just type
 checking E zero and we record its type T zero So the second premise is implemented
 like this Now were type_checking E1 and we expect it to have some type T1 but now
 the environment is different so were taking the original environment the
 overall environment of the expression and were adding a declaration that X has
 type T to that environment So were extending the environment with an
 additional variable declaration Okay And so we do that type_checking call and we
 get back a type T1 Now we have to check that T0 is a subtype of T So thats a
 thats a call to some function that implements the subtyping relationship
 and if if that passes if that check passes well then were done And we can
 return the type T1 And theres a little mistake here on the slide there shouldnt
 be a semicolon there S o we just return T1 as the type of the entire_expression
 In this video_were gonna talk_about static typing versus dynamic typing One
 way to think_about the purpose of static types system is to prevent common
 programming errors and they do this at compile times So they do this when the
 program is compiled And in particular they do it without knowing any input to
 the program So the only thing that is available is the program_text and thats
 why we call them static Because they dont involve any of the dynamic behavior the
 actual execution behavior of the program Now any type_system that is correct any
 static_type system that actually does the right thing is going to have to disallow
 some correct programs It cant reason completely precisely at compile_time about
 everything that could happen as the program runs Now what this means is that
 some correct programs by that I_mean some programs that would actually run correctly
 if you executed them are going to have to be disallowed by the type_checker And so
 for this reason some people argue for a dynamic_type checking instead and this is
 type_checking thats done solely when the program runs So at run_time we check
 whether the actual operations were executing are appropriate for the actual
 data that arises when the program executes Other people say well the
 problem is really just that the type systems just arent expressive enough and
 we should work on fancier static_type checking systems And Over time theres
 been a considerable development in both camps We see a lot of new dynamically
 type checked languages coming out so a lot of the modern scripting like languages
 and domain specific languages have some form of dynamic_type checking Other
 people have been working_on fancier and fancier type systems and actually theres
 been a lot of progress in static checking The disadvantage of the more expressive
 text Time checking systems they do tend to get more_complicated though and not
 all of these features that these people have develop have actually found their way
 yet into main stream languages Now one important idea that this discussion
 suggests is that there are two different notions of type There is the dynamic
 type That is the type that the object or the value that were talking_about
 actually has at run_time And then there is the static_type which is the compile_time
 notion what the type_checker knows about the object And there is some relationship
 that has to exist between the static_type and the dynamic_type if the static_type
 checker is to be correct In this relationship can be formalized by some
 kind of a theorem that proves something_like the following what wed like to know
 is that for every expression E for every program expression E that you can write in
 the programming_language the static_type that the compiler says that the the
 expression is going to have is equal to the dynamic_type of that expression
 Another way is saying that if you actually run the program Then you get something
 that is consistent_with what you expected to get from the static_type checker That
 the static_type checker is actually able to correctly predict what values will hap
 will will arise at run_time And in fact in the early days of programming_languages
 these were exactly the kinds of terms we had for the very_simple type systems in
 the languages at that time Now the situations a little more_complicated for a
 language like COOL So lets_take a look_at the execution of a a typical COOL
 program So heres a couple of classes class A and a class B that inherits_from
 A So B is going to be a subtype of A which well write like that And now we
 have a declaration here of X having type A and this is the static_type of X So the
 static_type of X is A And thats what the compiler knows about Xs value And then
 here when we execute this line of code we can see that we assign a new A object
 to X And the fact that its new is not important All thats important is the
 fact that its an A object And so at this point the dynamic_type of X is also
 A Okay So if this line of code when it actually execu tes A which was declared
 to have static_type A actually holds an object of class A But a little_bit later
 on down at this line of code the dynamic_type is actually different The dynamic
 type here of X Is going to be B K line of code executes x holds a b object even
 though its declared Have a different type And this is a very very important
 distinction to keep in mind So theres a static_type theres a type that the
 compiler knows about and thats invariant X has type A It always has type
 a All the uses of x for the entire scope of our typed with class A by the compiler
 But at run_time because we have assignments and we can assign different
 objects to x x can actually take on objects of different types different run
 time types Type b thats assigned x when the program executes
 In the last_video we talked_about the difference between static and dynamic
 typing and how one trend in static typing is towards increasingly expressive type
 systems In this lecture were_gonna talk_about self_type which will give you a
 taste or what those more expressive type systems can look like To begin_with
 lets motivate the problem that self types solves by looking_at a simple class
 definition so here we have class counts and it has a single field I which is an
 integer initialized to zero and it has one method increment And essentially the
 class count just increments a counter So you initially when you allocate a new
 count object the counter is zero and then every time you call ink the
 counters value is increased by one And notice that this can be thought of as a
 base class that provides counter functionality so whenever I wanted a
 counter for some specific purpose I get to find a new subclass and that of
 count and that subclass would automatically inherit the incmethod
 thereby allowing me to have counter without having to reimplement the code
 In this case the code is very very small but in general you can imagine having a
 class that implements something tricky or requiring a lot of code and its useful
 to be_able to reuse that in subclasses Now consider a sub class account that we
 might want to define called stock Say were implementing a warehouse accounting
 program and we want to keep_track of the number of items that are in stock of
 certain different_kinds So we define a new class stock that inherits_from count
 and now well have a new field in here to make this Object thats classed different
 from its parent will just have a name that corresponds to the name of the the
 item thats in stock And now down_here we can actually use this We can decla
 allocate a new stock_object we create a new object We increment it To indicate
 that we have one thing in stock And then we assign it to some variable that weve
 declared of typed stock And then later_on we can use this A object as we like Now
 the problem is that this code actually will not typecheck There is a type error
 in this code And why is that Well lets think_about it for a minute So what is
 the signature of inc So inc remember was declared to return things of type
 count Right and when the inc method is inherited by the stock class this
 signature doesnt_change it still returns things of type count So here we have a
 new stock_object we call the increment_method But the type of this whole_thing is
 a count and then we try to assign that to a stock but that doesnt work because
 count Is not a subtype of stock a variable of type stock cant hold a value
 of type count and so the type_system will report an error right here at the
 assignment statement And you can see that this is actually a serious problem
 because it has made the inheritance of the increment_method pretty useless I can
 define new subclasses of stock but I can never use the increment_method on them at
 least not without getting back something of the parent type And so its not as
 the inheritance of the incremental method is not as useful as one might have hoped
 So just to review new stock the the incremented new stock will have dynamic
 type stock Thatll actually be a stock_object that is returned okay So dont
 get confused here This is the dynamic_type Im talking_about So when we
 allocate the new stock_object and then we call the increment_method remember the
 increment_method returns self so the increment_method was implemented something
 like this Leave out the types but it was I gets I one and it returned the self
 object alright so its definitely returning whatever object is passed in
 here at the dispatch point so its returning something of dynamic_type stock
 And so this program will actually run if we didnt have typechecking If we
 actually run this and it would work just fine This would produce a dynamic stock
 object and would store it into the stock variable But its not well typed because
 the ty pe checker loses track of the fact that this is a stock_object All it knows
 is that increment is declared to have return type count and which is certainly
 correct because every stock_object is also a count object its just not useful in
 the context of this piece of code And so the type_checker loses information Which
 make it not very pleasant to try to put the increment_method in the count class to
 begin_with So to solve this problem were_going to look_at extending the type
 system The insight is going to be that the increment_method returns self In this
 case the increment_method actually returns the self_object and therefore the
 return value is going to have the same type as self Whatever self happens to be
 which could be count or it could be any subtype of count So the self_object only
 has to be dynamically something that holds a value thats a subtype of the declared
 type of the self_parameter and so it could be any one of the subtypes in this
 case of the count class And to do this were actually gonna have to introduce a
 new key_word called self_type that is gonna be used as the return of the type of
 the return value of such functions And were_gonna have to modify our typing
 rules to handle this new kind of type So the idea_behind self_type is its going to
 allow the type to change when inc is inherited or allow us to reason about how
 the actual return type dynamically of increment_method changes when the
 increment_method is inherited So we change the declaration of inc to read as
 follows Weve declared the return type now to be self_type meaning the return
 value Of the increment_method has whatever type Is the type of the original self
 parameter And when we do that now we can see That its possible We_dont we
 havent_said how we do it but you should be_able to see that it intuitively makes
 sense that we could prove facts of the following forms So when the self
 parameter has the type count remember that the thing we dispatch to the thing
 we call inc on is the self para meter So when we dispatch to account object we get
 something back of type count And we call when we when we dispatch on a stock
 object when we call increment on a stock_object well whats the type of self The
 type is stock and we get back something of type stock And now the program that we
 had before with this one change is well typed and would be accepted by the Cool
 type_system Now its very important to remember that self_type is not a dynamic
 type it is very much a static_type and part of the static_type system Its also
 important to realize that self_type is not a class name So unlike all the other
 static types in cool it is not a name of a class its its_own special thing and
 well say more about exactly what it is in future_videos but the purpose of self
 type as weve_seen is to enable to type_checker to accept more correct programs
 and effectively What self_type does is to increase the expressive power of the type_system
 In this video_were gonna continue_our discussion on selftype by talking_about
 the typelevel operations that are available on selftype And this will help
 to clarify what selftype actually is and its role in the type_system Lets begin
 by thinking_about the example that we discussed last_time and if youve
 forgotten what that is let_me just write it down quickly We had a class called
 count and count had one Field an_integer I that was initialized to zero and it had
 one method called INC that returned something itself type and all it did was
 to increment the counter field and return the selfobjects And Ive probably made
 some syntax errors here but thats not really important Thats basic code for
 the CAL class And the question is what can be the dynamic_type of the object
 thats actually returned by INC And the answer here is it could be whatever is the
 type of this selfobject Whatever is the dynamic_type of the selfobject And if we
 think_about a big program where there is multiple classes that inherit from count
 Then the answer is that INC could return count Or any subclass of count So its
 going to return something thats at_least at most count and but it could return
 something more specific The dynamic_type could be something more specific it
 could be a sub classic count or a sub class of a sub class of count Anything
 that inherits directly or indirectly from count is a possibility So whats the
 general case Well lets think_about a class c And in this class C theres some
 expression buried somewhere inside of it that has the type selftype It_doesnt
 really matter how that expression got the type selftype or what it is Lets just
 say that it has that type somehow Well what are the possible dynamic types of the
 expression e And from our_discussion on the previous_slide it seems clear the
 dynamic_type of e when you run e youre going to get back something thats a
 subtype of the class c the enclosing_class in which the selftype appears And
 thats interestin g because it shows us that the the meaning of selftype
 actually depends_on the context So what this selftype means this selftype means
 a subtype of the class C If Id written selftype in a class D in the de
 somewhere in the definition of class D there it would mean a subtype of the class
 D And so to remind ourselves what class were talking_about what enclosing_class
 were talking_about were_gonna subscript occurrences of selftype with a class
 name So selftypes of c here is going to refer to a syntactic occurrence of the
 keyword selftype in the body of the class c And this also suggests a very_simple
 typing rule And really the first useful fact about selftype which is that
 selftypes of c is a sub_type of c And this is really a key idea here that a
 selftyping class C is some subtype of the class C because it also helps
 illustrate what selftype really is The best way to think of an occurrence of
 selftype is that its a type variable that ranges over all the subclasses of
 the class in which it appears So selftypes of C you should think of as a
 type variable its something that doesnt have a fixed type but is guaranteed to be
 some type founded by C so its gonna be only one of the class That inherits
 directly or indirectly from the class c Now that rule that selftypes of c is a
 sub_type of the class c has an important consequence It means that when were
 doing type_checking with selftype it is always safe always safe to replace
 selftypes of c by c So I say its okay to promote any selftypes of c which
 could be c or a sub_type of c to just say okay were just going to say its c
 And that suggests one way to handle selftype which is just to replace all the
 occurrences of selftype sub C by C Now unfortunately that turns_out not to be
 very useful Its sound its correct to do that but thats really just like not
 having selftype at all Thats as if we went back to the example we did in the
 last_video where we started out without selftype and we found out we couldnt use
 inheritance in the way we expected So to do better_than just throwing all the
 selftypes away we need to incorporate selftype into the type_system And the
 way were_going to do that is by looking_at the operations that work on types in
 the type_system and there are two of them Theres the subtype relationship
 that weve talked_about before so when one type is a subtype of another and
 theres the least_upper bound operation that tells_us what the smallest type is
 thats bigger_than both of two argument types And all we have to do and what
 were_going to do now is extend these operations to handle the type selftype
 So lets begin_with a subtype relationship and in our definition here
 were_going to use subtypes T and T and these are just normal class names They
 are class names but not selftypes So one possibility is that we have self on both
 sides of Convince yourself of this Think of selftype again as a variable And we
 can plug in for that variable any subtype of C But just like variables in
 algebra if we plug in one particular class for an occurrence of this variable
 we have to pick the same one for every occurrence of the variable So in
 particular now if we pick some subclass A of C then we wind_up with A Is the sub
 type of A If we plug in A for both sides we can see if their relationship holds
 similarly C is the sub_type of C and for any sub_type we might pick if we bind the
 variable to that sub_type we can see that this relationship will be true Now
 another thing you might think is what if the selftype subC is compared with
 selftype from another class Say selftype subD And it_turns out in the
 cool type rules this will just never come up The cool type rules are written in
 such a way that we never need to compare selftypes from different classes And I
 havent shown you that thats the case yet But when we actually go_through the
 type rules for selftype youll_see that is true Now another_possibility is that
 we have selftype on one side and the n a regular type on the other side So when is
 selftypes of c a type of t Well were_going to say if thats true if C is the
 subtype of T And here were using our rule that its always safe to replace
 selftype by the class that that index is in So in this case C is a supertype of
 anything that selftype C could be Clearly if C is a subtype of T if T is at
 least C or possibly something higher in the class higher key then T would be a
 supertype of anything that selftypes of C could stand for Another case is when we
 have a regular class name on the left_hand side of the sub_typing relationship and
 selftype on the right_hand side And in this case it_turns out we have to say that
 this relationship is false That so T is never a regular class name is never A
 sub_type of selftype sub C And to see this just think_about the possibilities
 So where can C and T be in the type hierarchy So if T and C are
 unrelated You_know if they are inherent from object and they have nothing to do
 with each other Well than clearly T cant be a sub_type of selftype sub C
 They are just two unregulated classes So the only way that this could
 possibility work out is if they are related somehow Now if If T is a sub
 type of C well then you might think that could work out But it_turns out that we
 cant allow it even in that case And heres the reason why think_about a
 hierarchy where T has some subclass lets just say that it has a subclass A And now
 because selftypes of C ranges over all the possible subtypes of C we could plug
 in A here and T is not a subtype of A theyre in the wrong relationship And so
 it doesnt work for all the possible values of subtypes of C we cant say that
 this is true we have to say that it is false Now there is one very special case
 Where one could argue that we should allow this to be true And that is in the case
 where T is actually a leaf of the class hierarchy And let_me actually draw this a
 little_bit differently just to emphasize this Lets_say t hat C Is a class up
 here and then T you_know is through some chain of inheritance relationships is a
 subtype of C So its not immediate but there might be other classes in between
 Just emphasize this isnt doesnt this relationship doesnt have to be immediate
 inheritance It could be transitive inheritance And now if T is a leaf Of
 the hierarchy If n is the only leaf of c if c has no other sub classes then in
 fact T is a subtype of SELFTYPE sub C Because it is the unique minimal type that
 is in the subtype hierarchy of C But the problem is that this is extremely fragile
 and doesnt work if you modify the program In particular a programmer might
 come_along and add some class A over_here thats unrelated to T but is also a
 subclass of C And now this would no_longer work Because if I plug in A for
 SELFTYPE sub C then I see that T is not a subtype of A Right so we can allow
 it at a very special K That C had only a chain of inheritance Not a general tree
 under it And that T was the least of that chain But that is so fragile To
 future program extensions And we you_know if you if you broke it by
 adding another class over_here all of sudden you would get type errors and
 pieces of codes that had previously been typed check to work and hadnt changed at
 all It just wouldnt be a very nice language design So summarize T is
 never a sub_type of selftypes of C And finally if were comparing two normal
 types with not selftype then we just use the rules that we gave before So the
 selftyping rules we had for normal class names havent changed at all And that
 covers all four cases we can have selftype on both sides we can have
 selftype just on the left side or just on the right side and finally we can have a
 subtyping relationship with no selftype at all
 Now that weve_seen some of the operations on self_type in this video_were going to
 talk_about where self_type can be used in Cool The parser checks if self_type
 appears only where types are permitted but thats in fact a little_bit too
 permissive There_are places_where some other types can appear but self_type
 cannot and so the purpose of this particular video is to go over the various
 rules for the usage of self_type So lets begin_with a very_simple rule So self
 type is not a class name so it cant appear in a class definition can neither
 be the name of the class nor the class that is inherited from In attribute
 declarations the type of attribute in this case we have an attribute x and is
 declared to have type t it is okay for t to be self_type so its fine to have
 attributes that are declared to be the self_type of the class Similarly its
 fine to have local let down variables that have type self_type And its fine to
 allocate a new object of type self_type And what this actually does is that it
 allocates an object that has the same dynamic_type as the self_object So
 whatever the type of the self_object happens to be which is not necessarily
 the type of the enclosing_class at run_time the u t operation will create a new
 object of that dynamic_type The type named in aesthetic dispatch cannot be self
 type again because it has to be an actual class name Finally lets_consider method
 definitions So heres a very_simple method definition It has one formal
 parameter X of type T and the method returns something of type T prime And it
 turns_out that only T prime only the return type can be of type selftype No
 argument type can be of type selftype And to see why lets I can show it
 actually two different ways Why why this has to be the case And well do both
 because this is actually important So lets think_about a dispatch to this a
 method so lets_say we have some expression e and we call method m and we
 have some argument e prime And now lets_say the argument e prime As the
 type t zero So if you recall the rule for method calls t zero is gonna have to be a
 sub_type of the type of the fall parameter Were_gonna be passing this in
 so whatever type x is declared to have here has to be a super type of the type of
 the actual argument So that means that t zero is going to have to be a sub_type of
 now lets_assume that the argument can be of type self_type Some view that t zero
 has to be a subtype of self_type this is in some class c wherever this is defined
 and remember that we said This was always false that you couldnt have self_type on
 the right_hand side and a regular type on the left_hand side Because that would
 lead to problems that would that we would never be_able to prove that in general for
 a that that a type is actually a sub_type of self_type because self_type can
 vary over all the sub types of the class C So thats one way to see that we cant
 allow method parameters to be typed self_type but its also helpful to just think
 about executing the code or some example code and see what can go wrong So heres
 an example And let_me just walk you through what_happens if we allow a
 parameter to have type self_type in this example So there are two class
 definitions Class A has a method comp for comparison and it takes one argument of
 type selftype And it returns a bull So the idea here is that the comparison
 operation probably compares the this parameter with the argument and returns
 true or false Then theres a second class B and B is a subtype of A it
 inherits_from A And it has one new field B little b here of type int And now the
 comparison function in class B is overridden has the same signature as the
 comparison function or the comp function in class A But the the method body here
 accesses the field B And now lets_take a look_at what_happens with a piece of code
 that uses these two classes So here X is going to be declared to be of type A But
 were_going to assign it something of type B And here were notice that theres a
 gap between the static_type which will be A and the dynamic_type which will be B
 And thats actually key to the problem And now we Invoke the cup method on X and
 we pass it a new A object And so what_happens well this type checks just fine
 because X is in class A X is of type A and this argument is also of type A So if
 selftype if having an argument type selftype is ever going to work it has to
 work for this example where the two static types of the of the dispatched of this
 parameter and the former parameter are exactly the same So that clearly has to
 be allowed if we allow self_type as the type of the argument And now lets think
 about what_happens when it actually executes Is going to invoke the comp
 method in the b class okay Because X is of dynamic_type B And then its going to
 take the arguments and its going to access its B field But the argument is of
 dynamic_type A and it has no B field And so this is actually going to cause a
 runtime crash So and just to go over that one more time Just to make_sure that
 it is clear Here X has type A ut dynamic_type B The argument has static_type A and
 dynamic_type A and when this method gets invoked the argument That which is of
 dynamic_type A does not have the operations all the fields and methods of
 the class B And results in a run_time undefined behavior at run_time
 In this video we are going to use what we have learned so_far about selftype to
 incorporate the selftype into the type_checking rules for Cool First lets
 remind ourselves what the type_checking rules for Cool actually prove So the
 sentences in the type logic look like this and they prove things are the form
 that some expression has some type and they do that under the assumption that
 object identifiers have some types given by O methods have signatures given by M
 and the enclosing_class the current class in which E sits in in which we are doing
 our type_checking is class C And the whole reason for this additional piece
 here we havent actually discussed this before why we needed this C It is because
 selftypes meaning depends_on the enclosing_class So if you recall we
 Introduce this notation self_type sub C to record and what class a particular
 occurrence of selftype sits And this C in the environment is exactly that
 subscript it is tracking what class we are in So when we see occurrences an
 occurrence of selftype we know what kind of selftype Were talking_about so now
 Im ready to actually give the type rules that use selftype And for the most part
 this is really easy because the rules just remain the same That is they look
 the same but theyre actually a little_bit different because they use the new
 sub_typing and least_upper bound operations that we defined before So for
 example here is the rule for assignment and this looks identical to the rule for
 assignment that we discussed several_videos ago But notice that this use of
 subtyping here is now the extended definition of subtyping to incorporate
 selftype So now this rule works with selftype as well as with the normal class
 names Now there are some rules that have change in the presents of selftype And
 in particular the dispatch rules need to be updated So here is the old rule for
 dynamic_dispatch And this rule this part of the rule actually doesnt_change It
 stays the same But I just wanna point out the essential restriction in this
 rule is that the return type of the method could not be selftype And thats
 actually the place_where selftype buys us something So the whole purpose of having
 selftype is to have methods whos return type is selftype Because thats were we
 actually get the extra expressive power And know we have to consider the case now
 that we have self typing we done all this work what if the methods return type is
 self_type How are we going to type_check that Well heres the rule So as usual
 we type_check the The expression that were dispatching to thats E zero and all
 of the argument and we just get their type And their just type checked in the
 same environmental as the entire_expression And now just like before we
 look up in class T0 the type of E0 the method F and we get its signature Okay
 And then we have to check that the arguments conform That every actual
 argument E1 through En has a type thats compatible with the corresponding formal
 parameter in the method signature And if all of that works out then we can say
 that this dispatch is going to have type oh look T0 So where did that come from
 Well the return type is selftype And so the result of this entire dispatch is
 going to be the type of whatever e zero was E zero is the selfparameter
 Whatever type we got for e zero that is a sound static_type for the result of the
 entire_expression So we simply use the type of e zero as the type of the entire
 static the entire dynamic_dispatch Now recall the full parameters of a function
 cannot have type self_type but the actual arguments can have type self_type and the
 extended sub_typing relationship will handle that case just fine One
 interesting detail is that the dispatch expression itself could have a type
 selftype And so what do I_mean by that well lets think_about E zero
 Dispatching to method F and then what_happens if E zero has type self_type What
 if we can prove that E zero has type self_type And the problem here is that we need
 to lo okay up in the in the M environment in the method environment in
 some class The definition of or the signature of method F we have to get
 back that type signature so we can do the rest of that type_checking And if E0 has
 type soft type normally we use the type of E0 to do that to do that look up What
 type do we use here Well if this whole_thing is occurring in class C If we have
 if were type_checking in class C gtgt You just put the line there gtgt And its safe
 and this is a soft type sub c and as always its safe to replace soft type sub
 c by c So well just use the class c there The correct class that we are type
 checking in to look up the method name test We have to make similar changes to
 the rules for static dispatch So here is the original rule for the static dispatch
 and And again this part of the rule will not change Uh This This
 handles the case where the return type of the method is not the selftype But if
 the return type of the method is selftyped then the rule looks a little
 bit different So we once_again we type_check the expression that were
 dispatching to and all the arguments in the same environment as that of the entire
 expression We have to check that the class were dispatching to the type t
 zero is a sub_type of the class named in the static dispatch We have to look up
 the method It has to exist in that class that were statically dispatching to So
 we have to look up in class T the method F and get its signature And then we have
 to check that the actual arguments conform to the formal_parameters of their types
 If the types of the arguments match the types the declared types of the formal
 parameters And then the only thing thats kind of curious about this rule is
 that the result type here is again T zero And why is that right It could
 have been a a T It could have been the type to which we statically dispatched
 And its not because selftype is the type of the selfparameter And even_though
 were dispatching to a method in class T the selfparameter still has type_T0 And
 recall that T0 is a subtype of T So we use the static dispatch to reach a method
 definition thats hidden potentially by overwrite overwritten methods in the
 subclasses But that doesnt_change the type of the selfparameter The
 selfparameter still has type_T0 even_though were running a method Of the in
 a superclass of G0 There two new rules for self_type One involves the
 selfobject So the selfobject has type selftype subC And notice this is one of
 those places_where we need to know the enclosing_class So we know what kind of
 selftype were referring to And similarly theres a rule for allocating
 something of type selftype So a the expression new selftype also produces
 something of type selftype subC
 In the next several_videos were_going to apply what weve learned in the class to
 analysis of various features of Java This will give_us a chance to both look_at a
 real programming_language and how its designed and been done And also to talk
 about some features that art in Cool that we havent been able to cover thus far in
 the course For the perspective of this class Java is a kind of Cool on steroids
 Its Cool plus more features many more features At at their core Java and Cool
 are very_similar Java and Cool are both typed object_oriented garbage collected
 languages They were both designed in the early 1990s and so they share a common
 culture there with the ideas that were current at that time So in this video
 Im just going to give a little_bit of history of Java so that will be the focus
 really of this rather short video And then in the subsequent videos well talk
 about all the features or some of the features of Java that are not in Cool And
 well use the ideas of course that weve_been discussing all along through the
 class to explain a little_bit about those ideas But I_think these are all important
 language constructs that are unfortunately were too time consuming or too complicated
 to add to the course project So I_think its useful to use a language like
 inaudible to illustrate how these ideas work and what some of the issues are So
 Java began as a project called Oak at SUN Microsystems and it was originally
 targeted at set top devices And so what was a set top device This was a little
 box that was gonna sit on top of your TV So you had your TV screen and then there
 was going to be this little thing up here that was going to sit on top of the TV
 that was going to control all your cable programming So this would essentially
 would connect to some kind of network and it would help you to do you_know to make
 your TV more interactive Okay so this was back in the days before every TV was
 really a computer by_itself The initial develo pment of of Oak took several
 years I believe the project ran from about 91 to 94 and what happened at_least
 as I understand it is that this set top device market just never took off So this
 never became a popular never never became a popular idea with consumers and and so
 there really was a kind of limited up side or a limited potential for for Oak at the
 time And then something happened the Internet happened So also in the early
 90s the Internet revolution was really gathering steam Everybody was getting on
 the Internet and it became obvious sometime 93 94 that there was going to be
 a need for programming_languages that really addressed these specific issues on
 the Internet And in particular people were very concerned about security And
 they didnt want to be downloading lots of binaries that were programs written in C
 and passing those around on the Internet because it was just really no guarantee
 that those programs would work as intended and not crash your machine So you_know
 they needed to be_able to share code over the Internet from other people that you
 didnt necessarily trust completely meant that we needed safer languages than
 languages like C and C and so there was an opportunity there for a a new language
 and there were several candidates actually Besides Java Tickle and Python
 were very serious candidates to become the Internet programming_language eventually
 the backing of SUN Microsystems the backing of SUN gave to Java helped it to
 really gain a very very strong presence on the Internet But you_know the point
 of this story is that every new language needs a killer app Every programming
 language rides into the world on the back of some application So there has to be
 some kind of new application that people want to write that the existing languages
 dont serve very well and that provides the opportunity and therefore makes it
 worthwhile for people to learn a new programming_language And so the fact that
 Ja va was a very safe language it had the garbage_collection It had the type_system
 that made it well suited at the time to the the needs of the emerging needs of
 internet programming And it became very popular I_think primarily because of that
 reason And if you if you recall there was a lecture or a video early on on the
 economy of programming_languages And I would recommend actually that if you
 havent watched that one then you go_back and take a look because there I discuss
 some of these ideas about how languages are adopted in more_detail So Java also
 came into existence in a particular technical environment and its very
 common In_fact its normal for new programming_languages to borrow heavily
 from their predecessors and new languages are often mostly recombinations of ideas
 from previous languages in a new design perhaps with some innovations thrown in
 And the particular influences on Java Again this is at_least so_far as I
 understand it so the type_system in Java or its its commitment to types probably
 is traceable to Modula3 and the ideas there where people try to build a sort of
 programming_language that would scale in a realistic way to large systems but also be
 strongly typed The object orientation of Java is traceable to languages like
 Objective C and C and and also Eiffel which also had the idea of interfaces
 which is a prominent feature in Java And finally Java is quite a dynamic language
 meaning that there many things that are not done staticly Theyre done
 dynamically so features like reflection would be one example of that And there
 are actually quite a few other features and there is some history there Theres
 some shared culture there with Lisp This was a or is a functional family language
 but is also a very very dynamic language As I said at the beginning this video is
 just an introduction and overview and in the next few_videos well be looking_at
 specific features of Java and how they work And that will includes things_like
 exceptions interfaces and threads As well as a bunch of other features that
 well discuss at_least briefly One thing to realize is that Java is a big language
 It is not simple The language manual for Java runs to hundreds and hundreds of
 pages It has lots of features And perhaps more importantly lots of feature
 interactions So The hard part of designing a language of course is getting
 all the feature interactions right All the combinations of the features and you
 know making sure there are no surprising interactions between them
 In this video_were going to wrap_up our series on type_checking with a discussion
 of how to recover from type errors So as with all the other front end phases like
 flexing and parsing its important to recover from errors that happen during
 type_checking But unlike parsing its much much easier to recover from errors
 in the type_checker because we already have the up stacks and text tree and so
 theres_no need to skip over portions of the code as we did in parsing before we
 even knew what the structure of the program was The problem though is what
 type should we assign to an expression that has no legitimate type The type
 checker works by structural induction and it cant just get_stuck So if we find
 some sub expression that has no type that we can meaningfully give it we still have
 to do something with it so that we can type_check all the expressions that
 surround it One_possibility is to simply assign the type object as the type of any
 ill typed expression And the intuition here is that even if we couldnt figure
 out what the type or the expression was supposed to be certainly it was something
 that was a sub_type of object So it is certainly safe to assign any expression
 the type object So lets_consider what_happens with this strategy in a simple
 piece of code So here we have a little code fragment and we just assume here that
 X is undefined that actually theres a bug in this code and thats at X has no
 binding So theres_no type anywhere for X So what_happens when we type_check
 this Well were_going to recursively walk down the abstract_syntax tree
 eventually well get to the leaves and well try to type_check X And then well
 discover that there is no type for X anywhere and that will result in an error
 message Saying that X is undefined And then in order to proceed with type
 checking in order to recover well have to assign X a type And so well just
 assume that X has type object because thats our recovery strategy And now well
 continue to type_check as we walk up the abstr act syntax_tree and the next_thing
 well try to do is to type_check this plus operation Well see that were adding
 something of type object to an_integer And of course plus doesnt work on things
 of type object so well get an error Something like plus applied to an object
 And then well have to decide now that we couldnt type_check this plus what the
 type is of X2 so this whole subexpression and of course our recovery
 strategy is to say well that also has type object And now the next_thing up in
 the abstract_syntax tree is this initialization assignment Here were
 assigning Y the result of this expression But we couldnt type_check this expression
 so it has type object And now the type_checker sees that were assigning
 something of type object to something of type int and we get yet a third
 error Saying that we have a bad assignment of some kind So The nut of
 the the problem here is that this simple recovery strategy works If we do recover
 we continue type_checking But a single error leads potentially to lots of other
 errors So this is a workable solution It it it achieves the goal of recovering
 But in general it will lead to cascading errors Once you have on one type error
 that type error will just cause many more because not very many things can be done
 with something of type object And probably the code was written assuming some more
 specific type And these errors will just propagate up the abstract_syntax tree
 until some point just result in multiple errors Another_possibility is to introduce
 a new type a No type that is specifically for use with ill typed expressions And No
 type is very special Its not a type that is available to the programmer Its only
 available to the compiler and its just there for error_recovery and type
 checking And the special property of No type is that its going to be a sub_type
 of every other type So if you remember object was the opposite Object is a super
 type of every type and that had the bad property that there are very
 few methods that defined on object and so if you plug in type object Where you
 expected some other type probably its not going to type_check So we can fix that
 problem by introducing no type And no type will have this special property that
 every operation every operation is defined for no type And furthermore well
 say that it produces no type as a result So any operation in the language that
 takes an argument of type no type it will produce a result of type no type So the
 no types will propagate And now lets_take a look_at our same code fragment and
 lets work through what_happens if we use no type So again we walk down the
 abstract_syntax tree we get to this leaf X we see that X is undefined we
 produce an error saying X is undefined And then we have to assign a type to X so
 we say well X has type no type and now we consider the plus operation And now plus
 is taking an augmentative type no type integer And this is fine Were not gonna
 produce any errors thats consider to be well type and the results is also of type
 no type And now were doing an assignment And no type is compatible with N No type
 is a subtype of N So this assignment is also type correct And we dont produce any
 type of error in that stage either And so you can see what_happens here is that no
 types propagate up the abstract_syntax tree just like the object types did before
 But since no type is a special type its used only for error_recovery We can
 distinguish it from all the other regular types And we know that we shouldnt print
 out an error message after the first one is produced So a real compiler a
 production compiler would use something_like no type for error_recovery But there
 is an implementation issue with no type And in particular the fact that no type
 is a subtype of every other class means that the class hierarchy is no_longer a
 tree If you think_about it you have object at the top and then you have this
 tree structure branching out But then no type is a subtype of everything So no
 type becomes a bottom ele ment And This is now a DAG and not a tree and that makes
 it just slightly harder to implement Instead of being able to just have tree
 algorithms now you have to have either have a special case for no type or just do
 something more general And this is just enough extra hassle that I personally
 dont think its_worth doing for the course project and I recommend that you
 use the object solution and we just live with the propagating or compounding
 errors that that produces
 In this video_were going to begin our_discussion of run_time systems Now at
 this point we have actually covered the entire front end of the compiler which
 consists of the three phases lexical_analysis parsing and semantic_analysis
 And these three passes or these three phases together their job is to really
 enforce the language semantics or the language definition So we know After
 these three phases are done that if no errors have been generated by anyone of
 those phases then the program is actually a valid program in the programming
 language that were compiling And at this point the compiler is going to be_able to
 produce codes to produce a translation of the program that you can actually execute
 And I should_say that of course Enforcing the language definition is just one
 purpose of the frontend The frontend also builds the data_structures that are
 needed to do cogeneration as we seen but there is a real Once we get through the
 frontend we no_longer looking for errors in the program Were no_longer trying to
 figure_out whether its a valid program Now were really down to the point where
 were_going to generate_code And that is a job at the back end So cogeneration is
 certainly part of it The other big part of the back end is program optimization so
 doing transformations to improve the program But before we can talk_about
 either one of those things we need to talk_about Runtime organization And why is
 that well because we need to understand what it is were trying to generate before
 we can talk_about how we generated and have that makes_sense So first were
 gonna talk_about what the the translator program looks_like and how its organized
 and then well talk_about algorithms and code_generation algorithms were actually
 producing those things And this is a wellunderstood area or at_least some very
 standard techniques that are widely_used and those are the ones we wanted to cover
 and and encourage you to use in your project The main thing were_going to
 cover in this sequence of videos is the management of Runtime resources and in
 particular Im going to be stressing the correspondence and the distinction between
 static and dynamic structures So static structures are things that exist to
 compile_time and dynamic structures those are the things that exist or happen at
 Runtime And this is probably the most_important distinction for you to try to
 understand if you really want to understand how a compiler works What
 happens to the compile_time and what_happens at run_time Having a clear
 separation in your mind between what is done by the compiler and what is deferred
 to when the target program or the generated program actually runs that is
 key to really understanding how compilers work And well also be talking_about
 storage organization So how memory is used to store the data_structures of the
 executing program So lets begin at the beginning So initially there is the
 operating system and the operating system is the only thing that is running on the
 machine and when a program is invoke When the user says he wants to run a program
 what_happens while the operating system is going to allocate space for the program
 the code for the program is going to be loaded into that space and then the
 operating system is going to execute a job to the entry_point or the main function of
 the program and then your_program will be off and running So lets_take a look_at
 what the organization memory looks_like very roughly when the Operating System
 began execution of the compiled program So were_gonna draw our pictures of memory
 like this That would be just a big block and there will be a starting address at
 the a lower address and a higher address and this is all the memory that is
 allocated to your_program Now into some portion of that space goes to code for the
 program so the actual compiled code for the program is loaded usually at one end
 of the memory space allocated to the program And then there is a bunch Of
 other space that is going to be used for other things and well talk_about that in
 a minute Before going on I want to say a few words about this pictures of Runtime
 Organization because Im going to be drawing a lot of them over the next few
 videos So its just traditional to have memory drawn as a rectangle with the low
 address at the top and the high address at the bottom Theres nothing magic about
 that just a convention we could adjust it easily every verse or order of the
 address no big deal And then well be drawing lines to the limit different
 regions of this memory showing different_kinds of data and how theyre stored in
 the memory allocated to the program And clearly these pictures are simplifications
 if this is a virtual memory system for example theres_no guarantee that these
 data is actually laid_out contiguously but it helps to understand you_know what the
 different_kinds of data are And what the a compiler needs to do with them to have
 simple pictures like this So coming back to our picture of run_time organization
 we have some block memory and the first portion of that is occupied by the actual
 generated code for the program and then there was this other space and were what
 goes to that space Well what goes to that space is the data for the program So
 all the data is in the rest of the space and the tricky thing about code_generation
 that the compiler is responsible for generating the code but its also
 responsible for orchestrating the data So the compiler has to decide what the lay of
 the data is going to be and then generate_code that correctly manipulates that data
 so there are references of course in the code To the data and the code and data
 need to be designed the code and the layout of the data excuse_me need to be
 designed together so that the generated program will function correctly Now it
 turns_out that this actually more_than one kind of data that the compiler is going to
 be interested in and what well be talking_about In the next video is the different
 kinds of data and the different distinction between the kinds of data that
 go in this data area
 In this video_were going to being our discussions of run_time structures with
 the notion of procedure activations Before we begin the discussion of
 activations its_worth being explicit that we have two overall goals in code
 generation One needs to be correct to generate_code that actually faithfully
 implements the programmers program And the second is to be efficient that that
 code should made good use of resources and in particular we often care that it run
 quickly And is very easy to solve These problems in isolation If all we care
 about is correctness its not a hard problem to generate Code that is very
 simple but also very slow and correctly implements the program If all we care
 about is speed we dont care about getting the right answer the problem is
 even easier I can generate extremely fast programs that generate the wrong answer
 for any problem that you carry to me And so really all the complications in code
 generation arise from trying to solve These two problems simultaneously And
 what has grown up over time is fairly elaborate framework for how a code
 generator and the run and the corresponding run_time structures should
 be done to achieve both of these goals okay And the first step in talking_about
 that is to talk_about activations Were_going to make two assumptions about the
 kinds of programming_languages for which were generating_code The first
 assumption is that execution is sequential Given that we execute the
 statement the next statement that will be executed is easy to predict In_fact its
 just a function of the statement that we just executed So controls is going to
 move from one point in a program to another in some well defined order The
 second assumption is the one that procedure is called controllable always
 return to the point immediately_after the call That is if I execute a procedure f
 once f is done executing control will always return to the statement that
 followed Point where f was call And there are certainly programming_languages and
 programming lan guage features that violate this assumption So the most
 important class of programming_language is it violate assumption one are ones that
 have concurrency So the concurring program just because I execute one
 statement there is no easy way to predict what the next statement is to execute it
 because it might be in a completely_different thread And for assumption too
 Advanced control constructs things_like exceptions And Calls cough If you
 happen to know what call cc is its not important if you dont These kinds of
 constructs that affect the flow of control in fairly dramatic ways can violate
 assumption to So in particular if youre familiar_with catch and throw style
 exceptions in Java and C when we throw an_exception that exception might escape
 from multiple procedures before it is caught and so theres_no guarantee when
 you call a procedure if that procedure can throw an_exception that that it control
 whatever return to the point immediately_after the procedure call Now were_gonna
 keep these assumptions for the rest of the class We may later_on in future_videos
 briefly discuss how we would accommodate some of these more advanced features if
 the the material that were_going to cover Is basic to all implementation and
 even languages have concurrency and exception build upon the ideas that were
 going to discuss here So first the definition When we invoke the procedure p
 Were_going to say that is an activation of the procedure p and the life_time of an
 activation of p is gonna be all the steps are involved executing the procedure p and
 including all the steps in the procedures that p calls so its going to be all the
 steps in the procedures that p calls So its going to be all the statements that
 are executed between the moment that p is called and the moment that p returns
 including all the functions and procedures that p itself calls We could define an
 analogous notion of the lifetime of a variable So the lifetime of a variable x
 is gonna be the portion of the execution in which x is defined That_means that
 its all the step of execution from the time that x is first created until the
 time when x is destroyed or deal located and just note here that life_time is a
 dynamic concept so this is that implies to the executing program Were talking_about
 the time when the variable first comes into existence until the moment in time
 when it goes out of existence And scope on the other hand is a static concept that go
 prefers to that portion of the program_text in which the variable is visible
 Okay so this is a very different idea from the life_time of the variable and
 again Its very important to keep these two times what_happens at runtime and
 what_happens in compiler time or what is associated_with the static properties of
 the program distinct in your mind From the assumptions that we gave a couple of
 slides ago we can make a simple observation and that is when a procedure P
 calls the procedure Q Then Q is going to return before P returns And what that
 means is that the lifetime of procedures are going to be properly nested and
 furthermore that means that we can illustrate or represent activation
 lifetimes as a tree Lets illustrate activation with a simple example So
 heres a little cool program and as usual it will begin running by executing the
 main_method in the main class So the first activation and the root for our
 activation tree for this program is the method main And Main is going to call
 the method g and so gs lifetime the set of instructions were g exist where a
 period of time of the execution where g existed is gonna be properly contain
 within the execution of this call to main And so we can illustrate that by making g
 a child of main So this indicates that effect of g is a direct child of main
 indicates that main calls g and also the gs lifetime is properly contained within
 the lifetime of main After g returns main will call f and so f will also The a
 child of of main And then F as itself is going to call G again And so its gonna
 have another activation of G And so G Will also be a child of f And this tree that
 is actually the complete tree for this particular example illustrates the number
 of things First of all as we already said it shows the containment of life_time So
 again for example gs life_time is contained with a name but it also shows
 some other interesting lifetime relationships For example the life_time
 of this activation of g and the life_time of that activation of f are completely
 distinct because their siblings in the tree their lifetimes do no overlap at
 all And another thing to notice here is that there can be multiple occurrences of
 the same method in the activation tree So every time the method is called that is a
 separate activation so in this particular activation tree there are two activations
 of g So heres a somewhat more_complicated example the involves a
 recursive function Lets begin here at the at the first call So The call to
 main And all main does is call F with the argument three So there is an activation
 of F from Main And then what does f do well f asks if its argument is zero and
 if it is that calls g while the initial argument is three so thats not going to
 be true on the first call to f In otherwise it calls f with the argument
 minus one So I was making note over_here on the side about what the argument is
 because we need to keep_track of that So f is called with three clearly that is not
 zero and so then f is going to be called again with the argument two that will
 results in f being called yet another time with the argument one and finally f will
 be called With the argument zero Which will then result in a call to G And so
 this is the activation tree for this particular program And again notice that
 there is gonna be multiple activation of the procedure on the same run of the
 program It just indicates that the same procedure can be called multiple times and
 also note that the recursive procedure will result in nesting of activations of
 the same function within itself And so when f calls i tself and so the life_time
 say of the second call to f is properly contained within the life_time with the
 fist call to f
 In the previous_video we talked_about activation but we never said what
 information we actually need to keep for an activation Thats the topic of this
 video An activation_record is all the information thats needed to manage the
 execution of one procedure activation And often this is also called a frame that
 means exactly the same_thing as activation_record These_are just two names for the
 same_thing Now one interesting fact about procedure activations is that they have
 more information in them than you might expect So in particular when a procedure
 f calls a procedure g the activation_record for g will actually have
 information not only about g but very frequently also about the calling function
 f So typically the activation_record for a procedure will contain a mixture of
 information both about that procedure and about the procedure that called it Now
 up this point we havent_said why we need to keep information about activations at
 all And the reason is that there is some state associated_with this procedure
 activation that is needed on order to properly execute the procedure and we have
 to track that somewhere and thats the activation_record is gonna be forced Its
 gonna be the whole the information needed to properly execute the procedure So
 lets look_at that in a little_bit more_detail Lets consider this situation
 where a procedure F calls procedure G And what is going to happen so conceptually
 what_happens when f calls g is that f is suspended F is going to stop Executing
 while g is running So g is going to be using the processor and all the resources
 of the machine But when g completes we wanna start executing f again f is going
 to resume And so in between while g is running we have to save the state of the
 procedure activation of f somewhere so that we can resume it properly and thats
 again what the activation_record is for And so gs activation_record Is going to
 have to have information in it that will help us to complete the execution of g so
 there will be some inform ation about g that we just need in order to run g But
 also gs activation_record is going to have to store whatever we need to be_able
 to resume the execution of procedure f So lets work through an example Heres one
 of the programs that we looked_at in the last_video and here is a design for a
 concrete activation_record for the procedure f So well have one position
 for the results of f that will hold the return value after we finished execution
 of f There will be a position here for the argument to f so is it so fy takes
 one parameter so I only need one word here to hold the the argument to the function
 There will be a control link so a pointer to the previous or the callers activation
 and well also have a slot for the return_address so the address in memory or the
 address of the instruction that we are supposed to jump to after the execution of
 f completes So now lets just execute this program by hand and work out what the
 activation_records will look like down the stack So when the front program is first
 invoked it will call main There will be an activation_record for main And we were
 not gonna worry_about that Were just gonna focus_on that So theres some stuff
 for main but were not going to do to to to talk_about that And then main is going
 to call f all right And so when main calls f an activation_record will be
 pushed onto the stack and well have four slots and or four fields for values And
 what were_going now while the first lines for the result well is just starting to
 run if its just beginning execution so there is nothing to put there at the
 moment That gets filled in when f returns The second position will hold the
 argument to f so that would be the number three The third slot will hold the
 controlling so thats gonna point back To the activation for main and the fourth
 position will hold the return_address and this is actually not completely trivial
 because f is called in multiple places So if you look_at the program theres a
 called f in main and theres a call to f inside of f itself And so Depending on
 where the function is called from after that function completes with one or return
 to a different address In the case of the main when this called F completes we
 wanna return to the Whatever instructions comes_after the called f which is just
 gonna be something that wraps up the execution program since its the only exit
 point of main inside of f Its going to be the conclusion of the conditionals So
 this point double star here is going to be at whatever is left on the conditional
 then the return from F And so depending_on what F is called from we wanna return
 to one of two different places okay So in this case F is being called from main
 and so well put the single star address in that position of the activation_record
 All_right So then f is called the second time the body of f executes and the
 argument three is not zero thats way we wined up calling f again but that means
 that another activation_record will be pushed on to the stack that will also help
 for slots as an activation_record for f (I probably should label these) so thats an
 activation of f so its also an activation of f And what goes in this one
 well again the result doesnt have anything initially in it The argument in
 this case would be two The controlling in this case will point back to the previous
 activation of f and the return_address in this case will be the point double star
 So after two calls to f this is what the stack will look like with this particular
 activation_record design So here is the same picture just running a bit more
 neatly and theres one additional we want point out which is at this stack of
 activation_records and let_me Delineate the activation_records here Is not as
 extract as the kinds of stacks who were probably taught about in a data_structures
 class if youve had such a class So here there are distinct activation_records on
 the stack and we treat them as such in the Runtime system well treat them as
 such But this is also like one gigantic array All o f this Data is just laid_out
 in contiguous memory These were all contiguous addresses and one is activation
 record here just follows on with the next address merely after the previous
 activation_record And compiler is compiler writer will often play tricks to
 exploit the facts that these activations are adjacent to each other in memory and
 well see one such potential trick in just a moment To_summarize some of the
 highlights of these examples so_far I wanna repeat the main is not very
 interesting So it has no argument or local_variables and if results is never
 used And so while it does have an activation_record were just not focusing
 on that and were not concerning ourselves with what goes in at activation_record
 Were just focusing on the activation_record for f Just be true this clear the
 start and double star that I use in the example these are addresses in memory
 These_are actual memory addresses and they refer to addresses of code Those are the
 addresses of the instructions that come_after they call f because thats the place
 where f would return to And finally I want to stress that this really is only
 one of many possible activation_record designs You can design a different
 activation_record for f that has had different information that would work just
 fine depending_on the structure of the rest of the cogenerator in the runtime
 system So in particular many compilers dont use the controlling because they
 dont need inexpensive link to be_able to find the calling or the activation_record
 of the calling procedure and in fact in your class project the Khul compiler you
 wont be using a control link Most Activation records wont have the return
 value on the activation_record because itd be more efficient and convenient to
 return it in the register All_right this is just one possible design and with and
 you could just design other activation_records that will work just fine The
 important thing about the activation_record is that it just have to have
 sufficient information in it to e nable the generated code to properly execute the
 procedure thats being called And also to resume execution of the calling procedure
 So far weve only looked_at the procedure call for this activation_record We
 havent talked_about what_happens when activations return So lets_consider what
 happens in our example after the second called f that this one this activation
 down_here returns So whats going to happen is were_going to make the caller
 the current_activation so itll actually become the top of the stack so Ill have this big fat
 green error here indicated that this is now the current_activation this one up
 here Okay So this is the call this is the what was the caller and is now going
 to resume executing And the interesting thing here is to note that like I said
 before this isnt as abstract as a stack in a data_structures course So while we
 have restored this as the active procedure this data down_here this this
 activation that was running is still there in memory And in fact we can go and look
 at it if we want to And the way I set this example up in fact we need to because
 the results of the procedure that we called is now stored here In the first
 word of this activation All_right So when f begins executing again is going to
 have to look up that result in order to know what the result was of the procedure
 levels called So the advantage of placing the return value in the first_position in
 the frame that the call can find it at a fix offset from its_own frame Lets back
 up and just see that so here when the second call to f has returned and the the
 first call here has resumed executing this call the code for this call will know
 that the science of this activation_record is four There_are four words in this
 activation_record and so they can find the result to the procedure that it called in
 the four one position and five words passed the top of the frame So in
 particular theyll be_able to find this where in memory and even_though this has
 been popped out of the stack as I said before that data is still there at_least
 until another procedure is called And so if we immediately read the result of the
 function_call after we return from the function well be_able to pick up that
 result and then use it in the continuation of the execution of the call in procedure
 And once_again I just wanna stress I know this is a couple of times but its
 very important that theres absolutely nothing magic about this organization We
 could rearrange the order of elements in the frame We could divide the
 responsibility between the caller and the calling differently And really the only
 metric here is that one organization is better_than another if it results and
 faster code or in a simpler code_generator And I know I also mentioned this before but
 its also an important point in a production compiler we would produce much
 of the frame contents as possible in registers And in particular there would
 be a effort to pass the method results and the method arguments in registers because
 those are excess so frequently Finally to some up our_discussion of activations and
 activation_records the issue is that the compiler has to determine at compile_time
 okay so this happens statically The layout of the activation_record and also
 has to generate_code that correctly accesses the locations in that activation
 record And what does this mean this mean that the activation_record layout and the
 code_generator have to be designed together Okay So you cant just assign
 your code_generator and then figure_out later what your activation_record layout
 is gonna be or vice versa This two things needs to designed together because they
 depend_on each other
 In this video we are going to continue or discussion of run_time organization by
 talking_about how compilers handle global_variables and heap data_structures Lets
 start by talking_about global_variables The basic properties of the global
 variables that all the references point to the same object That what it means to be
 global And for this reason we cant store all the variables in the activation_record
 because the activation_record if of course is the allocated When the
 activation completes and that would be the allocated global variable So The way
 that little variables are implemented is that all global are signed the fix address
 once And these variables with fixed addresses are said to be statically
 allocated because theyre allocated essentially at compiled times So the
 compiler decides where they going to live and then they will live there in all
 executions of the program And depending_on the language there may be other
 statically allocated values and well actually see some later_on but they behave
 just the same as global_variables So I_think global_variables changes our run
 time organization picture a little_bit First we have the code as before and
 then immediately_after the code is typically all of the static data So these
 are the global_variables and other static object things that have fixed addresses
 for the duration of the execution of the program and then the stack comes_after
 that So the stack will start at the end of the static data area and grow towards
 the end of the programs allocated memory Trying out to the heat any value that
 that outlives the procedure that creates it also cannot be stored in the activation
 record Lets_take a look_at this example So here we have a procedure foo
 and lets_take a look_at the activation_record or frame for foo Now lets
 say that a foo allocates a bar object and that were_going to store that object in
 foo activation And now when this method returns of course the activation_record
 would be deallocated so the bar obj ect will also go away but that wont work here
 because notice that the dynamically_allocated object the object we allocated
 during the execution of foo is also the results of foo so this has to be this has
 to be accessible to foos caller After inaudible exits And so what that means
 is that this borrow object and all dynamically_allocated data has to be
 stored some place other than the activation_record and language is what
 dynamically_allocated data generally use a heap for that purpose At this
 point we can summarize the different_kinds of data that the language of
 implementation has to deal_with First there is the code and in many languages I
 shouldnt say most In many languages The code is fixed size and read only I_mean
 that the compiler creates all the code that will be used in the execution of the
 program and that could be allocated once It should_say that there are many
 languages also were this is not true and code can be dynamically created at one
 time The static area Would contain data with six addresses and this would be
 things_like global_variables and this is also typically fixed size and it was maybe
 readable and writable as opposed to the code which I generally dont want to be
 able to write And then a stack is used to contain an activation_record for each
 currently active procedure and the activation_record is generally fixed size
 so each activation_record for each particular kind of procedure has a fixed
 size and this will contain all the local information the local_variables
 contemporaries that needed to execute a particular activation And finally the
 heap is for everything else So the heap is just for all the data that doesnt fit
 into other categories This includes all of the dynamically_allocated data And if
 you are familiar_with C then the heap in C is managed by the programmer using
 inaudible in Java you have new For dynamically allocating data and then
 garbage_collection actually takes care of reclaiming data from the heap that is no
 longer used Now many lang uage implementations use both the heap and the
 stack and there is a little_bit of an issue here because both the heap and the
 stack grow And so we have to take care that they dont grow into each and step on
 each others data And there is a very nice and simple solution to this and as a start
 to heap and the stack at opposite ends of memory and let them grow towards each
 other So lets_take another look_at our Runtime Organization picture And just for
 review first we have the code and then we have the static data And then we have the
 stack which grows towards in this case the high address allocated to the program And
 notice that the stack doesnt necessarily just grow as procedure three terms stack
 will also shrink So as the program runs the stack will get bigger or smaller
 depending_on how many procedures are currently running And the heap will start
 at the other end of memory and grow towards the lower address and so we
 allocate objects well be allocating from the back memory or the end of the memory
 allocated the program up towards the top of stack And If these two points have ever
 become equal and whether the two pointers So we have a stack allocation_pointer
 which says where we are going to allocate the next stack frame And we have a heap
 allocation point where it says where will allocate the next object if we have
 another dynamically_allocated object As long as one of these two pointers dont
 cross as long as it never become equal then the program has memory to either add
 another stack frame or another dynamically_allocated object and the program can
 continue away If these programs ever become equal then the program is in fact
 out of memory and at that point the run_time system will abort the program or try
 to get more memory from the operating or take some other course of action to deal
 with the fact If there is no there is no more memory But as long as these two
 pointers dont cross notice that this design Allows the heap and the stack to
 share this this data area in whatever way suits the program best So this same
 design without any changes will work for programs that needed a lot of heap and
 only a little stack and for programs that need a lot of stack and only a little heap
 and things will have a rough balance between stack and heap as long as they
 dont exceed the total memory allocated to the program
 In this brief video_were going to talk_about alignment A very low level but very
 important detail of machine architecture that every compiler writer needs to be
 aware of First lets review a few properties of Contemporary machines
 Currently most modern machines are either 32 or 64 bit meaning you have the 32 or 64
 bits in a word and the word is actually subdivided into smaller units We would
 say that there are eight bits in a bye and then four or eight byes in word depending
 whether its a 32 or 64 bit machine And other important_property is that machines
 can be either byte or word addressable Meaning that in the native language of the
 machine in machine code it may be possible to either name only entire words or it may
 be possible to reference memory at the granule area of individual bytes They say
 that data is word aligned if it begins at a word_boundary So if we think_about
 Data in memory or the organization in the memory and is laid_out into bytes And
 lets_say That this is a 32bit machines so that four bytes make a word and one
 word begins here and the next word begins here and if data is allocated on a word
 boundary say it needs more bytes then that would be a word a line a piece of
 data If a piece of data begins in the middle of the word so lets_say for
 example that begins here and we have some data thats allocated here this data is
 not word aligned doesnt begin on a word_boundary And the important_property or the
 important issue is that most machines have some alignment restrictions So these
 restrictions come in one of two forms So on some machines if the data is not
 properly aligned meaning that you tried to reference data that isnt aligned the
 way the machines requires then the machine may just fail to execute that
 instruction Your program may hang or even the machine may hang and its but the
 important thing is that program will not execute correctly So theres a its
 incorrect to not have the data aligned properly Now there are other machines
 that well actually al low you to put the data anywhere you like but at a
 significantly cause And maybe that accessing data that is aligned in word
 boundaries is cheaper than accessing thats on nonword boundaries And these
 performance penalties Are often dramatic so it can easily be ten times lower to
 access missile line data than to access data that has the alignment favored by
 that particular machine So lets_take a look_at an example where data alignment
 issue tend to come up One of the most_common situations where we have to worry
 about the alignment is in the allocation of strings So lets_say we have this
 string the string Hello and then we want to put it in memory So let_me draw our
 memory as a linear sequence of bytes so Ill mark out some bytes here And lets
 assume this is a 32bit machine so let_me make the word boundaries a little_bit
 heavier boundaries So one two three four Okay So there are the the word
 boundaries And now lets_say there were we are trying to have aligned data a word
 aligned data and so allocate this string beginning in the word_boundary So the
 each character will go on the first byte when e then l then l then o And now
 we may have terminating null depending_on how strings are implemented And lets
 assume that we do And this is fine placement of the strings extremely begins
 in the word_boundary and That assess by presumably any alignment restrictions of
 the machine and now the question is where does the next data item go So we could
 begin the next data item right in the next available byte and that would be good if
 we are very concerned about not wasting memory But I noticed that that data
 item will then be were aligned We may either run into correctness or performance
 problems if the machine has restrictions on the alignment So the simple solution
 here is to simply skip to the next word_boundary and allocate the next data item
 whenever it is on the next word beginning at the next word_boundary And what
 happens to this two bytes here well these bytes are just junks T heyre not used at
 all they never reference by the program It_doesnt matter what theyre value is
 because the program should never refer to them Its just unused_memory And note
 that if we didnt have the terminating zero then there would be the terminating
 no character then and then would be three unused bytes after the string So to
 summarize this is the general strategy for dealing with alignment when you have
 alignment restrictions Data begins on the boundaries typically word boundaries that
 are required and if the particular data that youre allocating has a none integral
 length Meaning that it doesnt end directly on the next required boundary and
 you just skip over whenever bytes are in between to get the data the next data
 thats going to be allocated on the correct boundary
 In this video we are going to move beyond our_discussion of Runtime Organization
 and begin talking_about code_generation And in this first you_know it was
 probably quite a long series of videos on code_generation we are gonna talk_about
 the simplest model for code_generation which is called a stack_machine So in a
 stack_machine you might guess that the primary storage is some kind of a stack
 and you would be right In_fact the only storage that the stack_machine has is the
 stack And the way the stack_machine works is that it executed an instruction and
 all instruction have the form Theres some function of some arguments and they
 produce one result And what that does is itll pop in upper hands for the stack so
 the arguments a1 through an are stored at the top of the stack It will then compute
 the function f using those operands and it will push the result r back on top of the
 stack Okay So lets_take a look_at a simple example Lets see how we would
 compute seven_plus five using a stack_machine So we would have our stack And
 initially the stack might have already have some stuff on it but we dont care
 what that stuff is and so it will execute seven_plus five What we would do well
 first we will have to get the seven and the five out of the stack so as we get
 pushed on stack and well see more about how that happens in a minute And lets
 say that seven and five were both on the stack And so now we wanted to compute the
 addition on seven and five so addition takes two arguments so we would pop the
 two arguments off the stack And we wined up with the five and the seven Popup the
 stack We will perform the operation plus and then the result will get push back
 under the stack So this would be good to twelve and then twelve will get push back
 on to our stack Okay And I noticed that I did indicate that there might be some
 other stuff on this stack already Let_me give that stuff a name And let_me talk
 about one very important_property of the stack_machine So those we have evaluated
 seve n5 we round up in the situation_where the results of that operation was on
 top of the stop of the stack Okay and the initial stack_contents was unchanged
 This stack the stack that was below the arguments that we are interested in didnt
 get modified Okay So we have survived through all the operations unchanged And
 this is an important_property of the stack_machine That we will exploit and the
 general to say what the general property is when you evaluate an expression the
 result of the expression will be on top of the stack and the contents of the stack
 prior to the beginning evaluation of the expression will be preserved So now
 lets_take by how we could program a stack_machine So lets have a language with
 just two instructions in it We can push an engine run to the stack and then we
 have the operation add which will add the two integers on the top of the stack And
 now lets_take a look_at this program which pushes seven and then pushes five
 and then does an add So lets think_about how this program would work Okay
 so we have our stack_contents and now and the first instruction is to push seven So
 wined up with the seven on the stack added to the stack and now we push five
 Okay And so the next step well have five and seven on top of the stack then
 well perform the add and then well pop these two elements off the stack and add
 them and push the result back on And well wind_up with twelve on the stack and
 again the original stack_contents are preserved Now what interesting property
 of stack_machine code is that the location of the upper hands and result is not
 exquisitely stated in the instruction And thats because these instructions always
 refer to the top of the stack And this is in contrast were register machine or
 register instructions that explicitly name where they take their upper hands from and
 where they put the results So for example you might be familiar from seeing some
 machine code or assembly_code in the past or and add instruction by typically take
 three registers two for the arguments two for the registered
 arguments are gonna be added together and one for the destination for the result
 where in the stack_machine we just have A single word add and no explicit naming of
 the arguments because its fixed where the arguments will come from The
 arguments will always be popped from the stack and the result will always be placed
 back on top of the stack And The interesting property here is that it leads
 to more_compact programs because we have to say less in the instructions the
 programs themselves are actually quite a bit smaller than register machine
 programs And this is one of the reason reasons that Java bytecode uses a stack
 evaluation model because it leads to more_compact programs and especially in the
 early days of Java when it was very expensive to ship these programs around
 the Internet to download them having very small compact code was a good property
 And by we might wonder why would we prefer register machine and the answer is that
 register machine code is generally faster because we can place the data exactly
 where we wanted to be We will generally have fewer you_know immediate operations
 and less manipulation of the stack pushing and popping stuff to get to the
 data that we want And then it_turns out that there isnt intermedia point between
 a pure stack_machine and a pure register machine thats interesting This is
 called an N register stack_machine And conceptually the idea of the N register
 stack_machine is to keep the Top end locations of the stack in registers And
 the particular variant of the unresourced stack_machine that we particularly
 interested in is the one register stack_machine because the terms that you get
 widely benefit by even having a single register thats dedicated to the top of
 the stack This register is called the accumulator so the dedicated registry here
 is called the accumulator Its called that because intuitively it accumulates
 the results of operations and then all the other data lives on the stack So what is
 the advantage of a one register stack_machine Well lets think_about the add
 instruction and how it works in a pure stack_machine So in the pure stack
 machine what is the add instruction going to do its going to pop two arguments from
 the stacks a five and seven And its going to add them and then its gonna put
 the result back onto the stack And lets just name the rest of the stack_contents
 there And that requires three memory operations After load two arguments and
 then store one result But in the one razor stack_machine the add operation
 actually does a lot of its work out of the one register So the one of the arguments
 is already stored in the register because thats the conceptually the top of the of
 the stack And the result will be pushed back on the top of the stack which again
 is just the accumulated register So here one of the arguments in the right are both
 taking from registers and theres only one memory reference to get the second
 argument from the portion of the stack thats stored in the memory So in
 general lets think_about how we would evaluate and arbitrary expression using a
 stack_machine So now this isnt I should_say you_know just stack_machine called
 like were looking_at it before This is not just a sequence of bytecode level
 operations this is actually a full expression as you might find in Kuhl so
 there are other complex expressions nested inside of some operation All_right And
 so forget the operation that takes N arguments and those arguments are
 expression that themselves needs to be evaluated so heres a general strategy for
 doing that with the stack machines So for each of the subexpression each of
 the arguments in order were_going to evaluate it recursively using the same
 stack_machine strategy and that will end up putting the result when we evaluate EI
 recursively the results will be in the accumulator And so the results is in the
 accumulator alright And then were_going to push that results onto the memory
 stack So wer e going to take that results and were_gonna free up the
 accumulator and save it on the stack the portion of stack thats in memory okay
 So we do this evaluating the subexpressions for the first and one
 arguments So everything except the last one okay Were_gonna use the same
 strategy for the last one for en We just evaluate We_dont push the result on
 the stack That just means that the result is left in the accumulator okay so now we
 have one of the arguments of the accumulator The last one we evaluated and
 the other in line as one are o the top of the portion of the stack thats in memory
 So that what we all have to do is we pop in one values from the stack and combine
 any compute up using the one values plus the value of the accumulator and we store
 the result back into the accumulator okay So thats the general strategy for
 evaluating an expression using a stack_machine So lets do this now for a simple
 example Lets_take our same example that weve_been using and lets evaluate the
 expression seven_plus five So how were_gonna do that Well were evaluating a
 plus expression and that takes two arguments two expression as the way to
 evaluate each of those So first we evaluate the expression seven Let_me
 actually let_me draw our stack here Okay so we have our initial content to
 the stack we have our initial accumulator And so now were evaluating
 seven okay And of course a constant loose evaluate to itself and the result is
 toward the accumulator okay So thats the first step after evaluating seven And
 now because thats the first argument to plus it has to get pushed on to the
 stack the portion of the stack in main memory So Now we have a situation that
 looks_like this All_right in the course to seven is still in the accumulator but
 were now about to override it were not gonna use that value again Because the
 next_thing were_gonna do is evaluate the second_argument to plus and that happens
 to be in this case also a constant expression five and so that will get
 evaluated and then stored in the accumulator Okay so I will override the
 seven This will be five there all right And now we have evaluated both arguments
 Okay remember in the case of just having two arguments The first argument gets
 evaluated and saved on the stack so it doesnt so we dont lose the value when
 we evaluate the second_argument And the second_argument we uses is the last one we
 can just leave in the accumulator And that way actually evaluates the plus Okay so
 we do the accumulator gets the accumulator plus the top of the memory stack So in
 this case that results in adding seven and five And we line up and of course we
 pop the argument from the memory stack right So we have just the original
 contents there and now the value twelve in the accumulator So as I_think you would
 see from the example the invariant that were_gonna maintain with the stack
 machine is that after we evaluate an expression e the accumulator holds the
 value of e so the result of evaluating e winds_up in the accumulator and the stack is
 unchanged And so the stack the memory portion of the stack is whatever it was
 before we start of evaluating e And this is a very very important_property
 expression evaluation preserves the stack So now lets look_at a more elaborated
 example just slightly_more elaborate three75 And the interesting thing about
 this example Is that now one of the arguments to the other plus is itself a
 compound expression So it would have to be that would have to be evaluated
 recursively as part of evaluating the entire_expression so lets see how this
 works So the first_thing thats going to happen or evaluating the outer plus were
 gonna evaluate the first argument to that plus thats just the constant three so
 were_gonna load it into the accumulator And thats the result of evaluating three
 And now because its the first argument to the plus we have to save it before we can
 get around to evaluating the addition itself So that result is pushed on to the
 stack And now were g onna evaluate the second_argument to the outer plus and that
 itself has two arguments And the first argument to that to the inner plus is
 seven And so that winds_up getting stored in the accumulator thats the result of
 evaluating seven And then because the inner plus has two arguments we have to
 evaluate the second evaluate the second_argument to the inner plus the seven has
 to get saved to the stack So now the stack has seven three and whenever it had
 before we start it Next were_gonna evaluate the second_argument to the inner
 plus And so evaluating a constant five will result in five being loaded in the
 accumulator and now we have evaluated all the arguments to the inner plus okay And
 so we know from our stack discipline that the last arguments is in the accumulator
 and the first argument will be on top of the stack So the next_thing that will
 happen is that well pop that second_argument from the stack added to the
 accumulator and store back into the accumulator and so now we have the results
 of the inner plus in the accumulator We also have the pop the seven from the
 stack okay and finally now weve evaluated the second_argument to the outer
 plus So now we can perform the outer edition And what is that involve that
 takes the stack_contents then adds it to the value that is currently on the top of
 the stack which is the value three which is what we saved a long_time ago now to
 to remember it from what it was to do the other addition and we wind_up After we
 pop the stack with fifteen in the accumulator thats the results of the
 entire_expression and notice its the same stack that we started with Okay So
 evaluating this entire_expression resulted in the result in any accumulator
 and the stack being unchanged And if you looked_at that the subexpression you can
 see that the same things happened So lets_take a look_at the evaluation of
 seven_plus five So where that take place that started here Okay Started at this
 instruction And it lasted down to here and you can see that the evaluation of
 seven five which encompasses these five expressions resulted in twelve being put
 on top of the stack thats the result of seven five and it didnt affect the
 contents Im_sorry It resulted in twelve being placed in the accumulator and it
 will left the stack unchanged to where it was when the evaluation of seven_plus five
 began So here is where it began and the value we had saved three was on the top of
 the stack and when were done evaluating seven_plus five indeed again the value
 three and All the other stuff that was there before are still on the stack
 After numerous videos on run_time organization and stack machines we are
 finally ready to begin our_discussion of code_generation So as I mentioned in the
 previous_video were_going to focus_on generating_code for stack machines This
 is probably the simplest code_generation strategy It_doesnt generally yield
 extremely efficient code Its an interesting strategy and certainly not
 totally not an unrealistic one Its more_than complex enough for our purposes We
 want to run a real machine and were_going to the mix processor In particular were
 going to use a simulator from it which runs on about any kind of hardware so that
 will be very convenient for the course project And the basic_idea the basic
 strategy is going to be to simulate a stack_machine using Mipps instructions
 and registers So the first decision in designing our simulation is deciding
 where to put the accumulator in Well keep that in this register A0 Any
 register would have done but well just use A0 always for the accumulator And then
 the stack is going to be kept in memory And I should point out here that when we
 talk_about a one register stack_machine nominally that register in this case A0
 is the top of the logical stack of the stack_machine But just to avoid confusion
 in the terminology Im going to refer to A0 as the accumulator and the stack as all
 of the other data thats kept in a memory stack on the MISC processor so well just
 consider A0 the accumulator to be distinct from the stack which lives in memory And
 the stack on the MIPS will grow towards the lower addresses which is the standard
 convention on MIPS The address of the next location on the stack is going to be
 kept in the MIPS register sp and this register actually has a mnemonic name that
 stand for stack_pointer So normally on the MIPS machine compilers use SP to
 point to their stack and the top of the stack will always be at the address SP
 plus four So remember the stack is growing towards low addresses and the
 address in the stack_pointer is the ne xt unallocated location on the stack So the
 stack_pointer actually points to unused_memory and the top of the stack
 therefore is at the next higher word address which would be SP plus four Now
 the MIPS architecture is quite an old architecture It was designed in the
 1980s and it was or is the prototypical reduced instruction set computer or risk
 machine And the idea_behind RISC machines was to have a relatively simple
 instruction set Most of the operations used registers for operands and results
 And then load and store instructions are used to move values to and from memory So
 primarily all the computation takes place in registers and the memory operations
 are primarily are just loading and storing data There_are 32 purp there are 32
 generalpurpose registers on the MITS its a 32 bit machine Were only going to
 use three of those registers We already_talked about SP the stack_pointer A0 the
 accumulator and well need one more register for temporary values So some
 operations that take two arguments like plus and times will have to have two
 registers to hold the arguments to the operation So well use the accumulator
 for one of those and a temporary_register for the other And there is a lot more
 information on the MIPS architecture in the SPIM documentation Spim is the
 simulator that we well use to execute MIPS code Now of course to generate
 code for the mix Well also need some mix instructions And well be_able to get
 away with just a very small number of instructions Five in fact for our first
 example And here they are So the first instruction we need is load or load word
 And the way this works is it takes the value of register two takes the contents
 that are in register two Adds a fixed offset So this is a number thats
 directly in the code Adds a fixed offset to that to the contents of register two
 Thats a memory address It loads the value of that memory address into register
 one The add instruction adds the contents of register two and register three
 together and stores the results in register one again The store operation
 or store word operation takes the value in register one and stores it into memory So
 thats stored at a memory address and with the memory address is the contents of
 register two plus a fixed offset thats in the code And an add immediate
 unsigned takes is an unsigned add and it takes a value in register two an
 immediate value So this is just a number thats a constant thats directly
 embedded in the code It adds that to the value register two and stores the result
 in register one And the unsigned aspect here just means that the overflow is not
 checked were not were not checking whether we generate a number thats
 beyond beyond what we could represent if we had sine numbers Finally load
 immediate just takes a constant thats in the code and puts it into the register
 thats named as the first argument Alright So those are the five
 instructions that we need to do a one very_simple example So now were_ready to
 do our first program and not surprisingly its the same program that we looked_at in
 previous videos when we were talking_about stack_machine code So lets look heres
 the program for adding seven_plus five written out in our little abstract stack
 machine language Now our goal is to implement this program using MIPS
 instructions So over_here on the right Im going to layout the instructions we
 would use to simulate this program or implement this program on the MIPS machine
 Alright So the first instruction is to load seven into the accumulator And we
 can do that with a load_immediate Were_going to load_immediate the value seven
 A0 is our accumulator register and so this instruction puts seven in the
 accumulator Next instruction we want to push the value of the accumulator onto the
 stack How do we do that Well we have to store the value onto the stack and
 remember the stack_pointer points to the next unused_memory location And so were
 just storing directly at what the stack_pointer points to so thats at zero
 offset from the stack_pointer The value of the accumulator pushes the value onto
 the stack and now to restore the invariant That the stack_pointer points
 to the next unused location we have to subtract four from the stack_pointer
 Okay So these two instructions together implement a push they push the data value
 onto the stack and they move the stack_pointer to the next unused address
 Alright now Im ready to do the next instruction loading five into the
 accumulator Well we already know how to do that Well be a load_immediate into
 the accumulator register A0 the immediate value five Are now ready to do the add And
 how does that work Well first we have to load the value of thats on the top of
 the stack alright Because its like an argument is taken from the top of the
 stack And since MIPS can only do operations out of registers that value
 has to go somewhere into a register And this is where we use our temporary
 register So now this value is now at offset four from the stack_pointer
 because we subtracted four from the stack_pointer And we load it into register T1
 Okay And then we can actually perform the add And so we add the accumulator to the
 value of T1 and we store the result back into the accumulator And finally were
 going to pop the stack so were done with the value thats on the stack And how do
 we pop Well we just add four to the stack_pointer and that moves the stack
 pointer back popping that value off of the stack
 In the next two videos were_going to be looking_at code_generation for a language
 thats higher_level than a simple stack_machine language weve_been talking_about
 so_far So heres a language with integers and integer operation and this was the
 grammar So a program consists of a list of declarations and whats a declaration
 A declaration is a function definition so it has a function name the function takes
 a list of arguments which are just identifiers and the function has an
 expression which is the body of the function And what in function bodies look
 like well they expressions can be integers identifiers if then else and the
 only predicate that were_going to allow is an equality test between integers and
 then sums of expressions differences of expressions and function_calls Now well
 just say that the first function definition in the list is the entry_point
 This will be the main routine or the function that gets run when the program
 starts And this language is expressive enough to write a Fibonacci function And
 here it is And this is just a standard definition if X is one then the result is
 zero If X is two the result is one Otherwise its the sum of fib of X minus
 one and fib of X minus two Now its a two code_generation for this language We
 need to generate_code for each expression E we need to produce MIPS code for each
 expression E that accomplishes two things First of all that code is going to
 compute the value of E and leave it in the accumulator A zero Right So when the
 code for E is done the value of E will be stored in the accumulator And furthermore
 E is going to the code for E excuse_me the generated code for E is going to
 preserve the stack_pointer and the contents of the stack That_means whatever
 the stack is when we started executing E or the code for E the stack will be
 exactly the same after were done executing the code for E And were_going
 to write a code_generation function Cgen of E that produces code Okay So Cgen
 would be something that produces a program It produces code that will
 accomplish these two things Now our cogeneration function is just going to
 work by cases And to begin_with lets focus_on the expressions and were just
 going to have different kind of code or a certain kind of code thats generated for
 each kind of expression in the language So to evaluate an expression which is a
 just an integral constant all we have to do is load that constant into the
 accumulator So the code_generation for ‘I for the constant ‘I is the
 instruction load_immediate into the accumulator the value of ‘I And its
 easy to see that this preserves the stack as required so this doesnt modify the
 stack_pointer or the contents of the stack at all so the stack is exactly the same
 before and after the execution of this instruction gtgt And another thing I want
 to point out or I want to emphasize here is Im going to be following a convention
 that things that are in red are things that are done at compile_time And things
 that are in blue are things that are going to be done in run_time So in this case
 at compile_time we execute the function C gen of I And that produces code Here that
 will run at run_time okay So C gen of I something that would execute a compile
 time and it produces a program that will be executed at run_time And this is to
 help you separate in your mind and and to develop a very firm grip on the idea
 that we have a real division of time in these programs Theres stuff that happens
 inside the compiler and then theres computation thats deferred until the
 program that we are producing actually executes All_right now lets look_at
 another example Lets lets_take on the addition of two expressions and think
 about the code that gets generated for that So what are we going to do Well
 the first_thing that happens_when we execute E1E2 is that we have to compute
 the values of the sum expressions we have to know what integers were_going to add
 So we better generate_code for E1 And thats going to happen at com pile time
 Were definitely going to generate that code at compile_time And then once_weve
 got the value of E1 well remember we only have one register stack_machine so
 were_going to have to save that value somewhere until we also know the value of
 E2 and where were_going to put it Well do what we always do well put it on a
 stack So E1 The the the code for E1 is guaranteed to leave the value of E1 and
 the accumulator So what were_going to store the value of E1 onto the stack And
 we know how to do that We store A0 onto the stack and then we have to bump the
 stack_pointer gtgt And then we can generate_code for E2 Okay and again this stuff in
 blue is a part of the program that will be executed at at run_time These_are calls
 to the cogenerator that are happening at compile_time And so we generate the code
 for E2 and then that goes here after this code for pushing the value of E1 on the
 stack And once we have the value for E2 now we can perform the add So how do we
 do that Well first we retrieve the value of E1 So we load the value of E1 Which is
 on the stack And notice that This works because E2 is guaranteed the code for E2
 is guaranteed to preserve the stack You_know this code for E2 here and let_me
 digress for a moment this code for E2 can be arbitrarily complicated This could be
 a whole program It could go call functions It could allocate data
 structures It could print things out It could do all kinds of complicated things
 But because we have this invariant that all code_generation for all expressions
 will preserve the stack we know that no matter how complicated this is and how
 long it takes When its done executing the stack will be in the same state And
 thats what allows_us to know Where to find the value of E1 that we stored away
 Its going to be at the top of the stack Okay so we load the value B one back into
 a temporary_register now we can do the add Okay so we add T1 and A0 together
 and store that back in the accumulator And now we have to pop the stack And now
 notice that this is all the code here for E1E2 and when were done weve
 established our the value of E1E2 is in the accumulator That was established by
 this instruction And this pop here restores the state of the stacks Now the
 state of the stacks here is exactly what it was when we entered this block of code
 up here Now to be completely precise I really should write this code_generation
 function out a slightly different way And that would be like this So what were
 really doing here is we are generating_code for E1 and then were printing out
 into a file or something_like that the code to do the push Okay and then we
 generate the code for U2 And now these calls the code_generation are also
 printing in to the same file okay So here you_know they just printed out
 the instructions whatever the instructions are like security one this
 is printing out the code to execute to do the push You print_out the code to do U2
 And then we print_out the code to do the ad and the pop Fence Yes The add and the
 pop Okay and this is just a this is much_more verbose over_here and so Im
 trying to go in and leave out the prints and just indicate in blue the instructions
 that are deferred but I hope you understand what this means Everything in
 red here of course is being done in compile_time so you_know were calling
 these cogeneration functions a compile_time The print statements are being
 executed in compile_time and then were accumulating somewhere in some data
 structure or in a file all the instructions that will be executed at run
 time So lets think_about a possible optimization to this code Instead of
 pushing the result of E1 on the stack what if we stored the result of E1 in a
 temporary_register T1 What would the code for that look like gtgt Well in that world
 to generate_code for E1 plus_E2 what would we do Wed generate_code for E1 and
 that would be followed now by instead of pushing the result on the stack we would
 take the result of E1 which of course is in the accumul ator A0 we would store it
 in a temporary_register And then we would generate_code for E2 Alright that we
 followed_by the code for E2 and then we could just do the add We would take the
 result of E2 which is in the accumulator A0 add it to the contents of T1 and
 store that into the accumulator A0 and of course theres_no pushing and popping from
 the stack here so this code preserves the stack and it looks_like anyway that it
 actually puts the value of E1 plus_E2 into the accumulator Unfortunately this
 code is incorrect so this is actually wrong and you dont want to do this And
 to see why lets_consider what would happen If E2 Was itself the actually
 lets do it for a concrete example Lets do the example one plus two Plus three
 Parenthesize like that okay So whats going to happen so E1 here so were
 doing one plus two plus three So this will be a load_immediate the first the
 code for E1 will be a load_immediate into A0 of the number one Okay and then well
 have the move Well try to save that value I in temporary_register T1 And
 now were_going to generate_code for E2 And whats E2 Well E2 is itself a plus
 expression So were_going to recursively call the code_generator to generate_code
 for two3 So we generate_code for the new first expression So that will be a
 load_immediate into A0 of the value two And now you should be_able to
 see whats going to go wrong because Since this uses the same cogeneration
 strategy its also going to try to use T1 to hold the temporary value So its
 going to move the accumulator into T1 thereby clobbering the value of the
 previous self expression that we had evaluated the number one Okay so that
 values going to be overwritten and then were_going to do and add And oops I may
 have made a mistake were not going to do an ad let_me erase that Forgot to
 generate the code for the three so now we load the value of three I in to the
 accumulator And now we can do the add now comes the add And so we do A0 T1 A0 and
 when you execute this what do you get You get two three which is five and thats
 fine but now Now we have the value of this sub expression In the accumulator
 and now ready to do the outer add So that generates another add instruction Which
 is exactly the same But unfortunately the first value of T1 the first temporary we
 tried to restore has been overwritten And so whats in that whats in T1 at this
 point is the value two instead of the value one and we get that one237
 Which is not what we wanted And so the problem here of course is that in the
 presence of nested expressions and particularly nested expressions of the
 same kind if the expressions try to use a fixed register for their temporary
 values then if you try to generate a code for two different expressions that are
 nested sorry two expressions of the same kind that are nested beside each
 other they will step on each others temporary intermediate results And so
 thats why we have to use a stack to store intermediate values So this example
 illustrates a couple of features of code_generation that I just want to emphasize
 First of all notice that the code for plus is really a template that has holes
 in it for the code for evaluating E1 E2 that is there are some fixed instructions
 that we admit And then there are places_where we plug in the code for E one and
 the code for E two okay so thats what I_mean by a template so theres some fixed
 stuff which are the instructions that actually do the ad and then theres a
 place_where we can just plug directly in arbitrary code whatever it is for
 implementing E one and E two and well see the same pattern with all the other
 kinds of expressions The other important point is that stack_machine code
 generation is recursive That is you_know the code for E1 plus_E2 is code for E1 and
 E2 glued together and recursively regenerate code E1 and E2 which will have
 their_own templates and may even be other expressions of the same kind as we just
 saw And what this means is that code_generation can be written as a recursive
 descent of the abstract_syntax tree at_least for the expressions Alright so
 lets_consider another new instruction Lets add the subtraction instruction And
 this is just like addition instruction so sub just subtraction to register instead
 of adding them And code_generation then for subtraction expression as you might
 imagine look and awful like code_generation for a plus expression So what
 do we have first we have a place_where we plug in the code for E1 gtgt Then we have
 to store the value of E1 on the stack We have to remember that intermediate result
 And then we can go off and compute the value of E2 So this is where the code for
 E2 gets plugged in And then at the end we load the value of E1 back into a temporary
 register I actually do the operation the subtraction and then pop the stack And
 the thing to do note about this code is that its exactly the same as the code for
 addition except for this instruction right here where we do a subtraction instead of
 an add
 This video is a continuation of the previous_video where well be finishing up
 cogeneration for the simple language dealing with function_calls function
 definitions and variable_references So just to remind you what were working_on
 here is the simple language And again we have a bunch of different_kinds of
 expressions And we dealt with all of these last_time except for variable_references
 and function_calls And of course we also have function definitions So as I said
 in the introduction these are the three constructs well be looking_at in this
 video gtgt The main issue in designing the cogeneration for function_calls and
 function definitions is that both of these will depend intimately on the layout of
 the activation_record So really cogeneration for function_calls
 cogeneration for function definitions and the layout of the activation_record all
 need to be designed together Now for this particular language a very_simple
 activation_record will be sufficient Because we are using a stack_machine we
 are modeling a stack_machine in our code_generation The results of a function_call
 will always be in the accumulator and that means there is no need to store the
 results of the function_call in the activation_record And furthermore the
 activation_record will hold the actual_parameters So when we go to computer
 function_call with arguments X1 through XN we will push those arguments onto the
 stack And as it happens these are the only variables in this language that are
 no local or global_variables other than the arguments to a function_call And so
 those are the only variables that will need to be stored in the activation
 record Now recall that the stack_machine discipline guarantees that the stack
 pointer is preserved across function_calls So the stack_pointer will be
 exactly the same when we exit from a function_call as it was when we entered
 the function_call And this means we wont need a control link in our activation
 record The point of a control link is to help us find the previous activat ion and
 since the stack_pointer is preserved it will have no trouble finding it when we
 return from our function_call and well never need to look_at another activation
 during a function_call since there are no nonlocal variables in the language We
 will however need the return_address and that will need to be stored somewhere in
 the activation_record And one more thing It_turns out that a pointer to the current
 activation will be useful Now this is to the current_activation not to the
 previous activation And this pointer will live in the register FP which stands_for
 Frame Pointer This is a conventional this is a this is the register name on
 the MIPS and the name is chosen to denote the frame_pointer And by
 convention the compilers put the frame_pointer there What the frame_pointer is
 good for well it points to the current frame so thats what the name comes from
 But what its good for well see in a few_minutes Right so to summarize for
 this language an activation_record that has the callers frame_pointer The actual
 parameters and the return_address will be sufficient So lets_consider a call to
 the function F and has two arguments X and Y Then at the time the call is performed
 before we start executing the body of the function this is what the activation
 record will look like So well have the old_frame pointer So this is the frame
 pointer that points to the callers frame Not to the frame of the function that
 were executing And the reason that it does that is that we have to save it
 somewhere because the frame_pointer register will be overwritten with the
 frame_pointer for the current_activation so we have to save the old one so that we
 can restart the caller when we return to it from the current function And then
 there the arguments of the function and those that are pushed on the stack in
 reverse order So the last argument is pushed on first and the first argument is
 at the top of the stack And the reason for doing it this way is itll make the
 indexing to find the a rguments a little_bit easier A little_bit simpler And then
 We have the stack_pointer so theres a theres nothing here What will go here is
 the callee the function that were calling will push on the return_address
 So this is where the return_address will go And these elements the callers frame
 pointer the arguments to the function and the return_address of the call function
 will make up the activation_record of F A bit of terminology the calling_sequence
 is the sequence of instructions that both the caller and callee to set up a function
 invocation okay So thats referred to in compiler lingo as the calling_sequence And
 were_going to need a new instruction to show the calling_sequence for this for
 for function_calls And that will be the jump and link instruction So jump and
 link what it does is it jumps to the label that its given as an argument And it
 saves the address of the next instruction after the jump in link in the register
 RA Which stands_for return_address So what would happen in the jump in link
 instructions if I have jump in link to label L And then theres an add
 instruction that comes next I dont_know what it is Its the address of this
 instruction the one after the jump in the link that will be stored in the ret in
 the in the register RA So this instruction will jump to L It will store
 the address of this add instruction in RAb And it will execute whatever code is at L
 And then the code thats at L can execute a jump back to the address in here to
 execute the return to the caller So now were_ready to actually generate_code for
 a function_call expression So lets_say we have the call F of E1 To EN Where of
 course E1 through EN are expressions And let_me change colors here So these are
 expressions here not values So how are we going to do that gtgt Well the first
 thing were_going to do is were_going to start building the activation_record And
 so we save the current frame_pointer This is the frame_pointer for the collar gtgt
 Okay gtgt This is pointing to th_e collars frame gtgt Right gtgt And we store that at
 the stack_pointer We have to bump the stack_pointer And then we generate_code
 for the last argument for EN right And so that code gets inserted here And then
 we push it on the stack So we store the results of EN which will be in the
 accumulator A0 on the stack and then we bump the stack_pointer Alright and well
 do that for all the arguments finishing up with E1 So we generate_code for E1 and
 we push it onto the stack So now all the arguments are on the stack and now we just
 do the jump in link So weve done as much of the work or much of the calling
 sequence as we can do on the callers side So this code is executing in the
 function in the caller Okay so this is the caller side of the calling_sequence
 and it builds up as much of the activation_record as it can In particular its
 evaluating the actual_parameters and pushing them on to the stack to form part
 of the activation_record for the called function and then we do the jump and
 link And we jump to the entry_point of the function that were calling So were
 this is a call to to F and so we jump to Fs entry_point So a few more things to
 note First of all as we discussed on the previous_slide When we execute the jump in
 link instruction that is going to save the return_address in the register RA And that
 address will be this address here the one that comes_after the the address of the
 next instruction after the jump in link instruction And youll notice also that
 the activation_record weve built so_far is four times N plus four bytes So this
 is where N here is the number of arguments Each argument takes up four
 bytes and then four bytes for the old_frame pointer Now were_ready to talk
 about the callee side of the calling_sequence And were_going to need one new
 instruction for that The JR instruction stands_for jump register And it just
 jumps to the address in its register argument So now the callee side is the
 code for the function definition okay So this is the co de that actually executes
 the body of the function And how do we generate_code for that Well lets_take a
 look Now actually the very first_thing that should be here is that this first
 instruction of the call side is the entry_point So were missing the label
 here So this would be labeled F entry Okay So this is the target of the jump in
 link instruction And then the very first_thing we do is we set up the frame
 pointer So we copy the current_value of the stack_pointer into the frame_pointer
 That sets that points to the end of the frame for the callee for the new
 function thats being executed We also save the return_address at the current
 position on the stacks Remember there was one more thing to do one thing one thing
 that was missing On the caller side on the caller side of the sequences which is
 the return_address We_dont know the return_address until after the jumping
 link instructions executes And so the callee is the one that has to save that
 value Okay so after the jumping link the RA register contains the return_address
 and that we save it into the frame All_right and then we push the stack_pointer
 Kay And now we just generate_code for the body of the function So now the at
 this point the activation_record is completely set up and now we can just
 generate_code for the function body And after the function body executes of
 course the stack_pointer will be preserved and and that means that the
 return_address will be at four offset from the stack_pointer so we can load the
 return_address back into the return_address register And then we can pop the
 stack So here were_going to pop off The current frame from the stack And thats
 going to be song size z Which we I havent shown you what it is yet But
 well calculate The size of z in just a minute This is going to be an immediate
 value So its a constant that we plug in there And then we load the old_frame
 pointer Okay So once_weve incremented the stacks we popped off the existing
 frame and so now were pointing at the frame_pointer at the first were were
 were pointing at the first_thing beyond the previous stack frame and what was
 that well that was the first_thing that we saved in the stack frame for F and
 thats the old_frame pointer So now we restore the old_frame pointer so that the
 call the function that called us well have its frame_pointer back and then now
 were_ready to return it resume execution of the calling function We just do that
 by a jump register to the return_address All_right So note here that the frame
 pointer points to the top of the frame not the bottom of the frame Okay So that
 will actually be important when we talk_about how we use the frame_pointer When we
 get to talking_about the variable_references next And the callee pops the
 return_address The actual arguments in the saved value of the frame_pointer from
 the stacks So the callee pops off the entire activation_record and also
 restores the callers frame_pointer And whats the value of Z Well there are N
 arguments Each of which take up four bytes So theres at so the size of the
 activation_record is four times N Plus there are two other values In the
 activation_record One is the return_address And the other one is the old
 frame_pointer Okay and the space for two more words is eight bytes So thats the
 size of the activation_record So thats how much we have to add to the stack
 pointer to pop the activation_record for F off the stack Just to give you a sketch
 of what this looks_like before the call We have the frame_pointer for the caller
 and we have the The current_value of the stack_pointer And on entry to the
 function Okay after the calling after the calling functions side of the calling
 sequence has completed whats on the stack well we have the old_frame
 pointer and the two arguments and then the stack_pointer points to the next
 unused location Which is where the return_address will go Alright then we do
 the jump and link We jump over and the return_address gets pushed on to the
 stack a nd the frame_pointer gets moved to point two the current_value of the
 frame Okay youve got to point to the top of the frame Okay And then after the
 call what has happened Well weve popped everything off the stack weve
 popped the entire Your activation_record for the call function off of the stack And
 so now notice that were back in the same state So again function_calls have to
 preserve the invariant that The stack is preserved across the call so the stack
 should be exactly the same after the call as it was on entry to the call So we are
 almost done with code_generation for simple language The last construct we
 need to talk_about is how we generate_code for variable_references Now the variables
 of a function again are just its arguments just the parameters to the function There
 are no other kinds of variables in this simple language And these variables are
 all in the activation_record So we really all we have to do is generate_code to look
 up a variable in its appropriate place in the activation_record But there is one
 problem and thats that the stack does grow and shrink with intermediate values
 So when you call a function and you begin executing its body values will be popped
 and pushed onto the stack beside the activation_record So think back to the
 code_generation for plus and minus and if then else intermediate values were being
 pushed and popped from the stack And so what this means is that these variables
 that are in the activation_record are not at a fixed offset From the stack_pointer
 So we cant use the stack point very easily to the side or to find those
 variables So the solution is to use the frame_pointer The frame_pointer always
 points to the return_address in the activation_record and because it doesnt
 move during the execution of the function body we can always find the variables at
 the same place relative to the frame_pointer So how do we do that Well
 lets_consider the ith argument X of I and does the ith argument to
 the to the function So where is that going to be relative to the frame_pointer
 That will be at offset Z from the frame_pointer And Z is just four times I Right
 and this is actually the reason here for generating for pushing the arguments on
 the stack in reverse order starting_with the last argument to the function because
 it just makes this index calculation simple It wouldnt be that much_more
 complicated if we pushed the arguments in the other order It just makes it a little
 easier to see how the indexing works And anyway this index this offset is being
 calculated at compile_time So notice that this number this four times I is
 something that the compiler knows and what were putting in the code here is
 just a fixed offset So we are not actually doing that multiplication at run
 time See here is just a number as computed statically by the compiler So
 anyway We just load and off send Z which is the four times I where I is the index
 the position of the variable in the list of parameters At that offset from the
 frame_pointer thats where XI is stored in the activation_record And we just load
 it into the accumulator So that is the entire code_generation for a variable
 reference Heres a little example So for the function the hypothetical function
 that weve_been looking_at with two parameters x and y X is going to be at
 the frame_pointer four and y will be at the frame_pointer eight
 In this video_were going to generate_code for a small example program The program
 well take a look_at takes a positive imaginary X and sums all the numbers from
 O up to X So if X is O then the result is O Otherwise it is X plus the sum of all
 the numbers up to X minus one So this isnt a interesting program but it does
 illustration all of the features that we discuss in the previous couple of videos
 So lets dive right in and talk_about how were_going to generate_code for sum two
 So we begin by giving it a label for the entry_point to the function so thatll be
 the sum two entries Alright and now we have to generate_code for the callers
 side call callee side excuse_me of the calling_sequence So what was that So the
 first_thing we have to do is we have to set up the frame_pointer which would just
 be the value of the stack_pointer So thats the frame_pointer for this
 activation and Then were_going to have to store the return_address at the current
 value of the stack_pointer And then were_going to move the stack_pointer
 into the Whenever we store something on the stack we have to move the stack
 pointer to the next unused location Alright Okay And so now we have to
 generate_code for this if then else All_right And the very first_thing if you go
 back and look_at the code for if then else is to generate_code for the first sub
 expression of the predicate So were_going to generate_code for X and thats
 really easy And were generating_code for a variable just looks up the variable in
 the current position of the frame Sorry at the correct offset from the frame
 pointer alright Alright so once we do that now we are generating_code for the
 predicate And how do we do that Well we generate_code for this first sub
 expression and now we have to save that sub expression somewhere Because we are
 going to generate_code for another sub expression So the equality there is a
 binary operator so we have to save the value we just computed somewhere on the
 stack Alright So well do that so well st ore the value of a zero on the stack
 And that will involve as always moving the side pointer Okay and now we generate
 code for the second subexpression of the predicate All_right thats also easy
 Thats just load the immediate of the immediate value into the accumulator
 alright And now Im going to load the value that we said the first or we move
 the predicate back into a temporary_register and actually do the comparison
 So this is more code as actually part of the conditional alright so we do a load
 word Entity one Of the value that we saved before Okay and now we need to pop the
 stack okay Well do that here because were done with that value Alright and
 now were_going to do the branch So now we test whether The two subexpressions
 of the predicate are equal or not and if they are then we jump to the true_branch
 And here Im going to give the true_branch a unique label because this might be part
 of a larger program where there are many ifthenelses and so Im going to append
 some identifying number on the end Instead of writing out true_branch Ill
 call this true one Alright Okay and then if we fall through then were on the
 false_branch well call that false one And now were generating_code for the
 false_branch which is this summation here Alright And how are we going to do that
 Well this whole_thing is a plus expression which means we have the
 generic code first For the first subexpression which is just X Alright
 So what do we do Well we load To generate_code for x we look up x at its
 current offset And that is the appropriate offset in the frame using the
 frame_pointer Okay It is the only argument and so its at four from the
 frame_pointer Im_sorry the only argument to the procedure and so thats stored at
 the first_position for arguments which is always four from the frame_pointer in our
 scheme All_right and now that weve loaded it we have to save it because it is
 part of a binary operation so were_going to save that value on the stack Kay And
 now we will adjust the stack_pointer Okay And what are we going to do next Well
 now weve weve we computed this subexpression this X We cant do the
 plus yet until we compute the second subexpression which is the function_call
 Alright So now we have the generate_code for the function_call and Im going to
 move up here to the other side of the screen here to to show the rest of the
 code Okay And the first_thing we do to generate_code for the function_call Is to
 start setting up our activation_record Alright This is even setting up the new
 activation_record for the function_call that were about to make Alright So what
 do we do there We store The frame_pointer Kay use this to our old_frame
 pointer Add the stack on the stack sound Alright and now we have to
 compute the argument All_right We have to compute the x1 So that code gets
 inserted here in the template for our function_call So whats going to happen
 there Well were completing subtraction so the template for subtraction is to
 first generate_code for the first subexpression then generate_code for the
 second subexpression and then subtract them All_right so lets do that So
 first we generate_code for x again Okay and now since its the first argument of
 a binary operation were_going to save it on stack Alright now we generate_code for
 the second_argument of the subtraction Okay and now we perform the subtractions
 so we have to load the first argument back into a temporary_register Have to
 actually do the subtraction Excuse me here Alright and then we can pop the
 temporary value off the stack Okay now we have actually done subtraction Let_me
 see that There is everything from about here to down there is computing x minus y
 Okay So this is computing x And this was computing one And then this whole
 thing is computing the subtraction Alright So now we compute the argument
 What are we going to do Well we save it on the stack So now we save the result on
 the stack Were saving it into the new activat ion record that were building
 Alright And then we have to advance this or move the stack pointers as always And
 now were_ready we have to do the function_calls And now we do the jump in
 the link to the entry_point of sum two Okay And now when this returns what its
 going to return with its going to return what the result of computing the sum to in
 the accumulator all right And so then were_ready to perform the addition And
 now weve computed the second_argument to the addition and how do we do that Well
 look back at the template for addition the next_thing what_happens is we reload the
 temporary value that we saved on the stack Alright and now we got actually
 perform the edition Okay And then we could pop the temporary value of the stack
 Alright And that actually ends the the else branch the false_branch of the
 entire if and else And theres now a branch around the rest of the if and else
 code And well call that label if and one And now comes the code for the true
 branch And what we are going to put there well its not very complicated
 because all were doing true_branch is loading or generating codes for zero which
 is a single load_immediate load_immediate Alright And thats the entire true_branch
 and so now were at um there should not be a colon there excuse_me and in fact I can
 just erase that a little_bit Alright And now were at and actually I see it notice
 in the wrong place so lets fix that so this is a branch at the end of the false
 branch at the end of the else part of the if and were_going to to branch
 around per quote for the two branch which is only one instruction And so the very
 next instruction is the label end if So now whats left to do weve generated code for the whole if then else so
 now it goes here is the rest of the template for the function definition so
 now we have to generate the code returns back to the caller and how do we do well
 we have to load the return_address The on the stack okay And now we pop the stacks
 so we pop the entire activation_record off the stack and now because of the
 activation reco rd well remember theres always two words One for the return
 address and one for the frame_pointer and then a number of words equals to the
 number of arguments where theres only one argument here so we have three words so
 its twelve bytes So we increment the stack_pointer by twelve all right And
 then we load the old_frame pointer we store the frame_pointer Okay and then we
 return So one more instruction well do a jump register to the return_address And
 that is the entire code for this simple functions sum2 And theres a couple of
 things to point out So first of all the the code is constructed as a bunch of
 templates pasted together and I try to point out as we go along how that
 works But we do lined up with one linear sequence of code Alright and if if
 youre all confused as we work as to go_back and look_at those templates and look
 at this example and understand how the code all fits together and how it works
 And the other thing I would point out is just that this is your extremely
 inefficient code so later here where we were generating_code to check_whether x0
 Notice here that we we load x so this is a load Of x And then we immediately store
 the x again into the stacks we just loaded it now from the frame then we
 immediately store it back in the memory and then we and load the immediate value
 then we reload the value of x here So you_know moving the value of x we you
 know all around So we load it we store it we load it again and this was a lot of
 wasted motion here and thats a result of this very_simple cogeneration strategy
 where we want to be_able to compose code together We will be_able to compose these
 templates in a way that it will work properly This code does not have to be
 this inefficient in a lot of the techniques of what we discussed in
 subsequential lectures we talked_about in a smarter code_generation techniques and
 also optimizations like even improve the code further
 In the last_couple of videos we have talked_about code_generation for simple
 programming_language and I mentioned at the end of the last_video that realistic
 compilers do things a bit differently and in particular they do a better job of
 keeping values and registers and of managing the temporaries that have to be
 stored in the activation_record Were actually going to talk_about both of those
 problems In this particular video_were only going to talk_about the second one
 and so were_going to be covering a better ways for compilers to manage temporary
 values So the biggest idea which weve_already seen is to keep temporaries in the
 activation_record Now this is not as efficient as keeping temporaries in
 registers but thats the subject of a future video_were not going to talk
 about that today What were_going to talk_about is improving the language we manage
 temporaries that happened to be in the activation_record for whatever reasons So
 why it doesnt_matter why we want them to be into activation_record but given that
 its there thats the most efficient code that we can generate And the improvement
 that were_going to make Is have the cogenerator assign a fixed location In
 the activation_record for each temporaries Were_going to preallocate
 memory or a spot in the activation_record for each temporary and then we will be
 able to save and restore the temporary without having to do the stack_pointer
 manipulations So lets_take a look_at the inaudible program for a simple
 programming_language Here is the Fibonacci function again and let_me change
 colors to something that says more contrast and lets think_about how many
 temporaries we need to evaluate this functions So this function body when it
 executes well need a certain number of temporaries and if we know how many
 temporaries that needs in advance then we could allocate the space for those in the
 activation_record rather having to do push and pop pushing and popping from the
 stack at runtime So lets_take a look and if then else is going t o involve a
 temporary because it always do this predicate comparison here were_going to
 have to evaluate the the first argument to the predicate and then save the result
 of that while we evaluate the second_argument to the predicate So this one
 involve one temporary well need one temporary for that predicate Similarly
 for this predicate to evaluate it since its a two argument operation in
 comparison well also need one temporary for that 1010 Theres this expression
 over_here which is kind of complicated How many_temporaries will we need for
 these Well remember how this works So evaluate the first expression and then we
 save the results of that so this will require one temporary for the result of
 the called fib going to have to be saved and only evaluate the plus And while we
 are evaluating the call the fib though is actually before we evaluate to call the
 fib we have to evaluate the argument of fib and that involve the subtractions We
 also need one temporary here for the subtraction Okay And now we have about
 the second side of the this edition here Well this also involves a subtraction
 Okay So we got to have one temporary here to hold on to the value x while were
 evaluating the minus to compute the value of the argument before we call
 inaudible Okay So how many_temporaries do we need in total While we need one
 here for the predicate but notice that once the predicate is decided once we
 know the answer to whether this predicate is true or false we dont need that
 temporary anymore So in fact that temporary can be reclaimed we dont need
 the space for that temporary anymore by the time we get to the false_branch And
 again once this predicate is evaluated we dont need the space for that temporary
 anymore okay So now were down to the plus The first_thing that happens is we
 evaluate the argument to this first call the fib Once thats evaluated we dont
 need the temporary for it anymore Now the results of fib has to be saved somewhere
 while we do the plus okay And then wer e going to have to evaluate the argument
 to the second call of fib and then notice that this happens while we still need this
 temporary here so in fact we need both of these temporaries at the same time Okay
 because while were evaluating this argument the second call of fib we still
 need to be holding on to the first argument to the plus And so in fact this
 particular function can be evaluated with just two temporaries So all the space we
 need to compute the value of this function body So in general we can define a
 function nt of e that computes a number of temporaries needed to evaluate_e1 e2
 So thats going to need at_least as many_temporaries as e1 Okay so if we need a
 number of temporarys k to evaluate_e1 lets have at_least k temporaries to
 evaluate the whole expression And then well also need at_least as many
 temporaries as its needed to evaluate the two1 because we have to hold on to the
 value of e2 while we are evaluating so we have to hold on the value of e1 while
 were evaluating the two Okay And its going to be the maximum Over these two so
 itll be the maximum number with between the maximum number of temporaries need to
 evaluate a one and one the number of temporaries to evaluate two That would be
 the total number of temporaries the minimum number of temporaries needed to
 evaluate_e1 e2 And the reason is a max instead of a sum Is that once_weve
 evaluate_e1 we dont need any of the space that was used to evaluate_e1 anymore All
 those temporaries are done All we need is the answer We_dont need the immediate
 results and that means that the temporaries that were used to evaluate_e1
 can be reused to evaluate e2 So generalizing from that one example here
 is the system of equations that subscribes the number of temporaries needed to
 evaluate every kind of expression in our little language So lets_take a look
 So we already_talked about e1e2 is just the max of over the number or temporaries
 to value of e1 and one number of temporaries to value of e2 So e1e2 is
 exactly the same_thing because the same structure is a different computational
 operation but is a binary operation and we have to save the value of e1 while
 evaluated e2 So its the same formula inaudible Now for if and else well what
 do we need We need one Im_sorry we need its going to max again Its going
 to be max_over some number of different quantities How many_temporaries might we
 need Well we might need as many_temporaries or as needed to evaluate the
 value of e1 and we certainly need at_least as many alright So if you want to take
 a certain number of temporaries the whole f and l is going to require at_least as
 many_temporaries Now of course once e1 is done evaluating we dont need its
 temporaries anymore And and we can evaluate e2 okay And while we are
 evaluating e2 we have to hold on To the results of e1 thats where the one plus
 comes from So to that while were evaluating e2 we need one plus the number
 of temporaries to evaluating two to hold all the temporaries of the computation
 And then once the predicate is done we dont need any of those temporaries
 anymore at all ad were_going to evaluate either e3 or e4 And so then we just need
 however many_temporaries each of those requires and whatever the maximum is over
 these four quantities thats the minimum number of temporaries we can get away with
 to evaluate the entire if then else Lets_take a look_at a function_call So that
 the space needed for the function_call is number of temporaries the max_over the
 number of temporaries to evaluate anyone of the arguments and this is actually an
 interesting case because notice That we dont need we dont have anywhere in this
 formula space for the results for the e1 through en Of course once_weve evaluated
 the e1 then we need to save it somewhere and so you would think that we might see
 some numbers in here representing the temporary space needed to hold on to the
 results of the evaluating these expressions And the reason that we dont
 have that in here is that Even though those values are saved they are indeed
 saved theyre not saved in the current_activation record The space where the
 results of e1 and the results of all any of the arguments Yeah again is saved in
 the new activation_record that were building And so the space for the the
 results of e1 through en is that those values are stored in new activation_record
 and that storage of current_activation record and were trying to compute the
 number of temporaries needed to evaluate inside of the current_activation And then
 for integer that doesnt take any space at all to require any temporaries I_mean
 So theres zero temporaries required for that and also for a variable reference so
 it requires no temporaries So now lets go_through our example and work out
 systematically using the equations How many_temporaries we will need Okay So
 here for this if then else remember it was going to be the max_over the number
 required to evaluate_e1 well that zero One the number to evaluate e2 which is
 the second expression in the predicate so that would be one because the number one
 requires zero temporaries and the one the we have one hold on to x all right And
 then max_over the branches So to evaluate zero requires Zero temporaries
 and now We have to compute The number required here Okay so once_again to
 evaluate the first expression if and else requires zero temporaries to evaluate the
 second one we require one One the number required one zero to evaluate
 that constant we got zero temporaries and now for the last expression how many will
 this one will require Well this is going to require zero for this guy One for the
 second_argument so to evaluate fib is going to require one temporary okay and
 then its going to be one plus over_here We have to hold on to the results there
 The value of x two so how much that going to require That is going to require
 the max of zero and one zero okay so this would be one alright so we have over
 here we have one one two okay and now were taking the max_over two and one So
 thats two okay And this is the last expression in the our if and else So
 clearly this if then else here will require two temporaries okay Because the
 max_over the number required for either part of the predicate the then branch and
 the else branch And now this whole expression Requires two temporaries and
 thatll be the max of the four components of the outer if then else And so then for
 for the entire_expression we get two temporaries Once it computed the number
 of temporaries required to evaluate the function value we can add that much space
 to the activation_record So now our activation_record is going to require two
 n nt (e) elements And so the two of course are for the return_address for the
 frame_pointer The n is for the n argument of the function And then the rest of it
 is just the space required for the temporaries And now we can talk_about how
 were_going to layout the activation_record Well leave the first part of it
 the same so everything up to the return_address is laid_out just before First the
 color string pointer then the and arguments in reverse order and then the
 return_address And then after the return_address come the and locations or the
 nt(e) excuse_me locations for the temporaries
 In the last several_videos we have discussed code_generation for a various
 simple programming_language In this video we are going to take a look_at code
 generation for more advanced feature objects Fortunately this dated code
 generation strategy for objects is really just an extension of what weve_already
 learned So everything that you learn before were_going to be using and then
 theres going to be some additional things that we do specifically for objects And
 the important thing to know about objects is slogan that you hear When people
 talked_about object_oriented programming is this one So if b is a subclass of a
 then an object of class b can be used wherever an object of class a as expected
 So theres a substitutability property If I have a piece of code that can work on
 as then it could also work on bs and any_other subclass of a What this means for
 the for the case of code_generation is that the code that we generate for class
 a So the code that we produced for method in class a has to work unmodified
 for an object to class b And to see this keep in mind that when we compile a when
 we compile class a I we may not even know all the subclasses of a So those
 maybe not even have been defined yet So in the future some programs may come
 along To find a subclass of a then our compiled version of a will have to work
 with that new subclass So there really only two questions that we have to answer
 to give a complete description of how to generate_code for objects The first one
 is how our object represented in memory So we need to decide a layout and
 representation for objects And the second one is how is dynamic_dispatch implemented
 so thats the characteristic feature of using objects just so we can dispatch in
 the method of an object and we need an implementation of that So to be
 concrete were_going to use this little example throughout this video and Ill
 just take a moment here to to point out some features of it So we have three
 classes classes am b and c And notice that a is a base class and b and c both
 inherit from a And all three classes define some attributes some fields and
 also some methods Now a couple of important features here is that notice
 that because b inherits_from a and c inherits_from a they all they both
 inherit both of those classes inherit the attributes a and d from class a So these
 two attributes that are defined in class a are available in class b and in class c So
 even_though theres_no mention Of a and d in the definition say of class b The
 methods in class b can still refer to those attributes They are part of the
 attributes of class b They are just copied over or inherited from a Another
 feature of this example that I like to point out is that all of the methods refer
 to the attribute a so actually referred into this method and this one referred
 twice in this method and also in this method And the significance of this is
 just what we discussed a couple of slides ago For all of these methods to work
 attribute a is going to have to live in some place and some place_where all of
 them can find it they generate a code run Some particular less considered the method
 f So the method f exists in all three classes All three classes when it runs
 it will refer to attribute a and even_though the objects would be different In
 one case it might be running on an object and in another case on a c object It
 would need to be_able to find the attribute a and so therefore the
 attribute a has to be in the same place in each object And so how do we
 accomplish that Well the first principle is the objects are laid_out to in
 contiguous memory So an object is just a block of memory Okay with no gaps and all
 the data for the object is stored in the words of that lock of memory And each
 attribute is stored at a fixed off set in the objects So for example there may be
 a place in this object for attribute a On this case its at in the middle of the
 object is in the in the fourth position And no matter what kind of object it is
 whether its an a B or c objects and are example as with a we always live with that
 position so that any piece of code that refers to a any method that refers a can
 find can find the a attribute Now the other thing thats important to understand
 and this is you_know slight digression from what were talking_about but its a
 key aspect of code_generation for object is that when a method is invoked the
 object itself is the self_parameter So the self_parameter is the entire object so
 self When a function is involved it will refer to the entire object so you think
 itself is going to be appointed to the entire object Remember that self is like
 that this variable or this name in Java And then the fields we refer to particular
 or the attributes of the object will refer to particular position within the objects
 So for example the attributes we decided to leave it there So here is the
 particular object layout used in Kuhl So the first three words of a Kuhl object
 contain header information and every Kuhl object always has these three entries The
 first_position is a class tag and also at zero then the next word it also four is
 the size of the object and then something called the dispatch pointer and then all
 of the attributes Now the class tag is an_integer which just identifies the class of
 the object So the compiler will number all of the classes So in our example we
 have three classes a b and c and the compiler for example might assign them the
 numbers one two and three It_doesnt matter what these numbers are As long as
 they are different from each other So it doesnt have these numbers consecutively
 or anything like that The important thing is of the class tag is a unique identifier
 for a class each class has its_own unique bit_pattern that tells you what kind of
 class the object is And the other fields here the object size is also an_integer
 which is just a size of the object in words and the dispatch pointer Is a
 pointer to a table of methods so the methods are stored off to the side and the
 dispatch pointer is a pointer to that table and well talk_about this more later
 and then all the attributes are laid_out in the subsequence slots in some order
 thats determined by the compiler so the compiler will fix and order for the
 attributes in the class and then all the objects of that class will have the
 attributes of that class in the same order And again all of this is laid_out
 in the continuous chunk of memory Now Im ready to talk_about how inheritance
 works So the basic ideas like given a layout for class a a layout for a
 subclass b so this is a subclass of a can be defined by extending the layout of a
 So we dont need to move any of the attribute of a we can just add more
 fields onto the end of as layout And so thats going to leave the layout of a
 unchanged which is a great property because this is how the position of an
 attribute in the a object will always be the same for all the subclasses So
 essentially we will never once we decide where an attribute lives in a class it
 will never change for any of the subclasses of that object So b is just
 going to be an extension of the layout of a So lets_take a look_at our example
 here and see how that that works Let_me just write_down here a little_bit about
 these classes because we dont have the example on the screen So we have a class
 a and class a had two attributes a and d okay And it doesnt_matter what their
 types are or what the methods were here Were just looking_at the class names and
 the names of the attributes that are defined in the class And then we have b
 Which inherits_from a and b added a attribute b and then we had c which also
 inherits_from a but has no relationship to b And class c define an attribute little
 c Alright So thats the structure of our example is relevant to the layout of
 the objects Okay So Lets talk_about the layout of class a So in position zero at
 all sub zero therell be a tag for a that will be some small integer at the compiler
 picks Therell be a size of a well come_back to that in just a se cond There will
 be a dispatch pointer again which were_going to talk_about later And then come
 the attributes of a and it just laid_out the compiler the way its done in the
 the Kuhl c implementations is that they are laid_out in the order in which they
 appear textually in the class So in this case first the attribute a And then the
 attribute d all sets twelve and sixteen And now since the object there are two
 attributes and three header words that means the size of the object is five words
 and so its a five that goes in the size field for a objects Now lets_take a
 look_at b Okay So b is going to have a different tag b objects will have a
 different tag so they to distinguish them from a objects Theres going to be extra
 fields so the size will be one bigger But now the layout preserves the layout of a
 So the attributes of a appears in the same position So you can think of there being
 an a object Actually embedded inside of the b object If I were to strip off the
 end here that were just you_know cover up this last bit here b I would say that this
 object here has the same size and the same attributes as an a object so any piece of
 code that could work on an a object will also make_sense running on a b object Now
 Of course the tag is different because it actually is a subclass and you_know and
 there is an extra field so the the size is different but the point is that any
 code that it refer is just to the fields here will still work just fine So any a
 method that was compiled that refer to the methods of an a object will still find
 those attributes in the same place at the b object and afterwards there is also one
 more field here Which is the new attribute b It just gets laid_out after
 all of as fields So after all of as fields come all of bs fields in the same
 order which they appear textually in the class because theres just only one
 theres just one new field there And now looking with class c or the story with
 class c is very_similar so c has its_own distinct tag and also has one more
 attribute than a so it has size six And now again the a attributes were on the
 same position and now the c attribute just comes_after the a attribute And so notice
 here that a methods again will work just fine on c objects because the attributes
 are on the same places and so the methods will find the attributes where they expect
 to You cannot however call a method of class b on an object to class c Okay
 because they have different attributes in the third position We may have completely
 different types It may not make_sense to invoke a b method on c object but thats
 just fine because if we look_at our inheritance hierarchy over_here well see that b and
 c are actually unrelated They are both subclasses of a but they have no
 relationship to each other B is not a subclass of c and c is not a subclass of b
 and so anything beyond their shared ancestry with a can be completely
 different in the layout So more generally if we have a chain of
 inheritance relationship so lets_say we have a base class a1 and a1 inherits some
 a1 and a3 inherits some a2 and so on with some class a and inheriting at the bottom
 of this of this chain after some long sequence of of other intermediate Some
 classes you_know what is the layout of all these classes going to look like
 Well theres going to be a header Okay the three word header and that will be
 followed_by a1s attributes And then followed_by a2s attributes followed_by
 a3s attributes and so on all the way down to ans attributes down_here Okay And if
 you look again so what we talked_about before each prefix Of this header is
 essentially a valid object a valid one of these objects If I look_at the first set
 of attributes everything up to the end of a1 and attributes that forms a valid
 layout for one object is I stop with the a2 attributes I have a I have a I have
 a valid layout for a2 object going all the way from the header down to you_know
 including the a1 and a2 objects And then a3 includes all a1 a2 and a3s attributes
 and so on Okay So each prefix Of of this object Of this a and object has a
 correct layout for some for some super class of A n
 This is the first of the series of videos on programming_language semantics and in
 particular on the semantics of cool Before we dive into technical details
 though I want to spend a few_minutes talking_about what programming_language
 semantics are and why we need them The problem we have to address is that we need
 some way to say what behavior we expect when we run a Kuhl program So for every
 kind of Kuhl expression for everyone we have to say what_happens when its
 evaluated and we can regard this as the meaning of the expression Somehow give
 rules to specify what a particular what kind of computation of a particular
 expression does And I_think its useful to look back and see how we dealt with
 this with similar problems in defining other parts of cool okay the earlier
 things that we looked_at in this course So for example For Lexical Analysis we
 defined a family of family of tokens using regular_expressions And for the the
 syntax of the language we used Context Free Grammars to specify the the
 structure of the how words could be strong together to form valid sentences in
 Kuhl And then for the semantic_analysis we gave formal typing rules And now were to
 the point that we have to talk_about how the programs actually running so we have
 to give some evaluation rules and these are going to guide how we do code
 generation of optimization or you going to determine what the program should do and
 what kind of transformations we can do on programs to make them run_faster or use a
 space or what other what any_other kind of optimization that we would like to
 perform So far weve_been specifying the evaluation rules somewhat indirectly
 Weve been doing it by giving a complete compilation strategy down to stack_machine
 code and then weve_been talking_about the evaluation rules for the stack_machine or
 actually translation the stack_machine into assembly_code And that is certainly a
 complete description You can take the generated assembly_code and get it right
 out of the machine and see what the program do es and that would be a a
 legitimate description of the behavior of the program And the question then is you
 know why isnt that good enough Why isnt just having a code_generator for the
 language Why is that already a good enough transcription of what how the code
 is supposed to be executed And The answer to that is maybe a little hard to
 appreciate without having a written a few compilers But in a nutshell people have
 learned through hard experience that assembly_language descriptions of language
 implementations language implementations have a lot of irrelevant detail Theres a
 lot of things that you have to say when you get such a complete executable
 description that was not necessary to say about how the program executes So for
 example the fact that we use a stack_machine thats not intrinsic to the
 implementation of any particular programming_language There_are other
 cogeneration strategies that we could have used so you_know you dont have to do
 the stack_machine to implement the language which way the stack grows
 Whether it grows towards high addresses or low addresses you could implement it
 either way How it it yeah exact representation of integers in a particular
 instructions actually used to execute or to implement certain language constructs
 All of these things are are a are one way or or one particular way to implement
 the language but we dont want them to to be taken as the only way that the language
 could be implemented So what we really want than it has a complete description
 but one that is not overly restrictive One that will allow a variety of different
 implementations And when people have not done this when people have not tried to
 find some relatively high_level way of describing the behavior of languages
 theyve been inevitably gotten into the situation_where they a where people would
 just have to go and run the program on a reference implementation or to decide what
 it does And so this is not a very satisfying a situation because of the
 reference implementation is not completely correct itself It will have bugs and
 there will be artifacts of the particular way it was implemented that you didnt
 mean to be part of a language but because there was no better definition wind_up
 becoming fixed and have sort of accidents of the way the language was implemented
 the first time So there are many ways to actually specify semantics that would be
 suitable for our task and it_turns out that these are all equally powerful but
 some of them are more suited to various tasks than_others so the one that were
 going to be using is called operational_semantics So operational_semantics
 describes program evaluation via execution roles on an abstract machine we just gave
 a bunch of rules that say you_know from particular expression how it should be
 executed You can think of this as a very very high_level kind of cogeneration And
 this is most useful for specifying implementations and it is what were_going
 to use to describe the semantics of Kuhl I want to mention two other ways of Of
 specifying programming_language semantics because theyre theyre important and you
 may come across them at some point outside of this class One is the notational
 semantics and here the programs meaning is actually given as a mathematical function
 So the program_text is mapped to a function that goes from input and outputs
 and this this is this function is an actual function that exist in the
 mathematical sense And this is a very elegant approach but it uses complexities
 into finding an appropriate class of functions and we dont really need to
 consider for the purposes of just describing an implementation And another
 important approach is axiom semantics and here program behaviors described in some
 kind of logic And the basic kinds of statements that you write in this language
 or in this in this in the axiomatic semantics is that if execution begins in a
 state satisfying x then it ends in the state satisfying y where x and y are
 formulas in some logic And this is a very common foundation for syst ems that
 analyze programs automatically that tries to prove facts about programs either to
 prove theyre correct or to discover bugs in programs
 In this video_were going to begin our_discussion of formal operational_semantics
 Just as we did with lexical_analysis parsing and typechecking The first step
 in defining what we mean by a formal operational_semantics is introduced the
 notation and it_turns out that the notation we want to use for operational
 semantics is the same or very_similar in the notation we use in typechecking We
 are going to be using logical rule of inference So in the case of
 typechecking the kinds of inference rules we we presented proofings of the
 forms that in some context We could show that some expression had a particular type
 in this case to type c And for evaluation were_going to be doing something quite
 similar We will show that in some contacts now that this is going to be a
 different kind of context that we had in typing so this is going to be an
 evaluation context as oppose to a type context and so what goes in the context
 will actually be different But for the moment all that really matters is there is
 some kind of a context and in that context were_going to be_able to show some
 expression evaluates to a particular value b So as an example lets_take a look_at
 this simple expression e1_e2 and lets_say that using our rules which we I
 havent shown you yet but lets_say we had a bunch of rules and we could show in
 the initial context That e1 in that same context okay So these context are going
 to be the same that e1 evaluated to the a value of five and e2 also in that same
 context evaluated to the value of of seven then we could prove that e1_e2
 evaluated to the value of twelve If you think_about it what this rule was saying
 is that if e1 evaluates to five and e2 evaluates to seven then if you evaluated
 the expression e1_e2 youre going to get the value twelve And whats the
 context doing well it doesnt do a whole lot in this particular rule But remember
 what the context was for in type_checking The context was for giving values to the
 free_variables of the expression And so we need for an expres sion like e1_e2 to
 say something about what the values are the variable that might appear in e1 and
 in e2 in order to say what they evaluate to and and therefore to say what the
 entire_expression e1_e2 will evaluated to Now lets be a little more precise
 about whats going to go in the context So lets_consider the evaluation of a a
 expression or statement like y gets x1 Okay so we are going to assign y the
 value x1 and there are two things that we have to know in order to evaluate this
 expression First of all we have to know where In memory of valuable start So for
 example the variable x here were_going to have to go and look up excess value and
 then add one to it And then that value is going to be stored in whatever memory
 location holds the value for y okay so there is a mapping from variables Two
 memory locations Okay and that is called In operational_semantics the environment
 and this is a little confusing maybe because it use environment for other
 things Okay so now lets forget about as all we uses of the word environment We
 were talking_about the operational_semantics what the environment means is
 the mapping the association in between variables and where that variable store in
 memory And then in addition were_going to need a store and thats going to tell
 us what is in the memory So just knowing the location for a variable isnt quite
 enough When we if we know the value of x if we know the location for x for example
 or as as important because were_going to get the value of x but we also have to
 know exactly when value is stored there and so store Is going to be a mapping for
 memory locations Values These_are the values that are actually stored in the
 memory so its two levels of mapping We associate with each variable and memory
 location And then each memory_location will have a value in it So lets talk
 about the notation that were_going to use for writing down the environment and the
 store So as we said the variable environments have variables to locations
 and were_going to w rite that out In the following way were_going to just have
 this as a list of variables and location pairs separated_by columns and this
 environment for examples of variable a is it location l1 And variable b is in
 location l2 And another aspect of the environment is that its going to keep
 track of the variables that are in scalps and the only variables that will be
 mentioned in the environment are those currently in sculpted in the expression
 that is being evaluated Now as we said stores map memory_location to values and
 well also write_out stores as lists of pairs So in this case the memory
 location l1 in the store contains the value five and the memory_location l2
 contains the value seven And we will also separate these pairs by an arrow And just
 to make the stores look a little_bit different from the environment so that we
 wont confuse the two Theres an operation on stores which is to replace of
 value or update of value So in this case were taking the store s and were
 updating the value at location l1 to b12 And this defines a new store s_prime So
 keep in mind here that the stores are just functions list in our model and so we can
 define a new store by taking the old function or the old store has and
 modifying it at one point So this defines a new store as prime such if I apply s
 prime to the location l1 I get off the new value twelve and if I apply s_prime to
 any_other location any location different from l1 I get out the value that the store
 held in s sorry the value of the location in store s Now in Cool we have more
 complicated values and integers In particular we have objects and all the
 objects of course are instances of some class and were_going to need a notation
 for representing objects in our operational_semantics So well use the
 following way of writing down an object An object will begin_with its class name
 In this case the class name x and it would be followed_by a list of the attributes
 In this case the class x has n attributes a1 through an And associated_with each
 attribute will be the memory_location whether an attribute stored so attribute
 a1 is stored location l1 up through attribute and which is stored at location
 ln And this would be a complete description of the object because we know
 where in memory the object the object is stored We can use the store to look up
 the value of each of those attributes There_are few special classes in Kuhl that
 dont have attribute names and well have special way overriding them So integers
 only have a value and and that will be written as int with a single value in
 parens the value of the integer similarly for brilliance They have a single value
 true of false and strings have two properties the length of the string and
 the sting constant Theres also a special value void typed object and well use the
 term void in our operational_semantics to representative and briefly here so void
 is a a special and that there are no operations that can be before and on void
 except for the test is void So in particular you cant dispatch the void
 even_though it has typed objects that will generate runtime error The only thing you
 can do is to test whether the value is void or not And concrete implementation
 we typically use a null pointer which represent void Now were_ready to talk
 about in more_detail what the judgments will look like in our operational
 semantics so the context will consist of three pieces The first piece is a current
 self_object The second piece is the environment which is again the mapping
 from variables to the locations where those variables are stored and the third
 piece is the memory the store The mapping from memory locations to the
 values held at those locations All_right So in some context an expression e will
 evaluate to two things First of all e will produce a value so for example we saw
 before that the expressions seven five would produce the value twelve thats one
 result to the evaluation But the second thing is that well produce a modified
 store So the expression e maybe a complicated piece of code Maybe a whole
 program is on the right and it might have a slight statements that update the
 contents of the memory And so after e is evaluated there will be a new memory
 state that we have to represent and so s_prime here represents the state of memory
 after evaluation And now those are couple of things here First of all the current
 self_object and the environment dont change They are not changed by evaluation
 so which object is the self_parameter to the current method and Well the mapping
 between variables and memory locations that is not modified by running a running
 an expression and that makes_sense I_mean you cant update the self_object in
 Kuhl and we dont have access in in any form to relocations or variables stored
 and so those two things are in variant They dont they are variant under
 evaluation They dont change when you run a piece of code However the story does
 change so the contents in the memory may be modified so thats why we need a store
 for both before evaluation and after evaluation And one more_detail these
 judgments of this form always has a qualification That judgment only holds if
 the evaluation of e terminates So if e goes in to infinite loop then youre not
 going to get a value and youre not going to get a new store So this kind of the
 judgment must always be read as saying that if e terminates then e produces a
 value v and a new store s_prime Summarize the results of evaluation is a value and a
 new store And where the new store models the side_effects of the expression And
 once_again there are something dont change as a result of evaluation And this
 is actually important for compilation because well be_able to take advantage of
 the fact that they dont change to generate efficient code so the variable
 environment doesnt_change the value itself which object were talking_about
 doesnt_change and notice here as another detail That the contents of the self
 objects the attributes inside the self_object might change they might get
 updated but t he locations where the attributes are stored do not change So
 the layout of the object where the object stored doesnt_change and thats all were
 saying here the actual contents of the object which of course is a part of the
 mapping of the store those might get updated by evaluation And also the
 operational_semantics allows for nonterminating evaluations Thats the
 last point here and so the meaning that the judgments only holds on the assumption
 that the that the expression actually completes
 In the next couple of videos were_going to be looking_at the details of the Cool
 operational_semantics going over the semantics of each individual kind of
 expression Well start with easy ones and work our way up to the more_complicated
 ones So the easiest rules are the rules for the constants in Cool So the value
 true the expression true I should_say evaluates to a Boolean with the value true
 And it doesnt modify the store so the store is unchanged because it doesnt do
 any updates obviously And theres a corresponding rule for false And integers
 are very very_similar so if a integer expression integer literal i will
 evaluate to and integer object with the value i and again the store is not
 modified by such evaluation And finally strings if a if s is a string literal of
 length n then it will be evaluated by the string object of which the properties n
 and the string constant s The evaluation of identifiers is very straightforward
 given that we have both in environment in the store So to evaluate an identifier
 and this would be a variable lane y x or y or for What do we do Well first we look
 up in the environment where that identifier is stored so now we give_us
 back a memory_location l So by in this case And then we look up in the store what
 the value is at that memory locations So we use the same memory_location here as an
 argument to the store to get back the value that that that variable currently
 has And notice I just have a reference this is a read of memories so this is
 loading I_think it was loading the value of the variable This does not affect the
 store so the store is the same before and after This is just looking_at the value
 of the variable not updating the variable The expression self just evaluates to the
 self_object So this is a place_where we just make use of the fact that the self
 object is part of the environment so lets just copy them over_here as the result of
 expression and that was again that the store is unaffected by evaluation of self
 Now lets see of a more slightly_more complicated evaluation is evaluated in
 particular the assignment expression So an assignment consists of two parts an
 identifier that is being updated and an expression that is going to give_us the
 new value So for example just to remind you we might have something_like x gets
 one one so one one here would be the expression e and x would be the identifier
 All_right And so in order to evaluate the assignment the first_thing we have to do
 is we have to know what value were_going to be writing into the identifier So what
 is the what is the update were_going to perform So the first_thing to do is to
 evaluate e okay And notice here that e is evaluated in the same environment so it
 has the same three components here and here all right So it just says the first
 thing we do is we run e Okay That will give_us back a new value Were_going to
 get back on value b excuse and possibly an updated score so you can arbitrate a piece
 of code You could yourself have assignment statements in it so the story
 that we get out might be different Alright so e produces the value of e and
 an update store_s1 And now its actually due to assignment what do we do Well we
 have to know what memory_location was supposed to update so we look up the
 memory_location for id and that would give_us some location else of id And then we
 modify the store with the new we modify the store at that point with the new value
 so we replace the location_l i d or we update the value of l i d to be the
 value of e the value b and we do that in store_s1 which gives_us a new store_s2
 And Ive noticed That s2 is the store that results from the evaluation of e
 okay So after we do the assignment the assignment returns the value b which is of
 course the value of a of running e And it returns the updated_store s2 Next
 lets talk_about the operational rule for addition So to evaluate e1e2 what we
 are going to do So first we evaluate_e1 And notice that is done in the same
 context as the context of the entire e xpression okay So the components the
 context here for evaluating one are exactly the same as the components for the
 overall_expression e1_e2 So when we evaluate_e1 its going to give_us a value
 of e1 and its also going to give_us an updated_store s1 And then were_going to
 evaluate e2 and notice here And I_think context is different The soft objects in
 the environment are the same same but now were running e2 in the new store_s1 And
 what does its saying is that if e2 has has assignments or variable_references in
 it those assignments and variable_references have to be done on the store
 that resulted from running e1 okay So its very important that we get that any
 side_effects would happen in running e1 are visible or that are seen by the
 expression e2 So we run e2 in this environment were_going to get the value
 of e2 and the updated_store s2 And then the result of the entire_expression is
 going to be b1 b2 and the results it store will be the store_s2 And notice
 here how the stores tell you the order in which you have to evaluate the
 expressions So because e1 is evaluated in the same store as the overall_expression
 that tells you that e1 has to be evaluated first And then because e2 is evaluated in
 the store thats produced by e1 that tells you that e1_e2 excuse_me has to be
 evaluated after youve evaluated e1 and then the fact that S2 is the result of
 the whole_thing It tells you that E2 is also the last thing that you evaluate
 during the execution of this particular expression Okay lets_take a look_at the
 statement_block and just a variety here on the change my colors How are we going to
 evaluate the a statement_block of statements e1 through en okay so while
 this is semantics this is that we should run I_mean order beginning of e1 and the
 results of the entire execution will be the lets_say the value of the entire
 block with the value of the last expression And this this rule just says
 that So first we evaluate_e1 and also its done on the same store as the overall
 expressio n as it tells you it has to come first and that produces a new store_s1 and
 the value b1 Okay And then e2 is evaluated in the store_s1 and it produces
 the store_s2 and so on And then expression en is evaluated in the store sn one and
 it produces a value of bn and an updated_store s (N) Okay And then the result of
 the whole_thing is the value of vn and also the updated_store s (N) and this
 tells you this would really tell you the order which had to evaluate the sub
 expressions The dependencies here on the store of course you do evaluate_e1 and
 then e2 and then e3 and so on so you have to do them to net order to get the side
 effects to get You_know the side_effects in the correct order for all of these
 expressions And furthermore it also tells you the only value that youre going to
 keep is the value vn Notice that none of the other values that are produced here
 are used for anything They dont appear anywhere else in the rules Lets think
 what weve learned so_far and do a small example So we want to know what_happens
 when we evaluate the block X gets assigns seven five thats the first statement
 and the second and the last statement in the block is just the expression four
 And the first_thing we have to do is to say what the context did and which we are
 going to evaluate this and the context consists of three parts Therell be a
 self_object and in this case it doesnt really matter whats in the self_object
 because self is not referred to in the program and so it wont play any role in
 the evaluation But we we still need it so there still be therell be some self
 object out there just wont get used and Now in the new environment which tells_us
 the locations where all the free_variables in the programs So well just need a
 place for x is going to be stored and so s will be stored in some location_l And then
 we know our memory content is where our store is and lets just say that at l we
 have initially the value zero okay So now we can use our rules to run this
 program or to evaluate thi s program Im going to make this line here much longer
 And recall that that evaluation of block consist of the evaluation of the all the
 statements within the block Okay so the first one is going to be s gets seven5
 and that will be evaluated in the same environment as the overall_expression So
 we have up here So the same context excuse_me and I should_say I often slip
 and I realize and say environment for the entire left_hand side of one of these
 judgments Ill try to be consistent And just use environment for the for the
 second components of the context often in the literature people call the entire
 thing on the left_hand side of the environment thats why they make this
 mistake but you_know for this instead of notes Im trying to be consistent the
 entire all the components on left_hand side together are called the context and
 the environment is just the second component The mapping from variables to
 their locations Anyway coming back to the example The first statement in the block
 is s gets seven5 alright And then were_going to have the second statement as
 well And we know that the self_object and the environment wont change but we dont
 know what the store will be The store might be different so Ill leave the store
 empty for now and well figure that out later and were_going to be evaluating the
 expression four Okay so this is the structure of the evaluation now in
 progress We should look_at at this first statement trying to make some forward
 progress on that one So to evaluate the assignment what do we have to do Well
 the very first_thing we have to do is we have to evaluate the right_hand side so
 were_going to have the context for that is going to be the same And the context
 weve_been looking_at all on because its the first_thing thats actually going to
 happen is to evaluate Seven five okay And now Im leaving a little space down
 here for the rest of the assignment role which were not going to fill in just yet
 Now to evaluate the plus expression we have to evaluate the first express ion and
 the second expression okay And so how do we do that Well we know finally I
 think how to do that because were finally down and were_going to have a single
 integer there and that we already have a rule for okay and so an_integer
 literal evaluates to institute your object okay And inside that object is
 just about the value okay And the store is unmodified All_right And then
 similarly for the other argument here Okay So the five will also evaluate to an
 integer object with the value of five and the store will be unmodified okay so
 thats the two sum expressions of this edition and so now we can fill in the
 results here so to take the contents of the two integers well add them That will
 also be integer object so were_going to have the integer object twelve And the
 store has not been changed okay So the the store that we get out of here happens
 to be the same as the store that went in just because this expression had no
 assignments in it okay And now were_ready to do the assignment Okay So how
 do we do that Well we have to form a new store Alright so were_going to have a
 new store which will the L gets zero with the value of l Number which way my
 notation went here I_think its the number comes first and were_going to put
 twelve in the location_l and of course thats store was just equal to the store
 where l has the value twelve okay And so now what_happens down_here and we do the
 assignment and we get out The new value Okay so the value of the right_hand side
 is twelve and we have a new store where the location_l has twelve all right So
 now were_going to evaluate the second statement in the block and that will be
 done in the store where l has the location twelve and of course this is just an
 integer And so that will evaluate to the integer constant of four the integer
 value excuse_me 4or integer object containing the integer object with the
 value four and our store And its just going to fit not quite all right And
 thats then the result of the entire evaluation So this block will produce
 the value four an_integer object with the value four and an updated_store where
 location_l has the value twelve So the next expression I would like to take a
 look_at is the if then else expression and to evaluate if then else what do we
 do Actually there should be if then else See of course so evaluating if
 then else as well First we have to evaluate the the predicate and its done
 in the same store the same context as the overall_expression and if the result is
 true If the if the Boolean predicate returns the value true Then we want to
 evaluate just the true_branch and not the false_branch so thats why you only see
 here evaluation of e2 and e3 isnt mentioned anywhere and just know here that
 the predicate may have side_effects and so e2 is evaluated in whatever store that e1
 produces And then the results of the entire_expression is the value of e2
 Okay thats v and also just the final store is produced by running the then
 branch And there is a symmetric rule for what_happens if the predicate evaluates
 the false In that case you would evaluate e3 and not e2 Next well take a
 look with what_happens with while loops and cool So there are two cases First if
 the predicate of the while_loop evaluates the false okay Well in this case the
 loop_body is not going to execute alright so the first_thing we do is we evaluate
 the predicate and thats done in the same context as the evaluation of the overall
 expression and if the predicate is false then we exit the loop and so the results
 of the loop is void The value void and just whatever store resulting from
 evaluating the predicate The other_possibility is that the predicate
 evaluates the true So here we evaluate the predicate again in the same context as
 the overall loop And if the predicate evaluates the true then were_going to run
 the loop audio once Okay So well evaluate the loop audio and also thats
 done in this in whatever store results from evaluating the predicate Evaluating
 the loop audio is g onna give_us a value of v and a new store_s2 and then what we
 need to do is we need to go_back around and execute the loop again and how can we
 do that Well were really just running the whole loop in the new context So the
 next_thing we do is we evaluate the entire loop Right in the new store So after I
 execute the loop by loop_body one time then we go around and just evaluate the
 loop again And when this may run for zero or more iterations alright And when I
 finally terminate if it terminates it will produce it will produce a new store
 s3 evaluated while because always produced the value of a void And then what well
 produce for the entire loops for the entire_expression is the value void and
 the update and store s3 The next interesting expression to take a look_at
 is the let expression So recall how what this looks_like so let and cool has a
 variable thing declared and its type and an initializer which is optional so this
 is the value that the identifier will be initialized to and then the expression in
 which that new variable is available And so how do we evaluate this Well first
 were_going to evaluate the initial value of the of the new variables So we
 evaluate_e1 and as usual thats done in the initial store it produces possibly
 modified store And now the question is what are going to whats going to be the
 context here for the evaluation of e2 for the body of the latter And so it seems
 clear that its going to involve s1 because it has all the updates from e2 but
 it also has to have this new identifier in it And so how are we going to do that
 So what we want is to have a new environment e but with new binding of ID
 to refresh location So were introducing a new variable Remember that the
 environment has tracked all the free_variables so this is one situation_where
 you should going to extend the environment e with the new binding alright And that
 location the location for the new variable has to be a fresh location We
 dont want to conflict with any_other memory locations we are already using
 Okay And so were_going to allocate a new memory_location for the variable And then
 the store the new store will also will be like as one as we said we have to include
 all the the values for s1 But also we can have these new location for the
 variable and thats going to have the initial value of the variable e1 To
 express that we need a new location were_going to introduce a new operation on the
 store which gives_us a new fresh location So new lo applied to a store its just
 going to give_us some location that isnt being used by the store So the store has
 a domain where its a mapping from locations to values and well just pick
 some new location that isnt in the current list of locations within the store
 and that will be the one returned or that will be one that will be the one returned
 by new lo Okay so new lo if you can think of As modeling the memory allocation
 function in the runtime system So then here we can write_out the rule So this
 is the most complicated rule we seen so_far So Ill just take a moment to walk
 through it All_right So the first_thing we do is we evaluate_e1 the initializer
 for the new variable okay So just like before this is done in the same context
 as the overall_expression and this is going to give_us a value for e1 and an
 updated_store all right Then in the updated_store using the updated_store
 here we find an unused location_l new Okay And then were_going to create a
 store where that new location has has the value of e1 So were_going to store the
 value of e1 at that new location Were_going to update the store_s1 to reflect
 that and further more were_going to extend our environment with the new
 identifier which will be stored at this new location and this is the context then
 Okay with this updated environment in store in which we evaluate the body of the
 lab which will produce the value b2 and possible update in store_s2 and those are
 the results of the overall_expression
 In this video_were going to continue and complete our_discussion of cool
 operational_semantics Well be taking a look with the two most complex operations
 in cool the allocation of the new object and dynamic_dispatch Well begin by
 giving an informal discussion of what_happens when a new object is allocated in
 Kuhl So the first_thing that has to happen if we have to allocate space for
 the object and essentially that means having enough space for the object
 attributes Were_going to have to allocate a location for every attribute of
 the object of class t if what were doing is allocating a new t object Then were
 going to set the attributes of of that object to their default values and well in
 a few_minutes well see what the default values are and why we need to set the set
 the attributes to defaults And then we evaluate the initializers so every
 attribute in the class declaration can have an initializing expression Were
 going to evaluate those and set the resulting attribute values And then we
 return the newly allocated objects So these are the steps that are involved in
 setting a new object and as you can see its actually more_than just allocating a
 little_bit of memory Its actually quite a bit of computation going on in
 allocating new objects in cool Every class has a default_value associated_with
 that class So for integers the default_value is zero For Boolean the default
 value is a Boolean false and for strings the default_value is the empty_string And
 then for any_other class that isnt one of these three basic classes or any_other
 class the default_value is void In the operational rules were_going to need a
 way to repair to the attributes of a class So were_going to define a function
 called class that takes a class name and returns the list of attributes of of that
 class So here we have all the attributes of class a lets_say that a1 through and
 in addition this functions also going to tell_us for each attribute declared type
 of the attribute and the expression that initiali zes the attribute And one other
 important feature of this list is that it includes all the attributes of class a
 including the inherited ones And theres another detail which is in what order
 these attributes appear and these are actually become important when we define
 the semantics of how attributes are initialized and the rule is the attributes
 are listed in greatest ancestor first order And what do I_mean by that Lets
 say that we have three classes a b and c and a Im_sorry b inherits_from a And c
 inherits From b Okay lets_say that a defines two attributes a1 and a2 and b
 defines two attributes b1 b2 and c defines two attributes c1 and c2 Then
 class of c Well list the attributes in the following order First well come a1
 and then a2 Because a is the greatest ancestor okay its the the closest to
 the root of the object hierarchy and the attribute was in class a or within any
 class its always listed in the order that it textually appear So first comes
 a1 and a2 and of course the type in the initializer are also lets see here most
 of these attributes but were just concentrating here in the order in which
 the information appears So the next would come class b So the attributes of
 class b will be next and of course therell be the type and initialize for
 those attributes and then finally the attributes of class c Again in the order
 in which they are listed in the class definition okay So that defines the
 order of the attributes for any class Its always in the order of the greatest
 ancestor down the inheritance chain to the class itself which is the argument of the
 class functions At this point were_ready to actually define the formal semantics of
 new t and let switch_colors here So were_going to be allocating a new object of
 type and is going to be in a context with self_object as zero environment e and
 store s The first_thing we have to do were_going to figure_out what kind of
 object it is that were actually going to allocate and the only question is whether
 t is se lf type or not because remember self_type is not the name of an actual
 class If t is not self_type then the class that were_going to allocate is
 actually a t T is actually a class name and with that thats the kind of object
 that were_going to allocate If t is self_type then the kind of object were_going
 to be allocating Is whatever the class is of the self objects So were_going to
 look_at the dynamic_type here of the self_object called that x and that will be the
 class that we create That will be the kind of objects that we created all
 right So theres two possibilities Either object object allocating an
 object of type t if t is actually a class name Otherwise its an object of the same
 dynamic_type as the self_object Alright So now were_going to look up t0 is
 alright And we get out the list of the attribute types and initializers for t0
 So this tells that what we have to do to construct an object of this type Alright
 and the next_thing we do is we allocate locations for each of the attributes So
 because they were in attributes were_going to allocate n locations One for
 each attribute all right And then were_going to create an object with the class
 tag t0 and the attributes are going to be bound to these new locations So the i
 attribute will be abound to the i new location that we just allocated and that
 were_going to update the store Okay So were_going to take our initial store and
 know this is the same with the store we started with We take s and we are going
 to update it so that at these new locations those new locations hold the
 default values of for the type of each of the attribute Okay and that gives_us the
 store_s1 and now we have to evaluate the initializer The two actually initialize
 the attributes And we have to think_about what the environment is in which those
 attributes are initialized and remember the rule is that within initializer I
 mean attribute all the attributes of the class are in scope Alright so the
 environment in this case for the initializers will ju st consist of the
 initializer or the attributes excuse_me themselves Okay so these are the
 attribute names and the i attributes is bound to the is new memory_location
 holding the value the default_value initially of that attribute Alright and
 then finally to evaluate initializers we just evaluate them as a block in the order
 which they appear in the class function This is why it was important to specify
 the order in the class function So remember that these attributes include all
 the inherited attribute so well start by evaluating initializing attributes with
 the greatest ancestor and working our way down to the attributes declared within the
 class itself Notice that the environment here Which has all of the attributes in
 scope is an interesting point this environment has nothing to do with the
 environment in which new t is actually evaluation You_know these environments e
 and e prime are completely separate okay So new so e prime has in scope the names
 of the attributes the class e is a you_know is is some other environment
 Theres some functions somewhere thats calling new t and the variables are in
 scope there are just completely_different okay But anyway evaluating this block Of
 initializers will yield some value And the new store the value isnt used for
 anything okay But the new store is the final store Thats the store that we get
 out as a result of allocating the object and then what is the result of new t well
 it is the new object itself v To_summarize the semantics of New that was
 the first three steps allocate the object actually allocate the memory
 for the object and then the remaining steps initialize the objects by evaluating
 a sequence of assignments and the most_important thing probably to understand
 about initialization and one of the most_important things is the context in which
 or the stage in which the initializers are evaluated So know that only the attribute
 are in scope while we emphasize that and its the same rule as of typing So when
 you re type_checking a class declaration only the attributes are in scope of the
 you_know for the initializers of the class and then as the same naturally the
 same_thing that we use when we actually evaluate the initializers at runtime And
 the initial values of the attributes are the default values and then then we need
 the defaults because precisely because the attributes are insculpt inside their
 own initializers So it could be for example its perfectly reasonable like
 Kuhls to have an initializer lets_say like this And Im just going to I may
 leave all the types here just to save time but I can assign and attribute a the value
 of a and this is perfectly okay because the right_hand side of the intializer has
 all the attributes and scope and for this to make_sense a has to have some kind of
 default_value It has to have some initial value so because I might read it before I
 might read an attribute before I have actually finished computing its
 initializer All_right And the last point here is that notice that in the
 initialization or in the yeah in the initialization of an object self is the
 object itself is the self_object And what do I_mean by that I forgot to mention
 this on the previous_slide just flipping back to that slide for a moment notice
 here That in the evaluation of the initializers what is the context the self
 object is v the self_object is v this is the new object that we have just
 constructed And so its perfectly fine for e1 or en the initialization
 expressions over_here and refers to stealth and what they were referred to if
 they use self is the object that is being initialized Alright Returning to this to
 our summary you_know it might be a little_bit of a surprise how complicated the
 Semantics of new is in cool and its not just cool that has that property In_fact
 every object_oriented language language has a fairly complex semantics for the
 initialization of new objects and its a combination of features like inheritance
 and the ability of initializers to refer to the attributes that leads to this kind
 of complexity Now lets talk_about the semantics of dynamic_dispatch and well
 follow the same plan that we did the semantics of new for us giving for us have
 an informal discussion and high_level description of how the evaluation of
 dynamic_dispatch works and then well look_at the formal operational rule So the
 first_thing it happens in evaluating a dispatch is that well evaluate the
 arguments e1 through en and next well evaluate the target object e0 so that
 expression to get the actual object to which were dispatching Next were_going
 to look_at the dynamic_type of the target object So after we evaluate the zero
 were_going to look_at its class peg is And then were_going to use that type to
 figure_out which function which function f were supposed to use So were_going to
 go and look in the method table for the class x and see what method it has for f
 Then were_going to create new locations and an environment for the call Alright and
 were_going to set up a new locations for the actual_parameters Were_going to
 initialize the those locations with the actual arguments Where s itself to be the
 target object and then were_going to evaluate the body of f Now in order to do
 the look up of a method in a class were_going to need some representation of what
 methods exist and which class is in our operational rules So were_going to find
 a function eval stands_for implementation and the implementation in a class a of a
 method f is is going to be first of all the list of formal_parameters So its
 going to tell_us what the formal_parameters are of f and then the body of f
 Whatever the the function body of f is Now were_ready to actually discuss the
 details of the formal operational_semantics of method dispatch in Kuhl Im
 going to switch_colors here again just for contrast So as we said the first_thing
 we do is we evaluate the n arguments So this first in lines take care of that ad
 notice that each arguments thats evaluated may have side_effects So it
 starts in some store but it may produce a different store So after weve done all
 of this well have the n arguments evaluated and some store s (N) The next
 thing that happens is we evaluate zero This is the expression to which we are
 dispatching and that would give_us an object v0 and some updated_store s (n)
 one Okay And now we have to inspect v0 We want to know whats inside of v0 what
 v0 is made of and in particular were_interested in the classed tag of v0 and
 well also be interested in the contents of its attributes The locations
 associated_with its attributes but first lets focus_on the class tag Alright
 because were_going to use that class remember this is the dynamic_type of the
 zeros and what kind of objects the zeros actually is when the program is running
 And were_going to use that class to look up the definition of f that we should run
 So we look for the method f in class x We want to know its implementation and in
 particular we get the names of the former parameters Okay x1 through xn and we get
 the body of the function or method Alright So the next_thing we have to do
 is we have to allocate space in the memory or in the store for the actual_parameters
 of the method call So we allocate new locations Okay one for each actual
 argument and that were_ready to build an environment in which we can evaluate the
 method alright So what is this environment going to consist of So we
 have to think_about what names or inscoped inside of a method Well all
 the attributes of the class are inscope Okay So this is a class x with
 attributes a1 through an so the environment will have those names to find
 a1 through an And now what are the attributes or locations of those
 attributes Well those are the locations of The zero thats the object that were
 dispatching to that were_going to be the self_object and the attribute names will
 refer to the attributes of of self alright So those locations here are the
 locations of of the attributes in the object v0 Now in addition the formal
 paramete rs are also in scope inside of the method body So we add to this
 environment with just the attributes all of the formal_parameters okay and they are
 at the new locations l(x1) up to l(xn) Okay And notice one slight subtlety about
 the way this is defined were taking an initial environment which Ill show here
 with Ill Ill color these braces in blue So were defining and initial
 environment of the attributes and then were doing updates to that okay So
 were instead of just defining x1 to map two l sub x1 were saying were replacing
 The definition of x1 in this environment in the blue braces with one and maps x1
 and l(x1) Why do we do it that way Well the thing is that a method may have a
 formal_parameter that is the same as an attribute name so for example I could have
 a class a that has an attribute little a in it And it also has a method f that
 takes a formal_parameter named a Okay And if I do that and of course Im leaving
 out types and lots of other things here So here I have an attribute named
 a thats declared And then I have a method that takes the argument
 called a And then the question is when I refer to a Inside of the body of the
 method what a do I get Is this a is this a bind to the formal_parameters is it
 bind to the attribute And the answer we have to get one answer or the other the
 answer in Kuhl is that it binds to the formal_parameter that hides the the outer
 name Okay and thats and thats enforced here in the rule by these
 updates So if a formal_parameter has the same name as one of the attributes it
 will replace the definition of the attribute in the environment Okay Once
 we get the environment set up we need to set up our store what what are the
 changes to the store What we just have to store the actual value of each argument at
 the location for that argument And finally we are ready to evaluate the
 functioning body and the interesting part here is the context in which thats done
 So notice here that the that the self_object in in the context of running the
 method f is the object to which are dispatching Okay And then the
 environment is e prime the new environment we just set up and once_again
 notice that this is a complete change of context that e prime the environment e
 prime has nothing to do with the environment e E prime is built completely
 from scratch using only information about the method for calling it doesnt borrow
 anything from the from the environment where the method originated where the
 method was called from And finally all of this is done in the store that has
 reflects all the side_effects performed by evaluating the arguments by evaluating e0
 and by extending the store with the locations for the actual_parameters So to
 evaluate the body of the method we get back a value and another updated_store and
 that value in store are the results of the entire execution of the dynamic_dispatch
 In this video Im going to give a very brief introduction to Intermediate_Code
 and its use in compilers So the first question to address is what is
 Intermediate_Code or an Intermediate_Language And as the name suggests an
 Intermediate_Language is just that its a language thats intermediate between the
 source language and the target language So keep in mind what a compiler does So
 a compiler takes a program written in some source language And it provides a
 translation of that program into some target language And so in this class for
 example where often our source language is cool and our target language is mixed
 assembly_code Now an Intermediate_Language actually lives in between these
 two and a compiler that uses an Intermediate_Language will first translate
 its source language into the Intermediate_Language and then later translate the
 intermediate the code in the Intermediate_Language into the target language And you
 might wonder well why make life so difficult Why when why do something in
 two steps if you can do in one step And it_turns out that for many purposes this
 intermediate level here is actually quite useful precisely because it provides an
 intermediate level of abstraction So in particular the intermediate level may
 have more details in it than the source language So for example if we want to
 optimize register usage you_know a source language like Cool has no notion of
 registers at the source level and so theres_no way to even express the kinds
 of optimizations you might want to do with registers So an Intermediate_Language
 that exposes that at that amount of detail at_least have registers in it will
 allow you to talk_about and and write algorithms that could try to improve the
 use of registers in the program On the other hand the Intermediate_Language
 which will also have fewer details than the target And so it might be for
 example if the Intermediate_Language is a little_bit above the level of the parti
 cular instruction set of a particular machine and therefore its easier to
 retarget that that intermediate level of code to lots of different_kinds of
 machines Precisely because doesnt have all the grubby details in a of a
 particular machine And experience has shown that this is actually a pretty good
 idea to have Intermediate_Language And almost all compilers have an Intermediate
 Language I In_fact in their implementation and some compilers have
 more_than one Some compilers actually translate through an entire the series of
 Intermediate Languages between the source and target language Now were only going
 to consider one Intermediate_Language for the rest of this course The kind of
 Intermediate_Language which were_going to look_at is going to be a high_level
 assembly And so as I suggested on the previous_slide this language is going to
 use register names but it will have an unlimited number so we can use any number
 of registers that we like Were not bound to 32 or 64 registers The control
 structures will look a lot like assembly_language In particular there will be
 explicit jumps and labels on instructions And the language will also have op codes
 in it so itll look like assembly_language level op codes But some of these op codes
 will be higher_level So for example we might have an op code called Push And
 Push would end up translating into several concrete assembly_language instructions
 for a particular target machine In the intermediate_code that well be looking
 at every instruction will have one of two forms It will either be a binary
 operation or it will be a unary operation And always the arguments on the
 right_hand side in this case the y and the z will be either registers or
 constants They could also be immediate values And this is a very very common
 form of Intermediate_Code so widely_used and so widely_used it actually has a name
 Its called Three Address Code because every instruction has at most three
 addresses in it Two arguments at most two arguments and then a destination Now
 to see that this code is actually low level notice that you_know higher_level
 expressions that involve multiple operations will have to be translated into
 a sequence of instructions that do only one operation at a time So for example
 if I have the expression x sorry x y z and let_me put in parens here to show
 the association So the times binds more tightly than the plus were_going to have
 to this cant be written directly in an intermediate language of this form
 Instead we would have to write it something_like the following We have to
 first compute y z and assign that to a new register or a temporary or you_know a
 new register t1 to hold the intermediate value And then we would have to use t1 to
 compute x t1 which of course is the value of the entire_expression and that
 would end up getting stored in another register I noticed that one effect of
 forcing you to use only one operation at a time You see you do one primitive
 operation at time and then the result of that has to be restored in a register One
 effect of that is to give every subexpression of the program a name So
 if I look back at this expression here I see you_know like y z is anonymous
 That in this expression x y ltigtz the expression y ltigt z itself doesnt have a
 name And by rewriting it like this I actually name that intermediate result So
 again just to summarize this point one consequence of having to write_out
 compound expressions as a sequence of instructions that do a single operation in
 time is that every intermediate value will be given its_own name Generating
 Intermediate_Code is very_similar to generating assembly_code and were not
 going to go into this in any detail because it is so similar But I will
 sketch it for you you_know briefly The main difference between generating
 assembly_code and generating intermediate_code is that we can use any number of
 registers in the Intermediate_Language to hold intermediate results To generate
 intermediate_code we could write a function called IGEN for Intermediate_Code
 Generation that takes two arguments It takes the expression for which were
 generating_code and it takes the register into which the results of that expression
 should be stored And to give you just one example and this is the only example that
 Ill do Lets_take a look_at generating intermediate_code for a expressions I
 wanna generate_code for e1_e2 and I want the results of that to be stored in the
 register t okay So the first_thing Im going to do is Im going to generate_code
 for the subexpressions and I need some place to store the results of the sub
 expressions so Im just going to make up new register names for those results So
 Ill generate_code for e1 and store that in some register t1 and Ill generate
 code for e2 and Ill store the results of that in some register t2 And then we can
 just compute the sum So t t1 t2 and notice that this is a Three Address
 Instruction So were sticking to the rules here and only using three Address
 Instructions In our Intermediate_Code Generator And also notice that because we
 have an unlimited number of registers this actually leads to very_simple code
 generation of intermediate_code In_fact its even a little_bit simpler than
 generating_code for a stack_machine Recall that in a stack_machine we had to
 save the intermediate results here of e1 on the stack And that involved you_know
 more_than one instruction to actually push the result and adjust the stack_pointer
 and things_like that And here we can just save it in a register and and then just
 use that register name later_on So that is actually all I have to say about
 Intermediate_Code for this course You should be_able to use Intermediate_Code at
 the level in which we are going to be using it in in lectures The in the
 future_videos well actually be looking_at Intermediate_Code quite a bit and using it
 especially to express certain kinds of optimizations You should also be_able to
 write simple Intermediate_Code programs and you should be_able to write algorithms
 that work on Intermediate_Code But Im not going to expect you to know how to
 generate Intermediate_Code because were not going to discuss it any further And
 quite frankly it doesnt introduce any new any idea Thats really just a
 variation on the cogeneration ideas that weve_already discussed in quite a bit of detail
 We are now ready to begin our next major topic Program Optimization In this
 video_were just going to give overview discussing why we want to perform
 optimization and what the tradeoffs are for compilers and deciding what kind of
 optimizations to implement Optimization is the last compiler phase that were
 going to discuss Lets just very briefly review the compiler phases First there is
 lexical_analysis and then thats followed_by parsing Then we have semantic
 analysis And after that we talked_about code_generation And now were_going to
 talk_about optimization okay So optimization actually comes before code
 generation because we want to improve the program before we commit it to machine
 code but it is of course the last one that weve discussed But just point out here
 optimization fits in between generally semantic_analysis and code_generation and
 in modern compilers this is where most of the action is Its usually has by far the
 most code and its also the most complex part of the compiler Now a very basic
 question is when we should perform optimizations And we actually have some
 choices We could perform them on the abstract_syntax tree and a big advantage
 of that is that its machine independent but for many optimizations we want to do
 this it_turns out that the abstract_syntax tree will be too high_level that we
 cant actually even express the optimizations we want to perform because
 those optimizations depend_on lower level details of the machine or of the kind of
 machine that were generating_code for that arent present in the abstract_syntax
 tree Another_possibility would be to perform optimizations directly on assembly
 language and the advantage here that all the details of the machine are exposed We
 can see everything that the machine is doing We can talk_about all the resources
 of the machine and so in principle any optimization we want to perform can be
 expressed at the assembly_language level Now a disadvantage of doing optimizations
 on assembly_language is that they are machinedependent And then we would have
 to potentially reimplement our optimizations for each new kind of
 architecture And so as we mentioned in the previous_video another option is to
 use an intermediate language And the intermediate language has the advantage
 potentially if its designed well of still being machine independent Meaning
 it can it can be a little_bit above the level of the concrete details of very
 very specific architectures I_mean it can still represent a large family of
 machines but while at the same time exposing enough optimization opportunities
 that the compiler can do a good_job of improving the programs performance So
 we will be looking_at optimizations that work on intermediate language that has
 operations given by this grammar So in this case a program is a sequence of
 statements and a statement consists of either an assignment Which could be a
 simple copy or a unary or binary operation We can push and pop things from
 a stack and then we have a couple of different_kinds of jumps We have a
 comparison in jump where we compare the value of two registers and then
 conditionally jump to a label We have unconditional jumps and finally there are
 labels the targets of jumps And the identifiers here are the register names
 and we could also use immediate values on the right_hand side of operations instead
 of registers and the typical operators were just going to assume some typical
 family of operators like ltigt etcetera Now optimizations typicallyltigt
 work on groups of statements and one of the most_important and useful statement
 groupings is the basic_block So a basic_block is a sequence of instructions and
 typically we want it to be the longest possible sequence of instructions So we
 want it to be maximal and this sequence has two properties First of all there are
 no labels except possibly for the very first instruction And there are no jumps
 anywhere in this sequence of instructions except possibly for the last instruction
 And a basic_block the ide a behind a basic_block and the reason we require these two
 properties is that its guaranteed to flow the execution is guaranteed to
 proceed from the first statement in the block to the last statement in the block
 So the flow of control within a basic_block is completely predictable Once we
 enter the block once we begin at the first statement of the block which might
 have a label there will be a sequence of statements That must all execute before
 we reach the last statement which could potentially be a jump to some other part
 of the code But once we get here once we get to this very first statement then
 were guaranteed to execute the entire block without jumping out And furthermore
 theres_no way to jump into the block You couldnt just come from some other random
 part of the program and begin execution say at the second or third instruction
 The only way into the block is through the first statement and the only way out is
 through the last statement Say heres a example basic_block and just to show you
 why basic_blocks are useful Lets observ that we can actually optimize this piece
 of code Okay because three always executes after two This instruction here
 always execute after this instruction We could change that third instruction to be
 w three x Okay because we can see here that t is getting two x x or two
 x and here were adding in another x and so w is actually always equal to three
 x And a question then so that that is certainly a correct optimization and and
 its correct exactly because statement two is always guaranteed to execute before
 statement three Another question we might be is whether we can eliminate this
 statement so once we replace this by three x you_know maybe we dont need this
 assignment anymore if this was the only place that t was used if t was a temporary
 value that was computed only to compute the the value w And then we can delete
 this statement and this depends_on the rest of the program We have to know
 whether t has any_other uses someplace else in the program w hich we cant see
 just by looking_at the single basic_block The next important grouping of statements
 is a control_flow graph And a control_flow graph is a just a graph of basic
 blocks And so theres an edge from block a to block b If execution could pass from
 the last instruction in a to the first instruction of b So essentially the
 control_flow graph just shows how control_flow can pass between the blocks and there
 isnt of course no interesting control_flow within the block We know that the
 basic_block will just execute from the first instruction to the last instruction
 So the control_flow graph is a way of summarizing the interesting decision
 points in a in a procedure or a other piece of code showing where some
 interesting control_flow decision is actually made So heres a simple control
 flow_graph consists of two basic_blocks The first basic_block is outside of the
 loop and consists of some initialization code And then we have one basic_block
 here in the loop The basic_block consists of these three instructions And at the
 bottom of the block is a branch a testing branch where either we exit and go
 someplace else or we loop around and execute the loop_body again okay And the
 body of a method can always be represented as a control_flow graph The convention
 that well use is always a distinguished entry node so a distinguished start node
 of the control_flow graph and typically itll just be obvious itll be the one
 listed at the top And then there will be some return nodes or one or some nodes of
 which you can return from and you_know you have a return statements in the procedure
 And return nodes or places_where you exit the procedure will always be terminal
 Meaning there will be no edges out of those blocks Now the purpose of
 optimization is to improve a programs resource utilization And for the purposes
 of this classroom when we talk_about optimization in in our examples and in
 the videos were_gonna be talking_about execution time And were_gonna be talking
 about were g onna be talking_about making the program run_faster And this is
 mostly what people are interested in So most compilers do spent quite a bit of
 effort on making programs run_faster but its important to realize that there are
 many other resources that we could optimize for And actually for any
 resource that you can imagine there probably is a compiler out there that
 spend some effort optimizing for an insert domain domains of application So for
 example there are compilers we might care about code size We might care about the
 number of network messages sent other things that are commonly optimized for our
 memory usage disk accesses so so databases for example Try to minimize
 the number of times you access the disk and and power for battery powered
 devices And the important thing about optimization is that it should not alter
 what the program computes The answer still must be the same okay So were
 allowed to improve the programs resource utilization but we cant change what the
 program will produce Now for languages like C and Cool and all of the languages
 that youre probably familiar_with there are three granularities of optimization
 that people typically talk_about One is called local optimization and those are
 optimizations that apply to a basic_block in isolation So these are optimizations
 that occur within a single basic_block Then there are what are called global
 optimizations and this is really misnamed because its not global across the entire
 program What people mean by global optimization is that implies to a control
 flow_graph Its global across an entire function alright so so global
 optimizations would apply to a single function and optimizer across all the
 basic_blocks of that function And finally there are interprocedural optimizations
 these are optimizations that work across method boundaries They take multiple
 functions and move things around to try to optimize the collection of functions as a
 whole Many compilers do one in fact almost all compilers do one Many many
 compilers today do two but not very many actually do three okay So you see
 decreasing numbers of compilers doing these optimizations as you move up in the
 granularity and partly thats because the optimizations are more difficult to
 implement so its just more work to implement the interprocedural
 optimizations but also because a lot of the payoff is in the more local
 optimizations So expanding on that last point a little_bit more It_turns out
 that in practice while we know how to do many_many optimizations Often a
 conscious decision is made not to implement the fanciest optimization that
 is known in the research literature And thats kind of an unfortunate thing from
 my point of view being somebody whos really likes compilers and spent a lot of
 time thinking_about optimization And maybe its a little_bit hard to accept for
 the professional compiler researchers that that people dont always want to
 implement the latest and greatest optimization But its_worth understanding
 why that might not be the case and it boils down essentially to software
 engineering Some of these optimizations are really hard to implement I_mean
 theyre just complicated to implement Some of the optimizations are costly in
 compilation time So even_though the compiling happens offline it is not part
 of the running of the program you_know the programmer still has to wait while the
 optimizing compiler compiles does its compilation and if it takes hours or in
 some cases days to optimize a program you_know thats not necessarily great
 And some of these optimizations have low pay off They might always improve the
 program but they might only do it by a very small amount and unfortunately many
 of the fanciest optimizations in the literature have all three of these
 properties Theyre complicated they take a long_time to run and they dont do very
 much And so its not so surprising That and not all of these to get implemented in
 production compilers And this actually you kn ow points out what the real goal is
 in optimization What we really want is maximum benefit for minimum cost Were
 really talking_about a cost benefit ratio So like optimization costs a certain
 amount in code complexity complexity of the compiler In programmer time I_mean
 waiting for the compiler to run and and the benefit the amount that it improves
 the program has to be sufficient to justify those costs
 Now we are ready to begin talking_about actual program optimizations and we begin
 with local_optimizations Local optimization is the simplest form of
 program optimization because it focuses on optimizing just a single basic_block so
 just one basic_block and in particular there is no need worry_about complicated
 control_flow we are not going to be looking_at the entire method or procedure
 body Lets dive right in and take a look_at a couple of simple local_optimizations
 If x is an_integer valued variable And from here on well assume that x has
 typeins So let_me just write that down Were_going to assume that x has typeins
 in all of our examples on this slide Then the statement xx0 well that doesnt
 change the value of x Zero is the additive identity for Were just going
 to assign x the value it currently has And so this statement is actually useless
 It can just be_deleted from the program Similarly for xxltigt1 Multiplying by oneltigt
 will not change the value of X and so that statement can also be removed And in
 this case these are great optimizations because we actually save an entire
 instruction Now some statements cant be_deleted but they can be simplified A
 simple example of that is if we have xxltigt0 So that can be replaced_by theltigt
 assignment x0 And again we have we still have a statement here We still have
 to execute a statement But This statement may execute more_quickly because it
 doesnt involve actually running the the the times operator It_doesnt involve
 referencing the value of X Presumably X is registered that doesnt really cost
 anything But you_know its possible that this instruction over_here will execute
 faster than this instruction over_here Now on many machines thats not the case
 In_fact this assignment of this this assignment on the right will take the same
 amount of time as the multiplication on the left but as we will see Having a
 assignment of a constant to a variable will actually enable other optimization
 so this is still a very worthwhile transformation to do An example thats
 almost certainly an optimization is replacing the exponentiation operator
 Raising a value to the power of two by an explicit multiply So here were
 computing y2 And over_here we just replace that by yltigty Why is this a goodltigt
 idea Well this explanation operator here is almost certain not a built in machine
 instructions Probably this is gonna wind in our generated code being a call into to
 some built in math library And there will involve a functioning call overhead And
 then there will be some kind of general loop in there to do the right number of
 multiplies Depending on what the exponent is So in the special case where we know
 that the exponent is two Its much much_more efficient To just replace that call
 to exponentiation by an explicit multiply Another example of substituting one kind
 of operation for another In a in a special situation Is if we have a
 multiplication by a power of two We can replace that by a left bit shift So here
 multiplying by eight Thats the same as shifting the binary representation of x
 over by three bits And I and That will you_know in fact compute the same_thing
 And it doesnt even have to be a power of two If we have a inaudible location by
 some other number that is not a power of two that can be replaced_by some
 combination of shifting and and subtractions Okay So we can replace the
 multiply by some combination of shifts and and arithmetic operations Simpler
 arithmetic operations Now these last two here I should point out you_know these
 are interesting transformations On modern machines generally this will not result in
 any kind of speedup because on modern machines the integer multiply operation is
 just as fast as any_other single instruction Now on historical machines
 these were actually significant optimizations So all of these
 instructions together are examples of algebraic simplifications So that just
 means exploiting properties of the mathematical operators to replace more
 complex instruc tions or more_complex operations by simpler ones One of the
 most_important and useful local_optimizations is to compute the results of
 operations at compile_time rather_than at run_time if the arguments are known at
 compile_time So for example lets_say we have a threeaddress instruction xy op z
 And it happens that y and z are both constants These_are both immediate
 values These_are you_know literals in the instruction Then we can actually
 compute the results of the right_hand side at compile_time and replace this by an
 assignment to a constant So for example if we have the instruction x22 that can
 be replaced_by the assignment x4 And another example which is a very common and
 important one is if the predicate of a conditional consists only of immediate
 values Then we can precompute the result of that conditional And and decide what
 the target of the conditional will be What the next instruction will be at
 compile_time So in this case we have a predicate which is going to be false
 because two is not less_than zero And so we will not take the jump And so this
 instruction can just be_deleted from the program If we had the Otherwise if two
 is greater_than zero so if this is some predicate to valuate true Then we would
 replace this conditional by the jump Okay this would become an unconditional
 jump Alright And this class of optimizations is called constant_folding
 And as I said this is one of the most_common and most_important optimizations
 that compilers perform Now there is one situation that you should be aware of and
 which can be very dangerous and this situation is actually very instructive as
 well And so while it isnt that common I I wanted to mention it because it
 really illustrates some of the subtleties of program optimization and programming
 language semantics So what is this dangerous situation So lets_consider the
 scenario where we have two machines We have a machine X And we have a machine
 Why Okay and now the compiler is being run on machine X And the compiler is
 producing code Generated code this is the generated code produced as the output of
 the compiler over_here Thats gonna be run on machine Y So this is a cross
 compiler Okay So you are running the compiler On one machine but youre
 generating_code for a different machine and why would you want to do that Well
 The the common situation in which you want to do this is that this machine Y
 over_here is a very weak machine So weak in the sense that its very slow and has
 very limited memory Maybe very limited power then its beneficial to develop your
 program and even compile it on a much_more powerful machine So many embedded Systems
 codes are developed in exactly this way Code is developed on some powerful
 workstations that are actually compiling it for some small embedded device that
 well executes the code Now the problem comes If x and y are different So
 consider the situation_where x and y are different machines different
 architectures Alright And Ive been implying that they are but they dont
 have to be I_mean I_mean you could compile on one kind of architecture and
 run the same code on the same architecture But the interesting
 situation is when x and y are different architectures And so lets_consider
 something_like you_know in in you_know machine X lets_say we have the
 instruction A1537 Mmkay And you would like to constant fold that down to
 a52 Alright Now the problem is that if you simply execute this as a floating
 point operation on architecture x the round off and you_know the floating_point
 semantics in architecture x maybe slightly different from these semantics on
 architecture y It could be that if you do that in architecture y directly that you
 might get something_like a5 you_know a519 There might be a small difference
 in the floating_point result depending_on whether you execute the instruction here
 or here And this becomes significant in the case of constant_folding and and
 cross compilation Because some al gorithms really depend_on the floating
 point_numbers being treated very very consistently So if youre going to round
 off the operation one way you need to do it that way for every time you do that
 particular operation And by shifting the computation from comp from run_time when
 it would have executed an architecture y back into the compiler winds of executing
 architecture x You can change the results of the program So how do cross compilers
 actually deal_with this So so compilers that want to be careful about this kind of
 thing what they will do is they will represent the floating_point numbers as
 strings inside the compiler and they will do the obvious long form addition and
 multiplication division operations are the floating operations directly on the
 strings Keep the full precision Inside the compiler And then in the generated
 code produced the literal that is the full precision flowing point number And
 then let the architecture of the architecture y decide how it wants to
 round that off okay So thats the really careful way to do constant_folding of
 floating_point numbers if youre worried about cross compilation Continuing on
 with local_optimizations another important one is to eliminate unreachable
 basic_blocks So whats an unreachable basic_block That is one that is not the
 target of any jump or fall through So if I have a piece of code that can never
 execute and it might never execute because theres_no jump that jumps to the
 beginning of that piece of code and its not it doesnt follow after another
 instruction that can fall through to it Well than that piece of code that basic
 block is just not gonna be used its unreachable and it can be_deleted from the
 program This has the advantage of making the code smaller So obviously since the
 basic_block is unreachable its not contributing to the execution costs of the
 program in terms of the instruction count So the code is never executed So its not
 really slowing down the code because you_know extra instructions are being
 executed But making the program smaller can actually make it run_faster because of
 cache effects So the instructions have to fit into memory just like just like the
 data And if you make the program smaller it makes it easier to fit the program in
 memory and you may increase the spacial locality of the program Instructions that
 are used together may now be closer to each other And that can make the program
 run more_quickly Before continuing on I want to say a word or two about why
 unreachable basic_blocks occur So why would a programmer in their right mind
 ever write a program that had code in it that wasnt going to be executed And
 theres several actually ways in which unreachable code can arise and its
 actually quite common So this is an important optimization getting rid of the
 unreachable code is actually fairly important Perhaps the most_common
 situation Is that the code is actually parameterized with code that is only
 compiled and used in certain situations So for example in C It would be sorta
 typical to see some code that looks_like this If debug then you_know executes
 something where debug is a pound defying constant So in C you can define names
 for literals So you say something_like this You might define debug To be zero
 and so you might see a program that had this piece of code in it and what this
 literally means is that this piece of code is equivalent to if zero then blah blah
 blah Alright so so when youre compiling without debugging you have
 debug to find the zero when youre compiling with debugging you would change
 this line to define debug to be some non zero constant So in this case we are
 compiling without debugging What will happen Well well see that this predicate
 is guaranteed to be zero the constant_folding will take care of that And that
 will result in an unreachable basic_block on the ven branch and then that
 code can be_deleted And so essentially the compiler is able to go_through using the
 optimizer and strip out all of the debugging code That isnt going to be
 used since your compiler without debugging Another case where unreachable code comes
 up is with libraries So very frequently programs are written to use generic
 libraries But the program might only use a very small part of the interface So
 the library might supply 100 methods to cover all the situations that various
 programmers are interested in But for your_program you might only be using
 three of those methods And the rest of those methods could potentially be
 removed from the final binary to make the code smaller And finally another way
 that unreachable basic_blocks occur is as the results of other optimizations So as
 we will see optimizations frequently lead to other to more optimizations And it
 could be that just through other rearrangements of the code that the
 compiler makes some basic_block redundant and and able to be_deleted Now some
 optimizations are simpler to express if each register occurs only once on the
 lefthand_side of an assignment So that means if each register is assigned at
 most once then some of these optimizations are easier to talk_about So
 were_gonna rewrite our intermediate_code always to so that its in single
 assignment_form So this is called single_assignment form And all that means is
 that if we see a register being reused like over_here we have two assignments to
 the register X Okay Were just going to introduce another register name for one
 of those assignments So in this case Im just gonna rename the first use of X
 here definition of X here to be some new register B Ill replace the uses of that
 X by the name B and now I have an equivalent piece of code that satisfies
 single_assignment form Every register is assigned at most once Lets_take a look
 at an optimization that depends_on single_assignment form So were_going to assume
 the basic_blocks are in single_assignment form and if they are then were_going to
 know That a definition of a register is the first use of that register in th_e
 block And so in particular were also ruling out things_like this So there
 could be something_like this where X is read And then later_on X is used Okay
 Sorry X is read and then later_on X is defined So were not going to allow this
 This register here would have to be renamed to something else say Y And then
 uses of X later_on here are renamed to Y Alright so were_going to insist that
 whenever we have a definition Of a register in a basic_block That is the
 first use of that register in the block Alright and if if thats true if we
 main if we put things in that form and thats thats easy to do as weve_seen
 Then when two assignments have the same right_hand side theyre guaranteed to
 compute the same value So take a look here This example So lets_say we have
 an assignment xyz And then later_on we have another assignment wyz And we
 said that there could only be one assignment to x in any basic_blocks So
 all of these instructions that are alighted here they cant be assigning
 to X And they also cant be assigning to y and z Y and z already have their
 definitions So y and z cant be changed And that means that x and w here actually
 compute the same value And so we can replace the second computation Y plus C by
 just the name that we already have for it X Okay and this saves us having to
 recompute values Alright so this is called common sub expression elimination
 Common its a rather long name Sub expression The elimination And this is
 another one of the more important compiler optimizations This is actually
 something that comes up surprisingly often And saves quite a bit of work if
 if you perform this optimization So another use of single_assignment form is
 that if we see the assignment w equals x in a block So here the register w is
 being just copied from the register x Then all subsequent uses of w can be
 replaced_by uses of x So for example Here we have an assignment to b And then
 we have a copy a isto b And then down_here w e have a use of a in the last
 instruction Well that use of a in the last instruction can be replaced_by a use
 of B And this is called copy propagation okay Propagating copies through the code
 And by_itself notice that this makes absolute no improvement in the code its
 only useful in conjunction with some of the other optimizations So for example
 in this case after we do the copy propagation it might be the case that
 this instruction can be_deleted If A is not used any place else in the code then
 this instruction can be removed Now lets do a little more_complex example and use
 some of the optimizations that weve discussed so_far On a slightly bigger
 piece of code So we are starting_with this piece of code here on the left and we
 are going to wind_up with this piece of code here on the right And how does that
 work Well first we have a copy propagation so we have A is assigned the
 value five And so we can propagate that value forward And replace the use of a
 later_on by five and I should_say That when the value is propagated is a constant
 rather_than a registered name is called Constant propagation instead of Copy
 propagation but its exactly the same_thing We we we have a single value
 assigned on the right_hand side either a register name or constant and we are
 replacing uses of that in later instructions by that register name or
 constant Okay So once we have replaced a here by five now we can do constant
 folding and now we have two constant arguments for this instruction So this
 two times five can be replaced_by the constant ten Now notice we have another
 assignment of a constant to a register and so we can propagate that constant forward
 We can replace the subsequent uses of X by the number ten And now we have more
 opportunities for constant_folding ten plus six can be replaced_by the value
 sixteen Alright now we have another another value here which is a a constant
 assignment so another instruction here which is just an assignment of a constant
 to a register so we can p ropagate that constant forward Alright then we wind_up
 down_here with ten times sixteen And I see over_here in my final example here I
 didnt bother to propagate the ten to x But we can do that and this So we can
 either do this optimization So x times sixteen if we didnt do the propagation
 would be equivalent to x left shift four Or we can just replace this by ten times
 sixteen Thatd be even better We wind_up achieving the value 160 Returning to an
 idea I mentioned a couple of slides ago Lets_say there is an assignment in a
 basic_block Some registered W is assigned some value thats computed on the right
 hand_side Lets_say that W the registered name is not used anywhere else
 in the program It_doesnt appear anywhere not only in this basic_block but
 in any_other part of the procedure in which this statement appears Well then
 the statement is dead and can be just deleted from the program And dead here
 means it does not contribute to the programs result Since the value that we
 write into W is never referenced anywhere W is never used doing the computation of
 W in the first place was a waste of time so we can just delete that computation
 Heres a simple example Lets assume that the register a is not used any place else
 in the program And the first_thing we have to do so heres our initial piece of
 code The first_thing we do is we put it in single_assignment form And so Ive
 renamed here this register x to be register b Okay and once we do that let
 me do that so well say that BZY and AB and then we propagate this forward
 Alright so weve now replaced this use of A by B so this takes us to this state
 where we have this piece of code Now we can see that we have an assignment to A A
 is not used in the subsequent instruction We already said that A is not used
 anywhere outside of the basic_block and so the assignment ab can be_deleted and
 we wind_up with this shorter basic_block Now each local optimization actually does
 very little by_itself And some of these optim izations some of these
 transformations that are presented actually dont make the program run_faster
 at all They dont make it run slower either but by themselves they dont
 actually make any improvement to the program But Typically the optimizations
 will interact So performing one optimization will enable another And we
 saw this in the little example that I did a few slides ago So the way to think
 about an optimizing compiler is that it has a big bag of tricks It has a lot of
 Individual program transformations that it knows And what it is going to do when
 faced with a programs optimize its going to rummage around in its bag looking
 for an optimization that applies to some part of the code If it finds one it will
 do the optimization it will do the transformation and then it will repeat
 Itll go_back and look_at the program again and see if theres another
 optimization that reapplies Then it will just keep doing this until it reaches a
 point where none of the optimizations it knows about can be applied to the
 programming Next well take a look_at a bigger example and try applying some of
 the optimizations that weve discussed to it and see how far we get And of course
 this example has been constructed to illustrate many of the optimizations that
 we discussed So the first_thing we can do There_are a couple of opportunities
 for algebraic simplifications So we can replace the squaring up here by a
 multiply And down_here we had a multiply by two which we can replace by a left
 shift of one Next we can observe that we have some copies and constants So we have
 a constant assignment to b and a copy assignment to c And those can be
 propagated forward to the uses of b and c Once weve done that we can do constant
 folding So here the assignment to e The opera the arguments to the shift are all
 constants And so that can be replaced_by an assignment that e gets the value six
 Next we could observe that we have a common sub expression that we could
 eliminate that both a and d have the value x times x So the assignment to d could be
 replaced_by a copy that d now gets the value of a Now we have two opportunities
 again for copying constant_propagation the assignment to D and the assignment to E
 can be propagated forward And finally we can do a bunch of dead code elimination
 So assuming that none of these values B C D or E is used anyplace else in the
 program all four of these statements can be_deleted And this is where we actually
 get some real performance improvement So here we actually are now saving entire
 instructions and thats the best kind of savings that we can have And so we wind_up
 with this as our final form So notice that a is assigned the value xltigtx F isltigt
 then assigned the value aa And then g is assigned the value sixltigtf Now this isltigt
 not quite as fast as it could be alright Theres actually one more algebraic
 optimization that could be done We can notice here that f is actuallyto twoltigtaltigt
 And then we could do some rearrangement here to discover that g12ltigtf Sorry sorryltigt
 twelve x a Alright And then this statement assignment to F might become
 dead code and we could delete it from the program I_think some compilers would
 actually find this but I believe that even current state of the art compilers
 many of them would not discover this last rearrangement to the program
 In this short video Im going to say a few words about a variation on local
 optimization that applies directly to assembly_code called Peephole
 Optimization The basic_idea here is that instead of optimizing on intermediate_code
 we could do our optimizations directly on assembly_code And people optimization is
 one such technique The peephole is stands_for a short sequence of usually
 continuous instructions So the idea is that we have our program We can see we
 can think of it as a long sequence of instructions and our peephole is some
 window onto this program So if we have a peephole of size four we can think of
 ourselves as staring through a small hole at the program and all we can see is a
 short sequence of four instructions and then we can optimize that sequence So
 then we can slide the peephole around and optimize different parts of the program
 And the what the what the optimizer will do is it will you_know stare at this
 short sequence of instructions and if it knows a better sequence it will replace
 that sequence by the other one and then it will repeat this as I said You_know
 applying other transformations to to possibly the same or other parts of the
 assembly program So people optimizations are generally written as replacement
 rules So the well have the window of instructions on the left So itll be
 some sequence of instructions and well know some other sequence of instructions
 that we would prefer on the right So if we see this instruction sequence on the
 left then well replace by the one on the righthand_side So for example if I
 have a move from register b to register a and then I move back from register a to
 register b well thats the second move is useless can can just be_deleted as a way
 to replace this two instruction sequence by a one instruction instruction
 sequence And this will work provided that theres_no possible jump target here So
 if if theres_no possibility that the code would ever jump to this instruction
 then that instruction can be removed Another example If I add i to the
 register a and then I subsequently add j to the register a I can do a constant
 folding optimization here and combine those two add two additions into one
 addition where I add the sum of i j to the register A So many but not quite all
 of the basic_block optimizations that weve discussed in the last_video can be
 cast also as peephole optimizations So for example if we are adding zero to a
 register and were storing it in another register well that can be replaced_by a
 register move If were moving a value from the same register to itself so this
 is like a selfassignment well that instruction can just be_deleted replaced
 by the empty sequence of instructions And together for those two instructions would
 be those two optimizations excuse_me would be_able to eliminate adding zero to
 a register So first this would get translated into a move from a to a And
 then the move from a to a would get deleted And as this little example
 illustrates just like with local_optimizations people optimizations have
 to be applied repeatedly to get the maximum effect I hope this simple
 discussion has illustrated for you that many optimizations can be applied directly
 to assembly_code and that theres really nothing magic about optimizing
 intermediate_code So if you have a program written in any language source
 language intermediate language assembly_language It makes_sense to talk_about
 doing transformations of programs written in that language to improve the behavior
 of the program And its also a good time here to mention that program optimization
 is really a terrible term The compilers do not produce optimal code and its
 purely an accident if a compiler were to somehow generate the best possible code
 for a given program Really what compilers do is they have a bunch of
 transformations that they know will improve the behavior of the program And
 theyll just improve it as much as they ca N So really what program optimization is
 all about is program improvement Were trying to make the program better but
 theres_no guarantee that we will reach the best possible code for a given program
 In this video we are going to continue_our discussion on global data flow analysis by
 taking a look_at how global_constant propagation works in detail To begin
 lets review what the conditions are to do global_constant propagation So to replace
 a use of a variable x by a constant k we have to know the following property That
 on that on every path to the use of x the last assignment to the variable x is
 x equals the constant k okay And this has to be true again on every path to the
 use of x Now global_constant propagation can be performed at any point where this
 property holds What were_going to look_at in this video is the case of computing
 the property for a single variable x at all program points So were_going to take
 one were_going to focus_on one variable x and were_going to compute whether its
 a constant at every program point Its easy to extend the algorithm to compute
 this property for all variables One very_simple but very efficient way to do that
 is just to repeat the computation once for each variable in the method body The way
 we are going to compute the information that we want is to associate one of the
 following values with the variable x at every point in the program And lets
 start with the last one here we will assign x this special value here which is
 pronounced top if x is not a constant So if we cant figure_out whether x is a
 constant at a particular point in the program then well just say x is top at
 that point And this is going to be our safe situation Its always okay to say we
 dont_know what the value of x is and when we say that x has a value top and we
 could say we we were essentially saying we dont_know whether x is a constant or
 not at this point in the program x could have any value Alright Now another
 possibility is that we will say that x is some constant c okay So this is a
 particular constant and if we say that x is a constant c at a program point that
 means in fact at that program point we believe o r we have proven that x is
 always that constant Now there is a third possibility which is not immediately
 intuitive perhaps But as we will see plays a very important role in algorithms
 for for global_constant propagation And in fact in all global data flow analysis
 and that is bottom okay So this value is pronounced bottom and intuitively the idea
 anyway that is kind of opposite of top alright And the interpretation of bottom
 is going to be that this statement never_executes alright So when we dont_know
 whether a statement is even executed at all we will say that x at that point has a
 value bottom Meaning that as far as we know that point in the program is never
 reached It_doesnt matter what the value of x is at that point because that
 statement never_executes Alright so were_going to assign x one of these three
 kinds of values Either bottom some constant or top Lets begin by working
 through an example by hand and our goal is going to be for every program point to
 decide_whether x could be a constant definitely not a constant or whether we
 think that statement might not ever execute okay So execution will begin at
 the top of this control flip graph So this the entry_point and before executions
 begins we dont anything about the value of x So Im not making any assumptions
 about what code came before this basic_block and so to be safe I will say that
 at this point x has some unknown value We_dont know what the value of x is it
 could be anything So x T is the property that we want entry to the first
 basic_block Now after the assignment x three that was indicated there where
 what point were talking_about So after the assignment x three well definitely
 will know that x is the constant three Alright now theres something here thats
 worth pointing out which is that our program points the points that were
 attaching this knowledge to or these these facts to are in between the statem
 ents So when I say x three at this program point what I_mean is that after
 x after this assignment has executed x three but before this predicate of the
 conditional has executed I know that x three okay So the program points are in
 between statements and theres a program point before and after every statement
 Alright so the next_thing that happens is this conditional branch Notice that the
 branch doesnt update x doesnt even refer to x so after the branch executes
 well definitely knows that x three on both branches Alright now lets do the
 right_hand branch The next_thing that happens is the assignment to y that would
 not affect the value of x So after the assignment to y well still know that x
 three alright Now lets_take a look_at the left_hand branch so the first_thing
 that happens over_here is another assignment to y Well that wont affect
 the value of x After the assignment of Y well know that x three And now comes
 to the assignment of x alright So after this assignment happens at this program
 point were_going to know that the value of x is different Were_going to know
 that x four alright So now after this statement we know x four and after this
 statement over_here we know x three alright Now what do we know then about
 what_happens before this statement okay The a two x and I just want to point
 out here I said that theres a program point before and after every statement And
 so this program point here which is before this assignment to a is different
 from the program points that are after x four and y zero So intuitively after x
 four we know that were still on this path over_here on the left and so we know
 that x four and over_here after y zero we still know that were on this
 path is x three But when we reach the point before a two x we no_longer
 know which path were coming from This is the point of the merge of these two paths
 that both lead to this statement And what can we say about the value of x here
 Well there is no constant that we can assign to x because on o ne path x is
 three and on the other path x is four And so what we have to say here is that
 before this assignment executes a x sorry x T We_dont know what the value
 of x is Another way of saying it is we know we we dont_know that x is a
 constant alright So after the assignment executes it doesnt affect the
 value of x we will also have that x T Now notice that once we have the global
 constant information once we know for every program point what the state of x
 is its going to be very easy to perform the optimization We simply look_at the
 information associated_with the statement and then it will tell_us whether x is a
 constant when that statement executes or not And if x is a constant at that point
 then we can replace that use of x by the constant And crucial question of course
 is how do we compute these properties So we did this example by hand but how in a
 systematic fashion an_arbitrary control_flow graph do we actually compute these
 properties for x for every program point Now were_ready to talk_about data flow
 analysis algorithms and theres one basic principle that you see in all of these
 algorithms thats worth_mentioning right away And thats that the analysis of a
 complicated program can be expressed as a combination of very_simple rules that
 relate the change in information between adjacent statements So were just going
 to focus_on local rules and the way were_going to build our global data flow
 analysis is actually by a combination of rules that look only at a single statement
 and its neighbors The idea_behind the rules is going to be the push or transfer
 information from one statement to the next And so for each statement s were_going
 to compute information about the value of x immediately before and after s Remember
 thats where those are the program points that we want to attach information to So
 in particular were_going to have a function C It stands_for constant
 information and C will take three arguments takes the name of the variable
 x It takes the stat ement that were talking_about the particular statement in
 the program that were looking_at And then either in or out and this is what
 distinguishes the value of x before s executes versus the value of x after s
 executes Were_going to be defining a set of transfer functions that push
 information or transfer information from one statement to another And in the rules
 for constant_propagation we need to talk_about a statement and its predecessors So
 were_going to say that every statement s has some set of immediate predecessors p1
 through pn alright So its either of these statements that lead in one step to
 the statement s Lets do our first rule So we have a statement s and it has some
 set of predecessor statements P1 P2 P3 P4 And the situation that were
 interested in here is lets_assume that x is top at the program point after one of
 these predecessors So after some predecessor it doesnt_matter which one
 if it happens that x is top at the program point after that predecessor well then x
 has to be top before the execution of s okay So thats what this rule says It
 says if the out of any predecessor for x is top then the in of s for x is also
 top Alright and this makes_sense It says that if we dont_know whether x is a
 constant on some path that leads to s well then we dont_know that x is a
 constant at s Because for all we know execution came down that particular came
 from that particular predecessor and so we cant make any prediction about whether
 s is whether x is a constant before s executes Now lets look_at another
 situation Lets_say that x is some constant C after the execution of some
 predecessor And that on a after another predecessor a distinct predecessor x is a
 different constant D So D is not equal to C Well then what do we know about x at
 the program point before s executes Well we dont_know anything x has to be top
 because we dont_know which constant s will be since we dont_know which path
 will reach s at run_time And this is the situation that we saw in the example we
 did by hand Another_possibility is that the predecessors all agree on what the
 value of x could be So lets_say that we have you_know predecessor here and that
 after it executes x is known to be the constant C and x is known to be the
 constant C after this predecessor and x is known to be the constant C after this
 predecessor Theres one other_possibility Lets_say that after this
 predecessor over_here all we know is that x is bottom okay And so what the rule
 says is that if we have this situation_where either x has the property bottom
 after a predecessor or all the predecessors agree on the particular
 constant that x could be then before at the program point before s executes we
 know that x is going to guarantee to be the constant C And if you think_about it
 for a second its easy to see why this is correct First of all clearly if we come
 along one of the paths where x is known to be the constant C since they all agree
 and then when we get to s x will definitely have the value C What about
 the bottom case Well remember what that means That_means that this statement is
 never reached so theres some predecessor P here which never_executes Which means
 if P never_executes then we could never reach S along this path from P So the
 only paths that will reach s are the ones where x is known to be a constant
 alright So thats why its okay in this situation say that x if control if
 execution reaches s at all its guaranteed to reach it in a state where x is the
 constant C One last possibility is lets_say that x is bottom for all the
 predecessors okay And what does that mean Well that means that every
 predecessor of S never_executes so theyre all unreachable And therefore if every
 predecessor of x never_executes s itself can never execute and so we can conclude
 that entry to s x is bottom The first four rules that we just looked_at relate
 the out of one statement to the in of the next We also have to have rules that
 relate the in of a statement to the out of the same statement So we have to push
 information from the input of a statement to the output of the same statement So
 once_again there are several cases And lets_take a look_at an easy one first If
 x is bottom on an entry s if the program point before s well that says that at
 the that s is never reached that s never_executes And therefore x will be bottom
 after s after s as well So if the program point before s is never reached
 the program point after s definitely cant be reached either Another_possibility is
 that were assigning x to constant C in this statement In that case the out of
 the statement is going to be equal to C Alright so it doesnt_matter what the
 state of x was before the statement after we execute the statement x will be the
 constant C And I should_say there is a conflict with the previous rule Okay it
 could be that x is bottom before the statement So rule six has lower priority
 than rule five So we so if we could say that x is bottom after the statement we
 would prefer you to say that so rule five would be applied first and then if rule
 five does not apply So if x is some other constant D or x T then we would apply
 this rule and we would conclude that x is the constancy afterwards and that makes
 sense If x is d or x is the is top that means that control as far as we know can
 reach this statement And then what were saying here is that well after the
 execution of this statement if control can reach this statement after the
 execution of it x is guaranteed to be the constant C Another_possibility is that we
 have an assignment to x but the right_hand side is more_complicated than a constant
 So this case is for everything other than the constant assignment Okay so this F
 here just stands_for some more_complicated expression than just a simple constant
 And in this case we were just going to say we dont_know what the value is were
 not going to try to guess what the result of that computation is and well just say
 that x T X w e dont_know what the value of x is after the execution of this
 statement And once_again rule five takes precedence so if rule five applies then
 we would apply then then we would use that rule instead of rule seven But if
 control can reach this statement so up here x C or x T Then well apply rule
 seven and conclude that x is top after the statement And finally Rule eight another
 possibility is that were assigning to some variable other than x And in that
 case if x k before the statement then we just keep that value Okay so whatever
 x was before the statement bottom a constant or top if the assignment is to
 some other variable other than x then x will have the same property after the
 statement executes Now we can put these rules together into an algorithm For
 every entry_point for every entry statement to the program were_going to
 say on entry that we dont_know anything about the value of x So the program point
 before that entry_point were_gonna say that x has an unknown value top And then
 everywhere else were_going to say that the value of x is bottom okay And this
 is actually important so were_going what this intuitively is doing is its saying
 well as far as we know except for the entry_point to the program which can
 definitely be executed we dont_know whether any of the other statements in the
 control_flow graph are actually ever executed and so were_going to assume
 initially that theyre not And were just going to say that x has the value
 bottom everywhere except at an entry_point And now what were_going to do is a kind
 of constraint satisfaction algorithm Were_going to pick some statement that
 doesnt satisfy one of the rules one through eight And then were_going to
 update it using the appropriate rules So well look for places in the control_flow
 graph where the information is inconsistent according to the rules and
 then well update the information to make it consistent_with the rules Lets_take a
 look_at our example again So were_going to start out by saying x T at the entry
 point and then were_going to have all of our other program points And let_me
 indicate them here Okay so these are all the other program points that we have to
 be concerned with And there again theres a program point before and after
 every statement And we are going to say the x bottom for all of these So again
 what this means is that as far as we know control doesnt reach any of these
 points We have not yet proven to ourselves that any of these statements can
 execute And now we just look around in the program and try to find places_where
 the information is inconsistent according to the rules and then we update the
 information Let_me switch_colors here So when we begin the information is
 consistent everywhere except at this first statement because if x is T before and
 were assigning x to value three Well then we should not have x bottom as the
 result In_fact this should be x three It should be the appropriate information
 here and once we update that then we see that this next statement is inconsistent
 because now we know this statement is reachable We have a statement here and
 were concluding that the point after is not reachable which is not not correct
 according to the rules So that I believe that this is an application of rule eight
 We have a statement here that doesnt refer to x as and so whatever the value of
 x was before the statement becomes the value of x after the statement so that
 becomes x three And then now we can see that this information is inconsistent
 The out of the statement here is not consistent_with the in of the statement
 here In this case you_know its just one predecessor And so the the value
 should be the same so x should be three At this point And similarly x should be
 three at this point Here we have an assignment to a variable other than x
 That should information should be the same before and after the statements same
 thing here Now we have an assignment x The point before that assignment is
 reachable and so sin ce this is a constant assignment we should know that x is that
 constant after the assignment So here again we have a in and out issue so the
 out of this statement is not consistent_with the in of this statement So this is
 going to have to be updated but now what should this be Well we have two
 inconsistent predecessors and so this has to be top and then finally an assignment
 to x sorry an assignment to a state to a variable other than x so the information
 should just propagate across And that same is updated like this so now x is
 known to be top afterwards And now if we look around at all the program points
 wed see that all the information is consistent All the rules if you if you
 if you check_whether the information before and after a statement or across a
 statement Im_sorry or between predecessors and successors is correct
 its correct everywhere according to the rules and so were done
 In this video_were gonna continue_our discussion of analysis of controlled flow
 graphs by focusing on what is undoubtedly the most interesting aspect of the whole
 problem the analysis of loops Heres an example of control_flow graph with a loop
 in it And it_turns out that the need for the special element bottom in our analysis
 is intimately tied to the analysis of loops And so lets just think_about how
 we would do our constant_propagation example analysis with this particular
 control_flow graph all right So what do we know about x Okay So initially we
 dont_know anything so before we enter the control_flow graph its value its top
 and and after the assignment of three well know that x has the value three The
 conditional branch here the predicate wont affect the value of x So itll be
 three on both branches The assignment to y wont affect it so itll be three here
 as well And now we come here okay and lets focus_on this statement right here
 So the rule is that the analysis of x at y equals zero Okay So with a value of x
 right here before before the assignment to y Is a function of all the
 predecessors So we need to know what the value of x is on both of the incoming
 edges Okay Well we dont have a value down_here yet So the question is you
 know what is the value of x here on this edge And in order to figure that out
 wed have to look_at its predecessors Okay what are its predecessors Well
 theres this point here after the predicate theres this point here between
 the two statements and then theres this point here after the execution of y Were
 just following the edges backwards here Looking at you_know where we need to
 know information for x We need to know it here we need to know it here and we know
 it here alright And then because of this edge that means we again need to know it
 at both of the predecessors of y zero So now were in the loop and this isnt
 too surprising I_mean if you have if information about x depends_on t he
 predecessors of a statement and you do follow that recursively then youre gonna
 wind_up going around loops like this And and theres_no good way at_least theres
 no no particularly immediately obvious way to solve this problem So how do we I
 get information about the predecessor the predecessors of y zero when they depend
 on themselves So to be more precise looking_at that particular statement again
 in order to compute whether x is constant at the point right before the statement y
 zero we need to know_whether x is constant as the two predecessor and that
 information depends_on his predecessors which include y zero Okay so this is
 the conundrum So how are we to solve this recursive problem And theres a
 standard solution that that is actually used in many areas of Mathematics and not
 just in the analysis of of loops When you have these kinds of recurrence
 relationships or recursive equations And the standard solution is to break the
 cycle by starting_with some initial guess So you have some initial approximation
 that is really not perhaps even expected to be the final result but allows you to
 get going So and so what were_going to do is that because of the cycles all of
 the points all the program points have to have values at all times And so were
 going to assign an initial value and that is what bottom is for And the initial
 value bottom means so_far as we know control never reaches this point Remember
 this weve said this quite a while ago on several_videos ago And this will allow us
 to make progress And to see that lets go ahead and analyze this control_flow graph
 now where we assume that all points and at all points initially x has the value
 bottom except at the entry_point So the entry_point is special Here we assume
 that we dont_know anything about x because we know the control reaches the
 initial point But initially were_going to just say well x is bottom everywhere
 else Okay so inaudible the bottom there inaudible bottom there okay Im
 gonna just fill in all the values And Im just writing it everywhere here And
 theres really another one right here after the merge of these two paths So I
 indicate that All_right so there now we have our initial setup and now remember
 what the procedure is we go and look where the information is inconsistent and
 we update it So where is the place_where the information is inconsistent Well
 clearly its not correct here all right because we know that after if if control
 reaches the point before x three then after the assignment x will be equal to
 three Again the predicate will not change the value of x so we have to update the
 results of the two branches after the predicate and after its assignment that
 doesnt affect x to make that information consistent that we have that Now lets
 go_back to our interesting case Here we know that x three on this branch coming
 in to y zero And so_far as we know control never reaches the other
 predecessor So were_gonna start out by assuming that that that part that path
 is never taken And if that path is never taken then it wont contribute anything
 And s at this point in the program we will know that x three So assuming
 that all this information is correct we will be_able to conclude that x three at
 this point And notice how weve_been able to break the cycle here and get started
 So we just assume that the you_know this last edge in the cycle never_executes
 and if thats not correct well find out later and this value down_here will become
 something other than bottom and then well update the assignment again Alright so
 lets continue on So we have x three before y is assigned zero So the
 assignment of y will not affect the value of x So make the information afterward
 consistent well have to make x3 there Now we have a merge of two paths Okay
 So the at this point here before the execution of this assignment we will also
 know that x three The assignment a will not affect x Well update that point
 there and the predicate will not affect the value of x So well know that x
 three on the back edge And now this information has changed We now know the
 control can reach this edge cuz we followed the control path all the way
 here We have some new information about x and so now we have to double check that
 everything is still okay So here we have x three on this edge x three on this
 edge and our previous conclusion that x three on the entry to the statement y
 zero Well that is also consistent There_are no places left in the control_flow
 graph that are inconsistent So all the information is consistent_with all the
 rules And so were done and this is the final analysis Were able to conclude
 that all at all of these points here like I say every point except the entry
 point that x is in fact the constant three
 In the last several_videos weve_been talking_about doing a kind of abstract
 computation Computing with elements like Bottom the Constants and Top And in this
 video_were going to start to generalize those ideas a little_bit And the first
 thing were_going to talk_about the first step towards that generalization is to
 talk_about orderings of those values First Id like to introduce a technical
 term These values that we compute within program analysis things_like Bottom the
 Constants and Top these are called Abstract_Values And thats just to
 distinguish them from the Concrete Values so the Concrete Values are the actual
 runtime values that a program computes with Things_like actual objects and
 numbers and things_like that And the Abstract_Values here the program analysis
 uses are in general more abstract Some particular Abstract_Values can stand for a
 set of possible Concrete Values And in a particular set of Abstract_Values were
 using for concept propagation theres only one very Abstract Value and thats
 the Top and it stands_for any possible run_time value So it stands_for the entire
 set of run_time values Anyway it_turns out that there is a way to simplify the
 presentation of of the analysis that we have been discussing by ordering the
 Abstract_Values So were_going to say is that Bottom is less_than all the constants
 and that and all the Constants are less_than Top And so if we draw a picture with
 the lower values drawn towards at the bottom picture and the higher values drawn
 at the top And and edges between values where theres a relationship we get this
 diagram here So you have bottom down_here underneath all the other values
 Bottom is less_than every Constant Okay So notice that all the constants are here
 on the middle level alright And also notice that the constants are not
 comparable to each other alright So this ordering is different than the numeric
 ordering So zero is not less_than one for example Zero and one are inco mparable
 as are every other pair of Constants So you have you_know Bottom at the Bottom
 You have all the Constants in the middle and theyre incomparable And then bigger
 than everything else is Top Now with the ordering defined theres a useful
 operation we can define on collections of elements and that is the Least_Upper
 Bound or LUB alright And and this means is taking the smallest element that
 is bigger_than everything in the Least_Upper Bound So for example if I have
 the Least_Upper Bound of Bottom and one that is equal to one okay If I had the
 Least_Upper Bound of Top and Bottom that is equal to Top And perhaps more
 interesting one the Least_Upper Bound of one and two so two incomparable Constants
 here And remember the meaning of the Least_Upper Bound its the smallest
 element in the ordering thats bigger_than everything over which were taking the
 Least_Upper Bound So we just have two things here in our Least_Upper Bound But
 the Least_Upper Bound of one and two the smallest thing thats bigger_than both of
 them or greater_than or equal I should_say both of them is Top okay And so
 the Least_Upper Bound then if you think_about it if you draw draw our picture
 again So we had Bottom and we had Top and if you pick out some points here
 lets_say we want to take the Least_Upper Bound of Bottom and two youre just
 picking the smallest thing thats bigger_than both Well thats going to be two
 itself similarly two on Top you will get Top And then if have anything thats
 incomparable then you have to pick something thats bigger_than both of them
 and in this case that will always end up being Top for this very_simple ordering
 alright Then given this idea of the Least_Upper Bound it_turns out that rules one
 through four all theyre doing is computing the Least_Upper Bound So the in
 of a statement is just equal to the Least_Upper Bound of the out of all the
 predecessors Alright and thats all that rules one through four were saying And
 if you remember what we had there we had you_know we had a bunch of predecessors
 and then theres some kind of statement s and all were doing is whatever the
 information is on these predecessors were just taking the Least_Upper Bound
 over it all right And that is the information on entry to to s The
 ordering on the Abstract_Values also helps to clarify another important aspect of our
 analysis algorithm which is why it terminates So remember the algorithms
 termination condition is to repeat to repeatedly apply the rules until nothing
 changes until there are no more inconsistencies in the control_flow graph
 and theres_no information left to update Well just because we say were_going to
 repeat until nothing changes that doesnt guarantee that eventually nothing changes
 It could be that that goes on forever that we always introduce new
 inconsistencies with every update and we never actually get to the point where all
 the information is consistent So the ordering actually shows why that cant
 happen and the algorithm is guaranteed to terminate So remember that in every
 program point except the entry_point the values start as Bottom So they start at
 the lowest place in the ordering And then if you look carefully at the rules its
 easy to see that the rules can only make the values increase at a program point So
 Bottom can be promoted can be changed at a given program point up to some Constant
 and and and another update could raise that Constant to Top but of course once
 we get the Top theres_no greater element And if the rules can only make
 the elements increase then eventually we have to run out of elements that could be
 increased okay So what that says is that each piece of information were computing
 for every statement for every variable and for either in or out it can change at
 most twice okay So it can go from a Bottom to a Constant and from Constant to
 a Top but after that it will never be updated again And what this means is that
 the constant_propagation algorithm that weve described is actually linear in
 program size So the number of steps is gonna be bounded by the number of c values
 that were trying to compute times two cuz each one of those could change two
 times And since theres one value for the entry and exit over the in and out of
 every statement the total number of steps that the algorithm can possibly take is
 the number of program statements times four
 In this video_were going to look_at another global analysis called liveness
 analysis So in the past several_videos weve_looked at a procedure for globally
 propagating constants through a control_flow graph And lets heres heres one
 of the control_flow graphs weve_been looking_at and recall that this algorithm
 that we discussed would be sufficient to show that we could replace this use of x
 here by the constant three And once we do that This assignment x might no_longer be
 useful It might not be used anywhere And so we could potentially delete this
 statement from the program And that would be a real optimization an important
 optimization to do However we can only do that if x is not used elsewhere in the
 program So lets be a little more careful about what we mean by saying that x is not
 used So down_here is a use of x a reference to x in a statement And
 clearly this particular reference to x is use picking up the value thats
 defined by this right x here So we say that the right of x here is live This
 one is live Okay And what that means is that the value may be used in the future
 So live equals may be used In the future Okay So the value written to x at
 this line of code maybe used by some subsequent instruction And here its not
 just that it may be used Its actually guaranteed to be used because theres only
 one path And that one path has a reference to x on it before theres
 another assignment to x Okay So this particular value of x as written here is
 guaranteed to be used But in general we dont require that We just mean there has
 to be a possibility that it will be used Now in contrast lets_take a look_at this
 Other statement in this example Here we assign x a value three but this assignment
 x this value of x is never used This one is dead Alright Because the value
 three here is overwritten by the value four before theres any use of the
 variable x Okay So this particular right to x will never see the light of day
 Itll never get used by any part of the program And we say that it is dead So
 to summarize a variable x is live as a statement S if there exist some statement
 that uses x Okay So some other statement S_prime that uses x and there
 is some path from S to S_prime and there is no intervening assignments on that path
 to x Alright So there needs to be an assignment to x at some statement S there
 is some path through the program that reaches a read of x Add sum statement to
 S_prime and along that path there is no right to x Okay And if this situation
 arises then we say that this value written in this first statement s is live
 Now if a value is not live then it is dead And a statement that assigns to x is
 going to be dead code if x is dead after the assignment So if we know that
 immediately_after the assignment immediately_after this assignment to x
 there is no possibility that a value of x will be used in the future Well then the
 assignment was useless and the entire statement can be removed Alright So dead
 assignments can be_deleted from the program But notice that in order to do
 that we have to have the liveness_information We need to know_whether x is
 dead at this point So once_again what we want to do is to have global
 information about the control_flow graph In this case the property is whether x
 will be used in the future We want to make that information local to a specific
 point in the program so we can make a local optimization decision Alright And
 just like for constant_propagation were_going to define in a an algorithm for
 performing liveness analysis And its going to follow the same framework If
 were_going to express liveness in terms of information transferred between
 adjacent statements just as we did for copy of constant_propagation And its
 gonna turn out that liveness is actually quite If its simpler or somewhat
 simpler than constant_propagation since its just a Boolean property Eh you
 know its either true of false Alright So lets_take a look_at some of the rules
 for liveness So here were defining what it means for x to be live at this point
 here So were immediately_after p is x live And its going to be live Remember
 what the intuition is The intuition is that a the variable x is live right after
 p if the value of x is used on some path On one of the paths that begin at p
 Alright And so in order to know_whether its live were_going to take the
 liveness_information at each of the input points So that would be here here here
 and here So each of the successor statements after p And were_gonna ask is
 x live at any of those points So its just a big or over the liveness of x and
 all of the successors of p And thats the liveness of x at the out of p Next lets
 consider the effect of individual statements on the liveness of x So the
 first rule is that if we have a statement and it reads the value of x
 Okay So here we have an assignment statement and on the right_hand side it
 refers to x so its reading x Then x is live Before that statement Clearly x is
 just about to be used on the end of this statement and so x is live at that point
 Alright So if a statement or if if a statement reads the
 value of x then the in of that statement x is true Sorry the liveness of x is
 true A second case is when a statement writes the value of x So here we have an
 assignment to x And the rest of the statement does not refer x Does not read
 the value of x So theres_no x in E Okay So in this situation x is not live before
 the statement X is not live or we can say that x is dead Before the statement And
 why is that Well were overriding the value of x so whatever value x had
 before this statement is never gonna be read Okay Because the ee here the
 righthand_side of the assignment doesnt refer to x And so immediately before the
 statement the current_value of x is never gonna be used in the future And so x is
 dead at that point And finally the last case is what if we have a statement that
 does not refer to x Okay So it neither reads no r writes x Well then whatever
 the line this is of x after the statement it has the same liveness before this
 statement So if x is live here Then x will be live here Okay and similarly if
 x is dead After the statement Then x must be dead before the statement And thats
 because x if x is not use in the future after the statement S then it still want
 be use in the future before the statement S Since the statement S neither reads nor
 write x So those are the only four rules and now I can give the algorithm So
 initially we left the liveness_information for x be false at all program points And
 then we repeat the following until all the statements satisfy the rules one through
 four and just has its the same algorithm that we used for constant_propagation We
 pick some statement where the information is inconsistent and then up update the
 information at that statement with the appropriate rule So lets do a simple
 example something with a loop So lets begin say by initializing x to zero and
 then what should our loop_body do Well we can check_whether x is equal to ten
 and if it is well well exit the loop And lets_assume that x is dead on exit
 So x is not refer to outside of the loop In other wise if x is not ten Then we will
 increment x and well branch back to the top of the loop So this is a very very
 silly little program It just counts to ten and then exits Well lets do the
 lightness now to see where x is life So since x is dead here on exit its clearly
 gonna be dead on the out Of of this conditional on this branch Okay So I
 should_say that x is not live So were using booleans here so thats xs
 liveness would be false And were assuming And x is also not live everyplace else
 initially Okay And so theres a program point in there also Where the liveness of
 x is false Okay So now lets propagate the information Well so here we have
 read of x And let_me switch_colors here So here we have a read of x So in fact
 the informations inconsistent here because ri ght before this statement since
 we have a read of x x must be live So in fact x is live at this point Now notice
 that this statement both reads and writes x Okay But the rule that says x is live
 before when we do a read takes priority here Because the read happens before the
 write So well read the old value of x before we write the new value of x Okay
 So the old value of x does get used and thats why x is live immediately before
 this statement Okay so then heres another read of x Okay so on the so
 the point immediately before this when I left out one program point here x is also
 Y Okay And then following edges backwards well that means x is gonna be
 live on the back edge of the loop And its also gonna be live by going into the
 initialization block Alright Now we come_back around here and we see that were
 done cause x is already known to be live within the loop_body And now live x is
 also live here And then the question is you_know what about this point on the
 entrance at the entrance to the control_flow graph Well theres a right of x And
 with no read of x on the righthand_side So in fact x is not live on entry to
 this control_flow graph So in fact x is dead at this point So whatever value x
 has when we enter the control_flow graph it will never be used in the future
 Alright and so that is the correct liveness_information for every
 program point in this example Now another thing you can see from our little example
 is that values change from false to true but not the other way around So every
 value starts at false and it can change at most once To say that the value is
 actually live the property becomes true and then it wont ever change back to
 false again So going back to orderings We only have two values in this analysis
 false and true And the ordering is that false is less_than true Okay And we
 know so everything starts at the lowest possible element of the ordering and they
 only move up and so they can be promoted to true but no t vice versa And so since
 each value can only change once termination is guaranteed That eventually
 were guaranteed to have consistent information throughout the control_flow
 graph and the analysis will terminate
 In this video_were going to begin a discussion of Register Allocation which is
 one of the most sophisticated things that compilers do to optimize performance and
 also involves many of the concepts that weve_been discussing in global flow
 analysis Recall that intermediate_code can use unlimited numbers of temporaries
 and this simplifies a number of things Particularly it simplifies optimization so
 we dont have to worry_about preserving the right number of registers in the code
 But it does complicate the final translation into assembly_code cuz we
 might be using too many_temporaries and this is actually a problem in practice
 So its not uncommon at all for intermediate_code to use more temporaries
 than there are registers on the target machine The problem then is to rewrite
 the intermediate_code to use no more temporaries than there are machine
 registers and the way were_going to do that is were_going to assign multiple
 temporaries to each register So were_going to have a manyone mapping A many
 to one mapping from temporaries to registers okay And clearly theres a
 little_bit of an issue here if we really are using many_temporaries we will not be
 able to fit them all into a single register So there needs to be some kind
 of a trick and well say what that trick is in a few_minutes and there will be
 situations actually when this will fail well have to have some kind of back up
 plan But our default plan is to try to put as many_temporaries as possible into
 the same machine register And doing all of this without changing the behavior of
 the program So how can we do this Magic thing How can we actually make a single
 register hold multiple values Well the trick is that its fine for registers to
 have local values as long as it only has one value at a time So lets_consider
 this program Im going to switch_colors here Okay Simple three statement program
 and notice here that a is used in the first two statements So its written in
 the first statement read in the second stateme nt e is written in the second
 statement and read in the third statement and that is only written in the third
 statement And actually these three values a e and f they dont ever really
 coexist at the same time but at the time weve read a we are really done with it
 Weve all the uses that they are going to have in this little code fragment Here
 Im assuming that a and effort are not used anywhere else and so it_turns out
 that a e and f could all actually live in the same register Alright thats
 assuming that a and e are dead after their uses And what will that look like well
 lets allocate them all to a particular register r1 and lets assign c d and b
 into their_own individual registers and the code would like this r1 would be r2
 r3 and then r1 would be r1 r4 and r1 would be r1 one And so now notice how
 this is just a transliteration of the code over_here into registers but there is a
 many one mapping of names on the left to register names on the right A register
 allocation is an old problem In_fact it was first recognized way back in the 1950s
 in the original Fortran project but originally register_allocation was done
 with a fairly crude algorithms and who is rapidly or very quickly noticed that was
 actually a bottle neck in the quality of code_generation that actually limitations
 on the ability of register_allocation and do a good_job have a really significant
 effect on the overall equality overall quality of the code that compilers could
 produce And then about 30 years later in 1980 a breakthrough occurred where people
 discovered or a group of researchers at IBM discovered a register_allocation
 scheme based on graph coloring And the great thing about this scheme is that its
 pretty simple Its easy to explain Its global meaning it takes advantage of
 information from the entire control_flow graph at the same time and also happens to
 work well in practice And heres the basic principle that underlies the modern
 register_allocation algorithms So if I have two temporaries t1 and t2 I want to
 know when they can share register So theyre allowed to share a register and
 theyre allowed to be in the same register if they are not live at the same time
 okay So like I said any point in the program in most one of t1 or t2 as live
 And we are more concise which I already said was partially is is that if t2 t1
 and t2 are live at the same time okay Meaning that theres theres some program
 point were both are live then they cannot share a register alright So this is the
 negative form of the statement and it just tells you that if if you need two values
 at the same moment in time then they have to be in separate registers Lets_take a
 look_at a control_flow graph and now we know that in order to do the register_allocation
 to solve the register_allocation at_least in this in this way were_going to need
 liveness_information So lets compute the live variables for each point of this
 program So here it is and Ill just walk_through it very quickly Lets assume that
 on exit from this loop that only b is live So b is the output of this piece of
 the code and its used elsewhere but none of the other variables are live So now
 if we work backwards remember that line is a backward analysis Well see here
 that b is written so its not live before the statement but f and c are read So
 both c and f are live before this basic_block Okay and similarly if we if we go
 up another level here here we see that e is now alive and f is dead because f was
 written here and e was read And over on this path here we have another exit where
 b is live and now at this point here right after this basic_block the set of lot
 variables that are live is b c and f because b is live on one path and c and f
 are live on the other path Remember for something to be live it only has to be
 live on some in some future possible evolution of the execution So on some
 path out of this node is a variables live then its live at the exit from this
 Working backwards here B c and f are live here because e is read And b c and
 f are not referred to in this statement and so they just propagate upwards Here b
 is removed from the live f because its written but d is added and set here and
 similarly for the other edges in this graph If you go and check all the other
 edges you will see that the live set is correct and it just follows from the
 simple rules we gave in the previous_video But how are going to use the
 liveness_information to do register_allocation Well were_going to construct
 and undirected graph and in this graph there will be a node for each temporaries
 so each variable will have a node in the graph and therell be an edge between two
 temporaries if they are live simultaneously at some point in the
 program alright So backing up and looking_at our little example here we can
 see for example at this point in the program c and e are both live Theyre
 both in the live set after this basic_block executes So c and e cannot be in
 the same register Alright continuing on this is called this data_structure this
 graph is called the Register Interference Graph or RIG for short And again the
 basic_idea is that two temporaries can be allocated in the same register if there is
 no edge connecting them in the register_interference graph So heres a register
 interference_graph for our example This is the graph constructed from the code and
 the line analysis that were given a few slides ago and you_know its easy to read
 off from the graph what the constraints are So for example b and c cannot be in
 the same register because b and c are connected by an edge Okay seeing that
 theyre live simultaneously at some part some point in the program and so they have
 to live in different registers On the other hand there is at there is no edge
 between b and d okay So this edge is missing and therefore its possible that
 b and d could be allocated in the same register They are live ranges all the
 times in which they are alive do not overlapped So a great thing about the
 register_interference graph is that it extracts exactly the information needed to
 characterize a legal register assignment So it gives_us a representation of all
 the possible legal register assignments Now I havent_said I havent actually get
 a register assignment out of the register_interference graph but the first step is
 to characterize the problem in some kind of precise way And the graph of cannot
 live in the same register constraints does that for us The other thing that is good
 about is a is a global view of the register requirements meaning its over
 the entire control_flow graphs So takes into account information from every part
 of control_flow graph which will help us to make good global decisions about what
 value is very important to live in registers And finally the other thing to
 notice is that that after reconstruction the register_allocation for algorithm is
 going is architecture independent I havent shown you the algorithm so you
 just have to believe the statement for the moment but its going to turn out that
 were not going to depend_on any property of the machine except for the number of
 registers So thats the only thing we need to know about the machine in order to
 take a RIG and and do register_allocation using it
 In this video we are going to continue_our discussion of register_interference graphs
 and talk_about how to use RIGS to come up with register assignments for procedures
 And were_going to look_at one particular technique thats popular called graph
 coloring So first a couple of definitions A graph coloring is an
 assignment of colors to nodes such that the nodes connected by an edge have
 different colors So if I have a graph lets_say with with three nodes and its
 fully connected so every node connect to every other node And then and then a
 coloring of this graph would be an assignment of colors such that every pair
 of nodes are connected by an edge have a different color So for example I could
 color this node blue and I could color this node green and I could color this
 node black okay And then that would be a valid coloring of the graph because each
 pair of neighbours has a different color And then the graph is kcolorable if it
 has a coloring that uses k or fewer colors In our problem the colors
 corresponds to registers so we want to do is to assign colors or registers to the
 graph nodes And were_going to let k the number the maximum number of colors were
 allowed to use be the number of machine register So the actual number of
 registers present on the architecture for which were generating_code And then if
 if a RIG if a registered interference_graph is kcolorable then theres going
 to be a register assignment that uses no more_than k registers So lets_take a
 look_at an example rig and for this particular graph there is no coloring It
 turns_out that it uses fewer than four colors But there is at_least one for
 coloring of this graph And then here it is so Ive used colored labels but also
 register names so that you can see what registers we might assign to each of the
 nodes And just notice that although there are many more_than four temporaries or
 four nodes in this graph we do manage to color it with only four colors and some of
 the nodes have the same color So for example d and b are allocated the same
 color as are e and a Just to remind ourselves where this register_interference
 graph came from here is the original control_flow graph again And once we
 have the coloring of the graph now we can do the register assignment We can replace
 the temporaries by their corresponding register names and then we get this
 control_flow graph So here weve just renamed each of the variables of the
 program with its register that it was assigned to And now were very close as
 you can see to having code that we can emit and execute on the target
 architecture
 The graph coloring here is like that we discussed in the previous_video doesnt
 always succeed in coloring an_arbitrary graph And it may well get_stuck and not
 be_able to find a coloring And so in that case the only conclusion we can reach is
 that we cant hold all the values that wed like to register We have more
 temporary values and we have registers to hold them And those temporary values have
 to live somewhere so where should they live Well theyre going to have to live
 in memory Thats the only other kind of stories that we have And so were_going
 to pick some values and spill them into memory The ideas that we have the
 picture in your mind should be A bucket and it can hold a fixed amount of stuff
 Those are the registers and when it gets too full some of the stuff spills over
 and and ends up some place else Now when does the graph coloring here do get
 stuck Well this only situation which we wont be_able to make progress as if all
 the notes have k or more neighbors So lets_take a look_at our
 favorite register_interference graph when we will be using at our examples and now
 lets_say that our the machine we want to use only has three registers and so we
 instead of finding a 4coloring of this graph we need to find a 3coloring So
 lets think_about how to find the three coloring of this graph If we apply the
 heuristic well remove A from the graph but then were_going to get_stuck Because
 once you take A out of the graph and its edge is out and every node thats
 left has more_than has three or more neighbors as at_least three neighbors So
 theres_no node that we can delete from the graph and be guaranteed to be_able to
 find the coloring for it with the heuristic that we discussed in the previous_video
 So in this situation what were_going to do is were_going to pick and know that
 there is a candidate for spilling This is a know that we or a temporary that we are
 probably or we think we may have to assign into a memory_location rather_than to our
 register and let is assume for the sake of this example that we pick f and we talk
 later about how to choose a the know to spill theres a number of different ways
 to to chose the particular know to spill but for the illustration of this example
 it doesnt_matter how pick we just have to pick one to remove from the Graph As
 were_going to say were_going to remove that we going to spill F So what well do
 then is well remove f from the graph just like before and then well continue with
 our simplification and this will now succeed because once we move F from the
 graph we can see that all the nodes well actually several of the nodes have fewer
 than three neighbors and so B C and D Sorry B and D both only have two
 neighbors when inaudible E and C will only have one neighbor each and so clearly
 coloring will now succeed and heres one order that well succeed with this reduced
 graph After we decide to spill f and we successfully color the subgraph now we
 have to try to assign a color to f and it could be we could get lucky and discover
 that even_though f had more_than there neighbors or three or more neighbors when
 we remove it from the graph it could be that when we go to construct the coloring
 for the subgraph that Those neighbors actually dont use all of the register It
 could wind_up being at all those neighbors for example or assign to the
 same register and so there are plenty of registers left over to assign to f And
 so this is called optimistic coloring So we pick a candidate for spilling We tried
 to color the subgraph Once we have a coloring for the subgraph now we see if
 we just get lucky And are able to assign a register to F In which case we can just
 go ahead and continue the color of the rest of the graph as if nothing had
 happened So in this case lets_take a look what_happens Were_going add F back
 into the graph And And look_at all and look_at its neighbors and we see that we
 have a neighbor thats using r1 We have a neighbor thats using r2 and we have a
 neighbor thats using r2 and we have a neighbor thats using r3 And so on in
 this case optimistic coloring will not work so in fact F had more_than K
 neighbors and after we color the subgraph it_turns out that those
 neighbors are using all K In this case three all three of the register names
 And so F where there is no register left over for F and were_going to have to
 actually spill it and store in memory So if optimistic coloring fails as it does in
 this example then we spill f So what were_going to do is allocate the memory
 location for f and typically what that means is that well allocate a position in
 the current stack frame Lets call this address fa for the address of f And then
 were_going to modify the control_flow graph Were_going to change the code for
 that compiling So before each operation that reads f were_going to insert a load
 that loads from that address to current_value of f into a temporary name Okay
 that makes_sense because if the value is out of memory then if we have an
 operation that needs to actually use the value Were_going to have to load it from
 a memory first then to the register And similarly after each operation that writes
 F were_going to insert the store so were_going to save the current_value of F
 into its location in memory So here is the original code from which we
 constructed the registry interference_graph and notice that there are few
 references to f in here and we just highlight them alright So we have a
 couple of reads we have a right and so now what are we going to do So here
 we had the use of F the read of F in this statement and now we preceded that by
 a load And notice that Ive given a new name here Ive called this F1 And thats
 because the different uses of F in the control_flow graph dont all have to have
 the same temporary name And actually it would be a good idea to separate them so
 each distinct to use of F will get its_own name So here we load the value of F
 and then it get to use in the statement Here we have a right to f and so we store
 the current_value of f and those argument to a different name f2 So thats
 temporary is computed here as going to be stored and its called f2 And finally
 the third use of f theres another load of f right here Which is then used in this
 computation here of b Okay So that is the systematic way to modify the code to
 use f in storage And now we have to recompute the aliveness of f And so what
 happens there Well here is the original aliveness information from which we
 computed the register_interference graph okay And now notice that f is gone We no
 longer use f in the programs so we can delete all the places_where we mentioned
 that f was live and now we have the three new names f1 f2 and f3 And we have to
 add in their aliveness information so it creates a new program points here where we
 inserted statements And of course where we have a load of the current_value of f
 that value if live right before the use in the next statement Here we have the
 right of the current_value of f and thats live right before the store and then
 heres another load of the current_value of f which is live until the store Im
 sorry until the use in the next statement Okay And so now notice here
 that f used to be live in many_many many places in the in the code And now not
 only is f or the the different versions of f live in fewer places also weve
 distinguish them So it actually separate the different uses of f and so this will
 have their_own nodes in their_own set of interferences in the graph and they wont
 share them with the other users of f and that will actually also reduce the number
 of edges in the graph To_summarize the example on the previous_slide once we
 have decided that we are actually going to spill a temporary f that means were
 going to change the program where have loads and stores to the program and now
 were_going to have a different program and thats going to change our register
 allocation problems Were_going to have to recompute the aliveness of information
 were have to rebuild the restrain interference_graph and then were_going to
 have to try again to color that block graph Now it_turns out that this new
 aliveness information is almost the same as it was before So all the temporary
 names other than f are not much affected by the by the new statements that are
 added There_are a few program points where they might be live but I replaced
 they were alive before and theyre still alive And F itself has changed fairly
 dramatically Its like this information has changed really dramatically Certainly
 the old name F is no_longer used and so its like this information goes away and
 then weve also split F into three in this case three different temporaries One for
 each of the different uses of F in the control_flow graph And I noticed that
 each of these new uses of F or these new versions of F is live in a very very
 small area so a load In this video we are going to continue_our discussion of
 as filling For a load instruction The thing that were loading the temporary that
 were loading fi is live only between the load and the next instruction where its
 used and similarly for a store Its score of a temporary fi is live only between the
 store itself and the proceeding instruction The one they created fi And
 the effective is is to greatly reduce the live range of the spilled variable So
 whatever name we decide to spill by adding the load and stores right next to the
 places_where those values are used We dramatically reduced the live range and in
 addition as I mentioned in the previous live by splitting the name f into multiple
 different name we also you_know avoid sharing Those different liv e ranges
 between the different versions of F So because the live range of F is reduced by
 spilling It has fewer interferences in the new program than it did in the old
 program And so what that means the particulars in the rebuild inaudible
 interference_graph F will have fewer neighbors Some of the neighbors that it
 had before have gone away because it just live in fewer places So if we look_at the
 new register_interference graph we can see that among all the different versions
 of F Remember that F has been split into three temporaries in this graph We see
 that they only interfere with D and C Whereas before f have several other
 neighbors in the graph And now in fact this new graph is three tolerable Of
 course it might be the case that we cant just spill one name We might have to have
 just spill several different temporaries before the coloring is found And the
 tricky part is to siding what to spill So this is the hard decision that has to
 be made during restore allocation Now any choice is correct Its only a question of
 performance so you_know some choices of spilling will lead to better code than
 others but any choice of spilling is going to resolve in a correct program And
 theres heuristics that people use to pick which temporaries to spill and here are a
 few or I_think three of the most popular ones One is to spill the temporaries and
 have the most conflicts And the reason for that is that this is the temporary
 The one thing that you can move into memory that will most affect the number of
 interferences in the graph So the idea is by possible spilling justice on
 variable Well remove enough edges from the graph that they becomes tolerable with
 the number of registers we have Another_possibility is a spilled temporaries that
 have few definitions and uses And here the idea is that by spilling those since
 theyre not used very much the number of lows in storage will have to add will be
 relatively small and so if a variable just isnt used in many places then the actual
 cost in terms of additional instructions that are going to be executed to spill it
 is relatively small And another one and this is actually the one that I_think that
 all the compilers implement is to avoid spilling an inner loops So if you have a
 choice between spilling a variable thats used within the Innermost loop for the
 program and one that is used some place else You probably preferred this that you
 spill the one that is used not in the innermost loop absolutely because again
 that will result in fewer loads in stores You really want to avoid adding additional
 instructions to your inner_loop
 In the last few_videos weve talked_about managing registers In this video_were
 going take a few moments to talk_about another very important resource the cache
 and what compilers can and cant do to manage them Modern computer systems have
 quite elaborate memory hierarchies And so if we were to start at the closest
 level to the processor itself we would find that on the chip there are some
 number of registers And these are very fast access So typically that can be
 accessed in a single cycle so at the same rate as the clock frequency And the
 problem is that its very expensive to build such high performance memory And
 so we dont get to have very much of it typically You_know you might have 256
 say to 8K bytes of registers total available to you on a given processor
 Now a very significant portion of the die area and the modern processor would be
 devoted to the cache And the cache is also quite high performance but not quite
 as high performance as registers Maybe on average it would take three cycles just
 service something from the cache but you get a lot more of it And modern
 processors would have up to a megabyte of cache Then much further away from the
 processor is the main memory the DRAM and this is much_more expensive to
 allocate to access in time you_know typical values would be twenty to 100
 cycles and I_think you_know its more on 100 toward the 120 these days in most
 processors but you get quite a lot of it You get between 32 megabytes That would
 be fairly small machine up to four gigabytes for maximally provisions
 processor And finally farthest away is typically disk And this takes a very
 very long_time to get to hundreds of thousands or millions of cycles but you
 can have enormous amounts of storage out there gigabytes to terabytes of storage
 As I said there are limitations on the size and speed of registers and caches
 And these are limited as much by power actually as as anything e lse these days
 And I and so its you_know very important people would like to have as
 much register and cache as possible but there are real constraints on how big and
 how fast we can make these relative to the speeds of the processors Now
 unfortunately the cost of a cache miss is very high as we saw in the previous_slide
 If you you could get something in a couple of cycles from the cache But if
 its not in the cache then it could take you a couple of orders of magnitude longer
 to get it out of the main memory And so for this reason people you_know try to
 build caches in between the processor and the main memory to hide that latency of
 the main memory so that most of the data is in the cache And typically it
 requires more_than one level of cache these days to match a fast processor well
 with the speed of a very large main memory So you_know very common now to
 have two levels of cache and processors and some processors even have three levels
 of cache So the bottom line is that its very important to for high performance to
 manage these resources properly Particular to manage the registers and the
 cache as well if you want your_program to perform well Compilers have become very
 good in managing registers and in fact I_think today most people would agree that
 for almost all programs compilers do a better job at managing registers than
 programmers can And so its very worthwhile to leave the job of allocating
 registers or assigning registers to the compiler However compilers are not good
 at managing caches And while theres a little_bit that compilers can do and
 thats what were_going to talk_about in this rest of this video for the most part
 if programmers want to get good cache performance they have to understand the
 behavior of the cache is on the machine and have to understand what their program
 is doing you have to understand a little_bit about what the compiler is capable of
 doing and then they still have to write the program in such a way that is going
 to to be cache friendly So its still very much an open question How much a
 compiler can do to improve cache performance Although there are a few
 things that weve found compilers can do reliably So to see one of those things
 that compilers can actually do lets_take a look_at this example loop So what we
 have here we have an outer loop on j and inner_loop on i and then in each iteration
 of the inner_loop were reading from some vector you_know performing some
 computational net value and storing the results into the ith element of the A
 vector Now as it_turns out this particular program has really really
 terrible cache performance This is going to behave very badly And so lets think
 about whats going to happen So lets imagine our cache you_know as some block
 of memory okay And so whats going to happen here I_mean whats whats the
 first iteration going to be Well were_going to you_know load and store some
 function of that into And so whats going to get loaded into the cache is and
 All_right lets_assume they just go into different elements in this just for
 the sake of argument lets_say they land in the first two elements in the cache
 And then were_going to do the second iteration of this and well well load
 and write it into and so and will be loaded into the cache all right and so
 on And this will repeat over and over and over again loading one element of a and
 one element of b the important thing to notice is that all of these references to
 a and to b are misses okay Every single one of these is a cache miss because on
 each iteration of the loop we refer to new elements okay So were not referring to
 the same elements as we were on the previous ones So now lets ignore for
 the moment the fact that there may be multiple elements in the same cache line
 okay So some of you probably are aware already That when we fetch data from
 memory we dont just fetch the one word okay So typically when we refer to for
 example you_know is stored here will fetch an entire cache line which will be
 some block of memory and that may well have you_know other elements of b in it
 So we might get a couple other elements of b into the cache at the same time but
 the important thing here is that on every iteration of the loop were referring to
 fresh data okay And and if these data values are large enough if they take up
 an entire cache line then each iteration of the loop is going to be a cache miss
 for both elements and we wont get any benefit of the cache And this loop will
 run at the rate of at the rate of the main memory and not at the rate of the cache
 Now the other thing thats important here is that this loop bound here is very large
 and I picked it to be very large to suggest that its much larger than the
 size of the cache So as we get towards the end of the loop whats going to happen
 is we will have filled up the whole cache so this whole cache will be filled with
 values from a and b and then its going to start clobbering values that are
 already in the cache And if this loop you_know if the size of these vectors
 lets_say twice the size of the cache by the time we come around and complete the
 entire execution of the Inner loop Whats in the cache is the second half of
 the a and b arrays its not the first half of the a and b arrays And so then
 when we go_back around and execute another iteration of the outer loop now whats in
 the cache is also going to be not the data that were referencing And so when
 we come_back around and begin the execution of the inner_loop the second
 time And we refer to and and and Whats in the cache is the values from the
 high numbered elements of the a and b vector and not the low numbered elements
 And so these references are all misses again And so the the basic problem with
 this loop is a loop thats structured like this is that almost every memory
 reference and if and if the data values are big enough again that they fill an
 entire cache line then it will be every single memory reference is a cache miss
 Now instead lets_consider an alternative structure for the same
 program Here Ive put the i loop at as the outer loop and the j loop as the inner
 loop And here what we do is we load And we write and then we repeat that
 computation ten times on the same data values And so here well get excellent
 cash performance Well well have a miss on the first reference but then on the
 subsequent nine references the data will be in the cache or will completely exhaust
 our computation on those particular a and b values And then well go on to the next
 a and b values Well finish the inner_loop and go on to the other and do one
 more iteration of the outer loop And so the advantage of this structure is that it
 brings the data into the cache and then it uses that data as much as possible before
 going on to the next data Rather than doing a little_bit on every data item and
 then going back you_know doing one pass and then going back and sweeping over all
 items items again and doing another little_bit Alright so this particular
 structure where weve exchanged the order of the outer loops sorry exchanged the
 order of the inner and outer loops it computes exactly the same_thing but it has
 much better cache behavior And it probably run more_than ten times faster
 Now compilers can preform this simple loop interchange optimization This particular
 kind of optimization is called loop interchange where you just switch in the
 order of loops In this particular case its very easy to see that thats legal
 and the compiler could actually figure it out Not many compilers actually implement
 this optimization because in general its not easy to decide whe ther you can
 reverse the orders of of the loops And so usually a programmer would have to
 figure_out that they wanted to do this in order to improve the performance in the
 In this video_were going to start our_discussion of garbage_collection or
 automatic_memory management This will take us a few_videos to get through and
 this first video is just an overview of the problem And then well talk_about
 specific techniques in subsequent videos To set the stage lets first talk_about
 the problem that were trying to solve So if one has to manage memory manually
 meaning you have to do all the allocation and deallocation explicitly yourself that
 is a hard way to programming leads to certain kinds of bugs that are very
 difficult to eliminate from programs So in particular these days you see this
 primarily in C and C programs Those are the main languages that are used that have
 manual memory_management And the kinds of storage bugs that you can get because
 it has manual memory_management are things_like forgetting to free unused_memory so
 thats a it means a memory leak Dereferencing dangling pointers
 overriding parts of a data_structure unintentionally And actually theres a
 few more things although these are probably the three most_common problems
 that people have and these bugs are really hard to find And I want to emphasize that
 these kinds of bugs are often some of the very very last bugs to be found in in
 complex systems They often persist into production and sometimes for a very long
 time after the code is in production use And why is that The reason is that these
 these kinds of bugs storage bugs typically have effects that are far away
 in time and space from the source and so how can that happen Well lets think
 about some object in memory and now lets_say only on interesting you might have
 some fields lets_say you have a few fields and I am keeping some pointers to
 it So somewhere on program is a reference to this particular object and now I come
 along and free it So I am doing my own memory_management like free this object
 but I forget that I had this pointer And so now whats happen all the storage has
 been freed its no_longer really valid memory but the pointer still exist to it
 And then when I come_along and allocate something else it might allocate the same
 piece of memory So this might now be a different kind of object okay So I might
 have a different type here even In this memory might be used for something
 completely_different and now I have a pointer that says it thinks its a red
 object its pointing to a blue object And when I come in and write stuff into
 this object of course Im just writing nonsense So I this whatever piece of
 code holds this pointer thinks its still the old kind of object It will write some
 bits in here and when I go in some other part of the program possibly quite far
 away go out and read out this is a blue object Ill just get some random garbage
 and that will probably cause my program to cash So this is a very very old problem
 Its been studied since at_least the 1950s It was first thought about
 carefully in list And there are some wellknown techniques for completely
 automatic_memory management so you dont have to manage memory yourself And this
 only became mainstream actually in the 1990s so with the popularity of Java
 Prior to that time there was really no mainstream language that used automatic
 memory managements so thats really just in the last now almost twenty years that
 garbage_collection and automatic_memory management in general became a popular
 mainstream programming technique So the basic strategy in automatic_memory
 management is is pretty simple So when an object is created when we allocate a
 new object the system the run_time system will find some unused space for that
 object and it will just allocate it So whenever you say new of some class name in
 Cool Some memory is automatically allocated by the system some previously
 unused_memory is automatically allocated by the system for that object And if you
 keep doing this over and over and over again and after awhile youre going to run
 out of space So eventually there is no more unused space left for additional obj
 ects And at that point you have to do something You have to reclaim some of the
 space in order to allocate more objects and the observation that garbage
 collection systems rely upon is that some of the spaces being used is probably
 occupied by objects that will never be used again So they some of these objects
 are not going to be referred to again by the program and if we can figure_out which
 objects those are which objects are not longer going to be used Then we could
 deallocate them and reuse the space for new objects So the big question is how
 can we know that an object will never be used again And most of the garbage
 collection techniques that are out there today rely on the following observation
 then thats that a program can only use the objects that it can find and what do
 we mean by that So Im going to switch_colors so lets_take a look_at this piece
 of code so whats going to happen Well when we execute this the first_thing that
 happens is we allocate an A object alright And its assigned x so x will
 have a pointer to that object And then in the body of this let whats going to
 happen well were_going to assign x the value that y points to so y is another
 variable It points to some other objects in memory okay And whats going to
 happen when we execute this assignment is that were_going to remove the old value
 of x and x now is going to point to this object Now observe that this object a is
 unreachable Meaning it has no references to it There_are no_longer any pointers to
 it And how do I know that Well a brand new here when it was created I only
 created one pointer to it x and then I immediately assigned x to something else
 So I dropped the only pointer to A There is no reference to A anywhere in the
 program And so the program will never be_able to find it again The program if no
 variable or data_structure in the program has a pointer to A then A can never be
 referred to by the program in the future So any kind of subsequent execution of the
 program has no p ointers to A and therefore it will never use A again and so
 the space ray can be reclaimed and used for another object Now it_turns out that
 we need a more general definition of object reachability than this example
 illustrates so lets_take a look_at that Were_going to say that an object x is
 reachable if and only if one of the following two things is true So either A
 register contains a pointer to x So either the x is reachable immediately from
 some register Remember that the registers contain things_like the local_variables in
 there and the intermediate expressions and theyre just you_know the values that the
 program has immediate access to or another reachable object y contains a pointer to
 x And so what does this say Well this says youre going start at the register so
 you_know the program might be implemented using a few registers And then youre
 going to look_at all the things that those registers point to all the objects that
 they point to And you will look_at the pointers in those objects and everything
 they can point to okay And some of these things might overlap I_mean some of
 these there might be multiple things which are reachable by more_than one path
 starting at the registers But the complete side of things that you can
 reach beginning at the registers and following all the possible pointers those
 are all the reachable_objects And then the complement of that set an unreachable
 object is one that isnt reachable So all the other objects the ones that you were
 not able to reach by recursively starting at registers and following pointers as far
 as you could those objects can never be used Because clearly the implementation
 can only access things through registers and and then only find additional things
 by you_know loading pointers out of objects that it could reach from the
 registers So anything that it can reach by some sequence of substeps will never
 be used again and is garbage So lets_take a look_at another example that
 illustrates some interesting aspects of re achability and its use in automatic_memory
 management So what does this example do The first_thing it does it allocates an A
 object on the heap and assigns that to the variable x So x is a pointer to that
 object And then it allocates a B object and y will point to that object And then
 it assigns the value of y to x alright So well have this configuration and and
 now lets draw a line here okay and well come_back and lets remember this point in
 time what things look like at this point in time And then were_going to go off
 and were_going to execute this conditional And notice that this
 conditional is going to do Its going to always be true alright So the predicate
 will always be true so itll never take the false_branch All its going to ever
 do is take the true_branch and whats it going to do there is immediately going to
 overwrite x And so x is going to wind_up pointing at some other new object It
 doesnt_matter what it is And now lets_say that at this point right here is
 where we try to do a garbage_collection So you_know for some reason this is the
 point where the program stops and tries to collect unused_memory And what can it
 collect Well just like before cuz the example up to this point is essentially
 the same We can see that this object is unreachable okay So the first A object
 becomes unreachable at that point and it can be collected Now what about the
 second object Well it is reachable its clearly reachable Its reachable through
 x okay at that point and its also reachable as it happens through y And so
 its not garbage and its not going to be collected but notice that the x value is
 always going to be overwritten okay So the program the compiler doesnt know
 that this branch is always going to be true So it doesnt realize that the
 value that x has at this point wont ever be used again but that value is
 immediately going to be overwritten every time we take this conditional And
 furthermore if y is not used any place else in the program if y i s dead at this
 point Lets_say that y is dead here Then neither one of these references to B
 is ever gonna be touched again So in fact the B value will never be used again even
 though it is reachable And so what this tells you is that reachability is an
 approximation And by that I_mean its an approximation for the objects that will
 never be used again What were really interested in when we do garbage
 collection is collecting objects that will never be used in the future execution of
 the program Because obviously that space is wasted and could be put to some other
 use that might be better and reachability approximates that So if an object is
 unreachable it definitely wont be used again however just because an object is
 reachable its not a guarantee that it will be used again So now lets talk
 about how we do garbage_collection in Coolc So Coolc has a fairly simple
 structure It uses an accumulator in which of course points to an object and that
 object may point to other objects and so on So we have to trace all the objects
 reachable from the accumulator but we also have to worry_about the stack_pointer so
 theres also stuff reachable from the stack And each stack frame of course may
 contain pointers like and you_know for example the method parameters that are
 stored on the stack Each stack frame may also contain some nonpointers alright
 So if I_think about the layout of each activation_record there would be some mix
 of pointers and nonpointers Things_like the return_address so we have to know the
 layout of the frame But if we do know the layout and of course the compiler is
 deciding on the layout so it naturally does know the layout it can find all the
 pointers in the frame Essentially the compiler has to keep a record for each
 kind of activation_record it builds for each methods If activation_record for a
 method foo and lets_say that activation_record has four slots then the compiler
 would need to keep_track of which one of these were pointers to objects And
 perhaps the second and the fourth element of the frame are always pointers
 to objects and the other two are always nonpointers So the somewhere the
 compiler has to keep_track of this information so that the garbage_collector
 will know at Run time when its looking_at an activation_record for foo where the
 pointers that it needs to follow are So in Coolc we start tracing from the
 accumulator and the stack and these are called the roots okay So in garbage
 collection terminology the roots are the registers from which you begin tracing out
 all the reachable_objects And if we do that here what we can do so you see we
 have our object here we have our accumulator excuse_me and our stack
 pointer and so we can just walk_through This little diagram of memory and find all
 the reachable_objects so the acummulator points to object A so well mark that as
 reachable And A points to C so well mark it as reachable C points to E so well
 mark E as reachable The stack_pointer has a couple of frames on it The first frame
 has no pointers The second frame points to E Weve already touched that one Its
 already marked so we can mark it again but it doesnt_matter as long as it gets
 marked by somebody and now everything that is not marked is unreachable So what
 objects didnt we touch and are traversal of the reachable_objects Well those are
 objects B and D And so those are unreachable_objects and they can be
 reclaimed and we can reuse their storage Now one interesting thing to note here is
 that just because an object has pointers to it it does not mean it is reachable
 so notice here object D Object D actually has a pointer to it okay and yet object D
 is unreachable and why is that Well because the only pointers to it are from
 other unreachable_objects So its important here to you_know just
 understand that its not the case that every unreachable object has no pointers
 to it There will be some unreachable_objects or there may be some unreachable
 objects that actually do have pointers to it to them but they will on ly come from
 other unreachable_objects So every garbage_collection scheme has the
 following steps Were_going to allocate space as needed for new objects so we
 just go ahead and allocate new_space as long as we have space so whenever we need
 it And when space runs out we need to compute what objects might be used again
 And generally thats done by tracing objects reachable from a set of root
 registers and then were_going to free the complement of that set Were_going to
 free the space used by the objects not found in part A And I want to say that
 some strategies do perform garbage_collection before the space actually runs
 out and well actually look_at one of those in a future video
 In this video_were going to talk_about the first of three garbage_collection
 techniques that were_going to look_at in detail First one is markandsweep
 Markandsweep works in two phases And its called not surprisingly mark and
 sweep So the mark_phase is going to trace all the reachable_objects So when
 memory runs out and we stop to do the garbage_collection the first_thing were
 going to do is go and trace out all the reachable_objects And then the Sweep
 phase is going to collect all the garbage objects And to support this every object
 is going to have an extra bit somewhere in it called the mark_bit And this is
 reserved from memory_management and its not going to be used by anything except
 the garbage_collector And initially before we start a garbage_collection the
 mark_bit of every object will always be zero And thats going to be set to one
 for the reachable_objects in the mark_phase So when we mark an object we mark
 it with a And that indicates that the object is reachable So here is the mark
 phase Its going to be a work list based algorithm and so initially our work list
 consists of all the roots so all the initial pointers held in registers and
 then while the work list the todo list is not empty were_going to do the
 following We pick some element v out of the todo list well remove it from the
 todo list okay And then this is the crux of the algorithm If the object v is
 not already marked then we mark it okay So we say mark_bit to one and then we
 find all the pointers inside of it alright And we add those to our work
 list So everything every point gets added to work list Now if v is already
 marked well then we have already processed it and weve_already add all the
 things it points to to the work list And so we just need to do nothing there is no
 else branch and we just drop it from the todo list So once_weve completed the
 mark_phase and every reachable object has been marked then the sweep phase is going
 to scan th rough the heap looking for objects that have mark_bit zero And the
 sweep phase is just going to march through all of memory Its going to start at the
 bottom of the heap and walk over every object in the heap and check its mark_bit
 And so any of the objects that it finds that have mark_bit zero they were not
 visited in mark_phase and theyre clearly not reachable S all those objects will
 be added to a free_list And as we go_through the memory is one other detail
 thats important Any object that has its mark_bit set is gonna have its mark_bit
 reset to zero So that way its ready for the next garbage_collection So here is
 the pseudocode for the sweep phase and this will function size of p is going to
 size of block the size of the object that starts at pointer p alright And as
 youll_see this is actually the reason that we have the size of objects encoded
 in the object in COOL So remember in the header for COOL objects there is a size
 field that is so that the garbage_collector as its walking through memory
 can figure_out how big the objects are Anyway we start at the bottom of the
 heap And while we havent reached the top of the heap we do the following We look
 at where were pointing and then well always be pointing to the beginning of an
 object So we check to see if the mark_bit of that object is one And if it is
 well then it was a reachable object So we just reset its mark_bit to zero
 Otherwise if its mark_bit was zero then were_going to add that block of memory
 okay which is the size of the object to the free_list And finally in either
 case okay were_going to increment p by the size of the object that it points to
 so we point to the next object Then well just repeat that loop over and over again
 resetting the mark bits of things that were reached and adding things that were
 not reached for the free_list until weve touched every object in the heap Heres a
 little example So were starting out here with a a heap and were_gonna assume
 theres just one root for simplicity And here are all the objects and initially
 their marked bits are zero and we do have a free_list an initial free_list over
 here Notice that you_know theres a little_bit of memory that is on the free
 list Okay So after the mark_phase what has happened Well weve gone through
 and touched all the reachable_objects So we started with A and of course we set
 its mark_bit to one And then we followed pointers reachable from A set the mark
 bit there Follow the pointer reachable from C set the mark_bit there And so we
 wind_up A C and E being marked nothing else is marked okay And now the sweep
 phase will go_through memory its going to reset all the marked bits to zero And
 as it finds unreachable_objects in this case B and D its going to add them to
 the free_list and so what well wind_up the free_list will wind_up being a linked
 list of of of blocks of memory that are available for future allocations
 In this video we are going to look_at the second garbage_collection technique stop
 and copy In stopandcopy garbage_collection memory is organized into two
 areas We have an old_space thats used for allocation and so all of the data that
 the program is currently using lives in this area called the old_space And then
 theres a new_space which is reserved for the garbage_collector And so this is not
 used by the program its just for the GC And so the first decision in stopandcopy
 garbage_collection is that the program can only use half the space And there are
 some techniques more advance techniques for stopandcopy garbage_collection that
 allow the program to use more_than half the space So this isnt as bad as it
 sounds but fundamentally a fairly significant fraction of the space has to
 be reserved for the garbage_collector Now the way allocation works is that theres a
 heat pointer here in the old_space and everything to the left of the heat pointer
 is currently in use This is where all the objects have already been allocated in
 this area that I just shaded here in red And then when it comes time to allocate a
 new object we simply allocate it at the heap pointers So the heap pointer will
 simply bump up and some block of space will be allocated to a the next object
 that we want to do And it will just keep marching through the old_space allocating
 as you allocate more objects Okay so allocation just advances the heap pointer
 so one of the advantages actually of stopandcopy is a very_simple and fast
 allocation strategy Now eventually of course if we allocate over and over
 again were_going to fill up the old_space and so garbage_collection will start
 GC will start when the old_space is full And what its going to do is going to copy
 all the reachable_objects all the reachable_objects from the old_space into
 the new_space And the beauty of this idea is that when you copy the reachable
 objects the garbage is left behind So you simply pickup all the data that youre
 using and move it over to the new_space and all the junk that you didnt need
 anymore is left behind in the old_space And then after you copy stuff to the new
 space first of all since you left the garbage behind youre using less space
 than you did before the collection So theres some space available now in the
 new_space for allocating new objects And then you simply swap the roles of the old
 and new_space So the old and new spaces are reversed what was old becomes the new
 and what was new becomes the old and then the program resumes So lets_take a look
 at a quick example here just to get a idea of how this works Lets_say we have our
 old_space over_here this is the old_space and we have one root which is this
 object A And so what were_going to do well were_going to make a copy of all the
 objects reachable from A Were_gonna move them over to the new_space And what
 thats going to look like well here it is afterward But lets trace it out So
 we started A we follow pointers from A we can see theres a pointer to C okay
 so C is going to be reachable and theres a pointer to F okay And then F points
 back to A and thats all the reachable_objects so we copy them And notice when
 we copy them we also copy their pointers and now the pointers have all been
 changed So in the copy of A it now points to the copy of C okay And of
 course C will point to the copy of F and theres a little issue here this line is
 not in the right place so it should look like that And then F points back to the
 copy of A So what we know when the object and move their pointers and we
 adjust them so that weve really copied the whole graph of objects over to the
 news space Now were using less space so theres some free space here okay And
 now this will become the old_space This now our old_space and this is now the new
 space which we will use for the next garbage_collection To_summarize the
 discussi on so_far one of the essential problems in stopandcopy is to make_sure
 that we find all the reachable_objects and we saw this same problem with
 markandsweep garbage_collection Now the thing that really distinguishes
 stopandcopy is that were_going to copy these objects So when we find a
 reachable object we copy it into the new_space And that means that we have to find
 and fix all the pointers that point to that object and this is actually not
 obvious how to do alright Cuz when you find an object of course you cant see
 all the pointers that point into that object So how are we going to do that
 Well here is an idea Well we copy the object were_going to store in the old
 version of it it was called a forwarding_pointer to the new copy So lets_take a
 look_at what that would how that would how that looks_like So we have our old
 space we have our new_space And lets_say we discover some reachable object A
 in the old_space So what were_going to do is were_going to make a copy of it
 over_here in the new_space and thats easy enough to do But now what were_going to
 do is were_gonna take A and were_going to reuse its space and were_gonna store
 whats called a forwarding_pointer in it So were_going to yeah first of all
 were_going to mark somehow that this has been_copied So this will have some
 special mark on it which Ill just you_know indicate with here with a purple bar
 something This is were marking someway so that we can tell this object has
 already been_copied And then at a At a distinguished location in the object
 were_going to store the forwarding_pointer And you can think of this as like
 a forwarding address So if you_know where somebody lives you can go to their
 house and if they have moved you can ask for the forwarding address And thats
 exactly and then you can go off to their new house wherever theyve wherever
 theyve gone to and presumably find them And so thats whats going to happen
 here If we have a pointer that points into this object later_on and maybe much
 later_on in the garbage_collection we may discover this pointer we may follow this
 pointer find out it points in this object realize that this object has moved
 because weve marked it and the object was moved And then we can use the forwarding
 pointer to find out where the new object is and then update this pointer wherever
 it is to point to the new object Now just like with markandsweep we still
 have the issue of how to implement the traversal of the object graph without
 using any extra space Again when these garbage_collection algorithms they only
 get used they only get run in low memory situations And you cant assume that you
 can build unbounded data_structures to use with the garbage collectors The garbage
 collector really needs to work in constants base And now here is the idea
 that will that is used in stopandcopy algorithms to solve the problem So were
 going partition in new_space and this is just the new_space here into three
 contiguous regions Were_going to have well start with the one on the far right
 Were_going to have the empty region where were allocating new objects And theres
 an allocation_pointer that points to the beginning of that region So this is the
 region that were filling up with objects that were copying over and this is just
 empty unused space Now immediately to the left of that region are the objects
 that have already been_copied but not scanned okay This is copied and not
 scanned And what does that mean Well that means that the object has been_copied
 over And so weve actually you_know made a copy of the object into the new
 space But we havent yet looked_at its pointers We havent yet looked_at the
 pointers inside the object to see where they go And then to the left of that
 are the objects that have been_copied and scanned These_are objects that have been
 copied over And weve also processed all the pointers inside of those obje cts And
 so you can think of this area here between the scanned pointer and the
 allegation pointer this is the work quest So these are the objects that
 still need to be processed These_are the objects that have been_copied over but
 might yet still point to objects that havent been_copied And so these are the
 objects where we have to look_at their pointers to see whether they point to
 something that still needs to be copied over to finish the garbage_collection
 Returning to our little example Im now going to walk_through how a stopandcopy
 garbage_collector would collect this particular heap step by step So notice
 that we only have one root object and its A okay I just want to point out that A
 has one pointer which points to object C alright So at the very first step what
 were_going to do is were_going to copy the A object over to the new_space okay
 And this is literally a byte for byte copy So we just take the bytes of A and
 we do a copy without you_know doing any inspection of the interior of the object
 over to the new_space And hows that work Of course our allocation_pointer
 isnt in initially right here at the beginning of the new_space And then we
 add and we copy this one object over And then that means allocating an object and
 so now the allocation_pointer points to the first word of memory beyond the
 object that we just allocated okay Now what_happens when we copy it over Well
 because it is just a byte for byte copy all the pointers in A still point to the
 objects as they pointed to before which are the objects in old_space And notice
 now that this copy of A points to the object C in the old_space The other thing
 we do is we leave a forwarding_pointer in the old copy of A So we mark A as having
 been_copied thats why its grayed out Indicates that this object has already
 been_moved And that this dotted line here indicates that somewhere we stored a
 pointer to the new copy of A And now were_ready to begin the algorithms And
 not ice that we have some objects here that have been_copied but not scanned so
 this is our work list So now its going to repeatedly work off of those objects
 and how do we know theyre objects in there Well we just compare the scan and
 the allocation pointers So if theyre if they are different if theres an object
 in between the scan and the allocation_pointer at_least one object between the
 two then theres work to do Theres an object that needs to be scanned that and
 and possibly resulting in more objects being moved and allocated So what
 happens next So object we we process A so we walk over A and find all the
 pointers in A And we copy any objects that it points to that havent already
 been_moved And so before we said you_know the A this this copy of A pointed
 to the old copy of C So now what we discover the C object it hasnt been
 moved its still in the old_space So we copy it over and we update the pointer in
 A to point to the new copy of C Now of course and then the scan pointer moves
 over A Weve scanned all the pointers in A alright And the allocation_pointer
 also moves because we had to allocate space for C And of course C is just a
 byte for byte copy of what was in the old_space And so it any pointers that it has
 that point to objects that havent been_moved yet moved yet just point back into
 the old_space So in this case the object C points to the object F in the old_space
 And I probably should indicate here heres the original dividing line you
 know this is the old_space over_here and this is the new_space over there alright
 And finally we mark C as having been_copied having been_moved to the new_space
 and we left a forwarding_pointer to it in case so we can fix any pointers that point
 to C that we come across in the future And now we have to continue scanning
 objects that have been_copied but not scanned And we can see that there is an
 object between the scan and the allocatio n pointer namely C and so well now
 process all the pointers in C Next we scan C And we discover that it points to
 F Which hasnt been_moved yet and so we copy F over into the new_space and we
 update the pointer in C And now C has been_copied and scanned okay So the
 scan pointer moves past C and of course F again is a byte for byte copy and so all
 its pointers into old_space are still pointing to old_space in particular F
 points to A and the allocation_pointer is moved again because we moved F alright
 And now we have to process F And this will be the last object that we move And
 what_happens well we discover that F points to A okay And A is already marked
 as having been_moved and it has a forwarding_pointer So instead of copying
 A again we simply update the pointer in F that pointed to the old version of A to
 point to the copy of A okay And so now F is completely scanned All the pointers
 in F have been processed We didnt allocate any new objects so the allocation
 pointer didnt move and now the scan pointer and the allocation_pointer are
 equal There_are no objects in between them and so our work list is empty and
 this is the garbage collected heap This is a complete graph a complete copy of
 its A of the graph of reachable_objects from the old_space So now were done and
 we simply swap the role of the new and old_space and we resume the program so that
 when the program starts running again it will allocate out of this area and itll
 be on the allocation_pointer until it fills up what is now the old_space you
 know and then this will be the new_space that will be used for the next garbage
 collection Heres a pseudo code algorithm outlining how stopandcopy garbage
 collection should work So while the scan and allocation pointers are different
 remember we keep running until the scan pointer catches up with the allocation
 pointer and theyre equal What were_going to do is were_going to look_at the
 object that is at the scan pointer That well call that objec t and then for
 every pointer in O were_going to do the following Were_going to find the object
 O that that pointer points to And then there are two cases One is that there is
 no forwarding_pointer alright And if theres_no forwarding_pointer then we
 have to copy that object to new_space and that will involve allocating a new object
 and updating the allocation_pointer Then were_going to set and here it says the
 first word they really shouldnt emphasize the first word Set a word So
 its a distinguished word thats whats important We have to know which word
 were_going to use and will always be the same word But anyways we set a word of
 the old object to point to the new copy We mark the old object as copied Mark old
 object as copied okay So thats so that we can tell if we ever come to a pointer
 to it again we know its already been_moved and then we change p the pointer
 to point to the new copy of O alright So if there was thats what we do if
 there is no forwarding_pointer And if there is a forwarding_pointer then we
 simply update the pointer to point to the same place as the forwarding_pointer And
 we just repeat this loop over and over again until weve scanned all the copied
 options
 In this very short video Im going to say a few words about a technique called
 Conservative Garbage Collection that can be used for languages like C and C To
 review Automatic Memory Management relies on being able to find all the reachable
 objects And it also needs to be_able to find all the pointers in an object Now
 the difficultly with doing garbage_collection for a language like C or C is
 that its very difficult or even impossible to identify the contents of
 objects in memory with 100 percent reliability So if we see two words in
 memory you_know it might be a list cell that has a data and next field So we see
 just two words here And there are some bit patterns in here 0s and 1s Okay
 how do we know_whether these are both pointers It could be that one is a
 pointer and the the other is not in the case of a list cell So one of these
 fields is just data like an injure and another one is a pointer Or it could be
 something_like a binary tree node where both of these words are pointers And
 because of this weakness really in the C and C type systems we just cant
 guarantee that we know where all the pointers are Now it_turns out that it is
 possible to extend garbage_collection techniques to work with languages like C
 and C And the basic_idea or insight is that its always okay to be
 conservative And if were not sure whether something might be used in the
 future then we will just keep it around And remember that graph reachability is
 already a conservative technique What we really want is to keep around the objects
 that will just be used in the future but the reachability in the object graph is an
 approximation to that so because reachable_objects might be used And now
 the problem with C and C is that we dont_know where the pointers are We
 dont have a guarantee from the type_system about where the pointers are And
 so the basic trick is that if something looks_like a pointer then we will treat
 it as a pointer All we have to do is be conservative and if we are not sure wh
 ether a given word of memory is a pointer Then we can just treat it as a pointer
 and keep whatever it points to around If we and as long as we are not going to
 move it or change it that would be okay And so how how do we decide_whether a
 particular word of memory is a pointer Well it should be a line meaning you
 know it should end in some zeros to indicate that it was pointing if it was a
 pointer it was pointing to a word_boundary And then whatever pattern it
 is if we interpret it as an address it has to be a valid address So it should
 point to the data segment And Noah said you_know these two conditions will rule
 out all kinds of data and memory So for example any small integer is probably not
 going to be interpretable as a valid address in the data segment So you_know
 most likely only things that are pointers or very few things that are not
 pointers will be treated as pointers And what were_going to do then is if it
 looks_like a pointer were_going to consider it to be a pointer Well follow
 it and then well end up overestimating the set of reachable_objects We may keep
 around some stuff that isnt reachable at all But thats alright its always okay
 to keep around more stuff than necessary Now we still cant move the object
 alright Because we cant update the pointers to them If we dont_know that
 something is a pointer we certainly dont want to change it okay And you_know
 for example if we thought something was a pointer and it was actually an account
 number and then we updated the pointer when we move the object we would just
 completely change what the program does So this only really works when you mark
 this way
 In this video_were going to conclude our_discussion of automatic_memory management
 with the third and last technique were_going to talk_about for garbage_collection
 called Reference Counting So the basic_idea behind reference_counting is that
 rather_than waiting for memory to be completely exhausted were_going to try
 to collect an object as_soon as_soon as there are no more pointers to it So as
 soon_as we discard the last pointer to an object and it becomes unreachable we will
 try to collect it at that point in time And how can we do this Well as the name
 suggests were_going to count the number of references to each object So in each
 object we are going to store the number of pointers to that object So if I have an
 object in memory and it has say three pointers to it from other objects then
 somewhere in this object is going to be a dedicated field that contains the number
 three And if this number ever drops to zero if we discard these pointers and
 this number becomes a zero then we know that nobody is pointing to this object
 and it can be free And what this means is that every assignment has to manipulate
 the reference_count in order to maintain an accurate count of the number of
 pointers pointing to an object So allocating a new object returns an object
 with a reference_count of one So when a object is created by new it will already
 have a reference_count of one The pointer that is returned is the only reference to
 the object And so were_gonna write the reference_count of an object x is rc of x
 And now when we have an assignment x gets assigned y were_going to have to update
 the reference_counts of both the object that x points to and the object that y
 points to before the assignment So what_happens here So if y points to p so
 lets draw our objects here so y is a local variable and it points to some
 object p in memory and x is also a local variable and it points to some object o
 Okay So now x is getting the value of y and so thats going to move this po inter
 from where pointer before pointing to the same_thing as y So whats going to
 happen while ps reference_count is going to go up by one and os reference_count
 is going to go down by one And since we decremented os reference_counts as we
 dropped this pointer to the object o we need to do a check to see if the reference
 count has become zero And if the reference_count has dropped to zero then
 we can free the memory for o And then in addition to updating the reference_counts
 and checking whether the reference_count of o became zero we actually need to do
 the assignment itself alright So every assignment I want to stress that every
 single_assignment in the program its now translated into these four operations that
 need to be done to maintain the reference_counts There_are tradeoffs in reference
 counting It has advantages and disadvantages One of the big advantages
 is that it collects garbage incrementally without large pauses in the execution So
 for for kind for applications where large pauses would be problematic say
 real time applications or interactive applications reference_counting can
 really help because it minimizes the length of the longest possible pause
 Okay so your_program will never go to sleep And just appear to stop running for
 some period of time because its off collecting garbage It always collects the
 garbage in small incremental amounts and so that you never see a long pause
 Reference counting or at_least a basic implementation of reference_counting is
 also quite easy to implement Its very straight_forward to go_through and modify
 the code to add reference_counts So you can easily imagine a code_generator that
 would simply generate different code for for the assignment operation than it
 normally did if you were adding reference_counting to an implementation So really
 the the changes that are needed for a simple implementation of reference
 counting to a compiler are not that pervasive Now there are some
 disadvantages to reference_counting One is that manipulating the reference
 counts at each assignment is really quite slow So if you remember what_happens we
 have a couple of updates to reference_counts so we have to update you_know
 the reference_count of two objects To do an assignment This is this is the code
 to do an assignment and then we have an if statement And then we actually do the
 assignment itself So theres two reference_count updates thats has to see
 if our reference_count became zero and then we actually do the assignment So the
 overhead here is substantial Youre taking every single_assignment in the
 program and blowing up its cost by at_least four or five times And that will
 have a very noticeable impact on the performance of many programs Now it is
 possible to optimize reference_counting So for example if we had two updates to
 the same object say within a basic_block or even within a control_flow graph a
 compiler a smart optimizing compiler could frequently combine those reference
 count operations together So instead of updating the reference_count to the object
 two times it can just update it one time And similarly if there were even more
 reference_count updates potentially all of those could be coalesced within some
 region of the program The problem with that is that is becomes very tricky to
 get that right So a a simple implementation of reference_counting is
 quite slow But easy to get right A very sophisticated implementation of reference
 counting or highly optimized implementation of reference_counting is
 somewhat faster Still has a noticeable performance impact if youre reference
 counting all objects but it is substantially faster However its quite
 tricky to get it correct The other problem with reference_counting is that it
 cannot directly collect circular structures So to see this lets draw a
 little heap with a circular structure And so lets_say we have a local variable x
 and it points to some object in t he heap And that object has a pointer to another
 object alright And that object that second object then has a pointer back to
 the first object Okay so here x is pointing to say a circularly length list
 of length two alright And if we add in the reference_counts here what would
 those look like Well this object down_here the second object here actually one
 reference to it so its reference_count will be one And this first object has two
 pointers to it one from x and one from the other object and so its reference
 count is two Okay so here is our little heap and we can see that there is no
 garbage here because all the objects are reachable from a a local variable or
 variable of the program Now if we were to assign a new value to x lets_say that we
 have the assignment x gets null Alright so this pointer goes away But whats
 going to happen Well when we do that assignment were_going to change the
 reference_count here of this object its now gonna be one And if we look_at this
 heap we now see that these objects these two objects are unreachable Okay so
 these are unreachable But notice that the reference_counts are not zero so we cant
 collect them The garbage_collector or the reference_counting implementation will
 check the reference_counts and see oh these are one and so we cant delete them
 And then what it cant see is that the only references to these objects are from
 other unreachable_objects So the bottom line is that reference_counting cant
 collect circular structures and there is only really two ways to deal_with that
 One is if the programmer remembers whenever a circular structure is going to
 become unreadable to somehow break the circularity So for example before we
 clobbered the pointer to x here we remembered to go in and say set you_know
 this pointer here to null If we nulled out one of the pointers in this cycle so
 that there was no_longer a cycle then the reference_counting would work correctly
 because then the reference_count of this object would go to zero when when this
 pointer was dropped from x and that would cause the reference_count of this object
 also to go to zero after this object was deleted okay The other_possibility is to
 back reference_counting by some other garbage_collection technique that can
 collect cycles And so in some reference_counting systems for example most of the
 garbage_collection is done by reference_counting but every now and again once in
 a very very while you might want to mark and sweep collector to go_through and
 clean out any circular but unreachable data_structures
 In this video_were going to take a look_at Java Arrays Lets_say we have two
 classes a and b and that b is a sub class of a And lets think_about what
 happens_when we execute the following piece of code So the first_thing were
 going to do is were_going to allocate an_array of bs So this is an_array thats
 supposed to hold Bs okay And were_going to have an_array variable b here
 that points to it And then were_going to have another variable array variable
 A that also points to the same array as B But notice the type of a So a is an
 array of As little a here is an_array of As and b is typed as an_array of Bs
 And now what were_going to do is were_going to assign into a sub zero okay
 into the first_position of a a new a object And that should be fine right
 Because a is an_array of As and that seems like that should work out alright
 Alright so here there will be an a sitting in the first_position And then
 were_going to access b sub zero which because A and B point to the same array
 is the same element as a sub zero And were_going to call some method that is
 not declared in A Now remember B is a subtype of A alright So B has all the
 methods of A A but B might have more methods And since this is an_array of
 Bs we should be_able to call all the B methods on it and yet here when we call
 some methods thats declared in B but not in A we are going to get a runtime error
 because the object stored in the array is actually an A object at the first
 position To understand whats going on in this example we have to take a look_at
 the sub_typing rules in Java So if we use a subtype of A if B inherits_from A
 thats one case So if B and B inherits_from A then B is a subtype of A And
 thats just like in Cool and and most other object_oriented languages And
 were were very familiar_with that from our lectures in type_checking Further
 more type sub_typing is transitive So if C is a subtype of B and B is a subtype
 of A then C is also a subtype of A Okay a nd thats also completely standard But
 then theres this other rule thats not quite standard or is definitely
 nonstandard And thats that an_array of Bs is a subtype of an_array of As if
 the element types are in a subtype relationship So if B is a subtype of A
 then array of B is a subtype of array of A And Cool doesnt have anything like
 that Cool doesnt have arrays so it wouldnt even have the opportunity to have
 something_like that But this is also not the way its done in other languages that
 have objects and subtyping So lets_take a look_at our little example again and let
 me explain it in a slightly different way So the issue here is that we have a area
 of memory and it actually doesnt_matter here Its not essential that this be an
 array Whats important is thats an updatable part of memory so that we have
 pointers to it We have two pointers to it a and b and we can they can both read
 and write this part of memory So this could be just a single cell it doesnt
 have to be an_array of multiple cells But whats important is that there is some
 memory_location that both of these point to that they can both read and write
 okay And the trouble comes and by the way that theres a name that thats
 called Aliasing okay So when you have two names two program names for the same
 part of memory that is called aliasing and here you_know we have the the two
 arrays A and B that point to the same area of memory okay Now aliasing is
 very common in real programs since not bad by_itself but the problem in this example
 is that A and B have different types okay And in general if you have aliasing
 updatable references okay Meaning if two names for the same location that location
 is both readable and writable so it can be updated through the two names And
 those two names have different types then that is going to be unsound okay Were
 not going to have a sound type_system and to see the problem lets_say here in this
 case what was it We had that B type B was sub_type of A okay And what did that
 mean Well that meant is we could do a wright through this pointer okay And
 write an A object into this location and then we could read that out through this
 point over_here as a B object But now it doesnt have all the methods and and
 fields of A and treating it as the object we could potentially use an operation on
 it thats undefined And you can see that it doesnt help if we swap the roles of
 of A and B alright So in particular if we reverse the if we reverse the
 subtyping relationship so that A was a sub_type of B we can do exactly the same
 problem because aliasing is symmetric We just do the write through the B pointer
 and the read out of the A pointer swapping the roles of the recent right here and we
 have exactly the same problem So in general multiple aliases do updatable
 locations with different types is unsound okay And this problem actually has come
 up in many different programming_languages Java is not the only
 programming_language to have had this issue Its a fairly subtle aspect of type
 systems and in many languages have done things similar to Java where theyve
 created a problem really for the static_type system by wanting to have a
 subtyping work through arrays Now the standard solution or the solution thats
 used in I should_say in many languages and is probably most widely accepted in
 the programming_languages research community is that you need a different
 subtyping rule for arrays So we would say you_know the rule that is commonly
 used the standard solution to this problem at the type level is that to do the
 following things So you only allows subtyping on arrays So you_know an
 array of Bs is a subtype in array of As only if B and A are the same type If B
 A And if you think_about that for a second if we have an_array and now we
 have our two pointers to it A and B and we know the type of A the subtype or the
 type of B Well that only h appens if the element types are are equal And so we
 cant create two references to an updateable location with different types
 Okay and that will guarantee soundness of of the type of the type_system So
 Java fixes the problem differently So instead of statically checking that array
 accesses will all be type correct Java does this at run_time And so whenever an
 assignment is done into an_array at runtime Java checks whether the type of
 the object being assigned in compatible with the type of the array So when you
 say new B sub ten in Java Java will remember inside the array that this was
 supposed to be an_array of Bs And then whatever you assign into the array it
 will check that the thing youre assigning is either a B or a sub_type of B Now
 this obviously adds an overhead on array computations so every assignment to an
 array is going to have have a type_check on it at run_time And fortunately though
 the most kinds of arrays are arrays of primitive types in particular arrays of
 ints and arrays of floating_point numbers and these are not affected because the
 primitive types are not classes Theres no subtyping on them and so you can never
 create an_array say of floating_point numbers with any kind of subtyping
 relationship that would result in this problem So so that were saved or in
 better shape for the primitive types and they dont need these extra checks But if
 you have arrays of objects then we do assignments into those arrays in Java
 theres additional run_time overhead
 In this video_were going to talk_about programmer defined exceptions So think
 about the following typical programming scenario Youre deep into some fairly
 complex section of code and you come to a place_where you could experience an
 unexpected error That there could be something that could happen that would
 violate some important_property of your_program So for example maybe youre
 going to discover theres a place_where you could be out of memory or theres some
 data_structure that doesnt satisfy some variant So a list thats supposed to be
 sorted that is not or something_like that And the question is how do you
 handle these errors How do you write code that will handle the error gracefully and
 not make your_program really really ugly So a popular solution to this problem in
 many languages including Java is add a new kind of value to the language called an
 exception And well have a couple of control constructs for dealing with
 exceptions Here is the two that are probably the most popular And these are
 as they appear in Java So we can throw exceptions and what this does is it causes
 an_exception to be created at this point wherever the throw occurs And that
 exception will simply propagate out of the program It will basically halt the
 execution of the program at that point and any containing constructs will also throw
 the exception So the exception will you_know simply propagate up out of the code
 that thats currently executing until it hits a try catch So how does this work
 Well we can try something We can execute some expression here and this will be some
 expression And if this expression throws an_exception if the throw occurs
 somewhere inside this expression then we will catch that exception and there can be
 a binding here to say what we are going to call the exception value So this is like
 a let we will grab the exception that comes out of here Well call it X and
 then we can execute this piece of cleanup code to how to handle the exception in
 some way And so the basic_idea behind this thi design for handling exceptions
 is that the place_where you have the exception the place_where you actually
 detect the exception might be somewhere deep inside the code and not a very good
 place for actually dealing with the exception So what you want to do is get
 out of that part of the code get back to some higher_level point where you can
 clean things up deal_with the exception and then retry perhaps some larger block
 of code Heres a little example of using exceptions So here we have our main
 method And what are we going to do Were_going to have a tryblock that just calls
 a function X And if that raises a an_exception then we will catch the
 exception In this case we dont do anything with the exception we just print
 out a message saying there was an error and we terminate the program So we dont
 do anything very smart but we do catch the exception and at_least print_out an error
 rather_than just terminating So what does x do Well X simply throws an
 exception So this function X allocates an_exception object This is just a value
 its just a class like everything else But it has a special property that it can
 be thrown So when we throw it thats when X terminates abnormally and we end up
 in the catch block of the trycatch expression back in the main_method Now
 in the last_couple of slides I gave you a very informal description of how
 exceptions work and it it might not have been very clear and in fact its hard I
 think to give a very clear description without some kind of formal description of
 how exceptions are supposed to behave But fortunately weve_looked at operational
 semantics in this class and so now youre familiar_with those kinds of descriptions
 of language behavior and I can actually describe you pretty succinctly how
 trycatch actually really works alright So were_going to give operational rules
 here for TryCatch expressions And I just noticed I just poin t out here that I had
 some kind of font problem and so I had to write in the turnstiles by hand in this
 slide So those handwritten characters are all supposed to be the turnstile
 character Now to more important things theres a distinction with exceptions
 Okay there are two kinds of states that an_exception object could be in It could
 just be an ordinary value So when I say new exception object in Java so when I
 say something new something exception class is just an ordinary value At that
 point it just behaves like any_other object But then there is a distinction
 when the object is thrown So when the exception is actually thrown it becomes a
 special kind of value and it gets treated differently alright So were_going to
 distinguish between an ordinary object V okay And objects that have been thrown
 okay which are then active exceptions Alright So lets_take a look_at some of
 the operational rules for the exception constructs So heres one rule for its
 trycatchblock And what this rule says is that if an expression evaluates to an
 ordinary value If it doesnt throw an_exception then the results of the
 trycatchblock is just that value So the way the trycatchblock works is you
 evaluate the expression in the tryblock If that terminates normally with a value
 then the results of the whole expression is just that value alright Now the
 other_possibility is that youll evaluate a try catch block and when you go to
 evaluate the expression of the tryblock E1 it will throw an_exception So it
 could result in a thrown_exception And so in this case Okay that is that
 excuse_me that is this case where E1 evaluates to one of these special values
 has been marked as a thrown_exception What do we do in that case Well like
 unwrap the exception we pull out value that is in the thrown_exception alright
 We bind it to some local name alright thats named in the catch expression and
 then we evaluate the cleanup code So with the ex ception value available we
 evaluate E2 and whatever the result is of E2 that is the result of the
 trycatchblock How does throw work Its very_simple So throw just takes an
 expression it evaluates that expression against some value V And then it marks
 that value as a thrown_exception as a thrown_exception so it wraps the value in
 this T thing and that indicate that this exception now has been thrown Now the
 only other thing we need to talk_about is how the rest of the language all the
 other constructs in the language deal_with these thrown exceptions And thats very
 simple We want those thrown exceptions to simply propagate out of any_other kind of
 expression So for example well just do one example here because the idea is the
 same for every other language construct Lets_say that were evaluating E1 E2
 alright So the first_thing we have to do of course is to evaluate E1 and if that
 happens the thrown_exception So if something goes wrong in the evaluation of
 E1 and E1 evaluates to the thrown in exception well then we stop the
 evaluation of the plus right there We_dont even evaluate E2 notice that E2 is
 not mentioned here above the line of one of the things to be evaluated so that E1
 terminates normally with an_exception then the results of the entire addition is
 that exception And similarly for all the other constructs if if theres one of
 their subexpressions results in an_exception In_fact if the if as_soon as
 one of their subexpressions results in an_exception they stop evaluating and
 propagate that exception up The only thing that will stop the exception from
 propagating all the way out to become the result of the whole program is if it is
 caught in a trycatchblock There_are many ways to implement exceptions and here
 is one simple way to do it So we encounter a try expression we will mark
 the current location in the stack So we will mark the position in the stack where
 we encountered the try So for example here say is our s tack Lets_say that
 you_know the stack is going this way We encounter a try expression here so we put
 some kind of marker in the stack to indicate that theres a try that was
 encountered there And then you go on you_know evaluating things inside of the try
 which might add more stuff to the stack Now when we throw an_exception if down
 here all of a sudden a throw occurs and were at this point in the execution
 whats going to happen is were_going to unwind the stacks Were_going to knock
 everything off the stack Were_going to pop all of this stuff of the stack so
 its all gone back to the first try And then well execute the corresponding
 catch So here we marked you_know the place and the code where there was a try
 and we can use that to find the expression the piece of the code that has the
 corresponding catchblock and well unwind the stack to that point and then begin
 evaluation of the catch So at this particular design it has the disadvantage
 that try actually cost something So even if you dont throw an even if you dont
 throw an_exception you still pay something to execute a trycatchblock
 You have to at_least mark the stack and and remember to unmark it of course when
 you pop things off the stack when you leave the tryblock So more_complex
 techniques tries to reduce the cost of try and throw and also trade off between them
 And generally the thing you want to do is because exceptions are probably relatively
 rare in in most programs is to make the cost of try as low as possible possibly
 at the expense of making throws slightly_more expensive Now heres a little
 trivia question about Java So what_happens to an uncaught exception as thrown
 during object finalization So if you dont_know what object finalization is
 well when an object is collected when an object is garbage collected it is
 possible to run a method on that object to clean it up before the garbage_collector
 actually deallocates it and this is called the finali zation method So objects have
 finalization methods in in Java and those methods are essentially invoked by the
 garbage_collector Some garbage_collector discovers that some objects this garbage
 is going to be clean it up it will first invoke the finalization method And why
 would you wanted to do this unless say we have an object and it might have say
 a file handle It might have a pointer to an open file or something_like that And
 now when this object becomes unreachable it will be collected by the garbage
 collector But if you dont close the file well thats gonna cause problems
 Having lots of open files that are hanging around without the program using them it
 can cause problems later_on especially if you run out of file handles Usually
 theres a fixed number of file handles available from the operating system So
 the right thing to do is when this is garbage collected is to first close the
 file and essentially get_rid of this pointer okay and then deallocate the
 object and that it was object finalization is for So again you can
 define the method in Java that will be run by the garbage_collector to kinda clean up
 any resources the object has before its actually deallocated And now the question
 is if that finalization method raises an_exception who catches it Because it was
 invoked by the garbage_collector its unpredictable when its going to be
 invoked and its not clear where that exception should be handled And the
 answer to the question is no one handles that method or nothing handles that
 method The exception is simply dropped And so any exceptions that happen during
 object finalization that are not handled within the finalization method itself are
 simply thrown away One of the nice innovations in Java is that exceptions are
 actually part of the method interface and they are checked by the compiler So in
 in the example that I gave at the beginning of this lecture we had a method
 X that could raise an_exception my exception and notice that the declaration
 of X actually declares that X can throw that exception So its part of the
 interfaced X part of the checked interfaced X that it can raise a
 particular exception And why would you want to check this at compile_time Well
 the observation that was made actually in the original Java project was that there
 were many exceptions that could be raised by Java programs and people easily lost
 rack of what possible exceptions could be raised they didnt know what exceptions
 they had to handle And in fact when they added this to the language the compiler
 would actually enforce now that a method declared every exception it could raise
 They discovered lots of places in the in the compiler where there were exceptions
 being raised but not properly handled So this led to better air handling in in the
 Java compiler itself and and I_think people generally agree that this is een a
 good idea because it helps programmers to write more robust code because they can
 see exactly which exceptions they have to handle Now there are some exceptions to
 this rule In particular theres theres some kinds of runtime errors that dont
 have to be part of the method signature simply because its very hard to check
 statistically that the method would never raise them So things_like dereferencing
 null or interprocedural overflow dont have to be handled and declared in the
 interface But for the most part any exception that a a method can raise has
 to be declared as part of its interface in Java And then there are other
 mundanetype rules about the particular design for exceptions in Java So for
 example throw has to be applied to an object of type exception it cant be
 applied to an object of an_arbitrary type But overall exceptions are rather nicely
 done in Java and that this particular idea of of declaring the types of exceptions
 that a method can raise was a new idea in Java
 In this short video_were gonna take a look_at interfaces in Java Interfaces
 specify relationships between classes without using inheritance So here is an
 example uh we have an interface here called the point_interface And a point
 interface can have a a bunch of methods in it and and we just declare the the
 signature of those methods You can also have other things besides methods but uh
 the main thing that uh theyre used for usually is for a a method interface So
 uh heres an example of a particular method the move method and it takes some
 arguments and has a particular return type Now any_other class or any class
 excuse_me thats going to implement the point_interface has to provide a method
 uh with the same signature So if this see because the point_interface has a move
 method the A class will have to have a move method with the same signature as
 the move method in the declared interface And if the point_interface had other
 methods ah then the point class would also have to implement those methods by
 you_know having a method of the same name ah with the appropriate types of
 arguments and result Now it says in the Java language manual that Java programs
 can use interfaces to make it unnecessary for related classes to share a common
 abstract super class or to add methods to objects And the translation of that is
 that interfaces play the same role as multiple inheritance in c plus plus So
 interfaces uh are really analogous uh to to multiple inheritance And the
 reason for that is that a a class can implement multiple interfaces So if I
 have a class x and it implements a three interfaces a b and c This means an
 extra object can be treated as an A object or B object or a C object in the
 appropriate context So its like or almost as if X has three superclasses A B
 and C Now uh there are some important differences uh but there is the effect
 and so if I wanted to have a class that gets functionality or implements a
 functionality uh several uh uh interfaces thats I_mean we do very
 directly in java just by saying if the classes going to implement all those
 interfaces gtgt Now here is an example of an application of that so think_about a
 graduate student ah working at Stanford or some other university so ah typically
 graduate students are students okay they take classes and have propertys that
 students ah have they get degrees and grades and things_like that gtgt Graduate
 students also work for the University gtgt They are often teaching assistants in
 classes or research assistant inaudible so there inaudible another role which is
 university employee and if I have gone to ah trouble ah in my I university
 personnel management software to implement functionality to deal_with students and to
 implement functionality to deal_with employees Well then I would like to make
 use of that when I get around to thinking_about how Im going to implement uh the
 functionality for graduate students and one way to do that Would be if I had a
 class with implement if I had excuse_me an interface for employees and interface
 for student and I would say that graduate student could be both okay So a graduate
 student can implement both the employee interface and student interface And and
 the reason thats this is a good idea is it is actually hard to do this if ah you
 only have single inheritance If you think_about it if I had set things up so that I
 had some employee classes and some student classes and now I want to make graduate
 students Well now what am I going to do Well if I have my employee class I can
 make grad students a subclass of that but now how do I get the student functionality
 and similarly If I have a student class I can make graduate student a subclass of
 that But now how do I get the employee functionality So in single inheritance
 youre forced to choose a single class to inherit from And the advantage of
 interfaces is that it will let you get functionality or implement functionality
 or express the relationship at_least of functionality to multiple kinds of things
 And so I can have one uh graduate student class that implements both the
 employee and the student functionality So how are interfaces different from
 inheritance Well probably the biggest difference is that its not possible to
 implement interfaces as efficiently as inheritance And thats why you have both
 So youd prefer to use inheritance uh if you can because its going to be more
 efficient than interfaces And what makes interfaces less efficient Well the
 primary thing Is that if the class is implementing interfaces need not be at
 fixed offsets In_fact we will not be_able in general uh to assign methods in
 interfaces to fixed offsets inside of a class implementation or an object
 implementation So lets_take a look_at an example Heres our point_interface again
 Now say we have one class point when we saw this one before inaudible And it
 implements the move method has to implement the move method And then we
 have another class that also points to point_interface but it also implements
 some other stuff Okay so it might implement some other methods that arent
 part of that interface So now how would we decide you_know where to put the move
 method Well the natural thing that that weve discussed inaudible Say of course
 inaudible is that the methods ah would be laid_out in the order in which they are
 declared so if we did that clearly the move method will not be ah in the first
 position in both of these classes Now We could imagine uh a separate compiler
 pass that were trying to sort the methods So that say all the methods of
 the point_interface always appeared in the same position and in the same order in any
 class implements the point_interface But that doesnt work as_soon as we have um
 multiple interfaces being implemented So lets_say that the point two class here
 implemented another interface A So how can we then implement interfaces Well
 so its going to be a little more_complex than usual to implement in this batch say
 to a method f where e has some interface type So if e is typed as having some
 interface and now were calling it the f method of that interface then were_going
 to have to do a little_bit more work And so heres one approach this approach is
 actually quite inefficient but you will see that it will work And there are other
 approaches that are more efficient but thats not particularly important so
 heres one way that can work So each class of implements interface is going to
 have a lookup table associated_with it that maps method names the string names
 of methods to those methods themselves And then Uh we can hash the method names
 for faster lookup and we can actually compute Uh those hashes at compile_time
 And so the idea would be that if when we have an object Ah somewhere in the
 object probably at the dispatch pointer The dispatch pointer you_know will point
 off to a list of methods sort of the normal methods of the class But somewhere
 say maybe at the end of the dispatch table there will be another pointer to some kind
 of look up table that maps names Two two methods to quote Okay So somehow
 associated_with every object of every class ah we will have this look up table
 that will map ah the the names of interface methods to the actual codes for
 those methods that influence them All_right and wed already decided uh that
 the for the point_interface that the move method should go first It should be the
 first method in the class Well if we had made a similar decision for the a
 interface some method in that interface that should always be listed first in the
 give to all the methods and all the interfaces so that they can be maintained
 across all of the uh classes that implement those interfaces At least
 classes that are declared and all the interfaces that are declared And thats
 kind of unJava And that we dont want to uh force people to declare all the
 classes and the interfaces once and not be_able to extend them in the future Alright
 so the bottom line is that methods in interfaces do not live at fixed process in
 In this video_were going to talk_about Coercions which is a feature of type
 systems that appears in many languages and we will be looking specifically how
 coercions are done in Java Java allows primitive types to be coerced in certain
 context And coerced means converted from one type to another So heres an example
 lets_take the expression one 20 and the difficulty with this expression is that
 the the one here is is an_integer and the 20 is a floating_point number And
 there is no way to add an int to a float directly We either have to convert the
 integer to a float and then do the add as floating_point numbers or convert the
 floating_point number to an_integer and then do the addition as integer addition
 So they have to be converted to a common representation before we can actually do
 the operation And the normal thing to do and the thing that Java does is to
 convert the integer to the floating_point number 10 Now a coercion the right way
 I_think to think of coercions is theyre really just primitive functions that the
 compiler inserts for you So its like you left out a function_call and the
 compiler notices that and puts it in So in this particular example what would be
 the function_call Well there we can think of there being a primitive function
 that converts integers to floating_point numbers in the obvious way And so really
 this expression here gets converted to the expression into float applied to the
 number one plus 20 All_right So coercions are probably best thought of as
 a convenience for you the programmer to let you avoid having to write some
 function_calls And so where it is obvious that a type conversion is going on
 the compiler can insert the function that performs that type conversion for you And
 most languages really have extensive coercions so the conversions are very
 very common particularly between numeric types and so this is not just Java This
 is really many different programming_languages of all styles that have lots of
 differe nt kinds of coercions Now Java in particular distinguishes two kinds of
 coercions and casts You have widening coercions and these will always succeed
 Alright so that means that Java will always put them in and there will never be
 any complaining from the compiler or the runtime system about them and we already
 saw one of those so the conversion from int to float is an example of a widening
 cast Now narrowing casts may fail if the data cant be converted to the desired
 type So in particular float to int well this will work fine Something like 20
 can be converted in obvious way to two but if youre converting something that
 doesnt have an_integer representation something say like 25 you_know theres a
 question of what we should do here Okay and for such narrowing casts where there
 isnt a a clear mapping whether we should go or you_know or whether we
 should try here or round up or whatever then Java will actually complain and not
 let you do it Okay perhaps for better example of the kind of narrowing cast
 thats that Java will complain about is something_like a down cast So if I have
 two classes A and B And B is a subtype of A and then I have something of type A
 Well I can cast it to B I can say lets_say I have X which is a type A and then I
 can have an expression where I try to convert x to a B object So here I have a
 cast Ive indicated that I want to treat this expression x as a B object and this
 will type_check okay So the the compiler will let this through since B is
 subtype of A But at run_time its actually going to check_whether x is
 actually a B object and if its not youre going to get an_exception So this can
 fail at run_time if the object thatt x actually holds at the point of the cast is
 not a B object So the rule in Java is that narrowing cast must be explicit You
 have to put the function in yourself You have to put in the typecase in the code so
 that its obvious that you really want to do it but widening casts in coercions can
 be implicit so its alright If youre widening if youre either promoting to a
 super type or you are converting between initial type where its clear that the
 one type embeds in the other then those can be filled in for you by the compiler
 And heres a little Java trivia question So it_turns out that there is one type in
 Java for which there are no coercions or casts defined Okay so there are no
 implicit conversions or even explicit conversions from that type to any_other
 type And the answer to the question which is the only one is bool Okay so
 only the type boolean has no conercions or casts to another type Now personally Im
 not a big fan of coercions I_think that its clearly a convenience for
 programmers its clearly something that is widely accepted as being necessary in
 programming_languages because casts implicit casts and conversions are so
 ubiquitous but I do think that it tends to lead to programs that have behavior thats
 different from what the programmer probably expected And heres a good
 example from the language PL1 which recalls stood for Programming Language one
 designed by IBM in the 1960s And had many_many features in it so weve talked
 about PL1 a few times in this class And one thing that PL1 had was very extensive
 cast and coercions and this could lead us to some surprising behaviors So heres an
 example we have a A B and C are strings of three characters so its important to
 know here that the length three is part of the type So B is string 123 C is 456
 and then A is going to be B c and the question is what is A And and you
 probably wont guess so let_me show you what I_think is the right answer So first
 of all the question is what_happens with this operation here Well so that is
 going to be interpreted as an_integer so B and C are both going to be cast to
 integers and the and this will be done as a integer arithmetic So B will get
 converted to the number 123 C will get conver ted to the number 456 okay And
 then well add them and well get out the number 579 Okay so the result of this
 expression is 579 but A is also a string of three characters so this has to be cast
 back to a string Now it_turns out that this cast happens in two steps First
 this this number here is cast to a string of the default length okay And the
 default length happens to be six so this is cast to a string looks_like this
 Theres three_blanks followed_by 579 And then that string of six characters is
 converted to a string of three characters And we just take the first three
 characters and so we get out that and so the answer is that this program stores a
 string of three_blanks in A Which is probably not what was expected
 In this video_were gonna talk a little_bit about concurrency in programming
 languages and in particular Javas use of threads Java has concurrency built in
 through threads and Im not going to explain what a thread is from first
 principles in this video So Im going to assume a little_bit of background but
 well say a few words here about what threads are So a thread is like its_own
 program It has its_own program counter meaning it has an instruction that its
 executing and it has its_own set of local_variables and activation_records And a
 Java program or any program written in any language with threads may have
 multiple threads at the same time So abstractly we can think of threads as
 being a series of exec of of statements that are executed Each one of these
 threads again has its_own set of local_variables But the threads may refer to
 common data in the heap So they could refer to the same heap data_structures
 And each thread is executing a particular instruction so lets_say that the threads
 are all here we have three threads one two and three And theyre each at some
 instruction or some statement in the program And then there is a scheduler and
 at each_step of execution the scheduler will pick one thread to execute And itll
 execute one statement And this is conceptual Meaning this isnt exactly
 the way its usually implemented And then it will repeat this loop So itll pick a
 thread itll execute one statement of that thread and itll just keep doing that
 over and over again So we might for example the schedule might pick thread
 one and execute this first statement And then it might pick thread_two and execute
 this statement and then it might pick thread three and execute that statement
 And then it might decide well to execute another statement of thread_two and then
 it might execute several statements of thread one And then it might come_back
 and execute a couple statements of thread three and then thread_two might get to go
 again for a while and so on All_right so the threads execute in some order And
 its nondeterministic at each_step of execution which thread will execute how
 many of its instructions will be executed And and thus the threads may interlead
 the instructions in the threads may interlead in a relatively or in fact
 completely arbitrary order Alright Now coming back to how this is done in Java
 thread objects in Java all have class threads So theres a special class that
 you have to inherit from in order to be a thread And what you get when you inherit
 from the thread class is you will have start and stop methods for beginning and
 ending the thread Alright And theres some other special properties of threads
 And in particular one thing that threads can do is to synchronize on objects So
 a a a thread can acquire a lock on an object through the synchronized construct
 And so if I say synchronized xe in Java what that means is that the program will
 acquire a lock on x before it executes e So the procedure here will be to lock x
 then evaluate e and then unlock x alright So its a structured
 synchronization construct And within while it is executing the expression e it
 will hold a lock on x And this is the primary way really almost the only way in
 Java to get synchronization between multiple threads So this is how we one
 can control the set of interleavings because while one thread is executing
 this particular block of code no other thread can execute this block of code and
 also hold a lock on the same object x Now could two threads could execute this same
 syntactic construct if they were locking if if their local_variables actually
 referred to different objects But theyre guaranteed not to interfere with each
 other not to interweave if they tried to lock the same object x alright Now
 theres one shorthand in Java which is used more commonly than this form the
 synchronized construct And as the synchronization can be put on methods We
 can say synchronized f where this is a method definition Alright An d what this
 means is that when this method is called that this object will be locked So here
 the object thats going to be locked is implicit And when synchronized is
 attached to a method name or a method declaration that always means that this
 parameter will be the synchronized or locked object Lets_take a look_at the
 simple example and think_about what_happens if we have two methods one of
 which calls the method two of the class simple and one of which calls the method
 fro So lets_take a look_at that lets_say we have thread one and thread_two And
 now thread one is going to invoke the method two and thread_two is going to
 invoke the method fro All_right So one possibility here lets_say that that two
 gets to run all the way to completion before fro executes anything So then
 well have a three and b four okay And then fro will run and it will print
 out the string a three b four Okay So thats a relatively simple straight
 forward case You_know another_possibility is that thread_two gets to run before
 thread one ever does anything So thread_two executes all of its instructions
 before thread one executes anything at all In which case what will be printed
 Well the fro will print_out a one b two alright And two will then run and
 it will set after this executes So after this after fro finishes executing it
 will then set a three and b four Thats another_possibility and both of
 those are fine alright But then there are some other odd possibilities and
 lets_take a look_at one of those What happens if the thread is actually enter
 leave in a nontrivial way So lets_consider the following possibilities
 Lets_say that two executes these assignment a three And now fro
 executes the first part of the print So it does the read of a and starts building
 up this output string okay So its going to print_out here a three
 alright And then now lets_say that fro actually goes ahead and gets to run some
 more and also goes ahead and prints out the rest of this Okay So actually does
 the second read of b so the n it will print b two All_right And then one
 will run the rest of the way through excuse_me b four And so here we got an
 output that doesnt seem quite right Here we got we were able to see an
 intermediate state where thread one had only executed partially And so what came
 out over_here in fro show you_know just a partial update of the variables a and b
 So one had been written but not the other And if we didnt want to do that if we
 thought this was wrong we would have to use synchronization in order to control
 that So lets_take a look then at using synchronization to try to prevent this
 from happening And Ill tell you right upfront that this piece of code or this
 attempt is incorrect and actually it doesnt solve the problem at all But it
 also illustrates probably the most_common thread programming error that Java
 programmers make And lots of people including professional programmers make
 this mistake and lots of production Java programs have this particular mistake in
 them So its a very instructive example I_think So lets_take a look here Lets
 assume we have the the two threads again The thread is going to call two and the
 thread is going to call fro And lets_say that in our heap there is only one
 object simple and lets just call it s So this is globally in the entire heap
 just one object s of the simple class Alright So what is lets_say that thread
 one is going to go first alright and the first that its going to do because its
 synchronized method is its going to lock the this parameter of the call since
 theres only one simple only one object of the simple class that has to be the
 object s so its going to lock s Alright now well prevent any_other
 thread from acquiring the lock on s while while thread one holds that lock So then
 thread one can go ahead and execute the statement a three And now though we
 could have interruption and thread_two can get to run And notice here that thread
 two doesnt check the lock It goes ahead execute this code over_here In the f ro
 method but thats not synchronized there is no synchronize keyword there And so
 just the fact that somebody else holds the lock on a simple object doesnt prevent
 another method from accessing the fields or the data of that object if that other
 method doesnt itself check the lock So if the other method is not synchronized
 it will just go ahead and and and and execute ignoring the fact that another
 thread holds the lock on the object So in this case this will just this can
 just run to completion And well print_out a three b two Okay So we only
 see one of the two updates And and then the scheduler can come_back in Lets the
 other thread run and it would run to completion and unlock the object at the
 end And you could see here that this particular attempted fix has achieved
 nothing Actually all the possible inter leavings of the two methods that were
 that existed without any synchronization still exist if only one of the two methods
 is synchronized And the reason this error is common is because frequently people
 think well I you_know if reads are okay I can always read from things in parallel
 and that wont cause any problems because Im not altering any data Its my writes
 that have to be synchronized so if Im going to write to fields of objects well
 that needs to be coordinated with other methods because writes are dangerous but
 reads somehow dont interfere And the point here is that if having only one
 method or only having one half of the accesses to the of two accesses to shared
 data be synchronized doesnt help because synchronization only works if everybody is
 checking the lock So both the reader and the writer need to check the lock in order
 to restrict the set of possible interleavings of these two methods So
 what would be a correct way to do it Well is just to put the synchronized
 keyword on both methods And now Its not possible to have the interleaving we saw
 before its not just only two possible outputs One there are only two possible
 strings that could be p rinted One is that a one and b two So in this
 case the fro method executes before the two method so thats fro before two
 okay I_mean all of fro before all of the two method And the other_possibility is a
 three b four alright And that would be the two method executing in its
 entirety before the fro method And those become the only two possible
 interleavings when both methods here are synchronized Im going to conclude this
 video by making a couple of other comments on Javas threads So one property we
 would like is that even if there is no synchronization a variable should only
 hold values that were actually written by some threads So what do I_mean by that
 Lets_say that we have two assignments This is in thread one where were
 assigning a the value of 314 then in thread_two were assigning a the value
 278 And so after these assignments are done after theyve executed in some
 order what do we expect Well we expect that a ends up being equal either to 314
 or 278 alright Now what we dont want is for a to wind_up being some other
 value okay I_mean what if a turned out to be 378 for example okay This would
 be bad we dont want this right Because this value 378 was never written by
 either thread Okay this value was somehow manufactured And Ive chose 378
 to kind of indicate what could potentially go wrong If we somehow wound up with a
 mix of the bits or the the pieces of the number from thread one and thread_two If
 they were recombined in some strange way then we could create a number that was
 assigned to a that didnt exist in either thread Okay it was never actually
 written in either thread Now Java does guarantee that the rights of values are
 atomic Meaning that if I write to a value if I sign a primitive type to
 something that is going to happen atomically and wont be interfered with by
 another assignment to the same memory_location except for floating_point
 doubles So this does not hold writes to doubles or not necessarily atomic Now
 why would that be Well a double is a floating_point number but it consumes
 twice the memory Thats why its called a double it consumes two words Okay and
 what that means is that if a here is a double so lets_assume that a is a
 double That_means that this write of 314 actually translates into two machine
 instructions We have to write the high part of a equals something and then the
 lower part of a So the two machine instructions to write both of the words
 that represent a after writing the high half and the low half Okay because there
 isnt a a primitive double word write on most machines And the same_thing would
 happen in thread_two This would get broken up into two assignments to the two
 halves of a And then just from what we discussed before you can see that these
 could interleave in some way and you might wind_up with the unfortunate situation
 that the high half of the representation of a is written by thread one and the low
 half is written by thread_two and then you can get a number like this like you
 know something not exactly 378 but some mix of the bits from the write in thread
 one and the write at thread_two and you would create whats called an out of thin
 air value Okay and clearly out of thin air values are bad Okay you do not want
 those And and Java guarantees again that the rights of almost all the
 primitive data types are going to be atomic so you cant get out of thin air
 values But for performance reasons this is not the case for doubles All_right
 So so so for fro as a what it says in the manual is that as a concession the
 current hardware they do not require that rights to doubles be atomic unless you the
 programmer go and mark the type as volatile So you have to declare doubles
 to be volatile and if you do that then theyre guaranteed to be atomic writes
 Okay so if you were writing Java programs using Java threads and you were
 programming threads that read and write doubles concurrently then you need to be
 careful to declare those double types volatil e at_least currently and this
 may change in the future and Im sure theyd like to change it But currently
 you have to declare the doubles volatile to guarantee that the writes will be
 atomic More generally or actually somewhat separately this is actually a
 separate point here Java concurrency semantics are actually kind of hard to
 understand in detail And this this issue around out of thin air values is is one
 aspect of this There_are several other aspects of it And and and this is
 really not Javas fault It_turns out the concurrency semantics are hard and
 actually this is kind of at the frontier of research We_dont really understand
 exactly what we want or what the right thing is to do to specify the behavior of
 languages in concurrent settings And thats not to say that we dont understand
 anything We do have some languages in perfectly concurrency semantics but in a
 language flow in language feature is Java there are a number of things that
 are not completely clear how they should be implemented on certain machines So
 this has been a huge amount of work done on this problems specifically for Java and
 actually java was the first mainstream language to have first class threads in it
 and then try to integrate that with all other language features that all other
 modern languages features that we like It was so surprising actually that we have
 run into some trouble understanding how they are supposed to work in all
 situations So this is one area of Java that I would say still under debate And
 while for them while I figure_out straight_forward things with threads
 everything will work fine If you are doing there are some areas of the
 language where if you try to use them with threads you can get into a little_bit of
 trouble Alright so it surely pays to try to understand Java concurrency and the
 threads if youre writing significant concurrency Java programs
 In this video_were going to wrap_up our_discussion of Java by taking a look_at a
 couple of additional topics and how they are integrated into the language design
 Consistent with Javas dynamic nature Java allows classes to be loaded at runtime
 But this means that you can actually add functionality to an executing Java program
 so while its running by loading a new class And this creates potential issues
 with type safety and security because now there is a distinction between compiled
 time and load time So type_checking of the source takes place at compile_time and
 this is the kind of type_checking we discussed in earlier in earlier videos
 But the the loader when you actually go to load a class youre loading bytecode
 youre not loading source and its not being type checked again And it could be
 that this bytecode didnt come you_know from a trusted source This bytecode might
 not be the output of a compiler that did type_checking before it produced the
 bytecode So the bytecode might not actually satisfy the type assumptions of
 the Java implementation So essentially we have to check the bytecode again And
 a and a procedure called bytecode verification takes place when the class is
 loaded alright And and byte code verification is really a type_checking of
 bytecode Thats thats essentially what it does The procedure is a little_bit
 different because we dont have you_know the code here is much lower level and so
 the algorithms look a little_bit different But what theyre really doing
 is type_checking the thebytecode So now the loading policies are handled by
 something called the class loader And the class loader is a special class in Java
 and it decides what classes can be loaded and actually early on in Java a bunch of
 security problems were discovered Aware an attacker could get control of the class
 loader install its_own class loader that would be much_more permissive than the
 Java standard class loader and subve rt the system But those issues were fixed
 quite awhile ago alright And another interesting thing about Java is that the
 classes may also be unloaded So you dont you can not not only load classes
 you can also unload classes And the last_time I checked this was not particularly
 well specified in the definition and so its a little_bit unclear exactly what it
 meant when you unloaded the class and what happened to all the existing objects for
 example of that class Now Id like to spend a few_minutes talking_about
 initialization in Java which is quite complex and this shouldnt be too much of
 a surprise because if you remember initialization in COOL was also pretty
 complex and Java is just a superset of COOL so it has all the initialization
 issues that COOL has plus much_more And now the main source of complication is
 concurrency but other language features also add to the complexity of
 initialization in Java And in fact you could do worse If you want to understand
 a new object_oriented language then study how it does object initialization and
 class initialization Because essentially what_happens in initialization is that all
 the features of the language are going to be interacting and you have to explain
 what all those interactions are and how they are sorted out in order to have a
 welldefined initialization procedure alright So now lets talk_about class
 initialization We wont talk_about object initialization well just talk_about
 initializing classes So this is how the the object that represents a class
 actually gets initialized when that class is first brought into the program And so
 the first_thing to know is that a class is initialized when a symbol in a class is
 first used okay not when the class is loaded alright So if you reference any
 symbol in the class at the first time that happens that will cause the class to be
 initialized And the reason for doing this is if you are going to have an error in
 class initialization th is will cause that error to happen in a predictable
 place So if you have an error and you run the you you have an error in class
 initialization if you run the program five times you_know that error will probably
 happen in the same place every time So itll be repeatable and predictable where
 the error occurs If instead we had the error happened where you loaded the class
 at the time that you loaded the class well the class might be loaded at lots of
 different times And and and so this this this error here the error in the
 initialization of the class would become nondeterministic if we didnt if we
 didnt delay the initialization until some deterministic point in the execution So
 now Ill discuss the procedure for initializing class objects in Java And
 the first_thing I should stress is that this idea of a class object is something
 that Java has that COOL does not have I mentioned this on the previous_slide But
 just to be completely clear what is a class object A class object is just what
 it sounds like it is the object for a class It represents a class Okay this
 is not an instance of the class This is an object which is the class okay So
 this is an object which is the class it has all the information about the class
 so you_know it tells you what the type of the class is what the fields of the
 class are and everything else So this is used for introspection or reflection
 And its necessary in Java because of features like dynamic loading So if you
 know if you want if you dynamically load a class though you want to be_able to use
 that class you have to have some way of querying what the what kinds of methods
 and things the class has and that is what the class object is for So there is one
 object there is one class object for each class in Java alright So when you load
 a class the first_thing you have do is to initialize the class object And how is
 that done Well we lock the class object for the class alright And if th at if
 that object is already locked by another thread then well simply wait on the
 lock okay So we will wait until somebody tells_us that its okay to
 proceed Now once we obtain the lock on the class we have to do a check to see if
 the class is already being initialized alright So and it could turn out that it
 is our thread it is the same thread is already initializing the class And how
 could that happen Well remember that a class can have fields of the same type
 So I could have a class of class called X and then it could have a field of type X
 in it And the way classes are going to be initialize if were_going to have to
 initialize the class itself and then and were_going to do that by recursively
 initializing the classes for all the fields or at_least making sure of the
 classes for all the fields are initialize And if we have a recursive structure here
 with the same class mention in a field as in a name as the name of the enclosing
 class then we will get the situation_where the thread initializing the class
 may attempt to initialize the same class again So if we discover that were
 already initializing this class we simply release the lock and we turn Now another
 possibility is that the class is already initialized So if when we finally get
 the lock we discover that some other thread got in there and initialized the
 class before we have a chance to well then theres nothing to do and we just
 return normally alright Now if neither one of these things is true okay if we
 get the lock and we discover that the class is not already initialized and that
 were not already in the process of initializing the class then we will mark
 the class to to note the initialization is in progress by this thread okay So
 well indicate you_know this class is being initialized and that we are
 initializing it and then well unlock the class Alright the next_thing that
 happens is well have to initialize the superclass and that will m ean initial
 and then well initialize all the fields in textual order But because Java has
 what are called static and final fields we will initialize those first okay So
 static final fields will get initialized before any_other fields in textual order
 And of course we have to give every field of default_value before
 initialization just as in COOL So this step step five is very_similar to what
 goes on in COOL Now if theres an error during the initialization so some part of
 the initialization throws an_exception then were_going to mark the class as
 erroneous okay were_going to mark this class as no good and cant be used and
 and thats the best we can do So if theres an_exception during
 initialization we just have to give up on that class And so it gets a special mark
 on it saying that its erroneous And if there are no errors if we succeed in
 initializing the class and with and without any errors then were_going to
 lock the class again We will label the class as initialized alright And then
 well notify the threads that are waiting on the class object So Anybody who was
 locked waiting on the class object will now be alerted that the object is is
 ready and then well unlock the class Okay and so thats a rough outline of how
 class initialization in Java works I skipped over a few things and
 oversimplified it a bit So this isnt the complete description but these are the
 main points and they and they illustrate how the various features of the language
 have to interact So you have to worry_about concurrency you have to worry_about
 exceptions you have to worry_about static and final fields you have to worry_about
 inheritance I_mean all these things have to be dealt with together in the design of
 a single algorithm to do class initialization Stepping back for a moment
 this discussion of class initialization in Java illustrates a general point about
 designing complex systems So in any system with a certain number of featu res
 and every system is going to have some number of features lets call it N
 because you want to provide some functionality obviously the thing the
 systems suppose to d so its going to have features to do those things But as you
 add features you get lots of interactions potential interactions
 between the features and if we think_about even just the pairwise
 interactions If I have N features then Ill have I dont_know about N2
 pairwise feature interactions And the point there of course is that as I add
 features the number of possible interactions grows super linearly in the
 number of features I_mean it grows much_more quickly than the number of features
 And so adding the next feature youre going to have to consider all of the
 previous features that you already have in the system and how this new feature
 affects them and this is why it becomes very difficult to extend or build systems
 that have a lot of features alright And this is just the pairwise features These
 are just this is just considering pairwise interactions between one feature
 and another If I have to start worrying about subsets of features Im thinking
 about how all possible subsets of features might interact with each other well then
 this step this number of of potential interactions will grow not just it will
 grow in fact exponentially So itd be you_know way more_than quadratic And the
 bottom line here is that big featurefull systems are hard to understand You_know
 this is you_know a general lesson in Computer Science and any kind of
 discipline that wants to design complex systems and and this lesson applies to
 programming_languages It applies to every other kind of software system that you
 might want to build But and somehow it has a particular force in programming
 languages because these interactions between the features you_know these are
 the features of the programming_language they happen at a very fine grain And
 these things can be really can b e composed arbitrarily and so you really do
 have to work out in language design you_know what all the interactions are in
 order to have a language that people that programmers can actually understand and
 use productively Alright And that really I_think is the big big idea that one of
 the big ideas that weve talked_about throughout the course And and I hope one
 of the things that you would take away from this lecture at_least in particular
 So to summarize and to conclude our_discussion of Java I_think Java is a is
 a welldone language By production standards it is extremely welldone So
 its one of the best designed and best specified languages thats in in use
 today It brought several important ideas into the main stream So when it was new
 it brought ideas that had been around for a long_time but had not found their way
 into a production language that was very very widely_used and in particular Java
 was the first language to be very widely_used in in commercial settings They had
 strong sets of typing there that had real guarantees they were you_know provided
 by the typesystem and also there was a manage language and had a garbage
 collected memory But that doesnt mean its perfect And it and Java also
 includes some features that at the time that it was designed that we didnt fully
 understand and I would say you_know that this are probably the rough areas where
 theres still some roughness in the in in the Java design So things_like the
 way the memory semantics work in the presence of concurrency you_know
 probably still has most people would agree I_think you now has some problems and
 some some little gray areas that as a program you probably want to stay out of
 And the other thing is that Java has a lot of features And as I said before when you
 have a lot of features youre going to have even more feature interactions and
 that leads to complexity that becomes difficult to manage
 Hello In this video_were going to talk_about something that Ive referred to as the economy
 of programming_languages
 So the idea_behind this video is that before we get into the
 details of how languages are implemented or designed I wanted to say something about
 how languages work in the real world and why certain languages are used and others are not
 And if you look around theres actually a few obvious questions that come up to anybody
 who thinks about programming_languages for more_than a few_minutes One question is
 why are there so many of these things We have hundreds if not thousands of programming
 languages in everyday use and why do all these things need to exist Why wouldnt one
 programming_language for example be enough
 A related question but slightly different is why are there new programming_languages
 That given that we have so many programming_languages already what is the need for new
 ones to be created And finally how do we know a good programming_language when we see
 it So what makes a good programming_language and what makes a bad programming_language
 I just want to spend this video talking_about these three questions And as well see I
 think the answers to these questions are largely independent of the technical aspects of language
 design and implementation But very interesting in their_own right
 So lets begin_with the question of why there are so many programming_languages And at
 least a partial answer to this question is not too hard to come by If you think for
 a few_minutes youd realize that the application_domains for programming have very distinctive
 and conflicting needs That is its very hard to design one language that would actually
 do everything in every situation for all programmers
 And lets just go_through some examples One domain that you might not think_about very
 much is scientific computing So these are all the big calculations that are done for
 engineering applications primarily but also big science and long running experiments
 simulation experiments
 And what are the needs for such computations Well typically you need very good floating
 point support Ill abbreviate that as FP You need good support for arrays and operations
 on arrays because the most_common data type in most scientific applications is larger
 arrays of floating_point numbers And then you also need parallelism
 Today to get sufficient
 performance you really have to exploit parallelism in these applications
 And its not every language actually supports all of these things well This is actually
 not an exhaustive list of the things you need but a few distinctive things that are needed
 But one language that has traditionally done a very good_job of supporting these things
 is Fortran And Fortran is still heavily used in the scientific community
 It was originally designed for scientific applications If you recall that the name
 means formula translation And it has evolved over time It_doesnt really look much like
 the original language anymore but its always retain this core constituency in scientific
 computing and remains one of the leading languages in that domain
 Now a completely_different kind of a domain is business applications And so what do you
 need here Well so here youre going to need things_like persistence You dont want to
 lose your data Businesses go to a lot of trouble to get the data and they need a way
 to hold onto it and they want that to be extremely reliable
 Youre going to need good report facilities Because typically you want to do something
 with the data So you need good facilities for report generation And also you want
 to be_able to exploit the data The datas actually in many modern businesses one of
 the most valuable assets and so you need good facilities for asking questions about your
 data Lets call it data analysis
 And again this is not an exhaustive list of things that you need but it is representative
 I would say And probably the most_common or one of the most_common used languages for
 this class of applications is SQL the database query language So relational databases and
 their associated programming_language languages I should_say but most notably SQL really
 dominate in this application_domain
 And then another domain lets do one more is systems_programming
 So by this I_mean things_like embedded systems things to control devices operating systems things_like that
 And what are the characteristics here Well we need very low level control of the resources
 The whole point of systems_programming is to do a good_job of managing resources and
 so we really want fine grained control over the resources And often theres a time aspect
 so you might have some real time constraints So you need to be_able to reason about time
 Because these are actually again devices and they need to be_able to react within certain
 amount of time if its a network device or something_like that you need to be responsive
 to the network Lots and lots of examples where timing is important And these are just
 two aspects and Im a little_bit Im running out of space here so Ill just stop with that
 But again these are representative of the kinds of things you need in systems_programming
 And probably today still the most widely_used systems_programming language or family systems
 of programming_languages is the C and to some extent C family of languages
 And as you can see the requirements in these different domains are just completely
 different from each other Whats important in one domain or most_important in one
 domain is not the same as in another domain And its easy I_think to imagine at_least
 that it would be difficult to integrate all of these into one system that would do a good
 job on all of these things
 That brings us to our second question Why are there new programming_languages
 There_are so many languages in existence why would we ever need to design a new one And Im
 going to begin the answer to this question with an observation that at first glance has
 nothing to do with the question at all So let_me just take a moment to explain it
 I claim that programmer_training is the dominant_cost for a programming_language And I_think
 this is really important so just going to emphasize the bit thats important here Its
 the programmer_training The cost of educating the programmers in the language So we think
 about a programming_language there are several things that have to happen for that language
 to get used Somebody has to design it But thats really not very expensive Thats just
 one or very few people typically Somebody has to build a compiler but that is also
 not actually all that expensive Maybe 10 to 20 people for a really large compiler project
 can build quite a good compiler
 The real cost is in all the users and educating them So if you have thousands or hundreds
 of thousands or millions of users of the language the time and money that it takes to teach
 them all the language is really the dominant_cost And here I dont mean just the actual
 dollar expense of buying textbooks and taking classes and things_like that
 Its also the fact that the programmers have to decide its_worth it for them to learn
 this language and many programmers learn on their_own time but thats a use of their
 time and the expense of their time is a real economic cost And so if you think_about the
 number of hours that it takes to teach a population of a million programmers a language thats
 really quite a significant economic investment
 Now from this observation we can make a couple of predictions pretty easily And again these
 are just predictions now that follow from this claim If you believe that its true
 So let_me erase it and fix it So first prediction is that widely_used languages will be slow
 to change And why should that be true Well if I make a change to a language of lots of
 people use I have to educate everybody in that community about the change And so even
 relatively minor language extensions small changes to syntax small new features even
 just simple changes in the interface of the compiler if you have a lot of users it takes
 a very long_time and is quite expensive to teach them all about that
 So as languages become widely_used the rate of change their rate of change will slow
 down And this predicts over time as the world of programming grows as we have more more
 programmers in the world we would expect the most popular languages which will have
 larger and larger user bases so larger and larger programmer basis to become more and
 more ossified To evolve more and more slowly And I_think actually what you see in practice
 is very consistent_with that prediction
 Now at the other end of the spectrum this same observation makes an almost what appears
 to be contradictory prediction which is that easy to start its easy to start a new language
 That in fact the cost of starting up a new language is very low And why is that Well
 because you start with zero users and so there is essentially zero training_cost at the beginning
 and then even when you have just a few users the cost of teaching them the changes in the
 language is not very high And its so new languages can evolve much_more quickly They
 can adapt much_more quickly to changing situations And its just not very costly to experiment
 with a new language at all And theres a tension between these two things
 When is a programmer going to choose between a widely_used existing language that perhaps
 doesnt_change very quickly and a brand new language Well theyre going to choose it
 if the productivity if their productivity exceeds the training_cost So if they perceive
 that by spending a little_bit of time and money to learn this new language theyre
 going to be much_more productive over a relatively short period of time then theyre going to
 make the switch
 So when is this likely to happen Well putting this all together languages are most likely
 to be adopted to fill a void And again this is a prediction that follows from the fact that programmer
 training is the main cost What do I_mean by this Well what I_mean is that programming
 languages exist for purpose people use them to get work done And because were still
 in the middle of the information revolution and there are new application_domains coming
 along all the time
 So there are new kinds of programming that emerge every few years or even more often
 than that So just in terms of recent history mobile applications are now something thats
 relatively new And theres a lot of new technology being built up to support mobile computing
 A few years ago it was the internet itself was a new programming platform and a bunch
 of new programming_languages like Java in particular got started during that time
 So a new programming niche is open up because the technology changes that what people want
 to do with software changes And this creates new opportunities for languages The old languages
 are slow to change and so they have some difficulty in adapting to fit these new domains And
 they arent really necessarily well suited to them for the reasons we talked_about on
 the previous_slide with the previous question because its hard to have one language that
 incorporates all the features you would want
 And so there are so these languages are not necessarily perfect for these application
 domains Theyre slow to adapt to the new situation And this tends to call forth new
 languages So when theres a new opportunity and some application_domain If there are
 enough programmers to support the language often a new language will arise
 Just want to point out another prediction that can be made from this one observation
 That programmer_training and Ill underline that is a dominant_cost per programming_language
 And that is that new languages tend to look like old languages That is that new languages are rarely if ever completely
 new They have a family resemblance to some predecessor language sometimes a number of
 predecessor languages
 And why is that Well partly that its hard to think of truly new things But also I_think
 if theres an economic benefit to this namely that it reduces the training_cost by having
 your new language look like an old language by leveraging off what people already know
 about the old language you make it easier for people to learn the language and make
 them learn it more_quickly And the most classic example of this is a Java versus C where
 Java was designed to look a lot like C And that was I_think very conscious to make
 it easy for all of the existing C programmers to start programming in Java
 Finally we can ask ourselves what is a good programming_language And here unfortunately
 the situation is much less clear I would just make one claim that there is no and Ill
 emphasize no universally_accepted metric for language design And what I_mean by that
 Well I guess the most_important part of this statement is the universally_accepted bit
 So I_mean that people dont agree on what makes a good language
 There_are lots of metrics out there and people have proposed lots of ways of measuring programming
 languages but most people dont believe that these are very good measures and there is
 certainly no consensus If you just look_at the world of programmers they cant agree
 on what the best language is and to convince yourself of this just go and take a look
 at any of the many news group posts where people get into a semi religious arguments
 about why one group of languages or particular language is better_than another language
 But even in the research community in the scientific community in among people who design
 languages I would say there is no universally_accepted consensus on what makes a good language
 And to just kind of illustrate the difficulties in trying to come up with such a metric let
 me discuss one that Ive heard people propose in all seriousness and that is that a good
 language is one people use And let_me put a question mark on that because I dont believe this
 statement
 And I_think a moments reflection with a moments reflection I can convince you that this isnt
 a great measure On the positive side I guess the argument for this is that its a very
 clear measure It measures the popularity of the language How many people are actually
 using it and presumably languages are more widely_used for a good reason In some sense
 perhaps they are better languages But this would imply if you believe this and follow
 it its logical conclusion that Visual_Basic is the best language above all other programming
 languages
 And Ive nothing against Visual_Basic Its a well designed system but I dont even think
 the designers of Visual_Basic would claim that it is in fact the worlds best programming
 language And as we saw in the discussion that we just had there are many_many other
 factors besides technical excellence that go into whether a programming_languages is
 widely_used or not
 And in fact technical excellence is probably not even the most_important reason that a
 language might be used It has much_more to do with whether it addresses a niche or application
 domain for which there isnt a better tool And then once its established and has lots
 of users of course theres inertia in history that aided in surviving And thats why we
 still have Fortran and Cobalt and lots of other languages from long long ago that we
 could if we were starting over today designed much better
 So to conclude this video on the economy of programming_languages I_think the two most
 important things to remember are that application_domains have conflicting needs and therefore
 it is difficult to design one system that incorporates everything that you would like
 to have So you cant get all the features that you would like into a single system in
 a coherent design at_least its very hard to do that and so it takes a lot of time to
 add new features to existing systems
 And the second point is that programmer_training is the dominant_cost for programming
 language And together these two things these two observations these really explain why
 we get new programming_languages because the old languages are difficult to change
 and when we have new opportunities its often easier and more direct to just design a new
 language for those rather_than trying to move the entire community of programmers and an
 existing systems to accommodate those new applications
 Welcome_back This is the first video in our long series of the
 implementation of compilers The call from last_time that a compiler has five phases
 Were_gonna begin by talking_about lexical_analysis and this will probably take us
 three to four videos to get through at_least and then well we will be moving on
 in order to the other phases Lets start by looking_at a small code fragment The
 goal of lexical_analysis is to divide this piece of code up Into lexical units so
 things_like the keyword if the variable_names i n j and the relational operator
 doubleequals and so on Now as a human being this is As we discussed last_time
 this is a very easy thing to do because there are all kinds of visual clues about
 where the units lie Where the boundaries between the different units lie but a
 program like lexical_analyzer It_doesnt have that kind of luxury In_fact what
 the what the likes of analyzer will see is something that looks more like this So
 here I overwritten the code out just as a string with all the white_space symbols
 included and is from from this representation this is a linear string
 you can think of this as bytes in the file that the lexical_analyzer has to work and
 its going to mark through placing dividers between the different units So
 it will recognize that theres a division there between the white_space and the
 keyword Then a division after the keyword and theres more a wide space the open
 paren the i another wide space double_equals and so on and it goes through
 drawing these lines diving up The the string into its lexical unit So I wont
 finish the whole_thing but you should get the idea Now it doesnt just place these
 dividers in the string however It_doesnt just recognize the substrings It also
 needs to classify the different elements of the string according to their role We
 call these token_classes Or sometimes Ill just call it the class of the token
 And in English these roles are things_like noun verb adjective Okay and there
 is ther e are many more or at_least or some more And in the programming
 language the classes the token_classes would be things_like identifiers
 Keywords I and then individual pieces of syntax like an open_paren or a close
 paren those are the classes by themselves A numbers And again there
 are more classes but theres a thick set of classes and each one of these
 corresponds to some set of strings that could appear in a program So token
 classes correspond to sets of strings And inaudible strings can be described
 relatively straightforwardly so for example The token_class of identifiers in
 most programming_languages might be something_like strings of letters or
 digits starting_with a letter So for example a variable_name or identifier
 could be something_like a1 or it could be f00 or it could be b17 all of those
 would be be valid identifiers and often often theyll be additional characters
 that allowed identifiers but thats the basic_idea Very very often The main
 restriction identifiers that they have to start with a letter An integer and
 typical definition of an_integer is a nonempty string of digits So something
 like zero or twelve Okay One followed_by two I should_say is actually a string of
 number in this case And and yeah it is actually whether admit some numbers you
 might not think of Things_like 001 would be a valid representation of a number or
 even 00 could be a valid integer according to this definition Keywords are typically
 just a fix set of reserved words and so here Ive listed a few else if begin
 and so on And then white_space as itself a token_class so we actually have to say
 in that string which is the representation of the program what every character in
 that string what token or what token_class its a part of What every substring
 is a part of and that includes the white_space So for example if we have a series
 of three_blanks if I say if and then an open_paren and I have three_blanks in
 here these three blank s would be grouped together as white_space So the goal of
 lexical_analysis is to classify substrings of the program according to their role
 This is the the token_class okay Is it a keyword a variable identifier And then
 to communicate these tokens to the parser So drawing a picture here lets
 switch_colors The lexical_analyzer communicates with the parser Okay and the
 functionality here is that the lexical_analyzer takes in a string Typically
 stored up also just a sequence of bytes and then when inaudible to the parser is
 sequence or pairs which are the token_class And substring which I would say
 string here that that of which is the sets of string which is a part of the
 input along with the class the role that it plays in the in the language and this
 pair together is called a token So for example if my string is that f00 42
 all right then that will go to the lexical_analyzer and that will come Ill
 write_down here three tokens And these would be identifier Who Operator say
 equals And Integer excuse_me 42 And here I just left these things as strings
 to to emphasize that these are strings So this is not the number 42 at this point
 in time its its the string 42 which is a plays an_integer role in the programming
 language And then these and when the price that takes this input is this
 sequence of pairs So the lexical_analyzer essentially runs over the input string and
 chunks it up into the sequence of pairs where each pair is a token_class and a
 substring of the original input As we turn to the example from the beginning of
 the video here it is written out as a string And our goal now is to lexically
 analyze this fragment of code We want to go_through and identify the substrings
 that are tokens and also their token_classes So to do this were_gonna need
 some token_classes So lets give ourselves some of those to work with
 Well need white_space And and so this is sequences of blanks new lines tab
 things_like that with the keywords And well need variables which well call
 identifiers And well need integers and now Ill call those numbers Here and then
 were_going to have some other operations some other classes things_like open_paren
 close_paren and semi colon and these are interesting These three ae interesting
 because theyre single_character token_classes that is is a set of strings but
 is only is only one string in the set so the open_paren corresponds to exact
 inaudible strings that contain only open_paren So often the punctuation marks of
 the language are in token_classes all by themselves Another piece of punctuation
 that well add here is is assignments That will be a token_class by_itself
 because its such an important operation But the double_equals will class as a
 relational operator with this class as an operator put it up here Alright So now
 what were_going to do is were_gonna go_through and tokenized this string and Im
 going to write_down for each substring What class it is You_know Im just gonna
 use the first letter here of the class Its indicated just to save time so I
 dont have to write everything up Hence we change colors so we can do this in a
 different color So the first token here is white_space token and then that
 followed_by the F keyword So okay And then we have a blank here which is another
 white_space and then the open_paren which is its_own token_class so Ill just leave
 it to identify itself there and then we have an identifier Okay White space and
 then an operator the doubleequals Another blank so thats white_space
 followed_by another identifier followed_by close parens Again a punctuation mark in
 a token_class by_itself And then we have three white_space characters so those are
 group together as a white_space token Followed by another identifier and more
 white_space and then another single_character token the assignment operator
 white_space and a number And then sem i colon again and punctuation mark and a
 token_class by_itself Two white_space characters can group together What
 follows in is a keyword so it gets classified as in the keyword token_class
 Another run of white_space characters and then another identifier Theres actually
 a blank there where we almost covered it up without marks The assignment operator
 by_itself in a token_class white_space a number and finally the semi colon by
 itself And there is our tokenization Weve identified the substrings and weve
 also labeled each one with its token_class
 In this video_were gonna continue_our discussion of parsing with the idea of a
 derivation So a derivation is a sequence of productions so beginning_with the start
 symbol we can apply productions one at a time In sequence and that produces a
 derivation And a derivation can be drawn in a different way instead of as a linear
 sequence of replacements we can draw it as a tree So for example if I have a
 nonterminal x it appears in a derivation then when I replace x I can represent
 that by making the children Of x the left_hand side of the rule that I used to
 replace x So I applied production x goes to y1 to yn I add the y1 to yn is
 children of x in the tree that Im building up Lets do an example Here is
 our simple grammar of arithmetic expressions and lets_consider this
 particular string id_id id So what were_going to do now is were_going to
 parse this string and were_going to show how to produce a derivation for the string
 and also at the same time build the tree And here it is Over here there is
 derivation beginning in e and ending in the string that were_interested in with
 one production applied each_step along the way and here is the corresponding tree and
 this is called a parse_tree This is a parse_tree of this expression or this
 input string So lets walk_through this derivation in detail The right side in
 red were_going to have the tree that were building up And on the left side in
 blue were_going to have the steps in the derivation that weve taken so_far So
 initially our derivation consists of just the start_symbol e and our tree consists
 of just the root which is also the start_symbol So the first step is that we have
 a production e goes to e e and what that means is over on the tree we take the
 root of the tree and we make we give it three children e ne So now we replace
 the first t by e z We use the production e goes to e z and that means
 we take the first e in the tree and we give it to three children e
 inaudible Continuing along we take the fi rst e here that remains in this
 expression and we replace it by id which means we make id a child of the left most
 e in the tree that were building And then we replace the second e by id using
 the production e goes to id and finally we use the same_thing with the third e and
 now we have completed our Parse_Tree So here again from the start_symbol to the
 string we were_interested in parsing and in the process we built up this Parse_Tree
 of the expression Now there are lots of interesting things to say about parse
 trees So first of all parse_trees have terminals at the leaves and nonterminals
 at the interior_nodes and furthermore inorder traversal of the leaves is the
 original input So lets back up and look_at our example and confirm all this If we
 look_at the leaves we can see that they are all terminals Okay And the interior
 nodes are all nonterminals In this case its only one nonterminal in our language
 all the interior_nodes are e and the leaves are the terminals of the string
 And then we can see if we do an inward reversal of the leaves we get exactly
 this input string that we started with Furthermore the Parse_Tree shows the
 association of the operations and the input string does not So you may notice
 here that the way that this Parse_Tree is constructed the times binds more tightly
 than the plus because the times is a subtree Of the tree containing plus And
 so this means that we would do the e e first before we would add e and some some
 of you may have wondered well how did I know To pick this Parse_Tree because
 actually if you think_about it theres another derivation Actually there are
 several derivations that will give me a different Parse_Tree where the plus where
 the times is towards the root and the plus is nested inside the times So lets not
 worry_about that for right now and lets just say that somehow we knew that this
 was the Parse_Tree we wanted and I gave you a derivation that produces that Parse
 Tree Continuing on the previous derivation I_showed you is actually a very
 special derivation Its whats called a leftmost_derivation where each_step will
 replace the leftmost_nonterminal in our string of terminals and nonterminals And
 theres a natural and equivalent notion of a rightmost_derivation and here it is
 Here is a rightmost_derivation for the same string Again beginning_with the
 start_symbol ending with a string were_interested in And notice that at each
 step were replacing the rightmost nonterminal So here we replace the only
 nonterminal e and we get e c And then in the second step we replace the second
 nonterminal e with id and so on for the rest of the string So lets just
 illustrate this entirely with our little picture here of the tree and the
 derivation simultaneously so once_again over_here is our tree and this is the
 root the start_symbol e and and in blue is our derivation so we begin by replacing
 e by e e Thats the only nonterminal so its the rightmost one and then working
 from the right side of the tree we replace the right e by id and then the
 left id gets replaced_by e z And now the right most e that remains is replaced
 by id and finally the only e that remains is also replaced_by id Now I want to
 point out that the rightmost and leftmost derivations I_showed you have exactly the
 same Parse_Tree And this was not an accident Every Parse_Tree has a rightmost
 and a leftmost_derivation Its just about the order in which the branches are added
 So for example if I have the first production e goes to e e now I have a
 choice on how to build my tree I can either work on This subtree or I can
 work on that subtree And if I build this one first that will be a rightmost
 derivation If I continue to always work on the rightmost nonterminal of course
 And if I work on this one first I can use that to do a leftmost_derivation Now its
 important also to realize that there are many derivations besides rightmost and
 leftmost I could I could choose nonterminals in some random order to do
 my replacements But th_e rightmost and leftmost ones are the ones that were most
 concerned with
 In this video_were going to continue_our discussion of topdown parsing algorithms
 with another strategy called predictive_parsing So predictive_parsing is a lot
 like recursive_descent Its still a topdown parser But the parser is able to
 predict which production to use And its never wrong inaudible parser is always
 able to guess correctly which production will yield to will lead to a successful
 parse if any production Well it lead to a successful_parse And it does have some
 two ways first of all it looks at the next few tokens so it uses lookahead to
 try to figure_out which production should be used So based on whats coming_up in
 the input string but also it restricts the grammars So this this is only works
 for a restricted form of grammars And theres the advantage is that theres_no
 back tracking involved and so the parser is completely deterministic if you were to
 try alternatives The predictive parsers accept what are called the LLK grammars
 And this is a really cryptic name and so let_me explain it The first L stands_for
 lefttoright scan So that means were starting at the left end of the input and
 reading left to right And in fact thats what we always do so all the techniques
 that we looked_at look_at will have an L in the first_position The second L stands
 for a leftmost_derivation So we are constructing a leftmost_derivation That
 means were always working_on the leftmost_nonterminal in the parse_tree And K
 here stands_for K tokens of look_ahead And in practice while the theory is
 developed for arbitrary k in practice k is always equal to one And so in
 fact well only discuss the ks k equals to one in these videos To_review
 in recursive_descent parsing in each_step there may be many choices of production to
 use and so we need to use backtracking to undo bad choices In an LL1 parser in
 every step theres only going to be one choice of productions of possible
 production to use And and what does that mean Well it means that if I have an
 input string if I have a configuration of the parser where I have some terminal
 symbols omega and a non_terminal a you_know possibly now followed_by some other
 stuff there could be terminals and nonterminals but again a here is the
 leftmost_nonterminal And the next_input Is a token T Well then there is exactly
 one production A goes to alpha on input T Okay theres only one possible production
 that we can use And any_other production is guaranteed to be incorrect Now it can
 be that that even A goes to Alpha wont succeed It could be that we will be in a
 situation_where theres_no production we could use But in inaudible parser
 there will always be at most one that we could use So in this case we would chose
 to rewrite the string to Omega Alpha Beta Lets_take a look_at our_favorite grammar
 the one weve_been using for the last_couple of videos We can see an issue here
 with using this grammar for a predictive parser Take a look_at the first two
 productions for T They both begin_with Ns And so if I tell you that the next
 terminal in the input_stream as were parsing along is an_integer that doesnt
 really help you in trying to distinguish between these two productions in deciding
 deciding which one to use So in fact with only one token of look_ahead I cant
 choose between these two productions And that is not the only problem actually so
 we have a problem with T but the same problem exist with E We can see that here
 both production for E begin_with the nonterminal T and it is really clear
 what were to make of that because a T against a nonterminal terminal so how we
 even do the prediction but the fact that they begin_with the same_thing suggest
 that its not going to be easy for us to predict which production to use based of
 only a single token of look_ahead So what we need to do here is we need to change
 the grammar This grammar is actually unacceptable for predictive_parsing or at
 least for LL1_parsing And we need to do something thats called left factoring the
 grammar So the idea_behind left factoring is to eliminate the common prefixes of
 multiple productions for one non_terminal So thats a mouthful Lets do an example
 Lets begin_with the productions for E And we can see again that E that both
 productions for E begin_with the same the same prefix What were_going to do is
 just factor out that common_prefix into a single production So were_going to have
 one production where E goes to T And then were_going to have multiple suffixes So
 lets introduce a new non_terminal X that will handle the rest So here we have E
 goes to TX So it says that everything that E produces begins_with T and thats
 consistent_with these two productions And now we have to write another production
 for X that handles the rest And what would that be Well one possibility is if
 were in this production we need to have a Plus E and then in this production
 theres nothing So thats easy to handle right One_possibility for X as it goes to
 Plus E and the other_possibility as it goes to Epsilon And now you can see the
 general idea We factor other common_prefix we have one production that deals
 with the prefix and then we write and then we introduce a non_terminal or the
 different suffixes And then we just have multiple productions one for each
 possible suffix And you can see what this is going to do This is effectively going
 to delay the decision about which production were using So instead of
 having to decide immediately which production were_going to use for E Here
 in this grammar we wait until weve_already seen the T whatever is derived
 from the T And then we have to decide_whether the rest of the production is a
 plus E or the empty_string Lets do the other set of productions So we have tea
 goes to and now the common_prefix is int that we want to eliminate So were_going
 to have just one production that begins_with int and then well have a new a
 nonterminal to stand for the various possible suffixes And now we also have
 another production that doesnt h ave anything to do with int and so well just
 leave that one alone that production just stays here Because it already begins_with
 something different we wont have any trouble predicting between these two
 possible productions these two possible productions And now we have to write The
 productions for Y And again we just take the suffixes of the productions that we
 left_factored and write them down as alternatives So one is empty and the
 other one is times T So we wind_up with times T or epsilon
 This is the first of what will be a considerable sequence of videos on bottom
 up_parsing The first_thing to know is that bottom_up parsing is more general
 than deterministic top_down parsing So recall we talked_about recursive_descent
 which is a completely general parsing_algorithm but requires backtracking And
 now were focused on deterministic techniques and we talked_about LL one or
 predictive_parsing last_time And now were_gonna switch shift gears and talk
 about bottom_up parsing And it_turns out though even the bottom_up parsing is more
 general its just as efficient and it uses all of the ideas that we learned in
 top_down parsing And in fact bottom_up Parsing is the preferred method thats
 using most of the parser generator tools So one good thing about bottom_up
 parcers is they dont need left_factored grammars so we can revert to the natural
 grammar for our example and natural here is in quotes because we still have to
 encode the precedence of plus and times so bottom_up parcers arent going to deal
 with ambiguous grammars And lets just as an example consider how a bottom_up
 parcer would work on the following typical input string So the first_thing
 to know about bottom_up parsing is that it reduces what we call reduces a string
 into the start_symbol by inverting productions by running productions
 backwards So heres an example On the left_hand side is the sequence of states
 of the string On the right_hand side are the productions that were used And the
 thing to observe lets just look_at the very first step Is that we began_with the
 entire string We began_with the the the string of terminals And we picked some of
 those terminals In this case just one this particular Int right here And we ran
 a production backwards We replaced the Int here by the left side of the
 production We began_with we matched the right side of the production Int and we
 replaced it by the left side So Int went backwards here to T And then in the next
 step we took Int_times T this substr ing of The string that were working_on And
 we replace it by the lefthand_side of this production N times T was replaced_by
 T and so on At each_step here were matching some portion of the string And
 Im underlining the portion thats being replaced at each_step And were running
 and that matches the right_hand side of sum production And then were replacing
 that substring by the left_hand side And finally this entire string here is
 replaced_by E And we wind_up at the start_symbol So we began_with an input string
 This is our input string up here Alright put string of tokens and we end with the
 start_symbol down_here And if you read the moves in this direction If you start at
 the bottom and read towards the top Well these are just productions And in fact
 this whole_thing is a derivation This is just a normal derivation going from bottom
 to top But in this direction when we run it backwards beginning_with the string
 towards the start_symbol we call these reductions And I havent told you exactly
 how we decided what reductions to do and you might wonder well how I knew to do
 this particular sequence of reductions Well heres another interesting property
 of bottomup_parsing So if you read the productions_backwards they trace a
 rightmost_derivation so if we begin here with e so were gonnaso remember the
 parser is actually going in this direction so this is the direction of
 parsing here But now were_gonna look_at the steps the parser took in reverse and
 were_going to see that it was in fact a rightmost_derivation So here E went to
 TE Well E was the only non_terminal But then E here is the one thats
 expanded its the rightmost non_terminal And then this T is expanded its also the
 rightmost non_terminal to get int And now this T is the rightmost tom non
 terminal Its expanded to get Int_times T And then this is the only end right
 most non_terminal and so we wind_up with the whole input string Int_times Int plus
 int And this leads us to The first important f act about bottom_up parsing
 which is that a bottom_up parser_traces a rightmost_derivation in reverse all
 right So if youre ever having trouble with bottom_up parsing its always
 helpful to go_back to this basic fact Bottom up parser_traces a rightmost
 derivation but it does so in reverse by using reductions instead of productions
 So heres the series of reductions again Shown on the left And here is the parse
 tree that is constructed from those reductions And this is actually I_think
 a very helpful picture if we animate it to see the sequence of steps and to see
 what a bottom_up parser is really doing So here we begin_with the input string
 Over here And we show the same input string here And now were just going to
 walk_through the sequence of steps that the bottom_up parser takes A series of
 reductions And show how it builds an entire parse_tree And the basic_idea is
 that in each_step were performing a reduction And remember when we do a
 reduction we replace the children of the right_hand side of sum production by its
 left_hand side And just like when we were doing top_down parson well we will do the
 same_thing here In the input and then we make T its parent And now you can see
 whats going to happen A top_down parser begins_with the start_symbol and produces
 the tree incrementally by expanding some non_terminal at the frontier At the
 current at a current leaf of the partially constructed parse_tree The
 bottom_up parsers is going to begin_with all the leaves of the eventual pars tree
 The entire input And its going to build little trees on top of those And its
 going to be pasting together all the subtrees that its put together so_far to
 build the complete tree Lets walk a few more steps and see how that happens So in
 the next step we go from Int_times T to T so Int_times and the sub tree rooted at
 the other T become children of this non_terminal T and you can see weve taken
 these three sub trees here and pasted them together into a larger tree So as we
 go throug h the parcer bigger and bigger portions of the original input are gonna
 be pasted together into larger and larger trees And the next reduction takes the
 Int to the far into the input and reduces it to T And that gets reduced to E and
 then At the very end the three remaining sub trees are all pasted together into one
 parse_tree for the whole_thing with a start_symbol as the root
 In this video_were gonna continue_our discussion of bottomup_parsing with the
 main strategy used by all bottomup parsers socalled shiftreduce parsing
 Here is a quick review of the most_important thing that we learned last_time
 that a bottom_up parser_traces a right most innervations in reverse Now this
 particular fact has an important consequence So lets think_about a state
 of a shift_reduced parse where we have string alpha_beta and omega and lets
 assume the next reduction is going to be the replaced beta by X Okay so remember
 were running productions_backwards Then I claim that Omega has to be a string of
 terminals And why is that Well if you think_about it if this is a rightmost
 innervations in reverse then when X is replaced when we take this if we look_at
 the forward step is the the backward step So remember the parser is running
 this way replacing data by X But if we think_about the rightmost innervations in
 the other direction then X has to be the rightmost nonterminal which means there
 are no nonterminals to the right of X and so all the Character all the tokens
 whatever it is in this string have to be terminal_symbols Now it_turns out that
 those terminal_symbols to the right of the right most nonterminal are exactly the
 unexamined input in bottom of parsley implementations That is if I have alpha
 X omega and Im and X is my right most to nonterminal then this is the input
 that we havent read yet This is unexamined Input And its gonna be useful
 to mark where we are in the parse where our where our input focus is And were
 gonna do that by using a vertical_bar So were_gonna just place drop a vertical
 bar Between the place_where we read everything to the left and weve actually
 been working_on this So this stuff to the left here can be terminals and
 nonterminals and we the parts that weve_seen all of that stuff And the stuff to
 the right is after the parts hasnt seen Now we dont_know whats out there
 although we do know its all terminal_symbols An d the vertical_bar is just
 gonna mark the dividing line between the two substrings To implement bottom_up
 parsing it_turns out we only needs two kinds of actions Shift moves and reduce
 moves And weve_already talked somewhat about reduce_moves and so we have to
 introduce shift_moves So lets do that now So a shift move reads one token of
 input And we can explain that or represent that by moving the vertical_bar
 one token to the right So if our input focus is here and we want to read one
 more token of input then we just move the vertical_bar over And this signifies that
 now the parser knows about that next terminal_symbol And now we can start
 working_on it It can do things We can match against it for the purposes of
 performing reductions Again the stuff out here to the right of the vertical_bar the
 parser hasnt seen yet And then a reduce move is to apply an inverse production at
 the right end of the left string So if in production a goes to xy and we have x and
 y here immediately to the left of the vertical_bar So this is our focus point
 okay and x and y the right_hand side of the reduction is right there Then we can
 do a reduction we can replace that right_hand side by the left_hand side and this
 is a reduce move Here is the example from the last_video and this is exactly the
 example just showing the reduced_moves now with the vertical_bar also shown So
 this shows where the input focus was at the point where each of the reductions was
 performed And whats missing of course now we know is the sequence of shift
 moves So here is the sequence of shift_moves and reduce_moves that take the
 initial input string to the start_symbol So lets walk_through this in more_detail
 So were_going to go step by step And were_going to show each shift and each
 reduce move And now in addition to our input string down_here we also have a
 pointer showing where the where in the input we are So initially we havent_seen
 any of the input and our input pointer is to the left of the entire str ing So the
 first move is to do a shift And then we do another shift and then we do another
 shift And now just look_at the example from before if you look back at that at
 that example you_know the next_thing we need to do is to reduce So remember were
 only allowed to reduce to the left of the arrows So we can only reduce over on
 this side of the arrow So we always have to read enough of the input before we can
 perform a reduced move And then we perform another reduce move okay And
 then it_turns out the next_thing to do is two shift_moves and we havent explained
 yet how we know_whether to shift or reduce were_going to get there Im just showing
 that there exists a sequence of shift and reduce_moves that succeed in parsing this
 example Now weve shifted the entire input onto this sorry weve weve We
 shifted over the entire input so theres_no more input to read And now all we can
 do is reduce_moves But fortunately there is a sequence of reduce_moves from this
 point that we can we can perform So here we reduce int and then we reduce T
 plus T Oh forgot we first reduce T to E and then we reduce T plus E back to the
 start_symbol
 PROFESSOR In this video_were going to begin our_discussion of global program optimization
 And it_turns out that in order to talk_about global optimization theres another topic
 that we have to deal_with first known as data flow analysis
 Lets begin by reviewing the simple basicblock optimizations In particular constant_propagation
 and dead code elimination So heres a little piece of code And youll notice that theres
 an assignment of a constant X And we know from our lecture in local_optimizations that
 that constant assignment could be propagated forward to the uses of X If the basic_block
 is in single_assignment form this is particularly easy to do And then if the value of X here
 is not used anyplace else in the program that statement is dead and can be_deleted
 So heres a simple example over a basic_block combining constant_propagation and dead code
 elimination
 Now these optimizations can be extended to an entire controlflow_graph So here now
 we have a nontrivial controlflow_graph Remember a controlflow_graph is a graph of basic_blocks
 where the nodes are the basic_blocks and the edges show the transfers of control between
 basic_blocks And so this first basic_block here has a testament an if statement And
 itll go to one basic_block if the test is true Into a different basic_block if the
 test is false
 Now in this controlflow_graph we can observe that X is assigned a constant And then down
 here there are uses of X And in fact in this particular case it is going to be safe
 to replace this use of X by the constant 3 So just like we propagated constants in a
 single basic_block we can also at_least in some circumstances propagate constants
 across entire controlflow graphs
 Now it_turns out that there are some situations in which its not safe to propagate constants
 So here again lets observe that we have an assignment of X an assignment of a constant
 X and down_here we have a use of X But we cannot replace this X here by 3 And why is
 that Well thats because over_here we have another assignment X where X gets the value
 4
 And the interesting thing about this example is that notice that X is only assigned constants
 So X is assigned a constant here and X is assigned a constant here but this value of
 X down_here is not known We_dont know which constant its going to be because if we come
 from this path in the execution then X will be 4 And if we come from this path the value
 of X will be 3 And so we cant replace x here by either one of those values and its
 not safe in this case to propagate constants to this use of X
 And so the question then is how do we know when it is OK to globally propagate constants
 Now for constant_propagation it_turns out theres a simple criteria To replace a use
 of x by a constant k we have to know the following fact that on every path to the use
 of x So every path that leads to the use of x the last assignment that x is x is
 equal to k So every single path that goes to x and this make_sense I_think intuitively
 we must have assigned a constant x along that path And that must in fact be the last
 assignment to x on each path
 Lets_take a look_at our example again and Ill switch_colors here So here we have the
 assignment X equals 3 and down_here we have the use of X And now what we need to do is
 we need to check in order to replace this X by 3 that on every path it arrives at this
 X It reaches this X that X is assigned 3 along that path Well there are only two
 paths Theres this path and theres this path And its easy to see that this assignment
 is on both of those paths And therefore the last assignment done on both paths on all
 paths is X equals 3 And therefore we can replace this X by 3
 In contrast with this use of X in this example we have a path here where X is assigned 3
 and we have a path here that reaches along this path Let_me draw the whole path actually
 And along this path the last assignment to X is 4 and so we cannot propagate any constant
 value to this use of X
 In general the correctness condition that a variable is assigned the same constant to
 lock all paths to a use of that variable is not that easy to check because quotall pathsquot
 includes paths that go around loops as well as paths that go_through conditionals as
 we saw in the example So checking these conditions is done through a family of techniques called
 quotglobal dataflow analysisquot and its designed specifically to check conditions like this
 And essentially global dataflow analysis is called quotglobalquot because it requires an
 analysis of the entire controlflow_graph
 Stepping back for a moment there are many global optimization tasks that we would like
 a compiler to perform A constant_propagation global_constant propagation is just one of
 those Now it_turns out that all of these global optimization problems share several
 common traits So first of all the optimization always depends_on knowing some property X
 at a particular point in the program So we want to know some piece of very local information
 So for example is X at a particular point in the program guaranteed to be a constant
 OK Thats the property for a constant_propagation
 Now however even_though we want to know some local fact something thats specific
 to a particular point in the program proving this fact requires knowledge of the entire
 program so at_least the entire controlflow_graph So as we saw in the case of constant
 propagation to figure_out whether X is a constant at a particular point in the program
 requires reasoning about all the paths that lead to that statement And that is a global
 property So when you think_about allpaths that could be a path from the entry_point
 of the method all the way through loops and across conditionals to the particular statement
 So in general thats a very hard problem to solve And for certain kinds of questions
 its really extremely expensive to solve it exactly The thing that saves us is that it
 is OK always to be conservative So typically for these optimizations and if we want to
 know some property X then what we really want to know is either X is definitely true
 So if we say that the property is true then we have to be right We cant make a mistake
 But it is always OK to say that you dont_know So its OK to give up and say we dont
 know_whether the property holds or not Because of the worst case we just dont do the optimization
 If we cant establish the conditions that would mean it was definitely correct to the
 optimizations then we want to play it safe and not do the optimization
 So having approximate techniques or techniques that dont always give the correct answer
 to the questions that we wanted to ask is OK as long as we are always right when we
 say that the property holds and otherwise we just say that we dont_know whether the
 property holds or not
 To sum up global dataflow analysis is a standard technique or family of techniques for solving
 problems with the characteristics that we just discussed And global_constant propagation
 is one example of an optimization that requires global dataflow analysis And in the next
 several_videos well be looking_at global_constant propagation and another dataflow
 analysis in more_detail
 In this video_were going to introduce another important concept in bottomup
 parsing the notion of a handle To_review bottom_up parsing is these two
 kinds of actions we have shift_moves which just read one token of input and
 move the vertical_bar one to the right And reduced_moves which replace the right
 hand_side of a production inaudible to the left of the vertical_bar by a
 production left_hand side So in this case the production must have been A goes
 to XY And also reviewing what we did in the last_video the left string can be
 implemented by a stack where the top of the stack is marked by the vertical_bar
 So shift pushes the terminal on to the stack and reduce pops zero or more symbols
 of the stack and thats gonna be the right_hand stack of some production And
 then its going to push one nonterminal on to the stack which is the left_hand
 side of that same production And the key question in bottom of parsing and the one
 we havent addressed at all yet is how do we decide when to shift and when to
 reduce So lets_take a look_at this example grammar And lets think_about a
 step of a parse where weve shifted one token onto the stack We have Nth on the
 stack and then we have times N plus N still to go that we havent_seen yet Now
 at this point we could decide to reduce by T goes to N because we have the production
 T goes to Nth right here And so we could then get into this particul potential
 state or this particular state where we have T on the stack and then the rest of
 the input that looks_like that A but you can see that this would be a mistake
 There is no production in the grammar that begins Hence T times Theres no
 production up here that looks_like T times And therefore if we were to to to
 make this move we would get_stuck We could continue to do reductions to
 rummage around in the string But we would never be_able to get back to the start
 symbol Because there is no way to deal a sub string that has t times something in
 it So what that shows us is that we dont always want to reduce just because we have
 the righthand_side of a production on top of the stack To repeat that even if
 theres the righthand_side of some production sitting right there on top of
 the stack it might be a mistake to do a reduction We might want to wait and do
 our reduction someplace else And the idea about how we decide is that we only want
 to reduce if the result can still be reduced to the start_symbol So lets_take
 a look_at a right most innervations So beginning_with the start_symbol we get to
 some state after after some number of steps where that means just an_arbitrary
 number of steps We get to some state X is the right most nonterminal and then the
 next step is to replace X with by the right_hand side of some production And
 remember again with bottom_up parsing the parsers are actually going in this
 direction okay So this is the reduction direction The derivation direction the
 production direction Because thats the easiest way to talk_about what strings are
 derived We wanna begin_with a start_symbol But the inaudible but the
 parsers actually going against the flow of these arrows Anyway if this is a
 rightmost_derivation Then we say that alpha_beta is a handle of alpha_beta
 omega And that just means that yes it would be okay in this situation to reduce
 beta to X And we could replace beta by X because its not a mistake We can still
 by some sequence of moves get back to the start_symbol You_know by by doing more
 reductions So handles formulize the intuition about where it is okay to do a
 reduction A handle is just a reduction that also allows further reduction back to
 the start_symbol And we clearly only want to do reduction at handles If we do a
 reduction at a place that is not a handle even_though it looks_like its the right
 hand_side or maybe actually be the right_hand side of some production that does
 not mean That its actually a handle and we might if we could reduce there we may
 get_stuck So all we said so_far is what a handle is Weve defined a handle We
 havent_said anything about how to find the handles And actually how we find the
 handles is gonna consume much of the rest of our_discussion of parsing
