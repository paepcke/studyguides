 Three-Address Code

 3addr-sect



 In three-address_code there is at most one operator on the right

 side of an instruction that is no built-up arithmetic

 expressions are permitted Thus a source-language expression like

 xyz might be translated_into the sequence of

 three-address_instructions

 center

 tabularl

 y_z



 x

 tabular

 center

 where and are compiler-generated

 temporary names This unraveling of multi-operator arithmetic

 expressions and of nested flow-of-control_statements makes

 three-address_code desirable for target-code generation and

 optimization as discussed in Chapters codegen-ch

 and code-op-ch The use of names for

 the intermediate values computed by a program allows three-address

 code to be rearranged easily



 ex

 Three-address_code is a linearized representation of a syntax_tree

 or a DAG in which explicit names correspond to the interior

 nodes of the graph The DAG in Fig exp-dag-fig is

 repeated in Fig tree-3code-fig together_with a

 corresponding three-address_code sequence

 ex



 figurehtbf

 A

 DAG and its corresponding three-address

 codetree-3code-fig

 figure



 Addresses and Instructions

 3code-kinds-subsection



 Three-address_code is built from two concepts addresses and

 instructions In object-oriented terms these concepts correspond

 to classes and the various kinds of addresses and instructions

 correspond to appropriate subclasses Alternatively three-address

 code can be_implemented using records with fields for the

 addresses records called quadruples and triples are discussed

 briefly in Section quad-subsub



 An address can be one of the following



 itemize



 A name For_convenience we allow source-program names

 to appear as addresses in three-address_code In an

 implementation a source name is replaced_by a pointer to its

 symbol-table_entry where all information_about the name is kept



 A constant In_practice a compiler must deal_with

 many different types of constants and variables Type conversions

 within expressions are considered in

 Section coerce-3code-subsect



 A compiler-generated_temporary It is useful

 especially in optimizing compilers to create a distinct name each

 time a temporary is needed These temporaries can be_combined if

 possible when registers are allocated to variables



 itemize





 We_now consider the common three-address_instructions used in the

 rest of this_book Symbolic labels will be used by instructions

 that alter the flow of control A symbolic label represents the

 index of a three-address_instruction in the sequence of

 instructions Actual indexes can be substituted for the labels

 either by making a separate pass or by backpatching discussed

 in Section backpatch-sect Here is a list of the common

 three-address_instruction forms



 enumerate



 Assignment instructions of the form

 where op is a binary arithmetic

 or logical operation and and are addresses



 Assignments of the form where op is a unary operation Essential unary operations include unary

 minus logical negation and conversion operators

 that for example convert an_integer to a floating-point_number



 Copy instructions of the form where

 is assigned the value of



 An unconditional_jump The three-address

 instruction with label is the next to be executed



 Conditional jumps of the form

 and These instructions execute

 the instruction with label next if is true and false

 respectively Otherwise the following three-address_instruction

 in sequence is executed next as usual



 Conditional jumps such_as

 which apply a

 relational operator ( etc) to and

 and execute the instruction with label next if stands

 in relation relop to If not the three-address

 instruction following

 is executed next in sequence



 Procedure calls and returns are implemented using the

 following instructions for parameters

 and

 for procedure and function

 calls respectively and where

 representing a returned value is optional Their typical use is

 as the sequence of three-address_instructions



 center

 tabularl

 param



 param







 param



 call

 tabular

 center

 generated as part of a call of the procedure

 The integer indicating the number of

 actual_parameters in is not

 redundant because calls can be nested That is some of the first

 param statements could be parameters of a call that comes

 after returns its value that value becomes another parameter

 of the later call The implementation of procedure_calls is

 outlined in Section proc-3code-sect



 Indexed copy instructions of the form

 and The instruction

 sets to the value in the location memory units beyond

 location The instruction sets the

 contents of the location units beyond to the value of





 Address and pointer assignments of the form

 and

 The instruction

 sets the r-value of to be the

 location (l-value) of (From

 Section checking-subsect - and -values are

 appropriate on the left and right_sides of assignments

 respectively) Presumably is a name perhaps a temporary that

 denotes an expression with an l-value such_as Aij

 and is a pointer name or temporary In the instruction

 presumably is a pointer or a

 temporary whose r-value is a location The r-value of

 is made equal to the contents of that location Finally

 sets the r-value of the object

 pointed to by to the r-value of

 enumerate



 ex

 3code-ex Consider the statement



 center

 do i_i1 while (ai v)

 center

 Two possible translations of this statement are shown in

 Fig 3code-fig The translation in Fig 3code-fig(a) uses

 a symbolic label L attached to the first instruction The

 translation in (b) shows position numbers for the instructions

 starting arbitrarily at position 100 In both translations the

 last instruction is a conditional_jump to the first instruction

 The multiplication is appropriate

 for an array of elements that each take 8 units of space

 ex



 figurehtfb

 center

 tabularl_l l_l

 L i 1 100 i 1



 i 101 i



 i 8 102 i 8



 a 103 a



 if v_goto L 104 if

 v_goto 100



 tabular

 center



 center

 (a) Symbolic labels(b) Position numbers

 center



 Two ways of assigning labels to three-address_statements

 3code-fig



 figure



 The choice of allowable operators is an important issue in the

 design of an intermediate form The operator set clearly must_be

 rich enough to implement the operations in the source_language

 Operators that are close to machine_instructions make it easier to

 implement the intermediate form on a target_machine However if

 the front_end must generate long sequences of instructions for

 some source-language operations then the optimizer and code

 generator may have to work harder to rediscover the structure and

 generate good code for these operations



 Quadruples quad-subsub



 The description of three-address_instructions specifies the

 components of each type of instruction but it does_not specify

 the representation of these instructions in a data_structure In a

 compiler these instructions can be_implemented as objects or as

 records with fields for the operator and the operands Three such

 representations are called quadruples triples and

 indirect triples



 A quadruple (or just quad) has four fields which

 we call op and result The op field contains an internal code for the

 operator For_instance the three-address_instruction

 is represented_by placing in

 op in in and in

 result The following are some exceptions to this rule



 enumerate



 Instructions with unary operators like

 or do_not use

 Note_that for a copy_statement like

 op is while for most other

 operations the assignment operator is implied



 Operators

 like param use neither_nor result



 Conditional and unconditional_jumps put the target label in

 result



 enumerate



 exquads-ex

 Three-address_code for the assignment

 appears in

 Fig quads-fig(a) The special operator minus is used

 to distinguish the unary_minus operator as in

 from the binary minus operator as in

 Note_that the unary-minus

 three-address statement has only two addresses as does the

 copy_statement a



 The quadruples in Fig quads-fig(b) implement the

 three-address_code in (a)

 ex



 figurehtbf



 Three-address_code and its quadruple

 representationquads-fig

 figure



 Why Do We Need Copy Instructions A simple

 algorithm for translating expressions generates copy instructions

 for assignments as in Fig quads-fig(a) where we copy

 into a rather_than assigning

 to a directly Each

 subexpression typically gets its_own new_temporary to hold its

 result and only when the assignment operator is processed

 do we learn where to put the value of the complete expression A

 code-optimization pass perhaps using the DAG of

 Section dag-subsect as an intermediate form can discover

 that can be replaced_by a



 For readability we use actual identifiers like a b

 and c in the fields and result in Fig quads-fig(b) instead of pointers to their

 symbol-table entries Temporary names can either be entered into

 the symbol_table like programmer-defined names or they can be

 implemented as objects of a class Temp with its_own methods



 Triples

 triples-subsect



 A triple has only three fields which we call op

 and Note_that the result

 field in Fig quads-fig(b) is used primarily for temporary

 names Using triples we refer to the result of an operation

 by its position rather_than by an explicit

 temporary_name Thus instead of the temporary in

 Fig quads-fig(b) a triple representation would refer to

 position (0) Parenthesized numbers represent pointers into the

 triple structure itself In Section val-num-subsect

 positions or pointers to positions were called value numbers



 Triples are equivalent to signatures in

 Algorithm val-num-alg Hence the DAG and triple

 representations of expressions are equivalent The equivalence

 ends with expressions since syntax-tree variants and

 three-address_code represent control_flow quite differently



 extree-triple-ex

 The syntax_tree and triples in Fig tree-triple-fig

 correspond to the three-address_code and quadruples in

 Fig quads-fig In the triple representation in

 Fig tree-triple-fig(b) the copy_statement

 is encoded in the triple

 representation by placing a in the field and

 (4) in the field

 ex



 figurehtfb



 Representations of



 tree-triple-fig

 figure



 A ternary operation like requires

 two entries in the triple structure for example we can put

 and in one triple and in the next Similarly

 can implemented_by treating it as

 if it were the two instructions

 and where is a compiler-generated_temporary

 Note_that the temporary does_not actually appear in a triple

 since temporary values are referred to by their position in the

 triple structure



 A benefit of quadruples over triples can be seen in an optimizing

 compiler where instructions are often moved around With

 quadruples if we move an instruction that computes a temporary

 then the instructions that use require no change

 With triples the result of an operation is referred to by its

 position so moving an instruction may require us to change all

 references to that result This problem does_not occur with

 indirect triples which we consider next



 Indirect triples consist of a listing of pointers to

 triples rather_than a listing of triples themselves For_example

 let_us use an array instruction to list pointers to triples

 in the desired order Then the triples in

 Fig tree-triple-fig(b) might be represented as in

 Fig indirect-triples-fig



 figurehtbf



 Indirect triples representation of three-address

 codeindirect-triples-fig

 figure



 With indirect triples an optimizing_compiler can move an

 instruction by reordering the instruction list without

 affecting the triples themselves When implemented

 in Java an array of instruction

 objects is analogous to an indirect triple representation since

 Java treats the array_elements as references to objects



 Static Single-Assignment Form

 ssa-subsect



 Static single-assignment form (SSA) is an intermediate_representation

 that facilitates certain code optimizations

 Two distinctive aspects distinguish SSA from three-address_code

 The first is that all assignments in SSA are to variables

 with distinct names hence the term static single-assigment

 Figure 3ac-SSA-fig shows the same intermediate program

 in three-address_code and in static single-assignment form

 Note_that subscripts distinguish each definition of

 variables p and q in the SSA representation



 figurehtfb

 center

 tabularl_l

 p a b a b



 q p - c - c



 p q d d



 p e - p e -



 q p q



 tabular

 center



 center

 (a) Three-address code(b) Static single-assignment form

 center



 Intermediate program in three-address_code and SSA

 3ac-SSA-fig



 figure



 The same variable may be defined in two different control-flow

 paths in a program For_example the source_program



 center

 tabularl

 if_( flag ) x -1 else x 1



 y x a

 tabular

 center

 has two control-flow paths in which the variable x

 gets defined

 If we use different names for x in the true part

 and the false part of the conditional_statement

 then which name should we use in the assignment y x a

 Here is where the second distinctive aspect of SSA comes into play

 SSA uses a notational convention called the -function to combine

 the two definitions of x



 center

 tabularl

 if_( flag ) -1 else 1





 tabular

 center



 Here has the value

 if the control_flow passes_through the true part

 of the conditional and the value if the control_flow

 passes_through the false part

 That is to say the -function returns the value of its

 argument that corresponds to the control-flow path that was taken

 to get to the assignment-statement containing the -function

























 sexer

 int-code-exer

 Translate the arithmetic expression into

 itemize

 a) A syntax_tree

 b) Quadruples

 c) Triples

 d) Indirect triples

 itemize

 sexer



 exer

 Repeat_Exercise int-code-exer for the following assignment

 statements

 itemize

 a bi cj

 ai bc - bd

 x f(y1) 2

 x p y

 itemize

 exer



 hexer

 Show_how to transform a three-address_code sequence into

 one in which each defined variable gets a unique variable

 name

 hexer

 Program Abstraction

 secprog-abs



 Different kinds of transformations benefit from knowing different

 kinds of information_about a program For_example if we know that

 the operands of an expression have the same value every time it is

 executed then we can replace the dynamic computation statically by

 the result Similiarly if we know that a loop computes the same

 value in different iterations we can simply compute the value once

 outside of the loop and refer to it inside the loop



 Unfortunately most of the questions that we_wish to ask are

 undecidable Suppose we_wish to know if an_integer variable is always

 non-negative in a given program The answer can be one of the following

 enumerate

 yes it can be statically_determined that the variable is always non-negative

 no it can be statically_determined that the variable is sometimes negative

 don't_know because it depends_on the input

 don't_know because it is undecidable For_example a program

 may assign a negative number to the variable after invoking a function If

 the function halts then the variable will be negative However since

 the halting problem is undecidable in general we may not be_able to

 tell if a variable is negative

 enumerate

 Given that this problem is undecidable in general any compiler

 transformation must_be able to handle the case when the answer is

 don't_know A transformation must_be correct and therefore assume

 the worst_case In this case the compiler must assume

 that the variable may be negative That is the compiler can optimize

 the program only when the answer is yes and must treat all other

 answers similarly as if they are no Now even_though it may be

 possible to find the answer statically it may be very expensive

 Thus for the sake of expediency a program analysis usually

 approximates instead of trying to find the best answer possible The

 approximation must_be done in a conservative manner that is the

 analysis may report no even when the perfect answer is yes

 but never the converse In this way the compiler optimization may

 lose some optimization opportunities but never transforms the code in

 a way that renders it incorrect



 A program analysis approximates by creating an abstraction of the

 program where details are replaced_by simplifying but conservative

 assumptions A more_precise analysis is usually less efficient

 Program analysis design is all about choosing the right abstraction so

 as to strike the right balance between precision and efficiency

 Thus one must understand the nature of the optimization and choose an

 analysis that delivers most of the opportunities for optimizations

 without undue complexity



 In program analysis we try to find out about the state of the

 computation as defined by its program counter and the variables in

 the program It is impossible to keep_track of the complete state of

 the computation and the analysis is simplified by_applying

 abstractions in three major areas (1) data accesses (2) flow of

 control and (3) the contents of the memory state



 For_example a constant_propagation analysis tries only to find out if

 an operand of an instruction always has the same constant value if

 so the operand can be replaced_by the constant (1) The analysis may

 choose only to analyze simple local_scalar integer variables because

 other kinds of data are too hard to analyze and are unlikely to yield

 many opportunities for optimizations (2) It only analyzes one

 procedure at a time because local_variables cannot be altered by

 called procedures (3) The analysis only needs to keep_track of

 whether a variable has only one value and if so the value itself

 It does_not keep_track of all the possible values the variable can

 take on if it is not a singleton value Not_only is computing all the

 possible values infeasible and time consuming the information is

 useless anyway for that particular optimization



 The rest of the chapter will discuss the common data and control

 abstractions used in analyses How we abstract the state of the

 memory is more analysis-specific so we will defer discussing this

 until we talk_about the analyses themselves



 Data Accesses

 secdata-acc

 In most high-level programming_languages other_than C and C local

 scalar_variables or structures can be referred to by only one

 name They are thus relatively_easy to handle because an analysis

 knows precisely when these variables are accessed or not accessed In

 C and C however it is possible to take the address of a stack

 variable which can then be passed around and dereferenced

 subsequently Furthermore computer arithmetic is also allowed on

 pointers Thus many C and C compilers only apply optimizations on

 those local_scalar variables whose addresses have not ever been taken

 in the procedure



 All the other data accesses such_as parameters array_elements heap

 structures have aliases that is multiple names for the same

 memory_location Care must_be taken to track all the possible

 modifications to the same memory_locations Just take for example

 the following simple code snippet

 verbatim

 A3 1

 Ai 10

 t A3

 verbatim

 Even in this small code segment it is not possible to tell the value

 of t at the end of the execution if we do_not know the value of

 i If i has the value 3 then t gets the value of

 10 and 1 otherwise The analysis to disambiguate between array

 accesses is traditionally known_as array data_dependence

 analysis More details will be discussed in Chapter 10



 Similarly it is hard to resolve accesses that involve pointers For

 example

 verbatim

 p 1

 q 10

 t p

 verbatim

 Here if p contains the address of q then t 10 and

 1 otherwise The analysis to disambiguate between pointers is

 known_as pointer_alias analysis We will discuss this topic in

 Chapter 11



 Intraprocedural Control Flow Sensitivity



 To illustrate the different_ways of abstracting control_flow within a

 procedure we will use the same example code shown in

 Figure figch2-ex The program is very_simple and we can

 determine statically the value of every variable at every point in the

 program execution However each of the variables requires different

 degrees of sophistication of analyses and different representations



 Flow-insensitive analyses The value of variable a is the

 easiest to analyze in this example By scanning the execution

 statements in the program we observe that there are no assignments to

 variable a so a must retain the value it is initialized

 with 243 throughout the program To represent this kind of result

 we only need to attach one value to each variable We need not pay

 any attention to the control_flow at all for this example For that

 matter the answer is true even if every statement may be executed

 after every other statement in the program Analyses that do_not

 consider the flow of control in a program are known_as flow-insensitive analyses



 Flow-sensitive analyses In this example variable b is

 undefined until statement (5) after which it is given the value 10

 The answer is more elaborate than that for variable a because

 we need to distinguish_between the program points before and after

 statement (5) Furthermore the flow of control is important here

 The fact that statement (5) follows the statement i 10 and

 precedes further modifications to the variable i is critical to

 the reasoning Analyses that pay attention to control_flow are known

 as flow-sensitive analyses



 Iteration-sensitive analyses

 Variable i is the loop counter in the example code it starts at

 10 and is decremented every iteration of the loop until it reaches 0

 upon which the loop exits Here the answer

 is more_complicated still Variable i has value 10 right after

 executing statement (4) Its value varies with the number of iterations

 executed More_specifically i has

 the value 10- where is the number

 of iterations that have_been executed and tops out at 10

 Finally the value of i is 0 after the loop

 Analyses that distinguish_between the different iterations in a loop

 are known_as iteration-sensitive analyses



 Path-sensitive analyses Variable c in this program

 is the hardest

 to analyze Looking at the program we know that the value of c

 is 0 until the program executes the first iteration of the loop The

 comparison i -1 is always true because it follows the

 comparison i 0 The comparison would not have executed unless

 i 0 is true which implies i -1 is true We_say that

 the path of executing statements (678) is feasible but the

 path of executing statements (6710) is not Path-sensitive

 analyses take into_account the condition that governs the control

 flow



 In_contrast a flow-sensitive but path-insensitive analysis does_not

 analyze the predicates that govern control_flow They must assume

 that control may flow along every possible edge in the control_flow

 graph regardless of the history of the execution Such an analysis

 would conclude that variable c may have either value 1 or 2 upon

 reaching statement (11)



 In this particular case a path-sensitive analysis may find that

 the value of c 0 up to statement (6) after which it has the value

 0 or 1

 In_general a path-sensitive analysis may find the value of a variable

 to differ depending_on the path taken Path-sensitive analysis is

 expensive in both time and space The number of paths through a

 program is exponential and determining if a path is infeasible

 is undecidable in general

 Thus path-sensitive analyses are not used very often and

 when they are the results are usually computed on demand only for the

 paths of interest



 Interprocedural Context Sensitivity



 Most analyses used in compiler optimizations do_not analyze across

 procedural boundaries That is they conservatively assume that every

 procedure invoked may alter the state of all the variables visible to

 the procedure and that it may create all possible side_effects such_as

 raising all program exceptions



 Especially in programs_written in object-oriented_programming

 languages procedure invocations are frequent If we assume the worst

 case for all possible invocations the result may be imprecise and

 useless One relatively_simple but useful technique is to inline the procedures that is we replace the invocation of a

 procedure with the body of the procedure itself This method is

 applicable only if we know the targets of the function calls In the

 case_where procedures are invoked indirectly through a pointer or via

 the method dispatch mechanism in object-oriented_programming program

 analysis must first be applied to determine the targets of the

 indirect invocations It is not possible to inline all procedures if

 procedures call each other recursively Furthermore inlining can

 increase the size of the code and must_be used judiciously



 A more sophisticated alternative to inlining is interprocedural

 analysis An interprocedural_analysis analyzes across the whole

 program and information from the caller flows to the callees and vice

 versa There_are two major variants in interprocedural_analysis context-insensitive or context-sensitive The first computes a

 single result for a procedure regardless of its calling_contexts the

 latter is more_precise in that the analysis results of a procedure may

 be different under different calling_contexts



 One typical approach to writing a context-insensitive but

 flow-sensitive analysis is to create a super control_flow

 graph Besides the normal intraprocedural control_flow edges

 additional edges are created connecting (1) the call_site to the beginning

 of all called procedures and (2) the return statements back to

 the call_sites While simple this model abstracts out the

 fundamental stack-based nature of procedure invocations thus

 causing the analysis to be imprecise



 Consider the following simple example whose super control_flow graph is

 shown in Figure figsupergraph

 verbatim

 int_v

 f()

 v 4

 h()



 g()

 v 5

 h()



 h()



 verbatim

 Here procedures f and g assign values 4 and 5

 respectively to global variable v before invoking the empty

 procedure h Clearly v is unchanged after invoking h Unfortunately the context-insensitive_analysis is unable to

 infer this information

 figurehtpb

 figureuullmanalsuch2oldfigssuperfgeps

 A super control_flow graph example in flow-sensitive but context-insensitive_analysis

 figsupergraph

 figure



 Because the calls and returns are modeled as control_flow edges the

 values 4 and 5 flow from functions f and g respectively

 into procedure h Thus the variable v has values 4 and

 5 in procedure h Since there is a flow of control back to the

 call_sites of f and g the dual values flow back to both

 f and g This is grossly inaccurate but is correct

 That is it is not incorrect to say that v in procedure f

 may have values 4 and 5 even_though v can have only the value

 4 The imprecision is introduced because the analysis cannot

 tell that control cannot be passed from f to g by going

 through h The presence of unrealizable paths like this

 one makes context-insensitive_analysis imprecise



 A context-sensitive_analysis on the other_hand is more_precise

 because it faithfully models the procedure invocation semantics A

 typical context-sensitive_analysis approach would summarize the effect

 of a procedure and apply the summary to each calling_context For

 example in this case the analysis will first analyze function h and represent its action as a simple identity_function Then for

 each call to h it correctly infers that no side_effects have

 been made and that the value of variable v is unchanged



 The choice of precision at the interprocedural level is orthogonal

 to that at the intraprocedural level It is possible to build

 analyses that are sensitive to both flow and context analyses that

 are insensitive to both or sensitive to just flow and not context

 and vice_versa



 abstract



 This_paper presents the first scalable context-sensitive

 inclusion-based_pointer alias_analysis for Java programs Our

 approach to context_sensitivity is to create a clone of a method for

 every context of interest and run a context-insensitive

 algorithm over the expanded_call graph to get context-sensitive

 results For precision we generate a clone for every acyclic_path

 through a program's call_graph treating methods in a strongly

 connected_component as a single_node Normally this formulation is

 hopelessly intractable as a call_graph often has acyclic

 paths or_more We show that these exponential relations can be

 computed efficiently using binary_decision diagrams (BDDs) Key to

 the scalability of the technique is a context_numbering scheme that

 exposes the commonalities_across contexts We applied our algorithm

 to the most_popular applications available on Sourceforge and found

 that the largest programs with hundreds of thousands of Java

 bytecodes can be analyzed in under 20 minutes



 This_paper shows that pointer_analysis and many other queries and

 algorithms can be described succinctly and declaratively using

 Datalog a logic_programming language We have developed a system

 called that automatically_translates Datalog_programs into

 highly_efficient BDD implementations We used this approach to

 develop a variety of context-sensitive algorithms including side

 effect analysis type analysis and escape_analysis

 abstract



 Acknowledgments





 This material is based upon work supported by the National Science

 Foundation under Grant No 0086160 and an NSF Graduate Student

 Fellowship We thank our anonymous referees for their helpful comments



 Finding Synchronization-Free Parallelism

 ch11affine



 Having developed the theory of affine array_accesses their reuse of

 data and the dependences among them we_shall now begin to apply this

 theory to parallelization and optimization of real programs

 As_discussed in Section data-locality-subsect

 it is important that we find

 parallelism while minimizing communication among processors Let_us

 start by studying the problem of parallelizing an application without

 allowing any communication or synchronization between processors at

 all This constraint

 may appear to be a purely academic exercise how often can

 we find programs and routines that have such a form of parallelism In

 fact many such programs exist in real life and the algorithm for solving

 this problem is useful in its_own right In_addition the concepts

 used to solve this problem can be extended to handle synchronization

 and communication



 An Introductory Example



 Shown in Fig_figpsmoo1 is an excerpt of a C translation (with

 Fortran-style array_accesses retained for clarity) from

 a 5000-line Fortran multigrid algorithm to solve three-dimensional

 Euler equations The program spends practically all its time in a

 small number of routines like the one shown in the figure It is

 typical of many numerical programs

 These often consist of numerous for loops with different

 nesting levels and they have many array_accesses all of which are affine

 expressions of surrounding loop_indexes To keep the example short

 we have elided lines from the original_program with similar

 characteristics



 figurehtb

 verbatim

 for_(j 2 j_jl j)

 for_(i 2 i il i)

 APji

 T 10(10 APji)

 D2ji TAPji

 DW12ji TDW12ji



 for (k 3 k kl-1 k)

 for_(j 2 j_jl j)

 for_(i 2 i il i)

 AMji APji

 APji

 T APji - AMjiDk-1ji

 Dkji TAPji

 DW1kji T(DW1kji DW1k-1ji)





 for (k kl-1 k 2 k-)

 for_(j 2 j_jl j)

 for_(i 2 i il i)

 DW1kji DW1kji DkjiDW1k1ji

 verbatim

 Code excerpt

 figpsmoo1

 figure



 The code of Fig_figpsmoo1

 uses a number of different arrays with different dimensions

 Notice_that in each assignment_statement the computation of an array

 element depends only on other array_elements with the same values for

 the last two components ( and respectively) but sometimes also

 depends_on the scalar

 In the first two of the three loop_nests in Fig_figpsmoo1

 each iteration of the innermost_loop writes a value into and uses

 the value immediately_after in the same iteration As_discussed in

 Section ch11memory such reuses can constrain parallelism



 We can relax the constraint by_replacing the scalar by an array

 We then have

 each iteration use its_own array_element With this

 modification we can simply group all operations that operate_on the

 th_element of all arrays into one computation unit

 This modification produces

 units of computation that are all

 independent of one another

 Notice_that the portions of these units that are taken from the second

 and third nests involve a third loop with index However because

 there is no dependence_between dynamic_accesses with the same values for

 and we can safely perform the loops on inside the loops on

 and - that is within a computation unit



 Knowing that these computation

 units are independent enables a number of legal

 transforms on this code For_example instead of executing the code

 as originally written a uniprocessor can perform the same computation

 by executing the units of independent operation one_unit at a time

 The resulting code shown in Fig figpsmoo2 has improved

 temporal_locality because results produced are consumed immediately



 figurehtb



 verbatim

 for_(j 2 j_jl j)

 for_(i 2 i il i)

 APji

 Tji 10(10 APji)

 D2ji TjiAPji

 DW12ji TjiDW12ji

 for (k 3 k kl-1 k)

 AMji APji

 APji

 Tji APji - AMjiDk-1ji

 Dkji TjiAPji

 DW1kji Tji(DW1kji DW1k-1ji)





 for (k kl-1 k 2 k-)

 DW1kji DW1kji DkjiDW1k1ji



 verbatim

 Code excerpt transformed from Fig_figpsmoo1

 figpsmoo2

 figure



 The independent_units of computation can also be assigned to different

 processors and executed in parallel without requiring any

 synchronization or communication Parallel computations are often

 expressed in SPMD (single program multiple data) form In

 SPMD form the same program parameterized by the ID of the

 processor is executed on all processors Since there are

 independent_units of computation we can

 utilize at most processors By organizing the

 processors as if they_were in a 2-dimensional array with ID's where

 and

 the SPMD program to be executed by each processor is simply the body

 in the inner_loop in Fig figpsmoo2



 The above example_illustrates the basic approach to finding

 synchronization-free_parallelism We first split the computation

 into as many independent_units as possible This

 partitioning

 exposes the scheduling choices available We then assign computation

 units to

 the processors depending_on the number of processors we have

 Finally we

 generate an SPMD program that is executed on each processor



 Affine Partitions



 Like the iteration_spaces that describe loop_nests and the spaces that

 describe affine array_accesses we also may

 model the processor space as a multidimensional Cartesian

 processor space

 The dimension of the processor space reflects the degree of parallelism there is in the program it is not a property of

 the hardware We_say that a code has degrees of parallelism if we

 can split the computation into independent_units where

 is the number of iterations in a loop(As before we find it

 convenient to imagine that each loop_index runs from 1 to some variable

 limit )

 For_instance the code in

 Fig figpsmoo2 has 2 degrees of parallelism

 That is if we imagine both the limits and are really

 then there are computation_units



 We_shall assume initially that the dimensions of the processor array

 correspond to loop_indexes and each dimension of the processor array

 has as many processors as there are iterations of the corresponding

 loop After all the

 independent computation_units have_been found we_shall map these

 virtual processors to the actual processors

 In_practice each processor is expected

 to be responsible_for a fairly large_number of iterations because otherwise

 there is not enough work to amortize away the overhead of

 parallelization



 For_instance as in the matrix_multiplication example of

 Section ch11mm

 it may be useful for the sake of locality to partition a -dimensional

 iteration_space into -dimensional blocks and assign them to

 different_processors

 Initially maximizing the degree of

 parallelism gives_us the freedom to do so



 Suppose now that the program to be_parallelized is broken_down into

 elementary statements such_as 3-address statements

 For each statement we find an affine computation

 mapping that maps each dynamic_instance of the statement as

 identified by its loop_indexes to the ID of the processor array



 figurehtfb



 verbatim

 for (k 1 k n k)

 for_(i 1_i n_i)

 for_(j 1_j n_j)

 Xij_Xij Akij

 verbatim



 A loop_nest with two degrees of parallelism

 rank2-fig



 figure



 ex

 exrank2

 Consider the 3-deep loop_nest that accesses a two-dimensional_array

 seen in Fig rank2-fig

 Since the array is read-only we need to worry

 only about the accesses to

 It is not possible to divide this computation into

 independent computation_units since for a fixed value of and

 the units for each value of would access the same array_element



 However we can divide this computation into units of

 independent computation with each unit working on a different element

 of



 More formally the access function for





 arrayrrr

 0_0 0



 0_1 0



 0_0 1



 array





 arrayc

 k



 i



 j



 array





 has a rank of 2 The iterations can thus be divided_into

 independent groups with each group consisting of iterations

 operating on the same data



 With a degree of parallelism of 2 we

 model the processor array as a 2-dimensional space Let be

 a processor ID for and

 Then we map the dynamic

 instances of the one assignment_statement with the affine computation

 mapping





 arrayl

 p1



 p2



 array







 arrayrrr

 0_1 0



 0_0 1



 array





 arrayc

 k



 i



 j



 array





 With this mapping

 data element is managed by the processor with ID

 ex



 ex

 expsmoo2

 For Fig_figpsmoo1 the affine_partition for statements in

 the first loop_nest is





 arrayrr

 1_0



 0_1



 array





 arrayc

 j



 i



 array







 arrayc

 0



 0



 array





 The affine partitions for statements in the second and third loop_nest

 are the same





 arrayrrr

 0_1 0



 0_0 1



 array





 arrayc

 k



 j



 i



 array







 arrayc

 0



 0



 array





 ex



 The algorithm to find synchronization-free_parallelism consists of

 three steps



 enumerate



 Find for each statement in the program an affine_partition that maximizes

 the degree of parallelism Note_that we generally treat the statement

 rather_than the single access as the unit of computation The same

 affine_partition must apply to each access in the statement This

 grouping of accesses makes_sense since there is almost_always

 dependence among accesses of the same statement anyway



 Assign the resulting independent computation

 units among the processors and choose an

 interleaving of the steps

 on each processor This assignment is driven by locality

 considerations



 Generate an SPMD program to be executed on each processor



 enumerate



 We_shall discuss next how to find the affine_partition functions

 We also show_how to generate a

 sequential program that executes the partitions serially and how to

 generate an SPMD

 program that executes each partition on a different processor After

 we discuss_how parallelism with synchronizations is handled in

 Section ch11sync we return to Step_2 above in

 Section ch11block

 and discuss the optimization of locality for uniprocessors and

 multiprocessors



 Space-Partition Constraints

 ch11space



 To require no communication each pair of operations that share a data

 dependence must_be assigned to the same processor We refer to these

 constraints as space-partition_constraints Any mapping that

 satisfies these constraints creates partitions that are independent of

 one another Note_that such constraints can be satisfied by putting

 all the operations in a single partition

 Unfortunately that solution

 does_not yield any parallelism Our_goal

 is to create as many independent_partitions as possible while

 satisfying the space-partition_constraints that is operations are

 not placed on the same processor unless it is necessary



 When we restrict ourselves to affine mappings

 then instead of maximizing the number of

 independent_units we may maximize the degree (number of dimensions)

 of parallelism

 It is sometimes possible to create more independent_units

 if we can use piecewise affine mappings A piecewise affine

 mapping divides instances of a single access into different sets and

 allows a different affine mapping for each set

 However we_shall not consider such an option here



 Formally

 an affine_partition mapping of a program is synchronization-free if and

 only if for every two (not_necessarily distinct)_accesses sharing a dependence



 in statement_nested in loops

 and



 in statement_nested in loops

 the partition_mappings and

 for statements and

 respectively satisfy the space-partition_constraints



 itemize



 For all

 in and in such that

 itemize

 a)



 b)



 and

 c)





 itemize

 we have







 itemize



 Shown in Fig figspace is a diagram illustrating the essence

 of the space-partition_constraints That is we suppose there are static

 accesses in two

 loop_nests with index vectors and These

 accesses are dependent in the sense that they access at_least one

 array_element in common

 and at_least one of them is a write The figure shows particular

 dynamic_accesses in the two loops that happen to access the same array

 element according to the affine access functions

 and We also see that

 as a consequence of the space-partition_constraints the affine

 partition_mappings for the two static accesses

 and also have the same

 value for these two

 dynamic_accesses

 That is the two dynamic_accesses must_be executed by the same

 processor



 figurehtfb

 fileuullmanalsuch11figsspaceeps

 Space-partition constraints

 figspace

 figure



 The goal of the parallelization

 algorithm is to find for each statement

 the partition_mapping with the highest rank

 that satisfies these

 constraints

 If we choose an affine_partition mapping whose rank is the maximum of

 the ranks of all statements we get the maximum possible parallelism

 However under this mapping some processors may be idle at times while

 other processors are executing statements whose

 affine mappings have a smaller rank This situation

 may be acceptable if the

 time taken to execute those statements is relatively short

 Otherwise we can choose an affine_partition whose rank is smaller_than

 the maximum possible

 as_long as that rank is greater_than 0



 ex

 exredblack

 Consider the loop_nest in Fig redblack-code-fig As we

 mentioned the computation_units to be scheduled are the individual

 statements In this example there are two statements with array

 accesses which we have denoted and



 figurehtfb



 verbatim

 for_(i 1_i 100_i)

 for_(j 1_j 100_j)

 Aij_Aij Bi-1j_(s1)

 Bij Bij_Aij-1 (s2)



 verbatim



 A loop_nest exhibiting long chains of dependent operations

 redblack-code-fig



 figure



 Figure figredblack shows the data_dependences in the

 program That is each black dot represents an instance of statement

 and each white dot represents an instance of statement The

 dot is located at coordinates represents the instance of the

 statement executed for those values of the loop_indexes Note_however

 that the instance of is located just below the instance of

 for the same pair so the vertical scale of is greater_than

 the horizontal scale of



 figurehtfb

 fileuullmanalsuch11figsredblackeps

 Dependences of the code in Example exredblack

 figredblack

 figure



 Notice_that is written by that is by the instance

 of statement with index values and while it is later read

 by Thus must_precede This

 observation explains the vertical arrows from black dots to white dots

 Similarly is written by and later read by

 Thus must_precede which explains the arrows

 from white dots to black

 To parallelize this code without_synchronization we simply

 map each chain of dependent operations to the same processor



 The number of chains in Fig figredblack

 is so the partition of statements among

 processors is one-dimensional

 We can thus represent the affine_partition mapping for each statement by

 a matrix and a vector to translate the vector of

 indexes into a single processor number

 Let

 be the

 one-dimensional affine partitions for the statements and

 respectively



 It is easy to see that each instance of a static_access in this

 program accesses a different element However data_dependence exists between

 instances of accesses Aij and Aij-1 and between

 Bi-1j and Bij

 The constraints on the affine_partition imposed_by the

 data_dependence between Aij and Aij-1 are



 itemize



 For all pairs and such that

 enumerate









 and



 enumerate

 the following must hold





 arrayrr

 C11_C12

 array





 arrayc

 i



 j



 array







 arrayl

 c1



 array







 arrayrr

 C21 C22

 array





 arrayc

 i'



 j'



 array







 arrayl

 c2



 array







 itemize

 That is the first four conditions say that and lie within

 the iteration_space of the loop_nest and the last two conditions

 say that the dynamic_accesses Aij and Aij-1 touch the same

 array_element The concluding equation_says that the affine_partition

 puts these accesses on the same processor



 Expressing in terms of we get





 arrayrr

 C11-C21 C12-C22

 array





 arrayc

 i



 j



 array







 arrayc

 c1 - c2 - C22

 array





 0



 for all values of satisfying the bounds Therefore



 arrayrll

 C11-C21 0



 C12-C22 0



 c1 - c2 - C22 0



 array





 Similar considerations regarding the data_dependence

 between the accesses Bi-1j and

 Bij yield



 arrayrll

 C11-C21 0



 C12-C22 0



 c1 - c2 C21 0



 array





 Thus



 C11_C21 -C22 -C12 c2 - c1



 There is a two-dimensional space of solutions to the four equations in

 six variables described by the above chain of equalities For_instance

 one solution has and It then follows that

 and

 We thus get the partition_mappings



 arrayl

 s1



 arrayrr

 1 -1

 array





 arrayc

 i



 j



 array







 arrayc

 -1

 array











 s2



 arrayrr

 1 -1

 array





 arrayc

 i



 j



 array







 arrayc

 0

 array



 array



 That is the th_iteration of is assigned to the processor

 and the th_iteration of is assigned to processor



 ex



 The general method for finding the best partition_mapping

 for a program is summarized in the following algorithm



 alg

 algnosync

 Finding a highest-ranked synchronization-free affine_partition mapping

 for a program



 A program with affine array_accesses



 A partition_mapping



 Execute the following steps



 enumerate



 Find all data-dependent pairs of accesses in a program

 For each pair of data-dependent accesses



 in statement_nested in loops

 and



 in statement_nested in loops

 Let and

 represent the (currently unknown)

 partition_mappings of statements and respectively

 The space-partition_constraints state that if



 F1_i1 f1_F2 i2_f2

 then



 C1i1_c1 C2i2_c2



 for all within their_respective loop

 bounds We_shall

 generalize the domain of iterations to include all in

 and in

 that is the bounds are all assumed to be

 minus infinity to infinity This assumption makes_sense since an

 affine_partition cannot make use of the fact that an index variable can

 take on only a limited set of integer values

 For each pair of dependent accesses we do the following

 enumerate

 trick-item

 First note_that is the same vector as





 arraycc

 F f



 array





 arrayc

 i



 1



 array





 That is by_adding an extra component 1 at the bottom of column-vector

 i we can make the column-vector f be an additional last

 column of the matrix_F

 We may thus rewrite the equality of the access functions

 as





 arrayccc

 F1 -F2 (f1-f2)

 array





 arrayc

 i1



 i2



 1



 array





 0



 The above equations will in general have more_than one solution

 However we may still use Gaussian_elimination to solve the equations for

 the components of and as

 best we can

 That is eliminate as many variables as possible until we are left with

 only variables that cannot be_eliminated

 The resulting solution for and will have the

 form





 arrayc

 i1



 i2



 1



 array



 U



 arrayc

 t



 1

 array





 where U is an upper-triangular matrix and

 t is a vector of free variables ranging over all integers

 We may use the same trick as in Step (trick-item) to rewrite the

 equality of the partition_mappings

 However since we have_already solved for the vector

 we can write the constraints

 on the partition_mappings as





 arrayrrr

 C1 -C2 (c1-c2)

 array



 U



 arrayc

 t



 1

 array



 0



 Since t ranges over all integers it must_be that





 arrayrrr

 C1 -C2 (c1-c2)

 array



 U 0



 enumerate



 Collect all constraints generated in the form of

 where vector is all the unknown coefficients of the computation

 mappings



 Since the rank of an affine_partition mapping is independent of the

 value of the constant terms in the computational mappings we eliminate

 all the unknowns that come from vectors or thus

 replacing

 by simplified constraints





 Find the solutions to expressing them as B a set of basis_vectors spanning the null_space of



 Derive one row of the desired affine_partition mapping from each basis

 vector in B and derive the constant terms using



 enumerate



 alg



 Note_that the elimination of bounds at the beginning of

 Step 1 only increases the partitioning

 constraints and is therefore correct The data-dependence test in

 that step guarantees that there is an overlap between the accesses

 Since the partition_mappings must_be affine there is no

 loss of precision either unless the iteration_spaces of the two

 statements intersect at only one point Techniques

 discussed in Algorithm_algsync can be used to handle these

 special_cases if so desired



 A Simple Code-Generation Algorithm



 Algorithm_algnosync generates affine_partition mappings that

 split computations into independent_partitions Partitions can be

 assigned arbitrarily to different_processors since they are

 independent of one another A processor may be assigned more_than one

 partition and can interleave the execution of its partitions as_long

 as operations within each partition which normally have data_dependences

 are executed sequentially



 It is relatively_easy to generate a correct program given an affine

 partition We first introduce

 Algorithm algsimpleSPMD a simple approach to

 generating code for a single

 processor that executes each of the independent_partitions

 sequentially Such code optimizes temporal

 locality since array_accesses that have several uses are very close in

 time Moreover the code easily can be turned_into an SPMD

 program that executes a partition on a different processor The code

 generated is unfortunately inefficient we_shall next discuss

 optimizations to make the code execute efficiently



 The essential idea is as_follows We are given bounds for the index

 variables of a loop_nest and we have determined in

 Algorithm_algnosync a partition_mapping for the accesses of a

 particular statement We want to turn the iteration of the loop

 nest into an iteration over variables that represent the dimensions of

 the processor space In order to do so we need to turn the bounds on

 index variables into bounds on processor-ID variables That way each

 processor_executes the statements assigned to it for only those

 values of the index variables that the original code would execute

 Because different loop_nests may have different bounds it is not always

 possible to restrict our new loops to only those index values that have

 nonempty computation to perform It thus may be necessary to introduce

 additional tests to make_sure we execute only those instances of

 statements that the original_program would execute



 alg

 algsimpleSPMD

 Generating code that executes independent_partitions sequentially



 A program with affine array_accesses

 Each statement in the program has associated bounds of

 the form where i is the vector

 of loop_indexes for the loop_nest in which statement appears Also

 associated_with statement is a partition_mapping

 where p is an -dimensional vector of

 variables representing a processor ID is the maximum over all

 statements in program of the rank of the partition_mapping for that

 statement



 A program equivalent to but iterating over the processor space

 rather_than over loop_indexes



 Do the following



 enumerate



 For each statement use Fourier-Motzkin_elimination to project out

 all the loop_index variables from the bounds



 Use_Algorithm algenumerate to

 determine bounds on the partition ID's



 Generate loops one for each of the dimensions of processor

 space Let be the vector of variables

 for these loops that is there is one variable for each dimension of

 the processor space

 Each loop variable ranges over the union

 of the partition spaces for all statements in the program

 Note_that the union of the partition

 spaces is not_necessarily convex To keep the algorithm simple

 instead of enumerating only those partitions that have a nonempty computation

 to perform

 set the lower_bound of each to the minimum of all the lower

 bounds imposed_by all statements and the upper_bound of each to

 the maximum of all the upper_bounds imposed_by all statements Notice

 that some values of p may

 have no operations



 The code to be executed by each partition is the original_sequential

 program except that every statement is guarded by a predicate so

 that only those operations belonging to the partition are executed



 enumerate

 alg



 ex

 exredblack-cg

 Let_us generate code that executes the independent_partitions in

 Example exredblack sequentially

 The original_sequential program is from Fig redblack-code-fig is

 repeated_here as Fig redblack-code2-fig



 figurehtfb



 verbatim

 for_(i 1_i 100_i)

 for_(j 1_j 100_j)

 Aij_Aij Bi-1j_(s1)

 Bij Bij_Aij-1 (s2)



 verbatim



 Repeat of Fig redblack-code-fig

 redblack-code2-fig



 figure



 In Example exredblack

 the affine_partitioning algorithm found one degree of parallelism

 Thus the processor space can be represented_by a single variable

 Recall also from that example that we selected an affine_partition

 mapping that for all values of index variables and with

 and assigned



 enumerate



 Instance of statement to processor and



 Instance of statement to processor



 enumerate



 We combine the constraints and with one of

 the equations or and project_away and to get



 enumerate



 if we use the function that we get for

 statement and



 if we use from statement



 enumerate

 When we take the union of these ranges we get these

 bounds are sufficient to cover all instances of both statements

 and



 The code in Fig_redblack-unopt-fig

 iterates through the partitions sequentially at line_(1) Each

 partition goes_through the motion of generating the indexes of all the

 iterations in the original_sequential program in lines_(2) and (3)

 so that it can pick out

 the iterations the processor is supposed to execute

 The tests of lines_(4) and (6) make_sure that statements and

 are executed only when the processor would execute them

 ex



 figurehtfb



 verbatim

 1) for (p -100 p 99 p)

 2) for_(i 1_i 100_i)

 3) for_(j 1_j 100_j)

 4) if_(p i-j-1)

 5) Aij_Aij Bi-1j_(s1)

 6) if_(p i-j)

 7) Bij_Aij-1 Bij_(s2)



 verbatim



 A simple rewriting of Fig redblack-code2-fig that

 iterates over processor space

 redblack-unopt-fig



 figure



 Although the code of Fig_redblack-unopt-fig appears designed to

 execute on a uniprocessor we could take the inner_loops lines_(2)

 through_(7) and execute them on 200 different_processors each of which

 had a different value for from to 99

 Or we could partition the responsibility for the inner_loops among any

 number of processors less_than 200 as_long as we arranged that each

 processor knew what values of it was responsible_for and executed

 lines_(2) through_(7) for just those values of



 Optimizations

 affine-opt-subsect



 There_are two major sources of inefficiency in the code generated

 using Algorithm algsimpleSPMD First each partition executes

 many empty iterations that have no computation

 In terms of Fig_redblack-unopt-fig once we fix a value of

 the ranges for and in the original iteration_space for the code

 in Fig redblack-code2-fig are not 0 to 100 but usually something

 smaller



 Second the code of Fig_redblack-unopt-fig

 that it goes_through an unnecessary double loop in lines_(2) and (3)

 That is if the loop upper limits were rather_than 100 the code of

 lines_(2) through_(7) takes time to iterate over and

 while the number of times statements and are executed for a

 particular value of is only We_shall see shortly

 how to use the relationship_between

 and enforced by lines_(4) or (6) to replace the loop on

 by an assignment of the one value of that satisfies the test of

 line_(4) or (6) That change will make the running_time

 be per value of rather_than



 The first optimization is to tighten the bounds on the loop_indexes

 for each partition and eliminate all the empty iterations This

 optimization does_not change the structure of the code We use a

 procedure similar to the first three steps of

 Algorithm algsimpleSPMD For each statement use

 Algorithm_algenumerate on the loop-bound constraints and the

 partition_mappings to express bounds on the loop_indexes in terms of

 the partition ID's For each loop_index set the lower_bound to be the

 minimum imposed_by all statements and the upper_bound to be the

 maximum The expressions are simplified whenever possible if any

 loop-bound expression or the predicate guarding a statement is true

 given the loop_bounds of the surrounding loops the test is

 removed_from

 the generated code



 ex

 exredblack-opt

 Tightening loop_bounds of the code in Example exredblack-cg

 For statement

 the bounds for each of the variables are as shown in

 Fig redblack-bounds1-fig(a)

 For_instance one bound_on is obviously that because

 that is the range of the loop on

 However since also ranges from 1 to 100 the test of line_(4)

 can only be satisfied if and if



 figurehtfb



 arrayrrcl

 j

 i-p-1 j i-p-1



 1_j 100







 i

 p2 i_100 p 1



 1_i 100







 p -100 p 98



 array





 center

 (a) Bounds for statement

 center



 arrayrrcl

 j

 i-p j i-p



 1_j 100







 i

 p1 i_100 p



 1_i 100







 p

 -99 p 99

 array





 center

 (b) Bounds for statement

 center



 Tighter bounds on and for

 Fig_redblack-unopt-fig

 redblack-bounds1-fig



 figure



 Likewise

 for statement the bounds for each of the variables are shown in

 Fig redblack-bounds1-fig(b)

 The iteration_spaces for and in

 Fig redblack-bounds1-fig are similar but certain limits differ

 by 1 between the two

 We may take the union of the two iteration_spaces by using the smaller

 lower_bounds and the larger upper_bounds

 For_example for use as the lower_bound

 and as the

 upper_bound

 The code in Fig redblack-opt1-fig

 executes over this union of iteration_spaces

 Since the iteration_space executed is larger_than either that of

 and conditionals are still necessary to select when these

 statements

 are executed

 ex



 figurehtfb



 verbatim

 for (p -100 p 99 p)

 for_(i max(1p1) i min(100101p) i)

 for_(j max(1i-p-1) j min(100i-p) j)

 if_(p i-j-1)

 Aij_Aij Bi-1j_(s1)

 if_(p i-j)

 Bij_Aij-1 Bij_(s2)



 verbatim



 Code of Fig_redblack-unopt-fig improved by tighter loop

 bounds

 redblack-opt1-fig



 figure



 The second optimization is to remove conditionals from the inner_loops

 This change is especially important when the test is an equality

 involving indexes of the loop_nest such

 as in line_(4) of Fig_redblack-unopt-fig Then we

 find that we can solve the equation for the innermost_loop index and

 reduce the number of loops That change reduces by an order of

 magnitude the running_time of the program



 To_avoid the need for conditional tests

 we split the iteration_space into subspaces each of which executes

 the same set of statements

 Note_that as in our_running example different statements like

 and may have different iteration_spaces while the loops

 have iterations for the union of the iteration_spaces of the

 statements

 This optimization requires code to be

 duplicated and should only be used to remove conditionals in the

 innermost_loops The steps to split an iteration_space are



 enumerate



 Select a loop that consists of statements with different bounds



 Split the loop using a condition such that some statement is excluded

 from at_least one of its components We choose the condition from among

 the boundaries of the overlapping different polyhedra If some

 statement has all its iterations in only one of the

 half planes of the condition then

 such a condition is useful



 Generate_code for each of these iteration_spaces separately

 Note_that further splitting of one or both iteration_spaces may be

 necessary



 enumerate



 figurehtfb



 verbatim

 space (1)

 p -100

 i 1

 j_100

 Aij_Aij Bi-1j_(s1)



 space (2)

 for (p -99 p 98 p)

 for_(i max(1p1) i min(100101p) i)

 for_(j max(1i-p-1) j min(100i-p) j)

 if_(p i-j-1)

 Aij_Aij Bi-1j_(s1)

 if_(p i-j)

 Bij_Aij-1 Bij_(s2)





 space (3)

 p 99

 i_100

 j 1

 Bij_Aij-1 Bij_(s2)

 verbatim



 Splitting the iteration_space on the value of

 redblack-opt2-fig



 figure



 ex

 Let_us remove the conditionals from the code of Fig redblack-opt1-fig

 that was generated in

 Example exredblack-opt Statements and are mapped to

 the same set of

 partition ID's except for the boundary partitions at either

 end Thus we separate the partition space into three subspaces



 enumerate







 and







 enumerate



 The code for each subspace can

 then be specialized for the value(s) of contained

 Figure redblack-opt2-fig shows the code specialized for each of

 these three iteration_spaces



 Notice_that the first and third spaces do_not need loops on or

 because for the particular value of that defines each space these

 loops are degenerate they have only one iteration For_example in

 space (1) defined by the loops



 verbatim

 for_(i max(1p1) i min(100101p) i)

 for_(j max(1i-p-1) j min(100i-p) j)

 verbatim

 simplify to



 verbatim

 for_(i max(1-99) i min(1001) i)

 for_(j max(1i99) j min(100i100) j)

 verbatim

 and then to



 verbatim

 for_(i 1_i 1 i)

 for_(j max(1i99) j min(100i100) j)

 verbatim

 At this point we see that only takes on the value 1 whereupon we

 can simplify the loop on to



 verbatim

 for_(j max(1100) j min(100101) j)

 verbatim

 That is takes on only the value 100



 Next we split the loop with index in space (2) Again the first and last

 iterations of loop_index are different Thus we split the loop

 into three subspaces



 figurehtfb



 verbatim

 space (2)

 for (p -99 p 98 p)

 space (2a)

 if_(p 0)

 i p1

 j 1

 Bij_Aij-1 Bij_(s2)



 space (2b)

 for_(i max(1p2) i min(100100p) i)

 j i-p-1

 Aij_Aij Bi-1j_(s1)

 j i-p

 Bij_Aij-1 Bij_(s2)



 space (2c)

 if_(p -1)

 i 101p

 j_100

 Aij_Aij Bi-1j_(s1)





 verbatim



 Splitting space (2) on the value of

 redblack-opt3-fig



 figure



 itemize



 a)

 where only is executed



 b)

 where both and

 are executed and



 c)

 where only is executed



 itemize

 The loop_nest for space (2) in Fig redblack-opt2-fig

 can thus be written as in Fig redblack-opt3-fig



 figurehtfb



 verbatim

 space (1) p -100

 A1100 A1100 B0100 (s1)



 space (2)

 for (p -99 p 98 p)

 if_(p 0)

 Bp11 Ap10 Bp11 (s2)

 for_(i max(1p2) i min(100100p) i)

 Aii-p-1 Aii-p-1 Bi-1i-p-1 (s1)

 Bii-p Aii-p-1 Bii-p (s2)



 if_(p -1)

 A101p100 A101p100 B101p-1100 (s1)



 space (3) p 99

 B1001 A1000 B1001 (s2)

 verbatim



 Optimized code equivalent to Fig redblack-code2-fig

 redblack-opt4-fig



 figure



 Figure redblack-opt4-fig shows the optimized program We have

 substituted Fig redblack-opt3-fig for the loop_nest in

 Fig redblack-opt2-fig We also propagated out assignments to

 and into the array_accesses When optimizing at the

 intermediate-code level some of these assignments will be identified as

 common_subexpressions and re-extracted from the array-access code

 ex



 Code Transformation Examples

 ch11transforms



 In two simple steps finding affine_partition mappings and

 generating SPMD code that preserves the original order for each

 processor the affine-partitioning algorithm can transform code in

 significant ways

 Figure figtransforms gives some simple examples to demonstrate

 the power of these techniques

 Any combination

 of these primitive transforms can be_expressed as an affine_partition



 figurehtbp



 center

 tabularlcl

 Original Code Mapping Transformed Code



 tabularl

 for (i1 iN_i)



 Bi Ci (s1)



 for (j1 jN j)



 Aj Bj (s2)



 tabular



















 tabularl

 for (p1 pN p)



 Bp Cp



 Ap Bp







 tabular





 tabularl

 for (i1 iN_i)



 Bi Ci (s1)



 Ai Bi-1 (s2)







 tabular



















 tabularl

 for (p1 pN p)



 BpCp



 Ap1Bp







 tabular





 tabularl

 for (i1 i2N i)



 Bi Ci (s1)



 for (j1 jN j)



 A2jB2j (s2)



 tabular



















 tabularl

 for (p1 p2N p)



 Bp Cp



 if_(p mod 2)



 Ap Bp







 tabular





 tabularl

 for_(i0 iN_i)



 Bi Ci (s1)



 for_(j0 jN j)



 Aj BN-j (s2)



 tabular



















 tabularl

 for (p0 pN p)



 Bp Cp



 AN-p Bp







 tabular





 tabularl

 for (i1 iN_i)



 for_(j0 jN j)



 Aij



 Ai-1j (s)



 tabular



















 tabularl

 for (p0 pN p)



 for (i1 iN_i)



 Aip Ai-1p



 tabular





 tabularl

 for (i1 iN_i)



 for_(j0 jN j)



 Aij



 Aiij (s)



 tabular



















 tabularl

 for (p1 p2N p)



 for (imax(ip-N)i



 min(pN) i)



 Aip-i Aip



 tabular





 tabular

 center



 Examples of primitive code transforms using affine_partitioning

 figtransforms

 figure



 Notice_that in each case we have introduced a new loop constant

 Each statement involving array_accesses has p expressed in terms of the

 loop_indexes and one or_more loops involving the original loop_indexes

 and are replaced_by a loop on Of_course the transformation

 requires that we respect data dependencies among the accesses and

 therefore among the iterations of each statement



 For_instance consider the example of loop fusion in

 Fig figtransforms The only data dependency is between the

 access Bi in and Bj in Because the loop with

 is executed completely after the loop containing we must_be sure

 that for any integer the write of Bi by with occurs

 before the read of Bj in with Since we replace both

 and by and execute before in the transformed

 code we know these data dependencies are respected



 Note also that we

 have improved the temporal_locality greatly Instead of needing to hold

 all of in cache between the first loop and the second The value of

 computed by is used immediately by and need not be

 cached

 Heap Storage

 heap-sect



 Data whose existence is tied to one invocation of one function can

 be allocated on a stack as we discussed starting in

 Section stack-alloc-sect However many languages enable us

 to create data that lives indefinitely or until the program

 deletes it explicitly For_example C gives the programmer the

 functions malloc and free to obtain and give back

 arbitrarily sized blocks of storage C also uses new and

 delete to create and destroy objects The area of virtual

 memory from which these new data items are obtained is called the

 heap In this_section we discuss the memory_manager

 the subsystem that allocates and deallocates space within the

 heap We also introduce garbage_collection which is the

 process of finding spaces within the heap that are no_longer used

 by the program and that therefore can be reallocated to house

 other data items



 The Memory Manager



 A memory_manager is usually implemented as part of a language's

 run-time system and serves as an interface between the application

 programs and the operating_system The memory_manager responds to

 a program's request to allocate and deallocate arbitrarily sized

 chunks of memory It gets pages of (virtual) memory from the

 operating_system if necessary to provide the needed space and

 maintains information on the free_space in the heap



 For languages_like C or C that deallocate chunks of storage manually (ie by explicit statements of the program such_as free or delete) the memory_manager is also responsible_for

 implementing the deallocation

 For languages with automatic_garbage collection such_as Java it

 is the garbage_collector that deallocates memory

 In that case

 the garbage_collector is an important subsystem of the memory_manager

 one that we_shall study intensely in later sections



 In this subsection we describe what a memory_manager must do

 Section secallocperf gives the

 metrics by which its performance is measured

 In Section alloc-frag-subsect we consider how the memory

 manager can be designed to improve the performance of the user program it

 supports



 To_begin

 each process is given some initial heap_storage space which is allowed

 to grow to accommodate the needs of the process as it executes

 Starting with this initial space the memory_manager gets memory from

 the operating_system in units of pages when necessary These pages

 extend the virtual address space of the heap_storage in a contiguous

 manner that is in consecutive bytes of the virtual_memory

 The fact that the heap pages may not be contiguous in physical_memory is

 unimportant since the hardware and operating_system provide the

 appropriate mechanism to access bytes of the heap consecutively

 A limit is usually imposed on a process's heap_storage size to ensure

 that errant processes do_not grow unchecked ultimately the size of

 the heap_storage cannot exceed that of the address space of the

 machine



 A memory_manager keeps_track of all the free_space in the heap

 storage at all times and it performs the two following basic

 functions



 enumerate



 Allocation

 When a program requests memory for a variable or objectfootnoteIn

 what_follows we

 shall_refer to things requiring memory space as objects

 even if they are not true objects in the object-oriented_programming

 sensefootnote

 the memory_manager tries to produce a chunk of contiguous heap memory of the

 requested size It first tries to satisfy the request with the free_space

 in the heap if no chunk of the needed size

 is available it tries to increase the heap_storage

 space by getting memory pages from the operating_system

 If space is

 exhausted the memory_manager

 passes that information back to the application program



 Deallocation

 The memory_manager returns deallocated

 space to the pool of free_space so it can reuse the space to satisfy

 other allocation requests Deallocation can be either manual by

 explicit operations such_as free or it can be automatic -

 performed by the garbage_collector

 Memory managers typically do_not return

 memory to the operating_system even if the program's heap usage drops



 enumerate



 Performance Considerations

 secallocperf



 Life would be very_simple if



 enumerate



 All allocation requests were for chunks of the same size and



 Storage were released in a predictable pattern such_as

 first-allocated-first deallocated



 enumerate

 There_are some languages such_as Lisp for which condition (1) holds

 In principle

 Lisp uses only one data element - a two-pointer cell - from which

 all data_structures are built Condition (2) also holds in some

 situations the most_common being data that can be allocated on the

 run-time_stack

 because the second of two data elements to be allocated is always deallocated

 first (ie a first-allocated-last-deallocated discipline holds)

 However in most languages neither (1) nor (2) holds in general

 Rather data elements of different sizes are allocated and there is no

 good way to predict the lifetimes of all allocated_objects



 Thus the memory_manager must_be prepared to service in any order

 allocation and deallocation

 requests of any size ranging_from one byte to as large as

 the program's entire address space



 At the beginning of program execution the heap is one contiguous

 unit of free_space As the program allocates and deallocates memory

 this space is broken up into free and used chunks of memory and the

 free_chunks need not reside in a contiguous area of the heap We refer

 to the free_chunks of memory as holes With each allocation request

 the memory_manager must place the requested chunk of memory in a

 large-enough hole Unless a hole of exactly the right size is found

 we need to split some hole creating a yet smaller hole



 With

 each deallocation request the freed chunks of memory are added back

 to the pool of free_space We need to coalesce contiguous holes

 into larger holes as the holes can only get smaller otherwise If we

 are not careful the memory may end up getting fragmented

 consisting of large_numbers of small noncontiguous holes It is then

 possible that no hole is large_enough to satisfy a future request

 even_though there may be sufficient aggregate free_space



 From this discussion we can see the properties we

 desire of memory managers



 enumerate



 Space Efficiency A memory_manager

 should minimize

 fragmentation Doing_so minimizes the total heap space needed by a

 program and therefore

 allows larger programs to run in a fixed virtual address space



 Low Overhead Because memory allocations and deallocations are

 frequent operations in many programs it is important that these

 operations be as

 efficient as possible

 That is we_wish to minimize the overhead -

 the fraction of execution time_spent performing

 allocation and deallocation

 Notice_that the cost

 of allocations is dominated by small requests the overhead of

 managing large objects is less important because it usually can be amortized

 over a larger amount of computation



 Support for Spatial Locality The memory_manager can

 also affect the efficiency of the rest of the program

 Since the memory_manager is

 responsible_for placing objects in memory it affects the

 program's spatial_locality and hence its running_time It has_been

 found that data allocated together is often used and deallocated

 together Thus if the memory_manager places these objects close_together

 the space can be better utilized and the program may run_faster



 enumerate



 Object Placement to Reduce Fragmentation

 alloc-frag-subsect



 We reduce fragmentation by controlling how the memory_manager places

 new objects in the heap

 It has_been found empirically that a

 good strategy for minimizing fragmentation for real-life programs is to

 allocate the requested memory in the smallest available hole that is

 large_enough This best-fit algorithm tends to leave the large

 holes to satisfy subsequent larger requests

 An alternative called first-fit where an object is placed in the

 first (lowest-address) hole in which it fits takes less time to place

 objects but has_been found inferior to best-fit in overall

 performance



 To implement best-fit_placement more

 efficiently we can separate free_space into bins

 according to their sizes One practical idea is to have many more bins

 for the smaller sizes because there are usually many more small objects For

 example the Lea memory_manager used in the GNU C compiler gcc

 aligns all chunks to 8-byte

 boundaries

 There is a bin for every multiple of 8-byte chunks

 from 16 bytes to 512 bytes Larger-sized bins are logarithmically

 spaced (ie the minimum size for each bin is twice that of the

 previous bin) and within each of these bins

 the chunks are ordered by their size There

 is always a chunk of free_space that can be extended by requesting

 more pages from the operating_system Called the wilderness

 chunk this chunk is treated by Lea as the largest-sized

 bin because of its extensibility



 Binning makes it easy to find the best-fit chunk

 If as for small sizes requested from the Lea memory_manager there is a bin

 for chunks of that size only we may take any chunk from that bin

 For sizes that do_not have a private bin we find the one bin that is

 allowed to include

 chunks of the desired size Within that bin we either look for and

 select the first

 chunk that is sufficiently large or we spend more time and find the

 smallest chunk in the bin that is sufficiently large ie we can use

 either a first-fit or a best-fit strategy within the bin

 Note_that when the fit is not exact the remainder of the chunk will

 generally need to be placed in a bin with smaller sizes



 However it may be that the target bin is empty or all chunks in that

 bin are too small to accommodate the object for which space has_been

 requested

 In that case we simply repeat the search using the bin for the next

 larger size(s)

 Eventually we either find a chunk we can use or we reach the

 wilderness chunk from which we can surely obtain the needed space

 possibly by going to the operating_system and getting additional pages

 for the heap



 While best-fit_placement tends to improve space utilization it

 may not be the best in terms of spatial_locality Chunks allocated at

 about the same time by a program tend to have similar reference

 patterns and to have similar lifetimes Placing them close

 together thus improves the program's spatial_locality One useful

 adaptation of the best-fit algorithm is to modify the placement in the

 case when a chunk of the exact requested size cannot be found In

 this case we use a next-fit strategy trying to allocate the object

 in the chunk that has last been split whenever enough space for the new

 object remains in that chunk Next-fit also tends to improve the

 speed of the allocation operation



 Managing and Coalescing Free Space

 secallocfreelist



 When an object is deallocated manually the memory_manager must make

 its chunk free so it can be allocated again In some circumstances it

 may also be possible to combine (coalesce)

 that chunk with adjacent_chunks of the

 heap to form a larger chunk There is an advantage to doing_so since

 we can always use a large chunk to do the work of small chunks of equal

 total size but many small chunks cannot hold one large object as the

 combined chunk could



 If we keep a bin for chunks of one fixed size as Lea does for small

 sizes then we may prefer not to coalesce adjacent blocks of that size

 into a chunk of double the size It is simpler to keep all the chunks

 of one size in as many pages as we need and never coalesce them

 Then a simple allocationdeallocation scheme is to keep a bitmap with

 one bit for each chunk in the bin A 1 indicates the chunk is occupied

 0 indicates it is free When a chunk is deallocated we change its 1 to

 a 0 When we need to allocate a chunk we find any chunk with a 0 bit

 change that bit to a 1 and use the corresponding chunk

 If there are no free_chunks we get a new page divide it into chunks of

 the appropriate size and extend the bit_vector



 Matters are more_complex when

 the heap is managed as a whole without binning or if we are willing to

 coalesce adjacent_chunks and move the resulting chunk to a different bin

 if necessary

 There_are two data_structures that are useful to support coalescing of

 adjacent free blocks



 enumerate



 Boundary Tags

 At both the low and high ends of each chunk whether free or allocated

 we keep vital information

 At the extreme ends we keep a freeused_bit that tells whether or not

 the block is currently allocated (used) or available (free)

 Adjacent to each freeused_bit is a count of the total_number of bytes

 in the chunk



 A Doubly Linked Embedded Free List

 The free_chunks (but_not the allocated chunks) are also linked in a

 doubly_linked list The pointers for this list are within the blocks

 themselves say adjacent to the boundary_tags at either end Thus no

 additional space is needed for the free_list although its existence

 does place a lower_bound on how small chunks can get they must

 accommodate two boundary_tags and two pointers at_least

 The order of chunks on the free_list is arbitrary For_example the

 list could be sorted by size thus facilitating best-fit_placement



 enumerate

 Note_that the use of this structure does put a lower_bound on the

 effective size of objects

 That is all chunks much contain enough space to store two pointers

 plus the boundary_tags even if the object is a single byte



 ex

 Figure boundary-tag-fig shows part of a heap with three adjacent

 chunks and Chunk of size 100 has just been

 deallocated and returned to the free_list Since we know the beginning

 (left end) of we also know the end of the chunk that happens to

 be immediately to 's left namely in this

 example Inspecting the freeused_bit at the right

 end of we find that too is free We may therefore coalesce

 and into one chunk of 300 bytes



 figurehtfb

 fileuullmanalsuch7figsboundary-tageps

 Part of a heap and a doubly_linked free_list

 boundary-tag-fig

 figure



 It might be the case that chunk the chunk immediately to 's

 right is also free in which case we can combine all of and

 However if we always coalesce chunks when we can then there can

 never be two adjacent free_chunks so we never have to look further than

 the two chunks adjacent to the one being deallocated

 In the current case we find the beginning of by starting_at the

 left end of which we know finding the total_number of bytes in

 which is found in the left boundary tag of and is 100 bytes

 With this information we find the right_end of and the beginning of

 the chunk to its right At that point we examine the freeused_bit of

 and find it is not available for coalescing



 Since we must coalesce and we need to remove one of them from

 the free_list The doubly_linked free-list structure lets_us find the

 chunks before and after each of and Notice_that it should not

 be assumed that physical neighbors and are also adjacent on the

 free_list Knowing the chunks preceding and following and on

 the free_list it

 is straightforward to manipulate pointers on the list to replace and

 by one coalesced chunk

 ex



 Interaction with Garbage_Collection



 Our discussion so_far assumes manual memory_management The use of

 automatic_garbage collection offers some new opportunities for

 optimizations For_example a garbage_collector generally finds many

 objects to be freed at the same time Consequently deallocations

 can be batched and optimized differently Also to identify garbage

 in the program a garbage_collector needs to know where all the pointers

 are This information can be used to relocate objects Moving all

 the allocated_objects to contiguous storage gets rid of fragmentation

 altogether The interaction between garbage_collection

 and memory_management is discussed in more_detail in

 Section secgcrelocate



 exer

 Suppose the heap consists of seven chunks

 starting_at address 0

 The sizes of the chunks in order are

 80 30 60 50 70 20 40 bytes When we place an

 object in a chunk we put it at the high end if there is enough space

 remaining to form a smaller chunk (so that smaller chunk can remain on

 the linked_list of free_space easily) However we cannot tolerate

 chunks of fewer that 8 bytes so if an object is almost as large as the

 selected chunk we give it the entire chunk and place the object at the

 low end of the chunk If we request space for objects of the following

 sizes 32 64 48 16 in that order what does the free_space list look

 like after satisfying the requests if the method of selecting chunks

 is



 itemize



 a) First fit

 b) Best fit



 itemize

 exer

 Using Ambiguous Grammars



 ambig-gram-sect It is a theorem that every ambiguous

 grammar fails to be LR and thus is not in any of the classes of

 grammars discussed in the previous section Certain types of

 ambiguous_grammars however are useful in the specification and

 implementation of languages as we_shall see in this_section For

 language_constructs like expressions an ambiguous_grammar provides

 a shorter more natural specification than any equivalent

 unambiguous_grammar Another use of ambiguous_grammars is in

 isolating commonly occurring syntactic_constructs for special_case

 optimization With an ambiguous_grammar we can specify the

 special_case constructs by carefully adding new productions to the

 grammar



 Although the grammars we use are ambiguous in all cases we

 specify disambiguating rules that allow only one parse_tree for

 each sentence In this way the overall language specification

 still remains unambiguous We also stress that ambiguous

 constructs should be used sparingly and in a strictly controlled

 fashion otherwise there can be no guarantee as to what language

 is recognized by a parser



 Precedence and Associativity to Resolve Conflicts

 Consider the ambiguous_grammar (expr-ambig-display) for

 expressions with operators and repeated_here for

 convenience

 disp

 E_E E

 E_E ( E

 ) id

 disp

 This grammar is ambiguous because it does_not specify the

 associativity or precedence of the operators and

 The unambiguous_grammar (expr-gram-display) which includes

 productions EE

 TT and TT

 FF generates the same language but gives

 lower precedence_than and makes both operators

 left-associative There_are two_reasons why we might prefer to use

 the ambiguous_grammar First as we_shall see we can easily

 change the associativity and precedence of the operators

 and without disturbing the productions of

 (expr-ambig-display) or the number of states in the

 resulting parser Second the parser for the unambiguous_grammar

 will spend a substantial fraction of its time reducing by the

 productions ET and T

 F whose sole function is to enforce

 associativity and precedence The parser for the ambiguous_grammar

 (expr-ambig-display) will not waste time reducing by these

 single productions as they are called



 figurehtfb

 center

 tabularl_l c p l_l c_l

 E_E E_EE



 E_EE E_EE



 E_EE E_EE



 E_(E) E_(E)



 E_id E_id







 E_E E_(E)



 E_EE E_EE



 E_EE E_EE







 E_(E) E_EE



 E_EE E_EE



 E_EE E_EE



 E_(E)



 E_id E_EE



 E_EE



 E_id E_EE







 E_EE E_(E)



 E_EE



 E_EE



 E_(E)



 E_id

 tabular

 Sets of LR(0)_items for an augmented expression grammar

 items-ambig-fig

 center

 figure





 The sets of LR(0)_items for the ambiguous expression grammar

 expr-ambig-display augmented by E

 E are shown in Fig items-ambig-fig Since grammar

 (expr-ambig-display) is ambiguous parsing_action conflicts

 will be generated when we try to produce an LR_parsing table from

 the sets of items The states corresponding to sets of items

 and generate these conflicts Suppose we use the SLR

 approach to constructing the parsing_action table The conflict

 generated_by between reduction by E

 EE and shift on and

 cannot be_resolved because and are each in

 (E) Thus both actions would be called for on

 inputs and A similar conflict is generated_by

 between reduction by E

 EE and shift on inputs and In

 fact each of our LR_parsing table construction methods will

 generate these conflicts



 However these problems can be_resolved using the precedence and

 associativity information for and Consider the

 input ididid

 which causes a parser based_on Fig items-ambig-fig to enter

 state_7 after processing idid in

 particular the parser reaches a configuration

 disp

 tabularl_l r

 prefix stack input



 EE 0_1 4 7 id



 tabular

 disp

 For_convenience the symbols corresponding to the states 1 4 and

 7 are also shown under PREFIX



 Assuming that takes_precedence over we know the

 parser should shift onto the stack preparing to reduce

 the and its surrounding id symbols to an expression

 This is what the SLR_parser of Fig_action-goto-fig for the

 same language would do On the other_hand if takes

 precedence_over we know the parser should reduce

 EE to E Thus the relative precedence of

 followed_by uniquely determines how the parsing

 action conflict between reducing E

 EE and shifting on in state_7 should be

 resolved



 If the input had been idid

 id instead the parser would still reach a

 configuration in which it had stack after processing

 input idid On input there is

 again a shiftreduce_conflict in state_7 Now however the

 associativity of the operator determines how this conflict

 should be_resolved If is left-associative the correct

 action is to reduce by EE

 E That is the id symbols surrounding the first

 must_be grouped first Again this choice coincides with

 what the SLR would do



 In summary assuming is left-associative the action of

 state_7 on input should be to reduce by E

 EE and assuming that

 takes_precedence over the action of state_7 on input

 should be to shift Similarly assuming that is

 left-associative and takes_precedence over we can argue

 that state 8 which can appear on top of the stack only when

 EE are the top three grammar_symbols

 should have action reduce E

 EE on both and inputs In the

 case of input the reason is that takes_precedence

 over while in the case of input the rationale is

 that is left-associative



 Proceeding in this way we obtain the LR_parsing table shown in

 Fig ambig-table-fig Productions 1-4 are E

 EE_E

 EE_E

 (E) and E

 id respectively It is interesting that a similar

 parsing_action table would be produced_by eliminating the

 reductions by the single productions E

 T and ET from the SLR

 table for the unambiguous expression grammar

 (expr-gram-display) shown in Fig_action-goto-fig

 Ambiguous grammars like the one for expressions can be handled in

 a similar way in the context of LALR and canonical_LR parsing



 figurehtfb

 center

 tabularcc_c c_c c_c c_c

 -7pt0pt0pt

 state 6c



 2-9

 id (_) E





 0 s3_s2 1



 1 s4_s5 acc



 2 s3_s2 6



 3_r4 r4_r4 r4



 4 s3_s2 7



 5 s3_s2 8



 6 s4_s5 s9



 7 r1 s5 r1_r1



 8 r2_r2 r2_r2



 9 r3_r3 r3_r3





 tabular

 Parsing_table for grammar_(expr-ambig-display)

 ambig-table-fig

 center

 figure



 The Dangling-else Ambiguity

 Consider_again the following grammar for conditional_statements

 disp

 tabularl_c l

 stmt if expr_then stmt_else stmt



 if expr_then stmt



 other

 tabular

 disp

 As we noted in Section writing-grammars-sect this grammar

 is ambiguous because it does_not resolve the dangling-else

 ambiguity To simplify the discussion let_us consider an

 abstraction of this grammar where stands_for if

 expr_then stands_for else and stands_for

 all other productions We can then write the grammar with

 augmenting production as



 displaydangling-abs-display

 317ptstabularl_c l







 tabular

 (dangling-abs-display)

 display

 The sets of LR(0)_items for grammar (dangling-abs-display)

 are shown in Fig dangling-items-fig The ambiguity in

 (dangling-abs-display) gives rise to a shiftreduce_conflict

 in There calls for a shift of

 and since () item

 calls for reduction by on input



 figurehtfb

 center

 minipaget2in

 tabularp_c c_l







































 tabular

 minipage

 minipaget15in

 tabularp_c c_l





































 tabular

 minipage

 LR(0) states for augmented_grammar

 (dangling-abs-display)dangling-items-fig

 center

 figure



 Translating back to the ifthen

 else terminology given

 disp

 if expr_then stmt

 disp

 on the stack and else as the first input_symbol should we

 shift else onto the stack (ie shift ) or reduce

 if expr_then stmt (ie reduce by

 ) The answer is that we should shift else

 because it is associated_with the previous then In the

 terminology of grammar (dangling-abs-display) the on

 the input standing for else can only form part of the

 right_side beginning with the on the top of the stack If

 what_follows on the input cannot be_parsed as an

 completing right_side then it can be shown that there is

 no other parse possible



 We_conclude that the shiftreduce_conflict in should be

 resolved in favor of shift on input The SLR_parsing table

 constructed from the sets of items of Fig items-ambig-fig

 using this resolution of the parsing_action conflict in on

 input is shown in Fig table-ambig-fig Productions 1

 through 3 are and

 respectively



 figurehtfb

 center

 tabularcc_c c_c c_c

 -7pt0pt0pt

 state 4c



 2-7







 0 s2_s3 1



 1 acc



 2 s2_s3 4



 3 r3_r3



 4 s5 r2



 5 s2_s3 6



 6 r1_r1





 tabular

 LR_parsing table for the dangling-else

 grammartable-ambig-fig

 center

 figure



 For_example on input the parser makes the moves shown in

 Fig moves-ambig-fig corresponding to the correct

 resolution of the dangling-else At line_(5) state 4 selects

 the shift_action on input whereas at line_(9) state 4 calls

 for reduction by on input



 figurehtfb

 center

 tabularr_l l_r l



 STACK_SYMBOLS INPUT_ACTION





 (1) 0 shift



 (2) 0 2 shift



 (3) 0 2 2 shift



 (4) 0 2 2_3 shift



 (5) 0 2 2_4 reduce by



 (6) 0 2 2_4 5 shift



 (7) 0 2 2_4 5 3 reduce by



 (8) 0 2 2_4 5_6 reduce by



 (9) 0 2_4 reduce by



 (10) 0_1 accept





 tabular

 Parsing actions on input

 moves-ambig-fig

 center

 figure



 By way of comparison if we are unable to use an ambiguous_grammar

 to specify conditional_statements then we would have to use a

 bulkier grammar along the lines of Example matched-if-ex



 Error_Recovery in LR_Parsing



 An LR_parser will detect an error when it consults the parsing

 action table and finds an error entry Errors are never detected

 by consulting the goto table An LR_parser will announce an error

 as_soon as there is no valid continuation for the portion of the

 input thus far scanned A canonical_LR parser will not make even a

 single reduction before announcing an error SLR and LALR_parsers

 may make several reductions before announcing an error but they

 will never shift an erroneous_input symbol onto the stack



 In LR_parsing we can implement panic-mode error_recovery as

 follows We scan down the stack until a state with a goto on a

 particular nonterminal is found Zero or_more input symbols

 are then discarded until a symbol is found that can

 legitimately follow The parser then stacks the state

 and resumes normal_parsing There might be more_than one

 choice for the nonterminal Normally these would be

 nonterminals representing major program pieces such_as an

 expression statement or block For_example if is the

 nonterminal stmt might be semicolon or which marks

 the end of a statement sequence



 This method of recovery attempts to isolate the phrase containing

 the syntactic error The parser determines that a string_derivable

 from contains an error Part of that string has already_been

 processed and the result of this processing is a sequence of

 states on top of the stack The remainder of the string is still

 in the input and the parser attempts to skip_over the remainder

 of this string by_looking for a symbol on the input that can

 legitimately follow By removing states from the stack

 skipping over the input and pushing on the stack

 the parser pretends that it has found an instance of and

 resumes normal_parsing



 Phrase-level recovery is implemented_by examining each error entry

 in the LR_parsing table and deciding on the basis of language

 usage the most likely programmer error that would give rise to

 that error An appropriate recovery procedure can then be

 constructed presumably the top of the stack andor first input

 symbols would be modified in a way deemed appropriate for each

 error entry



 In designing specific error-handling routines for an LR_parser we

 can fill in each blank entry in the action field with a pointer to

 an error routine that will take the appropriate action selected by

 the compiler_designer The actions may include insertion or

 deletion of symbols from the stack or the input or both or

 alteration and transposition of input symbols We must make our

 choices so that the LR_parser will not get into an_infinite loop

 A strategy that assures at_least one input_symbol will be removed

 or eventually shifted or that the stack will_eventually shrink if

 the end of the input has_been reached is sufficient in this

 regard Popping a stack state that covers a nonterminal should be

 avoided because this modification eliminates from the stack a

 construct that has already_been successfully parsed



 exlr-error-ex

 Consider_again the expression grammar

 disp

 E_E E

 E_E ( E

 ) id

 disp



 Figure error-table-fig shows the LR_parsing table from

 Fig ambig-table-fig for this grammar modified for error

 detection and recovery We have changed each state that calls for

 a particular reduction on some input symbols by_replacing error

 entries in that state by the reduction This change has the effect

 of postponing the error_detection until one or_more reductions are

 made but the error will still be caught before any shift_move

 takes place The remaining blank entries from

 Fig ambig-table-fig have_been replaced_by calls to error

 routines

 figurehtfb

 center

 tabularcc_c c_c c_c c_c

 -7pt0pt0pt

 state 6c



 2-9

 id (_) E





 0 s3_e1 e1_s2 e2_e1 1



 1 e3 s4_s5 e3 e2 acc



 2 s3_e1 e1_s2 e2_e1 6



 3_r4 r4_r4 r4_r4 r4



 4 s3_e1 e1_s2 e2_e1 7



 5 s3_e1 e1_s2 e2_e1 8



 6 e3 s4_s5 e3 s9 e4



 7 r1_r1 s5 r1_r1 r1



 8 r2_r2 r2_r2 r2_r2



 9 r3_r3 r3_r3 r3_r3





 tabular

 LR_parsing table with error_routines

 error-table-fig

 center

 figure



 The error_routines are as_follows

 description

 e1

 This routine is called from states 0 2_4 and 5 all of which

 expect the beginning of an operand either an id or a left

 parenthesis Instead an operator or or the end

 of the input was found

 disp

 push_state 3 (the goto of states 0 2_4 and 5 on id)



 issue_diagnostic missing operand

 disp



 e2

 Called from states 0_1 2_4 and 5 on finding a right

 parenthesis

 disp

 remove the right_parenthesis from the input



 issue_diagnostic unbalanced right_parenthesis

 disp



 e3

 Called from states 1 or 6 when expecting an operator and an

 id or right_parenthesis is found

 disp

 push_state 4 (corresponding symbol ) onto the stack



 issue_diagnostic missing operator

 disp



 e4

 Called from state 6 when the end of the input is found

 disp

 push_state 9 (corresponding symbol )) onto the stack



 issue_diagnostic missing right_parenthesis

 disp

 description



 On the erroneous_input id) the

 sequence of configurations entered by the parser is shown in

 Fig error-moves-fig

 ex



 figurehtfb

 center

 tabularl_l r_l



 STACK_SYMBOLS INPUT_ACTION





 0 id)



 0 3 id )



 0_1 E )



 0_1 4 E ) unbalanced right_parenthesis



 e2 removes right_parenthesis



 0_1 4 E missing operand



 e1 pushes state 3 onto stack



 0_1 4 3 Eid



 0_1 4 7 EE



 0_1 EE





 tabular

 Parsing and error_recovery moves made by an LR_parser

 error-moves-fig

 center

 figure

 Using Ambiguous Grammars

 ambig-gram-sect



 It is a fact that every ambiguous_grammar fails to be LR and thus

 is not in any of the classes of grammars discussed in the previous

 two sections However certain types of ambiguous_grammars are

 quite useful in the specification and implementation of languages

 For language_constructs like expressions an ambiguous_grammar

 provides a shorter more natural specification than any equivalent

 unambiguous_grammar Another use of ambiguous_grammars is in

 isolating commonly occurring syntactic_constructs for special-case

 optimization With an ambiguous_grammar we can specify the

 special-case constructs by carefully adding new productions to the

 grammar



 Although the grammars we use are ambiguous in all cases we

 specify disambiguating rules that allow only one parse_tree for

 each sentence In this way the overall language specification

 becomes unambiguous and sometimes it becomes possible to design

 an LR_parser that follows the same ambiguity-resolving choices We

 stress that ambiguous constructs should be used sparingly and in a

 strictly controlled fashion otherwise there can be no guarantee

 as to what language is recognized by a parser



 Precedence and Associativity to Resolve Conflicts



 Consider the ambiguous_grammar (expr-ambig-display) for

 expressions with operators and repeated_here for

 convenience



 center



 center

 This grammar is ambiguous because it does_not specify the

 associativity or precedence of the operators and The

 unambiguous_grammar (expr-gram-display) which includes

 productions and generates the

 same language but gives lower precedence_than and makes

 both operators left_associative There_are two_reasons why we

 might prefer to use the ambiguous_grammar First as we_shall see

 we can easily change the associativity and precedence of the

 operators and without disturbing the productions of

 (expr-ambig-display) or the number of states in the

 resulting parser Second the parser for the unambiguous_grammar

 will spend a substantial fraction of its time reducing by the

 productions and whose sole function is

 to enforce associativity and precedence The parser for the

 ambiguous_grammar (expr-ambig-display) will not waste time

 reducing by these single productions (productions whose body

 consists of a single nonterminal)



 figurehtfb



 center

 tabularr_l p r_l





























































































 tabular

 center



 Sets of LR(0)_items for an augmented expression grammar

 items-ambig-fig

 figure



 The sets of LR(0)_items for the ambiguous expression grammar

 (expr-ambig-display) augmented by

 are shown in Fig items-ambig-fig Since grammar

 (expr-ambig-display) is ambiguous there will be

 parsing-action_conflicts

 when we try to produce an LR_parsing table from

 the sets of items The states corresponding to sets of items

 and generate these conflicts Suppose we use the SLR

 approach to constructing the parsing_action table The conflict

 generated_by between reduction by

 and shift on or

 cannot be_resolved because and are each in

 (E) Thus both actions would be called for on

 inputs and A similar conflict is generated_by

 between reduction by

 and shift on inputs and In

 fact each of our LR_parsing table-construction methods will

 generate these conflicts



 However these problems can be_resolved using the precedence and

 associativity information for and Consider the

 input

 which causes a parser based_on Fig items-ambig-fig to enter

 state_7 after processing in

 particular the parser reaches a configuration



 center

 tabularl_l r

 prefix stack input



 0_1 4 7



 tabular

 center

 For_convenience the symbols corresponding to the states 1 4 and

 7 are also shown under PREFIX



 If takes_precedence over we know the

 parser should shift onto the stack preparing to reduce

 the and its surrounding symbols to an expression

 This choice was made by the SLR_parser of Fig_action-goto-fig

 based_on an unambiguous_grammar for the

 same language On the other_hand if takes

 precedence_over we know the parser should reduce to

 Thus the relative precedence of

 followed_by uniquely determines how the parsing

 action conflict between reducing

 and shifting on in state_7 should be

 resolved



 If the input had been

 instead the parser would still reach a

 configuration in which it had stack 0_1 4 7 after processing

 input On input there is

 again a shiftreduce_conflict in state_7 Now however the

 associativity of the operator determines how this conflict

 should be_resolved If is left_associative the correct

 action is to reduce by

 That is the symbols surrounding the first

 must_be grouped first Again this choice coincides with

 what the SLR_parser for the unambiguous_grammar would do



 In summary assuming is left_associative the action of

 state_7 on input should be to reduce by

 and assuming that

 takes_precedence over the action of state_7 on input

 should be to shift Similarly assuming that is

 left_associative and takes_precedence over we can argue

 that state 8 which can appear on top of the stack only when

 are the top three grammar_symbols

 should have the action reduce

 on both and inputs In the

 case of input the reason is that takes_precedence

 over while in the case of input the rationale is

 that is left_associative



 Proceeding in this way we obtain the LR_parsing table shown in

 Fig ambig-table-fig Productions 1 through 4 are

 and

 respectively Notice_that a similar

 parsing_action table would be produced_by eliminating the

 reductions by the single productions and

 from the SLR

 table for the unambiguous expression grammar

 (expr-gram-display) shown in Fig_action-goto-fig

 Ambiguous grammars like the one for expressions can be handled in

 a similar way in the context of LALR and canonical_LR parsing



 figurehtfb

 center

 tabularcc_c c_c c_l c_c

 -7pt0pt0ptstate 6c



 2-9 id (_)



 0 s3_s2 1



 1 s4_s5 acc



 2 s3_s2 6



 3_r4 r4_r4 r4



 4 s3_s2 7



 5 s3_s2 8



 6 s4_s5 s9



 7 r1 s5 r1_r1



 8 r2_r2 r2_r2



 9 r3_r3 r3_r3



 tabular

 Parsing_table for grammar_(expr-ambig-display)

 ambig-table-fig

 center

 figure



 The Dangling-Else Ambiguity

 dang-else-subsect



 Consider_again the following grammar for conditional_statements



 center

 tabularl_c l

 stmt if expr_then stmt_else stmt



 if expr_then stmt



 other

 tabular

 center

 As we noted in Section elim-amb-subsect this grammar

 is ambiguous because it does_not resolve the dangling-else

 ambiguity To simplify the discussion let_us consider an

 abstraction of this grammar where stands_for if expr_then stands_for else and stands_for

 all other productions We can then write the grammar with

 augmenting production as



 displaydangling-abs-display

 317ptstabularr c_l









 tabular

 (dangling-abs-display)

 display

 The sets of LR(0)_items for grammar (dangling-abs-display)

 are shown in Fig dangling-items-fig The ambiguity in

 (dangling-abs-display) gives rise to a shiftreduce_conflict

 in There calls for a shift of

 and since item

 calls for reduction by on input



 figurehtfb

 center

 minipaget2in

 tabularp_l







































 tabular

 minipage

 minipaget15in

 tabularp_l









































 tabular

 minipage

 center



 LR(0) states for augmented_grammar

 (dangling-abs-display)

 dangling-items-fig



 figure



 Translating back to the if-then-else

 terminology given



 center

 if expr_then stmt

 center

 on the stack and else as the first input_symbol should we

 shift else onto the stack (ie shift ) or reduce if expr_then stmt (ie reduce by

 ) The answer is that we should shift else

 because it is associated_with the previous then In the

 terminology of grammar (dangling-abs-display) the on

 the input standing for else can only form part of the

 body beginning with the now on the top of the stack If

 what_follows on the input cannot be_parsed as an

 completing body then it can be shown that there is

 no other parse possible



 We_conclude that the shiftreduce_conflict in should be

 resolved in favor of shift on input The SLR_parsing table

 constructed from the sets of items of Fig dangling-items-fig

 using this resolution of the parsing-action conflict in on

 input is shown in Fig table-ambig-fig Productions 1

 through 3 are and

 respectively



 figurehtfb

 center

 tabularcc_c c_l c_c

 -7pt0pt0ptstate 4c



 2-7



 0 s2_s3 1



 1 acc



 2 s2_s3 4



 3 r3_r3



 4 s5 r2



 5 s2_s3 6



 6 r1_r1



 tabular

 LR_parsing table for the dangling-else

 grammartable-ambig-fig

 center

 figure



 For_example on input the parser makes the moves shown in

 Fig moves-ambig-fig corresponding to the correct

 resolution of the dangling-else At line_(5) state 4 selects

 the shift_action on input whereas at line_(9) state 4 calls

 for reduction by on input



 figurehtfb

 center

 tabularr_l l_r l

 STACK_SYMBOLS INPUT_ACTION



 (1) 0 shift



 (2) 0 2 shift



 (3) 0 2 2 shift



 (4) 0 2 2_3 reduce by





 (5) 0 2 2_4 shift



 (6) 0 2 2_4 5 shift



 (7) 0 2 2_4 5 3 reduce by



 (8) 0 2 2_4 5_6 reduce by



 (9) 0 2_4 reduce by



 (10) 0_1 accept



 tabular

 Parsing actions on input

 moves-ambig-fig

 center

 figure



 By way of comparison if we are unable to use an ambiguous_grammar

 to specify conditional_statements then we would have to use a

 bulkier grammar along the lines of Example matched-if-ex



 Error_Recovery in LR_Parsing

 lr-error-subsect



 An LR_parser will detect an error when it consults the parsing

 action table and finds an error entry Errors are never detected

 by consulting the goto table An LR_parser will announce an error

 as_soon as there is no valid continuation for the portion of the

 input thus far scanned A canonical_LR parser will not make even a

 single reduction before announcing an error SLR and LALR_parsers

 may make several reductions before announcing an error but they

 will never shift an erroneous_input symbol onto the stack



 In LR_parsing we can implement panic-mode error_recovery as

 follows We scan down the stack until a state with a goto on a

 particular nonterminal is found Zero or_more input symbols

 are then discarded until a symbol is found that can

 legitimately follow The parser then stacks the state

 and resumes normal_parsing There might be more_than one

 choice for the nonterminal Normally these would be

 nonterminals representing major program pieces such_as an

 expression statement or block For_example if is the

 nonterminal stmt might be semicolon or which marks

 the end of a statement sequence



 This method of recovery attempts to eliminate the phrase containing

 the syntactic error The parser determines that a string_derivable

 from contains an error Part of that string has already_been

 processed and the result of this processing is a sequence of

 states on top of the stack The remainder of the string is still

 in the input and the parser attempts to skip_over the remainder

 of this string by_looking for a symbol on the input that can

 legitimately follow By removing states from the stack

 skipping over the input and pushing on the stack

 the parser pretends that it has found an instance of and

 resumes normal_parsing



 Phrase-level recovery is implemented_by examining each error entry

 in the LR_parsing table and deciding on the basis of language

 usage the most likely programmer error that would give rise to

 that error An appropriate recovery procedure can then be

 constructed presumably the top of the stack andor first input

 symbols would be modified in a way deemed appropriate for each

 error entry



 In designing specific error-handling routines for an LR_parser we

 can fill in each blank entry in the action field with a pointer to

 an error routine that will take the appropriate action selected by

 the compiler_designer The actions may include insertion or

 deletion of symbols from the stack or the input or both or

 alteration and transposition of input symbols We must make our

 choices so that the LR_parser will not get into an_infinite loop

 A safe strategy will assure that at_least one input_symbol will be removed

 or shifted eventually or that the stack will_eventually shrink if

 the end of the input has_been reached

 Popping a stack state that covers a nonterminal should be

 avoided because this modification eliminates from the stack a

 construct that has already_been successfully parsed



 exlr-error-ex

 Consider_again the expression grammar



 center



 center



 Figure error-table-fig shows the LR_parsing table from

 Fig ambig-table-fig for this grammar modified for error

 detection and recovery We have changed each state that calls for

 a particular reduction on some input symbols by_replacing error

 entries in that state by the reduction This change has the effect

 of postponing the error_detection until one or_more reductions are

 made but the error will still be caught before any shift_move

 takes place The remaining blank entries from

 Fig ambig-table-fig have_been replaced_by calls to error

 routines

 figurehtfb

 center

 tabularcc_c c_c c_c c_c

 -7pt0pt0ptstate 6c



 2-9 id (_)



 0 s3_e1 e1_s2 e2_e1 1



 1 e3 s4_s5 e3 e2 acc



 2 s3_e1 e1_s2 e2_e1 6



 3_r4 r4_r4 r4_r4 r4



 4 s3_e1 e1_s2 e2_e1 7



 5 s3_e1 e1_s2 e2_e1 8



 6 e3 s4_s5 e3 s9 e4



 7 r1_r1 s5 r1_r1 r1



 8 r2_r2 r2_r2 r2_r2



 9 r3_r3 r3_r3 r3_r3



 tabular

 LR_parsing table with error_routines

 error-table-fig

 center

 figure



 The error_routines are as_follows

 description

 e1

 This routine is called from states 0 2_4 and 5 all of which

 expect the beginning of an operand either an id or a left

 parenthesis Instead or the end

 of the input was found



 disp

 push_state 3 (the goto of states 0 2_4 and 5 on id)



 issue_diagnostic missing operand

 disp



 e2

 Called from states 0_1 2_4 and 5 on finding a right

 parenthesis



 disp

 remove the right_parenthesis from the input



 issue_diagnostic unbalanced right_parenthesis

 disp



 e3

 Called from states 1 or 6 when expecting an operator and an id or left_parenthesis is found



 disp

 push_state 4 (corresponding to symbol ) onto the stack



 issue_diagnostic missing operator

 disp



 e4

 Called from state 6 when the end of the input is found



 disp

 push_state 9 (for a right parenthesis) onto the stack



 issue_diagnostic missing right_parenthesis

 disp

 description



 On the erroneous_input the

 sequence of configurations entered by the parser is shown in

 Fig error-moves-fig

 ex



 figurehtfb

 center

 tabularl_l r_l

 STACK_SYMBOLS INPUT_ACTION



 0



 0 3



 0_1



 0_1 4 unbalanced right_parenthesis



 e2 removes right_parenthesis



 0_1 4 missing operand



 e1 pushes state 3 onto stack



 0_1 4 3



 0_1 4 7



 0_1



 tabular

 Parsing and error_recovery moves made by an LR_parser

 error-moves-fig

 center

 figure



 hexer

 The following is an ambiguous_grammar for expressions with binary

 infix operators at different_levels of precedence



 center

 id

 center



 itemize



 a)

 As a function of what are the SLR sets of items



 b)

 How_would you resolve the conflicts in the SLR items so that all

 operators are left_associative and

 takes_precedence over which takes_precedence over

 and so on



 c)

 Show the SLR_parsing table that results from your decisions in

 part (b)



 d)

 Repeat parts (a) and (c) for the unambiguous_grammar which defines the

 same set of expressions shown in Fig n-ops-fig



 e)

 How do the counts of the number of sets of items and the sizes of the

 tables for the two (ambiguous and unambiguous) grammars compare What

 does that comparison tell you about the use of ambiguous expression

 grammars



 itemize

 hexer



 figurehtfb



 center

 tabularl_c l

















 id



 tabular

 center



 Unambiguous grammar for operators

 n-ops-fig



 figure



 hexer

 In Fig lr-ec-gram-fig is a grammar for certain statements

 similar to that discussed in Exercise td-ec-gram-exer

 Again and are terminals standing for conditional

 expressions and other statements respectively



 itemize



 a)

 Build an LR_parsing table for this grammar resolving conflicts in the

 usual way for the dangling-else problem



 b)

 Implement error

 correction by filling in the blank entries in the parsing_table with

 extra reduce-actions or

 suitable error-recovery routines



 c)

 Show the behavior of your parser on the following inputs



 center

 tabularl_l

 if then if then end



 while do begin if then end



 tabular

 center



 itemize

 hexer



 figurehtfb



 centertabularl c_l

 stmt if e then_stmt



 if then_stmt else_stmt



 while e do stmt



 begin list end







 list list stmt



 stmt



 tabularcenter



 A grammar for certain kinds of statements

 lr-ec-gram-fig



 figure

 A Simple Pointer-Analysis Algorithm

 andersen-sect



 In this_section we begin the discussion of a very_simple

 flow-insensitive pointer-alias_analysis assuming that there are no

 procedure_calls We_shall show in subsequent sections how to handle

 procedures first context insensitively then context sensitively

 Flow sensitivity adds a lot of complexity and is less important to

 context_sensitivity for languages_like Java where methods tend to be

 small



 The fundamental question that we_wish to ask in pointer-alias_analysis

 is whether a given pair of pointers may be aliased One_way to

 answer this question is to compute for each pointer the answer to the

 question what objects can

 this pointer point to If two pointers can point to the same

 object then the pointers may be aliased



 Why is Pointer_Analysis Difficult



 Pointer-alias analysis for C programs is particularly difficult

 because C programs can perform arbitrary computations on pointers In

 fact one can read in an_integer and assign it to

 a pointer which would render this pointer a potential alias

 of all other pointer variables in the program Pointers in Java known_as

 references are much simpler No arithmetic is allowed and pointers

 can only point to the beginning of an object



 Pointer-alias analysis must_be interprocedural Without

 interprocedural_analysis one must assume that any method called can

 change the contents of all accessible pointer variables thus rendering any

 intraprocedural pointer-alias_analysis ineffective



 Languages allowing indirect function calls present an additional challenge for

 pointer-alias_analysis

 In C one can call a function indirectly by calling a dereferenced

 function pointer We need to know what the function pointer can point

 to before we can analyze the function called And clearly after

 analyzing the function called one may discover more functions that the

 function pointer can point to and therefore the process needs to be

 iterated



 While most functions are called directly in C virtual methods in

 Java cause many invocations to be indirect Given an invocation

 xm() in a Java program there may be many classes to which

 object might belong and

 that have a method_named

 The more_precise our knowledge of the actual type of the

 more_precise our call_graph is Ideally we can determine at_compile time

 the exact class of and thus know exactly which method refers to



 ex

 Consider the following sequence of Java statements



 verbatim

 Object o

 o new String()

 n olength()

 verbatim

 Here is declared to be an Object Without analyzing what

 refers to all possible methods called length declared for all

 classes must_be considered as possible targets Knowing that

 points to a String will narrow interprocedural_analysis to precisely

 the method declared for String

 ex



 It is possible to apply approximations to reduce the number of

 targets For_example statically we can determine what are all the

 types of objects_created and we can limit the analysis to those But we

 can be more accurate if we can discover the call_graph on the fly

 based_on the points-to_analysis obtained at the same time More accurate

 call graphs lead not only to more_precise results but also can

 reduce greatly the analysis time otherwise needed



 Points-to analysis is complicated It is not one of those easy data

 flow problems where we only need to simulate the effect of going

 around a loop of statements

 once Rather as we discover new targets for a pointer all statements

 assigning the contents of that pointer to another pointer need to be

 re-analyzed



 For_simplicity we_shall focus mainly on Java We_shall start with

 flow-insensitive and context-insensitive_analysis assuming for now

 that no methods are called in the program Then we describe

 how we can discover the call_graph on the fly as the points-to_results

 are computed Finally we describe one way of handling context

 sensitivity





 A Model for Pointers and References



 Let_us suppose that our language has the following ways to represent and

 manipulate references



 enumerate



 Certain program variables are of type pointer to or

 reference to where is a type These variables are either

 static or live_on the run-time_stack We call them simply variables



 There is a heap of objects All variables point to heap_objects not

 to other variables These objects will be referred to as heap

 objects



 A heap_object can have fields and the value of a field can be a

 reference to a heap_object (but_not to a variable)



 enumerate

 Java is modeled well by this structure and we_shall use Java syntax in

 examples Note_that C is modeled less

 well since pointer variables can point to other pointer variables in C

 and in principle any C value can be coerced into a pointer



 Since we are performing an insensitive analysis we only need to assert

 that a given variable can point to a given heap_object we do

 not have to address the issue of where in the program can point to

 or in what contexts can point to

 Note_however that variables can be named by their full name In

 Java this name can incorporate the module class method and block

 within a method as_well

 as the variable name itself Thus we can distinguish many variables

 that have the same identifier



 Heap objects do_not have names Approximation often is used to name

 the objects because an_unbounded number of objects may be created

 dynamically One convention is to refer to objects by the

 statement at which they are created As a statement can be executed

 many_times and create a new object each time an assertion like can

 point to really means can point to one or_more

 of the objects_created at the statement labeled



 The goal of the analysis is to determine what each variable and each

 field of each heap_object can point to We refer to this as a points-to_analysis two pointers are aliased if their points-to sets

 intersect We describe here an inclusion-based analysis that

 is a statement such_as v_w causes variable to point to

 all the objects points to but not vice_versa While this approach

 may seem obvious there are

 other alternatives to how we define points-to_analysis For_example

 we can define an equivalence-based analysis such that a statement

 like v_w would turn variables and into one

 equivalence_class pointing to all the variables that each can

 point to While this formulation does_not approximate aliases well

 it provides a quick and often good answer to the question of which variables

 point to the same kind of objects



 Flow Insensitivity



 We start by showing a very_simple example to illustrate the effect of

 ignoring control_flow in points-to_analysis



 ex

 sens-ex

 In Fig sens-fig three objects and are created

 and assigned to variables and respectively Thus

 surely points to points to and points to by

 the end of line_(3)



 figurehtfb



 center

 tabularl_l l

 1) h a new_Object()



 2) i b new_Object()



 3) j c new_Object()



 4) a b



 5) b_c



 6) c a



 tabular

 center



 Java code for Example sens-ex

 sens-fig



 figure



 If you follow the statements (4)_through (6) you discover that after line_(4)

 points only to After line_(5) points only to and after

 line_(6) points only to

 ex



 The above analysis is flow sensitive because we follow the control

 flow and compute what each variable can point to after each statement

 In other_words in addition to considering what points-to

 information each statement generates we also account for what

 points-to_information each statement kills

 For_instance the statement b_c kills the previous fact

 points to and generates the new relationship

 points to what points to



 A flow-insensitive analysis ignores the control_flow which

 essentially assumes that every statement in the program can be

 executed in any order It computes only one global points-to map

 indicating what each variable can possibly point to at any

 point of the program execution If a variable can point to two

 different objects after two different statements in a program we

 simply record that it can point to both objects In other_words in

 flow-insensitive analysis an assignment does_not kill any

 points-to_relations but can only generate more points-to

 relations To_compute the flow-insensitive results we repeatedly add

 the points-to effects of each statement on the points-to relationships

 until_no new relations are found Clearly lack of flow sensitivity

 weakens the analysis results greatly but it tends to reduce the size

 of the representation of the results and make the algorithm converge

 faster



 ex

 insens-ex

 Returning to Example sens-ex lines_(1) through (3) again tell

 us can point to can point to and can point to

 With lines_(4) and (5) can point to both and and

 can point to both and With line_(6) can point to

 and This information affects line_(5) which in turn

 affects line_(4) In the end we are left with the useless conclusion

 that anything can point to anything

 ex



 The Formulation in Datalog



 Let_us now formalize a flow-insensitive pointer-alias_analysis based

 on the discussion_above

 We_shall ignore procedure_calls for now and

 concentrate_on the four kinds of statements that can affect pointers

 enumerate



 Object creation

 h_T v new_T()

 This statement creates a new heap_object and variable can point to

 it



 copy-item

 Copy statement

 v_w

 Here and are variables The statement makes point to

 whatever heap_object currently points to ie is copied_into





 Field store

 vf w

 The type of object that points to must have a field and this

 field must_be of some reference type Let point to heap_object and

 let point to

 This statement makes the field

 in now point to

 Note_that the variable is

 unchanged



 Field load

 v wf

 Here is a variable pointing to some heap_object that has a field

 and points to some heap_object The statement makes

 variable point to



 enumerate



 Note_that compound field accesses in the source code such_as v

 wfg will be broken_down into two primitive field-load statements



 verbatim

 v1 wf

 v v1g

 verbatim



 Let_us now express the analysis formally in Datalog_rules First

 there are only two IDB_predicates we need to compute



 enumerate



 means that variable can point to heap_object



 means that field of heap_object can point to heap

 object



 enumerate



 The EDB relations are constructed from the program itself Since

 the location of statements in a program is irrelevant when the analysis is

 flow-insensitive we only have to assert in the EDB the existence of

 statements that have certain forms In what_follows we_shall make a

 convenient simplification Instead of defining EDB relations to hold

 the information garnered from the program we_shall use a quoted

 statement form to suggest the EDB relation or relations that represent

 the existence of such a statement For_example new

 is an EDB fact asserting that at statement there is an assignment

 that makes variable point to a new object of type

 We assume that in practice there would be a corresponding EDB relation

 that would be populated with ground_atoms one for each statement of

 this form in the program



 With this convention all we need to write the Datalog_program is one

 rule for each of the four types of statements The program is shown in

 Fig_andersen-dl-fig Rule (1) says_that variable can point to heap

 object if statement is an assignment of a new object to

 Rule (2) says_that if there is a copy_statement V W and can

 point to then can point to



 figurehtfb



 centertabularr_r c_l

 1) - new







 2) -











 3) -















 4) -











 tabularcenter



 Datalog_program for flow-insensitive pointer_analysis

 andersen-dl-fig



 figure



 Rule (3) says_that if there is a statement of the form VF W

 can point to and can point to then the field of

 can point to

 Finally rule (4) says_that if there is a statement of the form V WF can point to and the field of can point to

 then can point to Notice_that and are

 mutually_recursive but this Datalog_program can be evaluated by either

 of the iterative_algorithms discussed in Section datalog-eval-subsect



 Using Type Information



 Because Java is type safe variables can only point to types that are

 compatible to the declared types For_example assigning an object

 belonging to a superclass of the declared type of a variable would

 raise a run-time exception Consider the simple example in

 Fig andersen-type-fig

 where is a subclass of This program will generate a run-time

 exception if is true because cannot be assigned an object of

 class Thus statically we can conclude that because of the type

 restriction can only point to and not



 figurehtfb



 verbatim

 S a

 T b

 if (p)

 g b new_T()

 else

 h b new S()



 a b



 verbatim

 Java program with a type error

 andersen-type-fig



 figure



 Thus we introduce to our analysis three EDB_predicates that reflect

 important type information in the code being analyzed We_shall use

 the following



 enumerate



 says_that variable is declared to have type



 says_that heap_object is allocated with type

 The type of a created object may not be known precisely if for

 example the object is returned by a native method Such types are

 modeled conservatively as all possible types



 means that an object of type can be assigned to a

 variable with the type This information is generally gathered from the

 declaration of subtypes in the program but also_incorporates information

 about the predefined classes of the language is always

 true

 enumerate



 We can modify the rules from Fig_andersen-dl-fig to allow inferences

 only if the variable assigned gets a heap_object of an assignable type

 The rules are shown in Fig_andersen-types-dl-fig

 figurehtfb



 centertabularr_r c_l

 1) - new







 2) -























 3) -















 4) -























 tabularcenter



 Adding type restrictions to flow-insensitive pointer_analysis

 andersen-types-dl-fig



 figure



 The first modification is to rule_(2) The last three subgoals say that we

 can only conclude that can point to if there are types and

 that variable and heap_object may respectively have

 such that objects of type can be assigned to variables that are

 references to type

 A similar additional_restriction has_been added to rule (4)

 Notice_that there is no additional_restriction in rule_(3) because

 all stores must go_through variables

 Any type restriction would only catch one extra case

 when the base object is a null constant

 our approach does

 not deal_with the types of fields in heap_objects However since these

 fields are normally declared to have a type we could extract additional EDB

 information from the code and have a restriction for rule_(3) as_well This

 extension is left as an_exercise





 figurehtfb



 verbatim

 h_T a new_T()

 g_T b new_T()

 T c a

 af b

 bf c

 T d cf

 verbatim



 Code for Exercise andersen-exer

 andersen-exer-fig



 figure



 exer

 andersen-exer

 In Fig andersen-exer-fig and are labels used to

 represent newly_created objects and are not part of the code You_may

 assume that objects of type have a field Use the Datalog_rules

 of this_section to infer all possible and facts

 exer



 hexer

 Applying the algorithm of this_section to the code



 verbatim

 h_T a new_T()

 g b new_T()

 T c a

 verbatim

 would infer that both and can point to and

 Had the code been written



 verbatim

 h_T a new_T()

 g b new_T()

 T c b

 verbatim

 we would infer accurately that can point to and and

 can point to Suggest an intraprocedural data-flow_analysis

 that can avoid this kind of inaccuracy

 hexer



 figurehtfb



 verbatim

 class T

 T f

 T p()

 h_T a new_T()

 af this

 return a







 void_main()

 g_T b new_T()

 b bp()

 b bf



 verbatim



 Example code for pointer_analysis

 andersen2-fig



 figure



 hexer

 We can extend the analysis of this_section to be interprocedural if we

 simulate call and return as if they_were copy operations as in rule_(2)

 of Fig_andersen-dl-fig That is a call copies the actuals to

 their corresponding formals and the return copies the variable that

 holds the return value to the variable that is assigned the result of

 the call Consider the program of Fig andersen2-fig



 itemize

 a)

 Perform a flow-insensitive and

 context-insensitive_analysis on this code

 b)

 Some of the inferences

 made in (a) are actually bogus in the sense that they do_not represent any

 event that can occur at run-time The problem can be traced to the

 multiple assignments to variable Rewrite the code of

 Fig andersen2-fig so that no variable is assigned more_than once

 Rerun the analysis and show that each inferred and fact can

 occur at_run time

 itemize

 hexer

 A Simple Pointer-Analysis Algorithm

 andersen-sect



 In this_section we begin the discussion of a very_simple

 flow-insensitive pointer-alias_analysis assuming that there are no

 procedure_calls We_shall show in subsequent sections how to handle

 procedures first context insensitively then context sensitively

 Flow sensitivity adds a lot of complexity and is less important to

 context_sensitivity for languages_like Java where methods tend to be

 small



 The fundamental question that we_wish to ask in pointer-alias_analysis

 is whether a given pair of pointers may be aliased One_way to

 answer this question is to compute for each pointer the answer to the

 question what objects can

 this pointer point to If two pointers can point to the same

 object then the pointers may be aliased



 Why is Pointer_Analysis Difficult



 Pointer-alias analysis for C programs is particularly difficult

 because C programs can perform arbitrary computations on pointers In

 fact one can read in an_integer and assign it to

 a pointer which would render this pointer a potential alias

 of all other pointer variables in the program Pointers in Java known_as

 references are much simpler No arithmetic is allowed and pointers

 can only point to the beginning of an object



 Pointer-alias analysis must_be interprocedural Without

 interprocedural_analysis one must assume that any method called can

 change the contents of all accessible pointer variables thus rendering any

 intraprocedural pointer-alias_analysis ineffective



 Languages allowing indirect function calls present an additional challenge for

 pointer-alias_analysis

 In C one can call a function indirectly by calling a dereferenced

 function pointer We need to know what the function pointer can point

 to before we can analyze the function called And clearly after

 analyzing the function called one may discover more functions that the

 function pointer can point to and therefore the process needs to be

 iterated



 While most functions are called directly in C virtual methods in

 Java cause many invocations to be indirect Given an invocation

 xm() in a Java program there may be many classes to which

 object might belong and

 that have a method_named

 The more_precise our knowledge of the actual type of the

 more_precise our call_graph is Ideally we can determine at_compile time

 the exact class of and thus know exactly which method refers to



 ex

 Consider the following sequence of Java statements



 verbatim

 Object o

 o new String()

 n ohashCode()

 verbatim

 Here is declared to be an Object Without analyzing what

 refers to all methods called hashCode declared for all

 classes must_be considered as possible targets Knowing that

 points to a String will narrow interprocedural_analysis to precisely

 the method declared for String

 ex



 It is possible to apply approximations to reduce the number of

 targets For_example statically we can determine what are all the

 types of objects_created and we can limit the analysis to those But we

 can be more accurate if we can discover the call_graph on the fly

 based_on the points-to_analysis obtained at the same time More accurate

 call graphs lead not only to more_precise results but also can

 reduce greatly the analysis time otherwise needed



 Points-to analysis is complicated It is not one of those easy data

 flow problems where we only need to simulate the effect of going

 around a loop of statements

 once Rather as we discover new targets for a pointer all statements

 assigning the contents of that pointer to another pointer need to be

 re-analyzed



 For_simplicity we_shall focus mainly on Java We_shall start with

 flow-insensitive and context-insensitive_analysis assuming for now

 that no methods are called in the program Then we describe

 how we can discover the call_graph on the fly as the points-to_results

 are computed Finally we describe one way of handling context

 sensitivity





 A Model for Pointers and References



 Let_us suppose that our language has the following ways to represent and

 manipulate references



 enumerate



 Certain program variables are of type pointer to or

 reference to where is a type These variables are either

 static or live_on the run-time_stack We call them simply variables



 There is a heap of objects All variables point to heap_objects not

 to other variables These objects will be referred to as heap

 objects



 A heap_object can have fields and the value of a field can be a

 reference to a heap_object (but_not to a variable)



 enumerate

 Java is modeled well by this structure and we_shall use Java syntax in

 examples Note_that C is modeled less

 well since pointer variables can point to other pointer variables in C

 and in principle any C value can be coerced into a pointer



 Since we are performing an insensitive analysis we only need to assert

 that a given variable can point to a given heap_object we do

 not have to address the issue of where in the program can point to

 or in what contexts can point to

 Note_however that variables can be named by their full name In

 Java this name can incorporate the module class method and block

 within a method as_well

 as the variable name itself Thus we can distinguish many variables

 that have the same identifier



 Heap objects do_not have names Approximation often is used to name

 the objects because an_unbounded number of objects may be created

 dynamically One convention is to refer to objects by the

 statement at which they are created As a statement can be executed

 many_times and create a new object each time an assertion like can

 point to really means can point to one or_more

 of the objects_created at the statement labeled



 The goal of the analysis is to determine what each variable and each

 field of each heap_object can point to We refer to this as a points-to_analysis two pointers are aliased if their points-to sets

 intersect We describe here an inclusion-based analysis that

 is a statement such_as v_w causes variable to point to

 all the objects points to but not vice_versa While this approach

 may seem obvious there are

 other alternatives to how we define points-to_analysis For_example

 we can define an equivalence-based analysis such that a statement

 like v_w would turn variables and into one

 equivalence_class pointing to all the variables that each can

 point to While this formulation does_not approximate aliases well

 it provides a quick and often good answer to the question of which variables

 point to the same kind of objects



 Flow Insensitivity



 We start by showing a very_simple example to illustrate the effect of

 ignoring control_flow in points-to_analysis



 ex

 sens-ex

 In Fig sens-fig three objects and are created

 and assigned to variables and respectively Thus

 surely points to points to and points to by

 the end of line_(3)



 figurehtfb



 center

 tabularl_l l

 1) h a new_Object()



 2) i b new_Object()



 3) j c new_Object()



 4) a b



 5) b_c



 6) c a



 tabular

 center



 Java code for Example sens-ex

 sens-fig



 figure



 If you follow the statements (4)_through (6) you discover that after line_(4)

 points only to After line_(5) points only to and after

 line_(6) points only to

 ex



 The above analysis is flow sensitive because we follow the control

 flow and compute what each variable can point to after each statement

 In other_words in addition to considering what points-to

 information each statement generates we also account for what

 points-to_information each statement kills

 For_instance the statement b_c kills the previous fact

 points to and generates the new relationship

 points to what points to



 A flow-insensitive analysis ignores the control_flow which

 essentially assumes that every statement in the program can be

 executed in any order It computes only one global points-to map

 indicating what each variable can possibly point to at any

 point of the program execution If a variable can point to two

 different objects after two different statements in a program we

 simply record that it can point to both objects In other_words in

 flow-insensitive analysis an assignment does_not kill any

 points-to_relations but can only generate more points-to

 relations To_compute the flow-insensitive results we repeatedly add

 the points-to effects of each statement on the points-to relationships

 until_no new relations are found Clearly lack of flow sensitivity

 weakens the analysis results greatly but it tends to reduce the size

 of the representation of the results and make the algorithm converge

 faster



 ex

 insens-ex

 Returning to Example sens-ex lines_(1) through (3) again tell

 us can point to can point to and can point to

 With lines_(4) and (5) can point to both and and

 can point to both and With line_(6) can point to

 and This information affects line_(5) which in turn

 affects line_(4) In the end we are left with the useless conclusion

 that anything can point to anything

 ex



 The Formulation in Datalog



 Let_us now formalize a flow-insensitive pointer-alias_analysis based

 on the discussion_above

 We_shall ignore procedure_calls for now and

 concentrate_on the four kinds of statements that can affect pointers

 enumerate



 Object creation

 h_T v new_T()

 This statement creates a new heap_object and variable can point to

 it



 copy-item

 Copy statement

 v_w

 Here and are variables The statement makes point to

 whatever heap_object currently points to ie is copied_into





 Field store

 vf w

 The type of object that points to must have a field and this

 field must_be of some reference type Let point to heap_object and

 let point to

 This statement makes the field

 in now point to

 Note_that the variable is

 unchanged



 Field load

 v wf

 Here is a variable pointing to some heap_object that has a field

 and points to some heap_object The statement makes

 variable point to



 enumerate



 Note_that compound field accesses in the source code such_as v

 wfg will be broken_down into two primitive field-load statements



 verbatim

 v1 wf

 v v1g

 verbatim



 Let_us now express the analysis formally in Datalog_rules First

 there are only two IDB_predicates we need to compute



 enumerate



 means that variable can point to heap_object



 means that field of heap_object can point to heap

 object



 enumerate



 The EDB relations are constructed from the program itself Since

 the location of statements in a program is irrelevant when the analysis is

 flow-insensitive we only have to assert in the EDB the existence of

 statements that have certain forms In what_follows we_shall make a

 convenient simplification Instead of defining EDB relations to hold

 the information garnered from the program we_shall use a quoted

 statement form to suggest the EDB relation or relations that represent

 the existence of such a statement For_example new

 is an EDB fact asserting that at statement there is an assignment

 that makes variable point to a new object of type

 We assume that in practice there would be a corresponding EDB relation

 that would be populated with ground_atoms one for each statement of

 this form in the program



 With this convention all we need to write the Datalog_program is one

 rule for each of the four types of statements The program is shown in

 Fig_andersen-dl-fig Rule (1) says_that variable can point to heap

 object if statement is an assignment of a new object to

 Rule (2) says_that if there is a copy_statement V W and can

 point to then can point to



 figurehtfb



 centertabularr_r c_l

 1) - new







 2) -











 3) -















 4) -











 tabularcenter



 Datalog_program for flow-insensitive pointer_analysis

 andersen-dl-fig



 figure



 Rule (3) says_that if there is a statement of the form VF W

 can point to and can point to then the field of

 can point to

 Finally rule (4) says_that if there is a statement of the form V WF can point to and the field of can point to

 then can point to Notice_that and are

 mutually_recursive but this Datalog_program can be evaluated by either

 of the iterative_algorithms discussed in Section datalog-eval-subsect



 Using Type Information



 Because Java is type safe variables can only point to types that are

 compatible to the declared types For_example assigning an object

 belonging to a superclass of the declared type of a variable would

 raise a run-time exception Consider the simple example in

 Fig andersen-type-fig

 where is a subclass of This program will generate a run-time

 exception if is true because cannot be assigned an object of

 class Thus statically we can conclude that because of the type

 restriction can only point to and not



 figurehtfb



 verbatim

 S a

 T b

 if (p)

 g b new_T()

 else

 h b new S()



 a b



 verbatim

 Java program with a type error

 andersen-type-fig



 figure



 Thus we introduce to our analysis three EDB_predicates that reflect

 important type information in the code being analyzed We_shall use

 the following



 enumerate



 says_that variable is declared to have type



 says_that heap_object is allocated with type

 The type of a created object may not be known precisely if for

 example the object is returned by a native method Such types are

 modeled conservatively as all possible types



 means that an object of type can be assigned to a

 variable with the type This information is generally gathered from the

 declaration of subtypes in the program but also_incorporates information

 about the predefined classes of the language is always

 true

 enumerate



 We can modify the rules from Fig_andersen-dl-fig to allow inferences

 only if the variable assigned gets a heap_object of an assignable type

 The rules are shown in Fig_andersen-types-dl-fig

 figurehtfb



 centertabularr_r c_l

 1) - new







 2) -























 3) -















 4) -























 tabularcenter



 Adding type restrictions to flow-insensitive pointer_analysis

 andersen-types-dl-fig



 figure



 The first modification is to rule_(2) The last three subgoals say that we

 can only conclude that can point to if there are types and

 that variable and heap_object may respectively have

 such that objects of type can be assigned to variables that are

 references to type

 A similar additional_restriction has_been added to rule (4)

 Notice_that there is no additional_restriction in rule_(3) because

 all stores must go_through a variable whose type already has_been checked

 Any type restriction would only catch one extra case

 when the base object is a null constant



 figurehtfb



 verbatim

 h_T a new_T()

 g_T b new_T()

 T c a

 af b

 bf c

 T d cf

 verbatim



 Code for Exercise andersen-exer

 andersen-exer-fig



 figure



 exer

 andersen-exer

 In Fig andersen-exer-fig and are labels used to

 represent newly_created objects and are not part of the code You_may

 assume that objects of type have a field Use the Datalog_rules

 of this_section to infer all possible and facts

 exer



 hexer

 Applying the algorithm of this_section to the code



 verbatim

 g_T a new_T()

 h a new_T()

 T c a

 verbatim

 would infer that both and can point to and

 Had the code been written



 verbatim

 g_T a new_T()

 h_T b new_T()

 T c b

 verbatim

 we would infer accurately that can point to and and

 can point to Suggest an intraprocedural data-flow_analysis

 that can avoid this kind of inaccuracy

 hexer



 figurehtfb



 verbatim

 t p(t x)

 h_T a new T

 af x

 return a





 void_main()

 g_T b new T

 b p(b)

 b bf



 verbatim



 Example code for pointer_analysis

 andersen2-fig



 figure



 hexer

 We can extend the analysis of this_section to be interprocedural if we

 simulate call and return as if they_were copy operations as in rule_(2)

 of Fig_andersen-dl-fig That is a call copies the actuals to

 their corresponding formals and the return copies the variable that

 holds the return value to the variable that is assigned the result of

 the call Consider the program of Fig andersen2-fig



 itemize

 a)

 Perform an

 insensitive analysis on this code

 b)

 Some of the inferences

 made in (a) are actually bogus in the sense that they do_not represent any

 event that can occur at run-time The problem can be traced to the

 multiple assignments to variable Rewrite the code of

 Fig andersen2-fig so that no variable is assigned more_than once

 Rerun the analysis and show that each inferred and fact can

 occur at_run time

 itemize

 hexer

 figurehtb

 tabularl

 VarRange(d lo hi)



 bitmask 1 (bits(d) - 1)



 result zero



 while (lo hi)



 v one



 n bits(d)-1



 do



 bit lo bitmask



 if (bit 0)



 v ithvar(d n) BDD with var n



 else



 v nithvar(d n) BDD with not n



 mask bitmask 1



 if ((lo mask) 0 (lo mask) hi)



 lo (lo mask) 1



 break



 bitmask 1



 while (true)



 result v



 return result



 tabular

 Pseudocode for the VarRange function

 figvarrange

 figure

 From Datalog to BDDs

 secapproach



 In this_paper we represent the program and analysis results as relations in a deductive database Analyses are specified in

 Datalog a declarative language similar to Prolog that operates on

 relations This formulation is powerful and succinct Furthermore

 these Datalog_programs can be_implemented directly using_BDDs leading

 to a highly_efficient implementation In_fact the algorithm

 descriptions we present in this_paper are copied verbatim from the

 actual implementation



 This_section is split into three_parts

 Section approachrelations gives a brief overview of relations

 Datalog and the notation we will use in the rest of the paper

 Section approachci presents the context-insensitive_points-to

 analysis of Berndl in Datalog and our notationberndl03

 Section approachbdd gives an overview of BDDs and outlines how

 to efficiently execute Datalog_programs using BDD_operations





 In this_section we start with a brief introduction to Datalog We

 then show_how Datalog can be used to describe the context-insensitive

 points-to_analysis due to Berndl at a high_level We then

 describe_how our bddbddb system translates a Datalog_program

 into an efficient_implementation using_BDDs



 Datalog

 approachrelations



 Some relations exist a priori for example

 relations that are extracted directly from the program code We call

 such relations input relations Likewise the relations that

 consist of the final results of the Datalog_program are called output

 relations



 A Datalog_program consists of a set of domains input relations

 extracted_from the program to be analyzed output relations and rules

 describing how the output relations can be computed from the input





 We represent a program and all its analysis results as relations

 Conceptually a relation is a two-dimensional table The

 columns are the attributes each of which has a domain

 defining the set of possible attribute values The rows are the

 tuples of attributes that share the relation If tuple is

 in relation we say that predicate is true



 A Datalog_program consists of a set of rules written in a

 Prolog-style notation where a predicate is defined as a conjunction

 of other predicates For_example the Datalog rule

 eqnarray

 D(wz) - A(wx) B(xy) C(yz)

 eqnarray

 says_that is true if and are

 all true Variables in the predicates can be replaced with

 constants which are surrounded_by double-quotes or don't-cares

 which are signified by underscores Predicates on the right_side of

 the rules can be inverted



 Datalog is more_powerful than SQL which is based_on relational

 calculus because Datalog predicates can be recursively

 definedUllmanDatabase If none of the predicates in a Datalog

 program is inverted then there is a guaranteed minimal_solution

 consisting of relations with the least number of tuples Conversely

 programs with inverted predicates may not have a unique minimal

 solution Our_system accepts a subclass of Datalog_programs

 known_as stratified programsstratum for which minimal

 solutions always exist Informally rules in such programs can be

 grouped into strata each with a unique minimal_solution that can be

 solved in sequence



 Context-Insensitive Points-to_Analysis

 approachci



 We_now review Berndl et al's context-insensitive_points-to

 analysisberndl03 while also introducing the Datalog notation

 This algorithm assumes that a call_graph computed using simple class

 hierarchy analysisDean1995OOP is available a priori Heap objects are

 named by their allocation_sites The algorithm finds the objects

 possibly pointed to by each variable and field of heap_objects in

 the program Shown in Algorithm_algci is the exact Datalog

 program as fed to that implements Berndl's algorithm To keep

 the first example simple we defer the discussion of using types to

 improve precision until Section approachimproving



 figurehtb

 alg Context-insensitive_points-to analysis with a precomputed

 call_graph

 algci

 alg



 Domains

 tabbing

 123456123456712345678









 tabbing

 Relations



 arraylll

 input 0 (variableV heapH)



 input (baseV fieldF sourceV)



 input (baseV fieldF destV)



 input (destV sourceV)



 output (variableV heapH)



 output (baseH fieldF targetH)

 array



 Rules

 equationarraylcl

 (vh)_- 0(vh) rule0



 (v1h) -_(v1v2) (v2h) rule1



 (h1fh2) -_(v1fv2)



 (v1h1) (v2h2) rule2



 (v2h2) -_(v1fv2)



 (v1h1) (h1fh2) rule3

 equationarray

 figure



 A Datalog_program has three sections domains relations and rules

 A domain declaration has a name a size and an_optional

 file name that provides a name for each element in the domain

 internally represented as an ordinal number from 0 to The

 latter allows to communicate with the users with meaningful

 names A relation declaration has an_optional keyword

 specifying whether it is an input or output relation the name of the

 relation and the name and domain of every attribute A relation

 declared as neither input nor output is a temporary relation generated

 in the analysis but not written out Finally the rules follow the

 standard Datalog syntax The rule numbers introduced here

 for the sake of exposition are not in the actual program



 We can express all information found in the intermediate

 representation of a program as relations To_avoid inundating readers

 with too_many definitions all at once we define the relations as they

 are used

 The domains and relations used in Algorithm_algci are

 description





 V is the domain of variables It represents all the allocation

 sites formal_parameters return values thrown exceptions cast

 operations and dereferences in the program There is also

 a special global variable for use in accessing static variables





 H is the domain of heap_objects Heap objects are named by the

 invocation_sites of object_creation methods To increase precision

 we also statically identify factory_methods and treat_them as object

 creation methods











 F is the domain of field descriptors in the program Field

 descriptors are used when loading from a field (v vf) or

 storing to a field (vf v) There is a special field

 descriptor to denote an array_access





 V_H is the initial variable points-to_relation

 extracted_from object allocation statements in the source_program

 means there is an invocation_site that assigns a

 newly_allocated object to variable





 V F V represents store statements

 says_that there is a statement vf v

 in the program





 V F V represents load statements



 says_that there is a statement v vf in the program





 V V is the assignments relation due to

 passing of arguments and return values means that

 variable includes the points-to set of variable Although

 we do_not cover exceptions here they work in an

 analogous manner





 V_H is the output variable points-to_relation

 means that variable can point to heap_object





 H_F H is the output heap points-to

 relation_means that field of heap_object

 can point to heap_object

 description



 Note_that local_variables and their assignments are factored away

 using a flow-sensitive analysiswhaley02sas The assign

 relation is derived by using a precomputed call_graph



 The sizes of the domains are determined by the number of variables

 heap_objects and field descriptors in the input program



 Rule (rule0) incorporates the initial variable points-to_relations into

 Rule (rule1) finds the transitive_closure over inclusion edges

 If includes and variable can point to object

 then can also point to Rule (rule2) models the effect

 of store_instructions on heap_objects Given a statement vf v

 if v can point to and v can point to

 then f

 can point to Rule (rule3) resolves load instructions

 Given a statement v vf if v can point to and

 f can point to then v can point to

 Applying these rules until the results converge finds all the

 possible context-insensitive_points-to relations in the program



 Improving Points-to_Analysis with Types

 approachimproving

 figurehtb

 alg Context-insensitive_points-to analysis with type_filtering

 algci-types

 alg

 Domains



 Domains_from Algorithm_algci plus

 tabbing

 12345612345678123456789

 tabbing

 Relations



 Relations from Algorithm_algci plus

 arraylll

 input (variableV typeT)



 input (heapH typeT)



 input (supertypeT subtypeT)



 (variableV heapH)

 array

 Rules

 equationarraylcl

 (vh)_- (vtv)_(hth) (tvth) vPfilterrule



 (vh)_- 0(vh) newrule0



 (v1h) -_(v1v2) (v2h)



 (v1h) newrule1



 (h1fh2) -_(v1fv2)



 (v1h1) (v2h2) newrule2



 (v2h2) -_(v1fv2) (v1h1)



 (h1fh2) (v2h2) newrule3

 equationarray

 figure



 Because Java is type-safe variables can only point to objects of assignable types Assignability is similar to the subtype

 relation with allowances for interfaces null values and

 arraysjvmspec By dropping targets of unassignable types in

 assignments and load statements we can eliminate many impossible

 points-to_relations that result from the imprecision of the analysis

 (We could similarly perform type_filtering on stores into

 heap_objects However because all stores must go_through variables

 such a type_filter would only catch one extra case - when the base

 object is a null constant)



 Adding type_filtering to Algorithm_algci is simple in Datalog

 We add a new domain to represent types and new relations to represent

 assignability as_well as type declarations of

 variables and heap_objects We compute the type_filter and modify the

 rules in Algorithm_algci to filter out unsafe assignments and

 load operations





 description





 T is the domain of type descriptors (ie classes) in the

 program





 V T represents the declared types of variables

 means that variable is declared with type





 H T represents the types of objects_created at a

 particular creation_site In Java the type created by a new

 instruction is usually known statically(The type of a

 created object may not be known precisely if for example the object

 is returned by a native method or reflection is used Such types

 are modeled conservatively as all possible types)

 means that the object_created at has type





 T_T is the relation of assignable types

 means that type is assignable to type





 V_H is the type_filter relation

 means that it is type-safe to assign heap_object

 to variable

 description



 Rule (vPfilterrule) in Algorithm_algci-types defines the

 relation It is type-safe to assign heap_object of type

 to variable of type if is assignable from

 Rules (newrule0) and (newrule2) are the same as Rules

 (rule0) and (rule2) in Algorithm_algci Rules

 (newrule1) and (newrule3) are analogous to Rules

 (rule1) and (rule3) with the additional_constraint that

 only points-to_relations that match the type_filter are inserted



 Translating Datalog into Efficient BDD Implementations

 approachbdd



 We first describe_how Datalog_rules can be translated_into

 operators from relational algebra such_as join and project

 then show_how to translate these operations into BDD_operations



 Query Resolution



 We can find the solution to an unstratified query or a stratum of a

 stratified query simply by_applying the inference_rules repeatedly

 until none of the output relations change We can apply a Datalog

 rule by performing a series of relational natural_join project and

 rename_operations A natural_join operation combines rows from two

 relations if the rows share the same value for a common attribute A

 project operation removes an attribute from a relation A rename

 operation changes the name of an attribute to another one



 For_example the application of Rule (rule1) can be

 implemented as

 tabbing

 123123123123











 tabbing

 We first rename the attribute in relation from variable to

 source so that it can be joined with relation assign to

 create a new points-to_relation The attribute dest of the

 resulting relation is changed to variable so that the tuples can

 be added to the tuples accumulated thus far



 The system uses the three following optimizations to speed_up

 query resolution







 Attributes naming Since the names of the attributes must match when

 two relations are joined the choice of attribute names can affect the

 costs of rename_operations Since the renaming cost is highly

 sensitive to how the relations are implemented the system

 takes the representation into_account when minimizing the renaming

 cost





 Rule application order A rule needs to be applied only if the input

 relations have changed optimizes the ordering of the rules

 by analyzing the dependences_between the rules For_example Rule 1

 in Algorithm_algci does_not depend_on any of the other rules

 and can be applied only once at the beginning of the query resolution





 Incrementalization We only need to re-apply a rule on those

 combinations of tuples that we have not seen before Such a technique

 is known_as incrementalization in the BDD literature and

 semi- fixpoint evaluation in the database

 literaturebalbin87generalization Our_system also identifies

 loop-invariant relations to avoid unnecessary difference and rename

 operations Shown below is the result of incrementalizing the repeated

 application of Rule (rule1)

 tabbing

 123456123123123



 repeat























 until



 tabbing







 Relational Algebra in BDD



 We_now explain how BDDs work and how they can be used to implement

 relations and relational operations BDDs (Binary Decision Diagrams)

 were_originally invented for hardware verification to efficiently

 store a large_number of states that share_many

 commonalitiesBryant



 They are an efficient_representation of boolean_functions A BDD is a

 directed acyclic_graph (DAG) with a single root node and two terminal

 nodes representing the constants one and zero Each non-terminal

 node in the DAG represents an input variable and has exactly two

 outgoing_edges a high_edge and a low_edge The high_edge represents

 the case_where the input variable for the node is true and the low

 outgoing_edge represents the case_where the input variable is false

 On any path in the DAG from the root to a terminal node the value of

 the function on the truth values on the input variables in the path is

 given by the value of the terminal node To evaluate a BDD for a

 specific input one simply starts at the root node and for each node

 follows the high_edge if the input variable is true and the low_edge

 if the input variable is false The value of the terminal node that

 we reach is the value of the BDD for that input



 The variant of BDDs that we use are called ordered binary

 decision_diagrams or OBDDsBryant Ordered refers to the

 constraint that on all paths through the graph the variables respect a

 given linear order In_addition OBDDs are maximally reduced

 meaning that nodes with the same variable name and low and high

 successors are collapsed as one and nodes with identical low and high

 successors are bypassed Thus the more commonalities there are in

 the paths_leading to the terminals the more compact the OBDDs are

 Accordingly the amount of the sharing and the size of the

 representation depends greatly on the ordering of the variables



 We can use BDDs to represent relations as_follows Each element

 in an -element domain is represented as an_integer between 0

 and using bits A relation

 is represented as a boolean function

 such that

 iff and iff





 A number of highly-optimized BDD packages are

 availablebuddycudd the operations they provide can be used

 directly to implement relational operations efficiently For_example

 the replace operation in BDD has the same semantics as the

 rename operation the relprod operation in BDD finds the

 natural_join between two relations and projects away the common

 domains For_example if and

 the operation is defined as



 relprod(XY) (d2d3) d1 (d1 d2) X (d1 d3) Y)







 Let_us now use a concrete example to illustrate the significance of

 variable_ordering Suppose relation

 contains tuples and relation

 contains tuples If in the variable

 order the bits for the first attribute come before the bits for the

 second the BDD will need to represent the sequence

 separately for each relation However if instead the bits for the

 second attribute come first the BDD can

 share the representation for the sequence

 between and Unfortunately the problem of finding the

 best variable_ordering is NP-completebollig96 Our

 system automatically explores different alternatives empirically to

 find an effective orderingbddbddb





 Incrementalization

 bddincremental



 When re-applying a rule we only need to apply it to those new tuples

 to which the rule has not been applied before Such a technique is

 known_as incrementalization in the BDD literature and

 semi- fixpoint evaluation in the database

 literaturebalbin87generalization



 To incrementalize a rule for each subgoal we combine the new elements

 in the relation with all elements of the other relations and union the

 results For_example if there are three subgoal relations

 and with new elements and the algorithm will

 calculate Obviously we can skip a

 calculation when a difference relation is empty



 The deductive_engine automatically transforms the rules to maximize

 opportunities for incrementalization It also identifies

 loop-invariant relations to avoid unnecessary difference operations







 We use a BDD-based deductive_engine that takes as input the Datalog

 programs exactly as they are presented in this paperbddbddb

 The deductive_engine transforms the rules into efficient_BDD

 operations automatically optimizing the evaluation order and

 incrementalizing the computation This_section gives a brief overview

 of BDDs and how the Datalog_programs map into BDD_operations







 A BDD can be thought of as a set of binary strings A particular

 binary_string is a member of the set if and only if the boolean

 function for that binary_string equals one A binary_string can

 partitioned_into substrings each of which corresponds to an attribute

 in a tuple A substring of bits can be used to represent

 elements in a domain



 By partitioning

 the bits into substrings each substring can be

 a binary_string can also represent strings of

 non-binary numbers For_example the bits are used for the

 first number bits are used for the second

 number etc These substrings are called physical_domains



 Each element in an -element domain is

 represented as an_integer between 0 and using

 bits A relation

 is represented as a boolean function

 such that

 if and

 if





 We represent relations in this manner Each element of a domain

 is given a unique number Each attribute of a relation is then assigned a

 physical_domain with a set of variables where is the number of

 bits necessary for the domain that corresponds to the attribute If

 and and each domain

 requires two bits the tuple is represented_by

 A relation is then a set of

 these binary strings If a relation does_not use a physical_domain

 those variables are don't-care effectively making those physical

 domains contain the universal_set





 Because ROBDDs exhibit shared substructure they can efficiently

 represent similar sets Furthermore as sets get larger the size of

 the BDD data_structure can become smaller because the number of

 don't-care bits increases (In BDDs the universal_set is

 represented_by a single_node the terminal node for 1) Luckily the

 results of program analyses especially context-sensitive program

 analyses often contain large sets and a lot of redundancy This

 makes BDDs a particularly apt data_structure to represent the results

 of many program analyses







 In ROBDDs for any given function

 there is exactly one node which represents

 that function Because this is true for all nodes in the ROBDD the

 ROBDD exhibits shared substructure where many paths can share

 the same nodes This leads to an efficient_representation of similar

 functions which is critical to our ability to handle the many

 contexts in context-sensitive_analysis





 The deductive_engine must also assign attributes to physical_domains

 such that there are no conflicting assignments within a relation or a

 rule We would also like to minimize the BDD rename_operations and

 avoid any rename_operations that change the relative order of physical

 domains in a BDD The deductive_engine models the assignment as a

 constraint problem taking into_account the variable_ordering and

 trying to avoid renamings within loops



 Applications of Syntax-Directed Translation

 apps-sdd



 The syntax-directed_translation techniques in this_chapter will be

 applied in Chapter_inter-ch to type_checking and

 intermediate-code_generation Here we consider selected examples

 to illustrate some representative SDD's



 The main application in this_section is the construction of syntax

 trees Since some compilers use syntax_trees as an intermediate

 representation a common form of SDD turns its input_string into a tree

 To complete the translation to intermediate_code the compiler may

 then walk the syntax_tree using another set of rules that are in

 effect an SDD on the syntax_tree rather_than the parse_tree

 (Chapter inter-ch also discusses approaches to

 intermediate-code_generation that apply an SDD without ever

 constructing a tree explicitly)



 We consider two SDD's for constructing syntax_trees for

 expressions The first an S-attributed definition is suitable

 for use during bottom-up_parsing The second L-attributed is

 suitable for use during top-down_parsing



 The final example of this_section is an L-attributed_definition

 that deals_with basic and array types













































































































 Construction of Syntax_Trees

 syn-tree-subsect



 As_discussed in Section ast-create-subsect each node in a

 syntax_tree represents a construct the children of the node

 represent the meaningful_components of the construct A

 syntax-tree_node representing an expression has label

 and two children representing the subexpressions and





 We_shall implement the nodes of a syntax_tree by objects with a

 suitable number of fields Each object will have an op field

 that is the label of the node The objects will have additional

 fields as_follows



 itemize



 If the node is a leaf an additional field holds the lexical

 value for the leaf A constructor function

 creates a leaf object

 Alternatively if nodes are viewed_as records then

 returns a pointer to a new record for a leaf



 If the node is an interior_node there are as many

 additional fields as the node has children in the syntax_tree A

 constructor function Node takes two or_more arguments

 creates an object

 with first field and additional fields for the

 children



 itemize



 exsyn-tree-ex

 The S-attributed definition in Fig sdd-constr-st-fig

 constructs syntax_trees for a simple expression grammar involving

 only the binary operators and As usual these operators

 are at the same precedence_level and are jointly left_associative

 All nonterminals have one synthesized_attribute

 which represents a node of the syntax_tree



 figurehtfb

 center

 tabularl_ll

 Production_Semantic Rules



 1)





 2)





 3)



 4)



 5)





 6)





 tabular

 center

 Constructing syntax_trees for simple expressions

 sdd-constr-st-fig

 figure



 Every time the first production is used its rule

 creates a node with for and two children

 and for the subexpressions The

 second production has a similar rule



 For production 3 no node is created since

 is the same as Similarly no node

 is created for production 4 The value of

 is the same as since parentheses

 are used only for grouping they influence the structure of the

 parse_tree and the syntax_tree but once their job is done there

 is no further need to retain them in the syntax_tree



 The last two -productions have a single terminal on the right

 We use the constructor Leaf to create a suitable node which

 becomes the value of



 Figure syn-tree4-fig shows the construction of a syntax_tree

 for the input The nodes of the syntax_tree are shown as

 records with the field first Syntax-tree edges are

 now shown as solid lines The underlying parse_tree which need

 not actually be constructed is shown with dotted edges The third

 type of line shown dashed represents the values of

 and each line points to the

 appropriate syntax-tree_node



 figurehtfb



 Syntax tree for syn-tree4-fig

 figure



 At the bottom we see leaves for and constructed by

 Leaf We suppose that the lexical value

 points into the symbol_table and the

 lexical value is the numerical value of a

 constant These leaves or pointers to them become the value of

 at the three parse-tree nodes labeled

 according to rules 5 and 6 Note_that by rule 3 the pointer to

 the leaf for is also the value of for the

 leftmost in the parse_tree



 Rule 2 causes us to create a node with equal to the

 minus_sign and pointers to the first two leaves Then rule 1

 produces the root node of the syntax_tree by combining the node

 for with the third leaf



 If the rules are evaluated during a postorder_traversal of the

 parse_tree or with reductions during a bottom-up_parse then the

 sequence of steps shown in Fig constr-st-steps-fig ends

 with pointing to the root of the constructed syntax_tree

 ex



 figurehtfb

 center

 tabularl_l

 1)



 2)



 3)



 4)



 5)



 tabular

 center

 Steps in the construction of the syntax_tree for

 constr-st-steps-fig

 figure



 With a grammar designed for top-down_parsing the same syntax

 trees are constructed using the same sequence of steps even

 though the structure of the parse_trees differs significantly from

 that of syntax_trees



 extd-syntree-ex

 The L-attributed_definition in Fig td-syntree-fig performs

 the same translation as the S-attributed definition in

 Fig sdd-constr-st-fig The attributes for the grammar

 symbols and are as discussed

 in Example syn-tree-ex



 figurehtfb

 center

 tabularl_ll

 Production_Semantic Rules



 1)









 15pt2)









 15pt3)









 15pt4)



 5)



 6)





 7)





 tabular

 center

 Constructing syntax_trees during top-down_parsing

 td-syntree-fig

 figure



 The rules for building syntax_trees in this example are similar to

 the rules for the desk_calculator in Example sdd-ll-term-ex

 In the desk-calculator example a term was evaluated by

 passing as an inherited_attribute since and

 appeared in different portions of the parse_tree Here the idea

 is to build a syntax_tree for by passing as an inherited

 attribute since and appear in different subtrees

 Nonterminal is the counterpart of nonterminal in

 Example sdd-ll-term-ex Compare the dependency_graph for

 in Fig dg-syntree-fig with that for in

 Fig dg-term-fig



 figurehtfb



 Dependency graph for with the SDD of

 Fig td-syntree-fig dg-syntree-fig

 figure



 Nonterminal has an inherited_attribute and a

 synthesized_attribute Attribute

 represents the partial syntax_tree constructed so_far

 Specifically it represents the root of the tree for the prefix of

 the input_string that is to the left of the subtree for At

 node 5 in the dependency_graph in Fig dg-syntree-fig

 denotes the root of the partial syntax_tree for

 the identifier that is the leaf for At node 6

 denotes the root for the partial syntax_tree for

 the input At node 9 denotes the syntax

 tree for



 Since there is no more input at node 9 points to

 the root of the entire syntax_tree The attributes

 pass this value back up the parse_tree until it becomes the value

 of Specifically the attribute value at node 10

 is defined by the rule_associated

 with the production The attribute value at

 node 11 is defined by the rule

 associated_with production 2 in Fig td-syntree-fig Similar

 rules define the attribute values at nodes 12 and 13

 ex



 The Structure of a Type

 array-type-subsect



 Inherited_attributes are useful when the structure of the parse

 tree differs_from the abstract_syntax of the input attributes can

 then be used to carry information from one part of the parse_tree to

 another The next example shows_how a mismatch in structure can be

 due to the design of the language and not due to constraints

 imposed_by the parsing method



 exarray-type-ex

 In C the type can be read as array of 2

 arrays of 3 integers The corresponding type expression

 is

 represented_by the tree in Fig array-type-fig The operator

 array takes two parameters a number and a type If types

 are represented_by trees then this operator returns a tree node

 labeled array with two children for a number and a type



 figurehtbf



 Type expression for

 array-type-fig

 figure



 With the SDD in Fig array-type-sdd nonterminal

 generates either a basic type or an array type Nonterminal

 generates one of the basic types int and float

 generates a basic type when derives and derives

 Otherwise generates array components consisting

 of a sequence of integers each integer surrounded_by brackets



 figurehtfb

 small

 center

 tabularr_c l_l r_c l

 3cProduction 3cSemantic Rules











 int



 float



 num











 tabular

 center

 small

 generates either a basic type or an array type

 array-type-sdd

 figure



 The nonterminals and have a synthesized_attribute

 representing a type The nonterminal has two attributes an

 inherited_attribute and a synthesized_attribute The

 inherited_attributes pass a basic type down the tree

 and the synthesized_attributes accumulate the result



 An annotated_parse tree for the input_string

 is shown in

 Fig ann-array-type-fig The corresponding type expression

 in Fig array-type-fig is constructed by passing the type

 integer from down the chain of 's through the

 inherited_attributes The array type is synthesized up the

 chain of 's through the attributes



 In more_detail at the root for nonterminal

 inherits the type from using the inherited_attribute

 At the rightmost node for the production is

 so equals The semantic_rules for the

 production form by

 applying the operator array to the operands

 and

 ex



 array(3integer)

 array(2array(3integer))

 figurehtfb



 Syntax-directed_translation of array types

 ann-array-type-fig

 figure





 exer

 Below is a grammar for expressions_involving operator and

 integer or floating-point operands Floating-point numbers are

 distinguished by having a decimal point



 center

 tabularl









 tabular

 center



 itemize



 a)_Give an SDD to determine the type of each term

 and expression



 b) Extend your SDD of (a) to translate expressions into

 postfix_notation Use the unary operator intToFloat to turn

 an_integer into an equivalent float



 itemize

 exer



 hexer

 Give an SDD to translate infix expressions with and

 into equivalent expressions without redundant parentheses For

 example since both operators associate from the left and

 takes_precedence over translates_into



 hexer



 hexer

 Give an SDD to differentiate expressions such_as

 involving the operators and the variable

 and constants Assume that no simplification occurs so

 that for example will be translated_into



 hexer

 Other Uses of Affine Transforms



 So far we have focused on the architecture of shared memory machines

 but the theory of affine loop transforms has many other applications

 We can apply affine transforms to other forms of parallelism including

 distributed memory machines vector instructions SIMD (Single

 Instruction Multiple Data) instructions as_well as

 multiple-instruction-issue machines The reuse analysis introduced in

 this_chapter also is useful for data prefetching which is an

 effective technique for improving memory performance



 Distributed Memory Machines



 For distributed memory machines where processors communicate by

 sending messages to each other it is even more important that

 processors be assigned large independent_units of computation such_as

 those generated_by the affine-partitioning algorithm Besides

 computation partitioning a number of additional compilation issues

 remain

 enumerate

 Data allocation If processors use different portions of an array

 they each only have to allocate enough space to hold the portion used

 We can use projection to determine the section of arrays used by each

 processor The input is the system of linear_inequalities

 representing the loop_bounds the array_access functions and the

 affine partitions that map the iterations to processor IDs We

 project_away the loop indices and find for each processor ID the set

 of array locations used



 Communication code We need to generate explicit code to send and

 receive data to and from other processors At each synchronization

 point

 enumerate

 Determine the data residing on one processor that is needed by

 other processors

 Generate the code that finds all the data

 to be sent and packs it into a buffer

 Similarly determine the

 data needed by the processor unpack received messages and move

 the data to the right memory_locations

 enumerate

 Again if all accesses are

 affine these tasks can be performed by the compiler using

 the affine framework



 Optimization It is not necessary for all the communications to take

 place at the synchronization points It is preferable that each processor

 sends data as_soon as it is available and that each processor does

 not start waiting for data until it is needed Such optimizations

 must_be balanced by the goal of not generating too_many messages since

 there is a nontrivial overhead associated_with processing each

 message

 enumerate



 Techniques described_here have other applications as_well For

 example a special-purpose embedded system may use coprocessors to

 offload some of its computations Or instead of demand fetching data

 into the cache an embedded system may use a separate controller to

 load and unload data into and out of the cache or other data buffers

 while the processor operates on other data In these cases similar

 techniques can be used to generate the code to move data around



 Multi-Instruction-Issue Processors



 We can also use affine loop transforms to optimize the performance of

 multi-instruction-issue machines As_discussed in

 Section secsp the performance of a software-pipelined_loop is

 limited by two factors cycles in precedence constraints and the usage

 of the critical resource By changing the makeup of the innermost

 loop we can improve these limits



 First we may be_able to use loop transforms to create innermost

 parallelizable loops thus eliminating precedence cycles

 altogether Suppose a program has two loops with the outer being

 parallelizable and the inner not We can permute the two loops to

 make the inner_loop parallelizable and so create more opportunities

 for instruction-level_parallelism Notice_that it is not necessary

 for iterations in the innermost_loop to be completely parallelizable

 It is sufficient that the cycle of dependences in the loop be short

 enough so that all the hardware resources are fully utilized



 We can also relax the limit due to resource usage by improving the

 usage balance inside a loop Suppose one loop only uses the adder and

 another uses only the multiplier Or suppose one loop is memory bound

 and another is compute bound It is desirable to fuse each pair of

 loops in these examples together so as to utilize all the functional

 units at the same time



 Vector and SIMD Instructions



 Besides multiple-instruction issue there are two other important

 forms of instruction-level_parallelism vector and SIMD operations

 In both cases the issue of just one instruction causes the same

 operation to be applied to a vector of data



 As_mentioned previously many early supercomputers used vector

 instructions Vector operations are performed in a pipelined manner

 the elements in the vector are fetched serially and computations on

 different elements are overlapped In advanced vector machines

 vector operations can be chained as the elements of the vector

 results are produced they are immediately consumed by operations of

 another vector instruction without_having to wait for all the results

 to be ready Moreover in advanced machines with scattergather

 hardware the elements of the vectors need not be contiguous an index

 vector is used to specify where the elements are located



 SIMD instructions specify that the same operation be performed on

 contiguous memory_locations These instructions load data from memory

 in parallel store them in wide registers and compute on them using

 parallel hardware Many media graphics and digital-signal-processing

 applications can benefit from these operations Low-end media

 processors can achieve instruction-level_parallelism simply by issuing

 one SIMD instruction at a time Higher-end processors can combine

 SIMD with multiple-instruction issue to achieve higher performance



 SIMD and vector instruction generation share_many similarities with

 locality optimization As we find independent_partitions that operate

 on contiguous memory_locations we stripmine those iterations and

 interleave these operations in innermost_loops



 SIMD instruction generation poses two additional difficulties First

 some machines require that the SIMD data fetched from memory be

 aligned For_example they might require that 256-byte SIMD operands

 be placed in addresses that are multiples of 256 If the

 source loop operates on just one array of data we can generate one

 main loop that operates on aligned data and extra code before and

 after the loop to handle those elements at the boundary For loops

 operating on more_than one array however it may not be possible to align

 all the data at the same time Second data used by consecutive

 iterations in a loop may not be contiguous Examples include many

 important digital-signal-processing

 algorithms such_as Viterbi decoders and fast Fourier transforms

 Additional operations to shuffle the data around may be necessary to

 take_advantage of the SIMD instructions



 Prefetching

 No data-locality optimization can eliminate all memory accesses for

 one data used for the first time must_be fetched from memory To

 hide the latency of memory operations prefetch_instructions

 have_been adopted in many high-performance processors Prefetch is a

 machine_instruction that indicates to the processor that certain data

 is likely to be used soon and that it is desirable to load the data into

 the cache if it is not present already



 The reuse analysis described in Section ch11reuse can be used

 to estimate when caches misses are likely There_are two important

 considerations when generating prefetch_instructions If contiguous

 memory_locations are to be accessed we need to issue only one

 prefetch instruction for each cache_line Prefetch instructions

 should be issued early enough so that the data is in the cache by the

 time it are used However we should not issue prefetch

 instructions too far in advance The prefetch_instructions can

 displace data that may still be needed also the prefetched data may

 be flushed before it is used



 ex

 Consider the following code



 verbatim

 for_(i0 ii3 i)

 for_(j0 j100_j)

 Aij

 verbatim

 Suppose the target_machine has a prefetch instruction that can fetch

 two words of data at a time and that the latency of a prefetch

 instruction takes about the time to execute six iterations of the loop

 above The prefetch code for the above example is shown in

 Fig prefetch-fig



 figurehtfb



 verbatim

 for_(i0 ii3 i)

 for_(j0 j6 j2)

 prefetch(Aij)

 for_(j0 j94 j2)

 prefetch(Aij6)

 Aij

 Aij1



 for (j94 j100_j)

 Aij



 verbatim



 Code modified to prefetch data

 prefetch-fig



 figure



 We unroll the innermost_loop twice so a prefetch can be

 issued for each cache_line We use the concept of software_pipelining

 to prefetch data six iterations before it is used The prolog

 fetches the data used in the first six iterations The steady_state

 loop prefetches six iterations ahead as it performs its computation

 The epilog issues no prefetches but simply executes the remaining

 iterations

 ex













 Backpatching

 backpatch-sect



 A key problem when generating code for boolean_expressions and

 flow-of-control_statements is that of matching a jump instruction

 with the target of the jump For_example the translation of the

 boolean_expression in

 contains a jump for when is false to the instruction

 following the code for In a one-pass translation must_be

 translated before is examined What then is the target of the

 goto that jumps over the code for In

 Section control-3code-sect we addressed this problem by

 passing labels as inherited_attributes to where the relevant jump

 instructions were generated But a separate pass is then needed

 to bind labels to addresses



 This_section takes a complementary approach called backpatching in which lists of jumps are passed as synthesized

 attributes Specifically when a jump is generated the target of

 the jump is temporarily left unspecified Each such jump is put on

 a list of jumps whose labels are to be filled in when the proper

 label can be determined All of the jumps on a list have the same

 target label



 One-Pass Code_Generation Using Backpatching



 Backpatching can be used to generate code for boolean_expressions

 and flow-of-control_statements in one pass The translations we

 generate will be of the same form as those in

 Section control-3code-sect except for how we manage labels



 In this_section synthesized_attributes and

 of nonterminal are used to manage labels in

 jumping_code for boolean_expressions

 In_particular will be a list of jump or conditional

 jump instructions into which we must insert the label to which control

 goes if is true likewise is the list of

 instructions that eventually get the label to which control goes when

 is false

 As code is generated for

 jumps to the true and false_exits are left incomplete with

 the label field unfilled These incomplete jumps are placed on

 lists pointed to by and

 as appropriate

 Similarly a statement has a synthesized_attribute

 denoting a list of jumps to the instruction immediately_following

 the code for



 For specificity we generate instructions into an instruction

 array and labels will be indices into this array To manipulate lists

 of jumps we use three functions



 enumerate

 creates a new list containing only

 an index into the array of instructions makelist returns a

 pointer to the newly_created list



 ) concatenates the lists pointed to by

 and and returns a pointer to the concatenated list



 inserts as the target label for

 each of the instructions on the list pointed to by

 enumerate





 Backpatching for Boolean Expressions

 bool-patch-subsect



 We_now construct a translation_scheme suitable for generating code

 for boolean_expressions during bottom-up_parsing A marker

 nonterminal in the grammar causes a semantic_action to pick

 up at appropriate times the index of the next instruction to be

 generated The grammar is as_follows



 center

 tabularr_c l











 tabular

 center



 The translation_scheme is in Fig bool-patch-fig



 figurehtfb

 center







 tabularl_l l

 1) to 10pt













 2) to 10pt













 3) to 10pt









 4) to 10pt









 5) to 10pt

















 6) to 10pt









 7) to 10pt









 8) to 10pt







 tabular

 center

 Translation scheme for boolean_expressions

 bool-patch-fig

 figure



 Consider semantic_action (1) for the production

 If is true then is also true

 so the jumps on become part of

 If is false however we must next test

 so the target for the jumps must_be

 the beginning of the code generated for This target is

 obtained using the marker_nonterminal That nonterminal

 produces as a synthesized_attribute the index

 of the next instruction just_before code starts being

 generated



 To obtain that instruction index we associate with

 the production

 the semantic_action



 center



 center

 The variable holds the index of the next

 instruction to follow This value will be backpatched onto the

 (ie each instruction on the list

 will receive as its target

 label) when we have_seen the remainder of the

 production



 Semantic action (2) for is similar to

 (1) Action (3) for swaps the true and false

 lists Action (4) ignores parentheses



 For_simplicity semantic_action (5) generates two instructions a

 conditional goto and an unconditional one Neither has its target

 filled in These instructions are put on new lists pointed to by

 and respectively



 figurehtfb





 Annotated parse_tree for

 patch-fig

 figure



 ex

 patch-ex

 Consider_again the expression



 x100 x200 x_y



 An annotated_parse tree is

 shown in Fig patch-fig for readability attributes truelist falselist and instr are

 represented_by their initial letters The actions are performed

 during a depth-first_traversal of the tree Since all actions

 appear at the ends of right_sides they can be performed in

 conjunction with reductions during a bottom-up_parse In response

 to the reduction of to by

 production (5) the two instructions



 center

 tabularl_l

 100 if x 100 goto



 101 goto

 tabular

 center

 are generated (We arbitrarily start instruction numbers at 100)

 The marker_nonterminal in the production



 B B1

 MB2



 records the value of

 which at this time is 102 The reduction of

 to by production (5)

 generates the instructions



 center

 tabularl_l

 102 if x_200 goto



 103 goto

 tabular

 center

 The subexpression corresponds to

 in the production



 B B1 M B2



 The

 marker_nonterminal records the current value of

 which is now 104 Reducing

 into by production (5)

 generates



 center

 tabularl_l

 104 if x_y goto



 105 goto

 tabular

 center



 We_now reduce by The

 corresponding semantic_action calls

 to bind the true exit of

 to the first instruction of Since is

 and is this call to

 fills in 104 in instruction 102 The six

 instructions generated so_far are thus as shown in

 Fig backpatch-bool-fig(a)



 figurehtfb



 center

 tabularl_l

 100 if x 100 goto



 101 goto



 102 if x_200 goto 104



 103 goto



 104 if x_y goto



 105 goto

 tabular

 center



 center

 (a) After backpatching 104 into instruction 102

 center



 center

 tabularl_l

 100 if x 100 goto



 101 goto 102



 102 if x_200 goto 104



 103 goto



 104 if x_y goto



 105 goto

 tabular

 center



 center

 (b) After backpatching 102 into instruction 101

 center



 Steps in the backpatch process

 backpatch-bool-fig



 figure



 The semantic_action associated_with the final reduction by

 calls

 (101102) which leaves the instructions as in

 Fig backpatch-bool-fig(b)



 The entire expression is true if and only if the gotos of

 instructions 100 or 104 are reached and is false if and only if

 the gotos of instructions 103 or 105 are reached These

 instructions will have their targets filled in later in the

 compilation when it is seen what must_be done depending_on the

 truth or falsehood of the expression

 ex



 Flow-of-Control Statements

 stmt-patch-subsect



 We_now use backpatching to translate flow-of-control_statements in

 one pass Consider statements generated_by the following grammar

 center

 tabularr_c l







 ''''





 tabular

 center

 Here denotes a statement a statement list an

 assignment-statement and a boolean_expression Note_that

 there must_be other productions such_as those for

 assignment-statements The productions given however are sufficient to

 illustrate the techniques used to translate flow-of-control

 statements



 The code layout for if- if-else- and while-statements is the same

 as in Section control-3code-sect We make the tacit

 assumption that the code sequence in the instruction array

 reflects the natural flow of control from one instruction to the

 next If not then explicit jumps must_be inserted to implement

 the natural sequential flow of control



 The translation_scheme in Fig stmt-patch-fig maintains

 lists of jumps that are filled in when their targets are found As

 in Fig bool-patch-fig boolean_expressions generated_by

 nonterminal have two lists of jumps and

 corresponding to the true and false_exits

 from the code for respectively Statements generated_by

 nonterminals and have a list of unfilled jumps given by

 attribute that must eventually be completed by

 backpatching is a list of all conditional and

 unconditional_jumps to the instruction following the code for

 statement in execution order is defined

 similarly



 figurehtbf

 center

 tabularl r_c l_l l

 1)













 2)

 3l























 3)

 3l























 4) ''''







 5)









 6)







 7)











 8)











 9)





 tabular

 center



 Translation of statements

 stmt-patch-fig



 figure



 Consider the semantic_action (3) in Fig stmt-patch-fig The

 code layout for production

 is as in

 Fig code-layout-fig(c) The two_occurrences of the marker

 nonterminal in the production

 center



 center

 record the instruction numbers of the beginning of the code for

 and the beginning of the code for The corresponding

 labels in Fig code-layout-fig(c) are and

 respectively



 Again the only production for is Action

 (6) in Fig stmt-patch-fig sets attribute

 to the number of the next instruction After the body of the

 while-statement is executed control_flows to the beginning

 Therefore when we reduce

 to we

 backpatch to make all targets on that list

 be An explicit jump to the beginning of the

 code for is appended after the code for because control

 may also fall out the bottom is

 backpatched to go to the beginning of by making jumps on

 go to



 A more compelling argument for using and

 comes when code is generated for the

 conditional_statement

 If

 control falls out the bottom of as when is an

 assignment we must include at the end of the code for a

 jump over the code for We use another marker_nonterminal to

 generate this jump after Let nonterminal be this marker

 with production has attribute

 which will be a list consisting of the

 instruction number of the jump that is

 generated_by the semantic_action (7) for



 Semantic action (2) in Fig stmt-patch-fig deals_with

 if-else-statements with the syntax



 center





 center

 We backpatch the jumps when is true to the instruction

 the latter is the beginning of the code for

 Similarly we backpatch jumps when is false to go to the

 beginning of the code for The list

 includes all jumps out of and as_well as the jump

 generated_by (Variable is a temporary that is

 used only for merging lists)



 Semantic actions (8) and (9) handle sequences of statements In



 L L1MS



 the instruction following the code for

 in order of execution is the beginning of Thus the

 list is backpatched to the beginning of the

 code for which is given by In

 is the same as



 Note_that no new instructions are generated anywhere in these

 semantic_rules except for rules (3) and (7) All other code is

 generated_by the semantic_actions associated_with

 assignment-statements and expressions The flow of control causes

 the proper backpatching so that the assignments and boolean

 expression evaluations will connect properly



 Break- Continue- and Goto-Statements



 The most elementary programming_language construct for changing

 the flow of control in a program is the goto-statement In C a

 statement like goto_L sends control to the statement

 labeled L - there must_be precisely one statement with label

 L in this scope Goto-statements can be_implemented by

 maintaining a list of unfilled jumps for each label and then

 backpatching the target when it is known



 Java does away with goto-statements

 However Java does permit

 disciplined jumps called break-statements which send control out

 of an enclosing construct and continue-statements which trigger

 the next iteration of an enclosing_loop The following excerpt

 from a lexical_analyzer illustrates simple break- and

 continue-statements



 center

 tabularr_l

 1) for ( readch() )



 2) if(_peek '_' peek_') continue



 3) else_if( peek_') line line 1



 4) else_break



 5)

 tabular

 center

 Control jumps from the break-statement on line_4 to the next

 statement after the enclosing for-loop Control jumps from the

 continue-statement on line 2 to code to evaluate

 and then to the if-statement on line 2



 If is the enclosing_loop construct then a break-statement is a

 jump to the first instruction after the code for We can

 generate code for the break by (1) keeping_track of the enclosing

 loop statement (2) generating an unfilled jump for the

 break-statement and (3) putting this unfilled jump on

 where is as discussed in

 Section stmt-patch-subsect



 In a two-pass front_end that builds syntax_trees

 can be_implemented as a field in the node for

 We can keep_track of by using the symbol_table to map a

 special identifier to the node for the enclosing

 loop statement This_approach will also handle labeled

 break-statements in Java since the symbol_table can be used to map the

 label to the syntax-tree_node for the labeled construct



 Alternatively instead of using the symbol_table to access the

 node for we can put a pointer to in the

 symbol_table Now when a break-statement is reached we generate

 an unfilled jump look up through the symbol

 table and add the jump to the list where it will be backpatched

 as discussed in Section stmt-patch-subsect



 Continue-statements can be handled in a manner analogous to the

 break-statement

 The main difference_between the two is that the target of the generated

 jump is different



 exer

 Using the translation of Fig bool-patch-fig translate each of

 the following expressions Show the true and false lists for each

 subexpression You_may assume the address of the first instruction

 generated is 100



 itemize

 a) ab (cd ef)

 b) (ab cd) ef

 c) (ab cd) ef

 itemize

 exer



 figurehtfb



 center

 tabularl_l

 while () Code for



 if () Code for



 while () Code for



 Code for



 else Code for



 if () Code for



 Code for



















 (a)_(b)



 tabular

 center



 Control-flow structure of program for

 Exercise bp-cf-exer

 bp-cf-fig



 figure



 exer

 bp-cf-exer In Fig bp-cf-fig(a) is the outline of a

 program and Fig bp-cf-fig(b) sketches the structure of the

 generated three-address_code using the backpatching translation of

 Fig stmt-patch-fig Here through are the

 labels of the generated instructions that begin each of the

 Code sections When we implement this translation we

 maintain for each boolean_expression two lists of places in

 the code for which we denote by and

 The places on list are those

 places_where we eventually put the label of the statement to which

 control must flow whenever is true similarly

 lists the places_where we put the label that control_flows to when

 is found to be false Also we maintain for each statement

 a list of places_where we must put the label to which control

 flows when is finished Give the value (one of through

 ) that eventually replaces each place on each of the

 following lists



 itemize

 (a)_(b)

 (c) (d)

 (e)

 itemize



 exer



 exer

 When performing the translation of Fig bp-cf-fig using the

 scheme of Fig stmt-patch-fig we create lists

 for each statement starting_with the

 assignment-statements and and proceeding to

 progressively_larger if-statements if-else-statements

 while-statements and statement blocks There_are five constructed

 statements of this type in Fig bp-cf-fig



 itemize

 while ()

 if ()

 The block consisting of and

 The statement if () else

 The entire program

 itemize



 For each of these constructed statements there is a rule that

 allows_us to construct in terms of other

 lists and the lists and

 for the expressions in the program Give the

 rules for



 itemize

 (a)_(b) (c)

 (d) (e)

 itemize

 exer

 Optimization of Basic_Blocks

 bb-opt-sect



 We can often obtain a substantial improvement in the running_time

 of code merely by performing local optimization within each

 basic_block by itself More thorough global optimization

 which looks at how information flows among the basic_blocks of a

 program is covered in later chapters starting_with

 Chapter_code-op-ch It is a complex subject with

 many different techniques to consider



 The DAG Representation of Basic_Blocks

 dag-bb-subsect



 Many important techniques for local optimization begin by

 transforming a basic_block into a (directed acyclic graph) In

 Section dag-subsect we introduced the as a

 representation for single expressions The idea extends naturally

 to the collection of expressions that are created within one basic

 block We construct a for a basic_block as_follows



 enumerate



 There is a node in the for each of the initial values

 of the variables appearing in the basic_block



 There is a node associated_with each statement

 within the block The children of are those nodes

 corresponding to statements that are the last definitions prior

 to of the operands used by



 Node is labeled by the operator applied at

 and also attached to is the list

 of variables for which it is

 the last definition within the block



 Certain nodes are designated output nodes These are

 the nodes whose variables are live_on exit from the block

 that is their values may be used later in another block of the

 flow_graph Calculation of these live_variables is a matter

 for global flow analysis discussed in

 Section live-var-subsect



 enumerate



 The representation of a basic_block lets_us perform several

 code-improving_transformations on the code represented_by the

 block



 itemize



 a) We can eliminate local common_subexpressions that

 is instructions that compute a value that has already_been

 computed



 b) We can eliminate dead_code that is instructions

 that compute a value that is never used



 c) We can reorder statements that do_not depend_on one

 another such reordering may reduce the time a temporary value

 needs to be preserved in a register



 d)

 We can apply algebraic laws to reorder operands of three-address

 instructions and sometimes thereby simplify the computation



 itemize



 Finding Local Common_Subexpressions



 Common subexpressions can be detected by noticing as a new node

 is about to be added whether there is an existing node

 with the same children in the same order and with the same

 operator If so computes the same value as and may be

 used in its place This technique was_introduced as the

 value-number method of detecting common_subexpressions in

 Section dag-subsect



 ex

 dag1-ex A for the block

 center

 tabularl

 'a_b c'



 'b a -_d'



 'c b_c'



 'd a -_d'



 tabular

 center

 is shown in Fig dag1-fig When we construct the node for

 the third statement cbc we know that the use of

 b in bc refers to the node of Fig dag1-fig

 labeled because that is the most recent definition of

 b Thus we do_not confuse the values computed at statements

 one and three



 figurehtfb







 for basic_block in Example dag1-ex

 dag1-fig



 figure



 However the node corresponding to the fourth statement da-d has the operator and the nodes with

 attached variables a and as children Since the

 operator and the children are the same as those for the node

 corresponding to statement two we do_not create this node but

 add d to the list of definitions for the node_labeled

 ex



 It might appear that since there are only three nonleaf nodes in

 the of Fig dag1-fig the basic_block in

 Example dag1-ex can be replaced_by a block with only three

 statements In_fact if b is not live_on exit from the

 block then we do_not need to compute that variable and can use

 d to receive the value represented_by the node_labeled

 in Fig dag1-fig The block then becomes

 center

 tabularl

 'a_b c'



 'd a -_d'



 'c d c'



 tabular

 center



 However if both b and d are live_on exit then a

 fourth statement must_be used to copy the value from one to the

 other( In_general we must_be careful when

 reconstructing code from 's how we choose the names of

 variables If a variable is defined twice or if it is

 assigned once and the initial value is also used then we

 must make_sure that we do_not change the value of until we

 have made all uses of the node whose value previously held)



 ex

 dag2-ex When we look for common_subexpressions we really

 are looking for expressions that are guaranteed to compute the

 same value no_matter how that value is computed Thus the

 method will miss the fact that the expression computed by the

 first and fourth statements in the sequence



 center

 tabularl

 'a_b c'



 'b b -_d'



 'c c d'



 'e b_c'



 tabular

 center

 is the same namely That is even_though

 b and c both change between the first and last

 statements their sum remains the same because

 The for this sequence is shown in Fig dag2-fig but

 does_not exhibit any common_subexpressions However algebraic

 identities applied to the as discussed in

 Section alg-identities-subsect may expose the equivalence

 ex



 figurehtfb





 for basic_block in Example dag2-ex

 dag2-fig

 figure



 Dead Code Elimination



 The operation on 's that corresponds to dead-code_elimination

 can be_implemented as_follows We delete from a any root

 (node with no ancestors) that has no live_variables attached

 Repeated application of this transformation will remove all nodes

 from the that correspond to dead_code



 ex

 If in Fig dag2-fig a and b are live but c and e are not we can immediately remove the root labeled

 e Then the node_labeled c becomes a root and can be

 removed The roots labeled a and b remain since they

 each have live_variables attached

 ex



 The Use of Algebraic Identities

 alg-identities-subsect



 Algebraic identities represent another important class of

 optimizations on basic_blocks

 For_example we may apply arithmetic identities such_as



 center

 tabularl p l









 tabular

 center

 to eliminate computations from a basic_block



 Another class of algebraic optimizations includes local reduction in

 strength that is replacing a more_expensive operator

 by a cheaper one as in



 center

 tabularc_c c

 Expensive Cheaper















 tabular

 center



 A third class of related optimizations is constant_folding

 Here we evaluate constant expressions at_compile time

 and replace the constant expressions by their values(

 Arithmetic expressions should be evaluated the same way at_compile time

 as they are at_run time

 K Thompson has suggested an elegant solution to constant_folding

 compile the constant expression

 execute the target code on the spot

 and replace the expression with the result

 Thus the compiler does_not need to contain

 an interpreter)

 Thus the expression would be replaced_by 628

 Many constant expressions arise in practice because of the frequent use of

 symbolic_constants in programs



 The -construction process can help us apply these and other

 more general algebraic transformations such_as commutativity and

 associativity For_example suppose the language reference manual

 specifies that is commutative that

 is Before we create a new node_labeled

 with left_child and right_child we always check_whether

 such a node already exists However because is

 commutative we should then check for a node having operator

 left_child and right_child



 The relational operators such_as and sometimes generate

 unexpected common_subexpressions For_example the condition

 can also be tested by subtracting the arguments and performing a

 test on the condition code set by the

 subtractionfootnoteThe subtraction can however

 introduce overflows and underflows while a compare instruction

 would notfootnote Thus only one node of the may

 need to be generated for and



 Associative laws might also be applicable to expose common

 subexpressions For_example if the source code has the

 assignments



 center

 tabularl

 'a_b c'



 'e c_d b'



 tabular

 center

 the following intermediate_code might be generated



 center

 tabularl

 'a_b c'



 't c d'



 'e t b'



 tabular

 center

 If t is not needed outside this block we can change this

 sequence to



 center

 tabularl

 'a_b c'



 'e a d'



 tabular

 center

 using both the associativity and commutativity of



 The compiler_writer should examine the language reference manual

 carefully to determine what rearrangements of computations are

 permitted since (because of possible overflows or underflows)

 computer arithmetic does_not always obey the algebraic_identities

 of mathematics For_example the Fortran standard states that a

 compiler may evaluate any mathematically equivalent expression

 provided that the integrity of parentheses is not violated Thus

 a compiler may evaluate as but it

 may not evaluate as A Fortran compiler must

 therefore keep_track of where parentheses were present in the

 source_language expressions if it is to optimize programs in

 accordance with the language definition



 Representation of Array References



 At first glance it might appear that the array-indexing instructions

 can be treated_like any other operator Consider for instance the

 sequence of three-address_statements



 center

 tabularl

 'x ai'



 'aj y'



 'z ai'



 tabular

 center

 If we think of ai as an operation involving a and

 i similar to then it might appear as if the two uses

 of ai were a common_subexpression In that case we might

 be tempted to optimize by_replacing the third instruction z ai by the simpler z x However since j could

 equal i the middle statement may in fact change the value

 of ai thus it is not legal to make this change



 The proper way to represent array_accesses in a is as

 follows



 enumerate



 An assignment from an array like x_ai is

 represented_by creating a node with operator and two

 children representing the initial value of the array

 in this case and the index i Variable x becomes a

 label of this new node



 An assignment to an array like aj y is

 represented_by a new node with operator and three children

 representing j and y There is no

 variable labeling this node What is different is that the

 creation of this node kills all currently constructed nodes

 whose value depends_on A node that has_been killed

 cannot receive any more labels that is it cannot become a common

 subexpression



 enumerate



 ex

 array-dag1-ex The for the basic_block



 center

 tabularl

 'x ai'



 'aj y'



 'z ai'



 tabular

 center

 is shown in Fig array-dag1-fig The node for x is

 created first but when the node_labeled is created

 is killed Thus when the node for is created it cannot

 be identified with and a new node with the same operands

 and must_be created instead

 ex



 figurehtfb







 The for a sequence of array assignments

 array-dag1-fig

 figure



 ex

 Sometimes a node must_be killed even_though none of its_children

 have an array like in Example array-dag1-ex as

 attached variable Likewise a node can kill if it has a

 descendant that is an array even_though none of its_children are

 array nodes For_instance consider the three-address_code



 center

 tabularl

 'b 12 a'



 'x bi'



 'bj y'



 tabular

 center



 What is happening here is that for efficiency reasons b

 has_been defined to be a position in an array a For

 example if the elements of a are four_bytes long then b represents the fourth element of a If j and i represent the same value then bi and bj

 represent the same_location Therefore it is important to have the

 third instruction bj y kill the node with x as

 its attached variable However as we see in

 Fig array-dag2-fig both the killed node and the node that

 does the killing have as a grandchild not as a

 child

 ex



 figurehtfb







 A node that kills a use of an array need not have that

 array as a child array-dag2-fig

 figure



 Pointer Assignments and Procedure Calls



 When we assign indirectly through a pointer as in the assignments



 center

 tabularl

 'x p'



 'q y'



 tabular

 center

 we do_not know what p or q point to In effect x p is a use of every variable whatsoever and q y is

 a possible assignment to every variable As a consequence the

 operator must take all nodes that are currently

 associated_with identifiers as arguments which is relevant for

 dead-code_elimination More_importantly the operator

 kills all other nodes so_far constructed in the



 There_are global pointer analyses one could perform that might

 limit the set of variables a pointer could reference at a given

 place in the code Even local analysis could restrict the scope

 of a pointer For_instance in the sequence

 center

 tabularl

 'p x'



 'p y'

 tabular

 center

 we know that x and no other variable is given the value of

 y so we don't need to kill any node but the node to which

 x was attached



 Procedure calls behave much like assignments through pointers In

 the absence of global data-flow information we must assume that a

 procedure uses and changes any data to which it has access Thus

 if procedure is in the scope of variable a call to

 both uses the node with attached variable and kills

 that node



 Reassembling Basic_Blocks From DAG's



 After we perform whatever optimizations are possible while

 constructing the or by manipulating the once

 constructed we may reconstitute the three-address_code for the

 basic_block from which we built the For each node that has

 one or_more attached variables we construct a three-address

 statement that computes the value of one of those variables We

 prefer to compute the result into a variable that is live_on exit

 from the block However if we do_not have global live-variable

 information to work from we need to assume that every variable of

 the program (but_not temporaries that are generated_by the

 compiler to process expressions) is live_on exit from the block



 If the node has more_than one live variable attached then we have to

 introduce copy statements to give the correct value to each of those

 variables Sometimes global optimization can eliminate those copies

 if we can arrange to use one of two variables in place of

 the other



 ex

 Recall the of Fig dag1-fig In the discussion

 following Example dag1-ex we decided that if b is not

 live_on exit from the block then the three statements



 center

 tabularl

 'a_b c'



 'd a -_d'



 'c d c'



 tabular

 center

 suffice to reconstruct the basic_block The third instruction

 cdc must use d as an operand rather_than

 b because the optimized block never computes b



 If both b and d are live_on exit or if we are not

 sure whether or not they are live_on exit then we need to compute

 b as_well as d We can do so with the sequence



 center

 tabularl

 'a_b c'



 'd a -_d'



 'b d'



 'c d c'



 tabular

 center

 This basic_block is still more_efficient than the original

 Although the number of instructions is the same we have replaced

 a subtraction by a copy which tends to be less expensive on most

 machines Further it may be that by doing a global analysis we

 can eliminate the use of this computation of b outside the

 block by_replacing it by uses of d In that case we can

 come back to this basic_block and eliminate bd later

 Intuitively we can eliminate this copy if wherever this value of

 b is used d is still holding the same value That

 situation may or may not be true depending_on how the program

 recomputes d

 ex



 When reconstructing the basic_block from a we not only

 need to worry_about what variables are used to hold the values of

 the 's nodes but we also need to worry_about the order in which we

 list the instructions computing the values of the various nodes

 The rules to remember are



 enumerate



 The order of instructions must respect the order of nodes in

 the That is we cannot compute a node's value until we

 have computed a value for each of its_children



 Assignments to an array must follow all previous assignments to or

 evaluations from the same array according to the order of these

 instructions in the original basic_block



 Evaluations of array_elements must follow any previous

 (according to the original block) assignments to the same array

 The only permutation allowed is that two evaluations from the same

 array may be done in either order as_long as neither crosses over

 an assignment to that array



 Any use of a variable must follow all previous (according to the

 original block) procedure_calls or indirect assignments through a pointer



 Any procedure call or indirect assignment through a pointer must follow

 all previous (according to the original block) evaluations of any

 variable



 enumerate

 That is when reordering code no statement may cross a procedure call

 or assignment through a pointer and uses of the same array may cross

 each other only if both are array_accesses but not assignments to

 elements of the array



 sexer

 dag1-exer Construct the for the basic_block



 center

 tabularl

 'd b_c'



 'e a b'



 'b b_c'



 'a e -_d'



 tabular

 center

 sexer



 exer

 Simplify the three-address_code of Exercise dag1-exer assuming



 itemize



 a) Only is live_on exit from the block

 b) and are live_on exit from the block



 itemize

 exer



 sexer

 Construct the DAG for the code in block of

 Fig_identity-fg-fig Do not forget to include the comparison



 sexer



 exer

 Construct the DAG for the code in block of

 Fig_identity-fg-fig

 exer



 sexer

 Extend Algorithm nu-alg to process three-statements

 of the form

 itemize

 a) ai b

 b) a bi

 c) a b

 c) a b

 itemize

 sexer



 exer

 dag2-exer Construct the for the basic_block

 center

 tabularl

 'ai b'



 'p c'



 'd aj'



 'e p'



 'p ai'



 tabular

 center

 on the assumption that



 itemize



 a) p can point anywhere



 b) p can point only to b or d



 itemize

 exer



 hexer

 If a pointer or array expression such_as ai or p

 is assigned and then used without the possibility of being

 changed in the interim we can take_advantage of the situation to

 simplify the For_example in the code of

 Exercise dag2-exer if p cannot point to d then

 the fourth statement e p can

 be replaced_by e c

 Revise the -construction algorithm to take_advantage of such

 situations and apply your algorithm to the code of

 Exercise dag2-exer

 hexer



 exer

 Suppose a basic_block is formed from the C assignment

 statements



 center

 tabularl

 'x a b_c d e f'



 'y a c e'



 tabular

 center



 itemize



 a)_Give the three-address_statements (only one addition per

 statement) for this block



 b) Use the associative and commutative laws to modify the

 block to use the fewest possible number of instructions assuming

 both x and y are live_on exit from the block



 itemize

 exer

 Basic_Blocks and Flow_Graphs

 bb-sect



 This_section introduces a graph representation of intermediate

 code that is helpful for discussing code_generation even if the

 graph is not constructed explicitly by a code-generation

 algorithm Code generation benefits from context We can do a

 better job of register_allocation if we know how values are

 defined and used as we_shall see in Section ra-sect We can

 do a better job of instruction_selection by_looking at sequences

 of three-address_statements as we_shall see in

 Section tile-sect



 The representation is constructed as_follows



 enumerate



 Partition the intermediate_code into basic_blocks

 which are maximal sequences of consecutive three-address

 instructions with the properties that

 enumerate

 The flow of control can only enter the basic_block through

 the first instruction in the block That is there are no jumps

 into the middle of the block Control will leave the block

 without halting or branching except possibly at the last

 instruction in the block

 enumerate



 The basic_blocks become the nodes of a flow_graph whose edges

 indicate which blocks can follow which other blocks

 enumerate



 Starting in Chapter_code-op-ch we discuss transformations

 on flow_graphs that turn the original intermediate_code into

 optimized intermediate_code from which better target code can

 be generated The optimized intermediate_code is turned_into

 machine code using the code-generation techniques in this_chapter



 Basic_Blocks



 Our first job is to partition a sequence of three-address

 instructions into basic_blocks We begin a new basic_block with

 the first instruction and keep adding instructions until we meet

 either a jump a conditional_jump or a label on the following

 instruction In the absence of jumps and labels control proceeds

 sequentially from one instruction to the next This idea is

 formalized in the following algorithm



 The Effect of Interrupts

 The notion that control once it reaches the beginning of a basic_block

 is certain to continue through to the end requires a bit of thought

 There_are many reasons_why an interrupt not reflected explicitly in the

 code could cause control to leave the block perhaps never to return

 For_example an instruction like x_yz appears not to affect

 control_flow but if is 0 it could actually cause the program to abort



 We_shall not worry_about such possibilities The_reason is as

 follows The purpose of constructing basic_blocks is to optimize

 the code Generally when an interrupt occurs either it will be

 handled and control will come back to the instruction that caused

 the interrupt as if control had never deviated or the program

 will halt with an error In the latter case it doesn't matter

 how we optimized the code even if we depended on control reaching

 the end of the basic_block because the program didn't produce its

 intended result anyway



 alg

 bb-alg

 Partitioning three-address_instructions into basic_blocks



 A sequence of three-address_instructions



 A list of the basic_blocks for that sequence in which each instruction is

 assigned to exactly one basic_block



 First we determine those instructions in the intermediate

 code that are leaders that is the first instructions in

 some basic_block The instruction just past the end of the

 intermediate program is not included as a leader The rules for

 finding leaders are



 enumerate



 The first three-address_instruction in the intermediate_code is a leader



 Any instruction that is the target of a conditional or unconditional

 jump is a leader



 Any instruction that immediately follows a conditional or unconditional

 jump is a leader



 enumerate

 Then for each leader its basic_block consists of itself and all

 instructions up to but not including the next leader or the end of the

 intermediate program

 alg



 figurehtfb



 center

 tabularr_l

 1) i 1



 2) j 1



 3) t1 10 i



 4) t2 t1 j



 5) t3 8 t2



 6) t4 t3 - 88



 7) at4 00



 8) j j 1



 9) if j 10 goto (3)



 10) i i 1



 11) if i 10 goto (2)



 12) i 1



 13) t5 i - 1



 14) t6 88 t5



 15) at6 10



 16) i i 1



 17) if i 10 goto (13)

 tabular

 center



 Intermediate_code to set a matrix to an

 identity matrix identity-matrix-code-fig

 figure



 ex

 identity-matrix-ex The intermediate_code in

 Fig identity-matrix-code-fig turns a matrix

 a into an identity matrix Although it is not important

 where this code comes_from it might be the translation of the

 pseudocode in Fig identity-matrix-code2-fig In generating

 the intermediate_code we have assumed that the real-valued array

 elements take 8 bytes each and that the matrix a is stored

 in row-major form



 figurehtfb



 center

 tabularl

 for from to do



 for from to do







 for from to do







 tabular

 center



 Source code for Fig identity-matrix-code-fig

 identity-matrix-code2-fig



 figure



 First instruction 1 is a leader by rule (1) of

 Algorithm bb-alg To_find the other leaders we first need

 to find the jumps In this example there are three jumps all

 conditional at instructions 9 11 and 17 By rule_(2) the

 targets of these jumps are leaders they are instructions 3 2

 and 13 respectively Then by rule_(3) each instruction

 following a jump is a leader those are instructions 10 and 12

 Note_that no instruction follows 17 in this code but if there

 were code following the 18th instruction would also be a leader



 We_conclude that the leaders are instructions 1_2 3 10 12 and

 13 The basic_block of each leader contains all the instructions

 from itself until just_before the next leader Thus the basic

 block of 1 is just 1 for leader 2 the block is just 2 Leader 3

 however has a basic_block consisting of instructions 3 through 9

 inclusive Instruction 10's block is 10 and 11 instruction 12's

 block is just 12 and instruction 13's block is 13 through 17

 ex





 Next-Use Information



 Knowing when the value of a variable will be used next is

 essential for generating good code If the value of a variable

 that is currently in a register will never be referenced

 subsequently then that register can be assigned to another

 variable



 The use of a name in a three-address statement is defined as

 follows_Suppose three-address statement assigns a value to

 If statement has as an operand and control can flow

 from statement to along a path that has no intervening

 assignments to then we say statement uses the value

 of computed at statement We further say that is live at statement



 We wish to determine for each three-address statement

 what the next uses of and are For the present we

 do_not concern ourselves with uses outside the basic_block

 containing this three-address statement



 Our algorithm to determine liveness and next-use information makes

 a backward_pass over each basic_block We store the information in

 the symbol_table We can easily scan a stream of three-address

 statements to find the ends of basic_blocks as in

 Algorithm bb-alg Since procedures can have arbitrary side

 effects we assume for convenience that each procedure call starts

 a new basic_block



 alg

 nu-alg

 Determining the liveness and next-use information for each statement in a basic

 block



 A basic_block of three-address_statements We assume

 that the symbol_table initially shows all nontemporary variables

 in as being live_on exit



 At each statement in we attach to

 the liveness and next-use information of and



 We start at the last statement in and scan backwards

 to the beginning of At each statement in

 we do the following



 enumerate



 Attach to statement the information currently found in

 the symbol_table regarding the next use and liveness of

 and



 In the symbol_table set to not live and no next

 use



 In the symbol_table set and to live and the

 next uses of and to



 enumerate

 Here we have used as a symbol representing any operator If

 the three-address statement is of the form or

 the steps are the same as above ignoring Note_that the

 order of steps (2) and (3) may not be interchanged because may

 be or

 alg



 Flow_Graphs

 fg-subsect



 Once an intermediate-code program is partitioned_into basic

 blocks we represent the flow of control between them by a flow

 graph The nodes of the flow_graph are the basic_blocks There is

 an edge from block to block if and only if it is possible

 for the first instruction in block to immediately follow the

 last instruction in block There_are two ways that such an edge

 could be justified



 itemize



 There is a conditional or unconditional_jump from the end of

 to the beginning of



 immediately follows in the original order of the

 three-address_instructions and does_not end in an

 unconditional_jump



 itemize



 We_say that is a predecessor of and

 is a successor of



 Often we add two nodes called the entry and exit

 that do_not correspond to executable intermediate instructions

 There is an edge from the entry to the first executable node of the

 flow_graph that is to the basic_block that comes_from the first

 instruction of the intermediate_code There is an edge to the exit

 from any basic_block that contains an instruction that could be

 the last executed instruction of the program If the final

 instruction of the program is not an unconditional_jump then the

 block containing the final instruction of the program is one

 predecessor of the exit but so is any basic_block that has a jump

 to code that is not part of the program



 figurehtfb

 Flow

 graph from Fig identity-matrix-code-fig

 identity-fg-fig

 figure



 exfg-ex

 The set of basic_blocks constructed in

 Example identity-matrix-ex yields the flow_graph of

 Fig_identity-fg-fig The entry points to basic_block

 since contains the first instruction of the program The

 only successor of is because does_not end in an

 unconditional_jump and the leader of immediately follows

 the end of



 Block has two_successors One is itself because the leader

 of instruction 3 is the target of the conditional_jump at

 the end of instruction 9 The other successor is

 because control can fall_through the conditional_jump at the end

 of and next enter the leader of



 Only points to the exit of the flow_graph since the only

 way to get to code that follows the program from which we

 constructed the flow_graph is to fall_through the conditional_jump

 that ends

 ex



 Representation of Flow_Graphs



 First note from Fig_identity-fg-fig that in the flow_graph it

 is normal to replace the jumps to instruction numbers or labels by jumps

 to basic_blocks Recall that every conditional or unconditional_jump is

 to the leader of some basic_block and it is to this block that the jump

 will now refer The_reason for this change is that after constructing

 the flow_graph it is common to make substantial changes to the

 instructions in the various basic_blocks If jumps were to

 instructions we would have to fix the targets of the jumps every time

 one of the target instructions was changed



 Flow graphs being quite ordinary graphs can be represented_by any of

 the data_structures appropriate for graphs The content of nodes (basic

 blocks) need their own representation We might represent the content

 of a node by a pointer to the leader in the array of three-address

 instructions together_with a count of the number of instructions or a

 second pointer to the last instruction However since we may be

 changing the number of instructions in a basic_block frequently it is

 likely to be more_efficient to create a linked_list of instructions for

 each basic_block



 Loops

 loops-subsect



 Programming-language constructs like while-statements

 do-while-statements and for-statements naturally give rise

 to loops in programs

 Since virtually every program spends most of its time in

 executing its loops it is especially important for a compiler

 to generate good code for loops

 Many code transformations depend upon the identification of loops in

 a flow_graph

 We_say that a set of nodes in a flow_graph

 is a loop if contains a node called the loop entry

 such that



 enumerate



 is not ENTRY the entry of the entire_flow graph



 No node in besides has a predecessor outside That is every

 path from ENTRY to any node in goes

 through



 Every node in has a nonempty

 path completely within to



 enumerate



 ex

 The flow_graph of Fig_identity-fg-fig has three loops



 enumerate



 by itself



 by itself







 enumerate

 The first two are single nodes with an edge to the node itself For

 instance forms a loop with as its entry Note_that

 the last requirement for a loop is that there be a nonempty path

 from to itself Thus a single_node like which does

 not have an edge is not a loop since there is no

 nonempty path from to itself within



 The third loop has as its loop entry

 Note_that among these three nodes only has a predecessor

 that is not in Further each of the three nodes has a

 nonempty path to staying within For_instance

 has the path

 ex



 exer

 mm-code-exer

 Figure mm-exer-fig is a simple matrix-multiplication program



 itemize



 a)

 Translate the program into three-address_statements of the type we have

 been using in this_section Assume the matrix entries are

 numbers that require 8 bytes and

 that matrices are stored in row-major_order



 b)

 Construct the flow_graph for your code from (a)



 c)

 Identify the loops in your flow_graph from (b)



 itemize

 exer



 figurehtfb



 center

 tabularl

 'for (i0 in i)'



 ' for_(j0 jn j)'



 ' cij 00'



 'for (i0 in i)'



 ' for_(j0 jn j)'



 ' for (k0 kn k)'



 ' cij cij aikbkj'



 tabular

 center

 A matrix-multiplication_algorithm mm-exer-fig



 figure



 sexer

 primes-code-exer Figure primes-exer-fig is code to

 count the number of primes from 2 to using the sieve method

 on a suitably large array That is is TRUE at

 the end only if there is no prime or less that evenly

 divides We initialize all to TRUE and then set

 to FALSE if we find a divisor of



 itemize



 a)

 Translate the program into three-address_statements of the type we have

 been using in this_section Assume integers require 4 bytes



 b)

 Construct the flow_graph for your code from (a)



 c)

 Identify the loops in your flow_graph from (b)



 itemize

 sexer



 figurehtfb



 center

 tabularl

 'for (i2 in i)'



 ' ai TRUE'



 'count 0'



 's sqrt(n)'



 'for (i2 is i)'



 ' if (ai) i has_been found to be a prime '



 ' count'



 ' for (j2i jn j ji)'



 ' aj FALSE no multiple of i is a prime '



 '_'



 tabular

 center



 Code to sieve for primes primes-exer-fig



 figure

 Datalog Implementation by BDD's

 bdd-sect



 Binary Decision Diagrams (BDD's) are a method for representing

 boolean_functions by graphs Since there are boolean

 functions of variables no representation

 method is going to be very succinct on

 all boolean_functions However the boolean_functions that appear in

 practice tend to have a lot of regularity It is thus common that one

 can find a succinct BDD for functions that one really wants to

 represent



 It_turns out that the boolean_functions that are described by the

 Datalog_programs that we have developed to analyze programs are no

 exception While succinct BDD's representing information_about a

 program often must_be found using heuristics andor techniques used in

 commercial BDD-manipulating packages the BDD approach has_been quite

 successful in practice In_particular it outperforms methods based_on

 conventional database-management systems because the latter are

 designed for the more irregular data patterns that appear in typical

 commercial data



 It is beyond the scope of this_book to cover all of the BDD technology

 that has_been developed over the years We_shall here introduce you

 to the BDD notation We then suggest how one represents relational

 data as BDD's and how one could manipulate BDD's to reflect the

 operations that are performed to execute Datalog_programs by algorithms

 such_as Algorithm seminaive-alg Finally we describe_how to

 represent the exponentially_many contexts in BDD's the key to the

 success of the use of BDD's in context-sensitive_analysis



 Binary Decision Diagrams



 A BDD represents a boolean function by a rooted DAG The interior

 nodes of the DAG are

 each labeled by one of the variables of the represented function At

 the bottom are two leaves one labeled 0 the other labeled 1

 Each interior_node has two edges to children these edges are called

 low and high The low_edge is associated_with the case that the

 variable at the node has value 0 and the high_edge is associated_with

 the case_where the variable has value 1



 Given a truth_assignment for

 the variables we can start at the root and at each node say a node

 labeled follow the low or high_edge

 depending_on whether the truth

 value for is 0 or 1 respectively If we arrive at the leaf_labeled

 1 then the represented function is true for this truth_assignment

 otherwise it is false



 figurehtfb

 fileuullmanalsuch12figsbdd1eps

 A binary_decision diagram

 bdd1-fig

 figure



 ex

 In Fig bdd1-fig we see a BDD We_shall see the function it

 represents shortly Notice_that we have labeled all the low edges

 with 0 and all the high edges by 1 Consider the truth_assignment

 for variables that sets and Starting at the

 root since we take the low_edge which gets us to the leftmost of

 the nodes labeled Since we again follow the low_edge from

 this node which takes us to the leftmost of the nodes labeled

 Since we next move to the leftmost of the nodes labeled Now

 since we take the high_edge and wind_up at the leaf_labeled 1

 Our conclusion is that the function is true for this truth_assignment



 Now_consider the truth_assignment that is and

 We again start at the root Since we again move to the

 leftmost of the nodes labeled But now since we follow the

 high_edge which jumps to the 0 leaf That is we know not only that

 truth_assignment 0101 makes the function false but since we never even

 looked at or any truth_assignment of the form will also

 make the function have value 0 This short-circuiting ability

 is one of the reasons BDD's tend to be succinct representations of

 boolean_functions

 ex



 In Fig bdd1-fig the interior_nodes are in ranks - each rank

 having nodes with a particular variable as label Although it is not an

 absolute requirement it is convenient to restrict ourselves to ordered BDD's

 In an ordered BDD there is an order to the

 variables and whenever there is an edge from a parent node_labeled

 to a child_labeled then We_shall see that it is

 easier to operate_on ordered BDD's and from here we assume all BDD's

 are ordered



 Notice also that BDD's are DAG's not trees Not_only will the leaves 0

 and 1 typically have many parents but interior_nodes also may have

 several parents For_example the rightmost of the nodes labeled in

 Fig bdd1-fig has two parents This combination of nodes that

 would result in the same decision is another reason that BDD's tend to

 be succinct



 Transformations on BDD's



 We alluded in the discussion_above to two simplifications on BDD's that

 help make them more succinct



 enumerate



 Short-Circuiting If a node has both its high and low edges go

 to the same node then we

 may eliminate Edges entering go to instead



 Node-Merging If two nodes and have low edges that go to

 the same node and also have high edges that go to the same node then we

 may merge with Edges entering either or go to the

 merged node



 enumerate

 It is also possible to run these transformations in the opposite

 direction In_particular we can introduce a node along an edge from

 to Both edges from the introduced node go to and the edge

 from now goes to the introduced node Note_however that the

 variable assigned to the new node must_be one of those that lies between

 the variables of and in the order

 Figure bdd2-fig shows the two transformations schematically



 figurehtfb

 fileuullmanalsuch12figsbdd2eps

 Transformations on BDD's

 bdd2-fig

 figure



 Representing Relations by BDD's



 The relations with which we have_been dealing have components that are

 taken from domains A domain for a component of a relation is the

 set of possible values that tuples can have in that component For

 example the relation has the domain of all program variables

 for its first component and the domain of all object-creating statements

 for the second_component If a domain has more_than possible

 values but no more_than values then it requires bits or

 boolean variables to represent values in that domain



 A tuple in a relation may thus be_viewed as a truth_assignment to the

 variables that represent values in the domains for each of the

 components of the tuple We may see a relation as a boolean function

 that returns the value true for all and only those truth assignments

 that represent tuples in the relation An_example should make these

 ideas clear



 ex

 rel-bdd-ex

 Consider a relation such that the domains of both and

 are We_shall encode by bits 00 by 01 by

 10 and by 11 Let the tuples of relation be



 center

 tabularcc

















 tabular

 center

 Let_us use boolean variables to encode the first () component

 and variables to encode the second () component Then the

 relation becomes



 center

 tabularcccc





 0_0 0_1



 0_0 1_0



 1 1 1_0



 tabular

 center

 That is the relation has_been converted into the boolean function

 that is true for the three truth-assignments 0010 and

 1110 Notice_that these three sequences of bits are exactly those that

 label the paths from the root to the leaf 1 in Fig bdd1-fig

 That is the BDD in that figure represents this relation if the

 encoding described above is used

 ex



 Relational Operations as BDD Operations



 Now we see_how to represent relations as BDD's But to implement an

 algorithm like Algorithm seminaive-alg (incremental evaluation of

 Datalog programs)

 we need to manipulate BDD's in a way that reflects how the

 relations themselves are manipulated Here are the principal operations

 on relations that we need to perform



 enumerate



 Initialization We need to create a BDD that represents a single

 tuple of a relation We'll assemble these into BDD's that represent

 large relations by_taking the union



 Union To take the union of relations we take the logical_OR of

 the boolean_functions that represent the relations This operation is

 needed not only to construct initial relations but also to combine the

 results of several rules for the same head predicate and to accumulate

 new facts into the set of old facts as in the incremental

 Algorithm seminaive-alg



 Projection When we evaluate a rule body we need to construct

 the head relation that is implied by the true tuples of the body In

 terms of the BDD that represents the relation we need to eliminate the

 nodes that are labeled by those boolean variables that do_not represent

 components of the head We may also need to rename the variables in the

 BDD to correspond to the boolean variables for the components of the

 head relation



 Join To_find the assignments of values to variables that make a

 rule body true we need to join the relations corresponding to each

 of the subgoals For_example suppose we have two subgoals

 The join of the relations for these subgoals

 is the set of triples such that is a tuple in the

 relation for and is a tuple in the relation for We

 shall_see that after renaming boolean variables in BDD's so the

 components for the two 's agree in variable names the operation on

 BDD's is similar to the logical AND which in turn is similar to the OR

 operation on BDD's that implements the union



 enumerate



 BDD's for Single Tuples



 To initialize a relation we need to have a way to construct a BDD for

 the function that is true for a single truth_assignment Suppose the

 boolean variables are and the truth_assignment is

 where each is either 0 or 1 The BDD will have

 one node for each If then the high_edge from

 leads to the leaf 0 and the low_edge leads to or to the leaf

 1 if If then we do the same but the high and low edges

 are reversed



 This strategy gives_us a BDD that checks_whether each has the correct

 value for As soon_as we find an incorrect value we

 jump directly to the 0 leaf We only wind_up at the 1 leaf if all

 variables have their correct value



 As an example look ahead to Fig bdd3-fig(b) This BDD

 represents the function that is true if and only if ie the

 truth_assignment 00



 Union



 We_shall give in detail an algorithm for taking the logical_OR of BDD's

 that is the union of the relations represented_by the BDD's



 alg

 bdd-union-alg

 Union of BDD's



 Two ordered BDD's with the same set of variables in the same order



 A BDD representing the

 function that is the logical_OR of the two boolean_functions represented

 by the input BDD's



 We_shall describe a recursive_procedure for combining two BDD's

 The induction is on the size of the set of variables appearing in the

 BDD's



 Zero variables The BDD's must both be leaves labeled either 0 or 1

 The output is the leaf_labeled 1 if either input is 1 or the leaf

 labeled 0 if both are 0



 Suppose there are variables found among the

 two BDD's Do the following



 enumerate



 If necessary use inverse short-circuiting to add a new root

 so that both BDD's have a root labeled



 Let the two roots be and let their low children be and

 and let their high children be and

 Recursively apply this algorithm to the BDD's rooted_at and

 Also recursively apply this algorithm to the BDD's rooted_at and

 The first of these BDD's represents the function that is true

 for all truth assignments that have and that make one or both

 of the given BDD's true The second represents the same for the truth

 assignments with



 Create a new root node_labeled Its low child is the root of the

 first recursively constructed BDD and its high child is the root of the

 second BDD



 Merge the two leaves labeled 0 and the two leaves labeled 1 in the

 combined BDD just constructed



 Apply merging and short-circuiting where possible to simplify the BDD



 enumerate

 alg



 figurehtfb

 fileuullmanalsuch12figsbdd3eps

 Constructing the BDD for a logical_OR

 bdd3-fig

 figure



 ex

 bdd-union-ex

 In Fig bdd3-fig(a) and (b) are two simple BDD's The first

 represents the function OR and the second represents the

 function



 center

 NOT AND NOT

 center

 Notice_that their logical_OR

 is the function 1 that is always true

 To apply_Algorithm bdd-union-alg to these two BDD's we consider

 the low children of the two roots and the high children of the two

 roots let_us take_up the latter first



 The high child of the root in Fig bdd3-fig(a) is 1 and in

 Fig bdd3-fig(b) it is 0 Since these children are both at the

 leaf level we do_not have to insert nodes labeled along each edge

 although the result would be the same had we chosen to do so The basis

 case for the union of 0 and 1 is to produce a leaf_labeled 1 that will

 become the high child of the new root



 The low children of the roots in Fig bdd3-fig(a) and (b) are both

 labeled so we can compute their_union BDD recursively These two

 nodes have low children_labeled 0 and 1 so the combination of their low

 children is the leaf_labeled 1 Likewise their high children are 1 and

 0 so the combination is again the leaf 1 When we add a new root

 labeled we have the BDD seen in Fig bdd3-fig(c)



 We are not done since Fig bdd3-fig(c) can be simplified The

 node_labeled has both children the node 1 so we can delete the node

 and have the leaf 1 be the low child of the root Now both

 children of the root are the leaf 1 so we can eliminate the root That

 is the simplest BDD for the union is the leaf 1 all by itself

 ex



 Using BDD's for Points-to_Analysis



 Getting context-insensitive_points-to analysis to work is already

 nontrivial The ordering of the BDD variables can greatly change the

 size of the representation Many considerations as_well as trial and

 error

 are needed to come_up with an ordering that allows the analysis to

 complete quickly



 It is even harder to get context-sensitive_points-to analysis to

 execute because of the exponentially_many contexts in the program In

 particular if we arbitrarily assign numbers to represent contexts in

 a call_graph we cannot handle even small Java

 programs It is important that the contexts be numbered so that the

 binary encoding of the points-to_analysis can be made very compact

 Two contexts of the same method with similar call_paths share a lot of

 commonalities so it is desirable to number the contexts of a

 method consecutively Similarly because pairs of

 caller-callees for the same call_site share_many similarities we_wish

 to number the contexts such that the numeric difference_between each

 caller-callee pair of a call_site is always a constant



 Even with a clever numbering_scheme for the calling_contexts it is

 still hard to analyze large Java programs efficiently Active machine

 learning has_been found useful in deriving a variable_ordering

 efficient enough to handle large applications



 exer

 Using the encoding of symbols in Example rel-bdd-ex develop a BDD

 that represents the relation consisting of the tuples

 and You_may order the boolean variables in whatever way gives

 you the most succinct BDD

 exer



 hexer

 As a function of how many nodes are there in the most succinct BDD

 that represents the exclusive-or function on variables That is

 the function is true if an odd number of the variables are true and

 false if an even number are true

 hexer



 exer

 Modify Algorithm bdd-union-alg so it produces the intersection

 (logical AND) of two BDD's

 exer



 vhexer

 Find algorithms to perform the following relational operations on the

 ordered BDD's that represent them



 itemize

 a) Project out some of the boolean variables That is the

 function represented should be true for a given truth_assignment

 if there was any truth_assignment for the missing variables that

 together_with made the original function true

 b) Join two relations and by combining a tuple from

 with one from whenever these tuples agree on the attributes that

 and have in common It is really sufficient to consider the case

 where the relations have only two components and one from each relation

 matches that is the relations are and

 itemize

 vhexer

 For a more in-depth discussion on processor architecture and design we

 recommend Hennessy and Patterson hp03x



 The concept of data_dependence was first discussed in

 Kuck Muraoka and Chen km72 and

 Lamport lamport74 in the context of compiling code for

 multiprocessors and vector machines



 Instruction scheduling was first used in scheduling horizontal microcode

 (dasgupta79 fisher81 tokoro81 and wood79)

 Fisher's work on microcode compaction led him to

 propose the concept of a VLIW machine where compilers directly can

 control the parallel_execution of operations fisher81 Gross

 and Hennessy gh82 used instruction scheduling to handle the delayed

 branches in the first MIPS RISC instruction set This

 chapter's algorithm is based_on Bernstein and Rodeh's br91

 more general

 treatment of scheduling of operations for machines with

 instruction-level_parallelism



 The basic idea_behind software_pipelining was first developed by Patel

 and Davidson pd76

 for scheduling hardware pipelines Software

 pipelining was first used by

 Rau and Glaeser RG81 to compile for a machine

 with specialized hardware designed to support software_pipelining

 The algorithm described_here is based_on Lam lam88 which

 assumes no specialized hardware support



 enumerate



 br91

 Bernstein D and M Rodeh

 Global instruction scheduling for superscalar machines

 Proc_ACM SIGPLAN 1991 Conference

 on Programming_Language Design and Implementation_pp 241-255



 dasgupta79

 Dasgupta S The organization of microprogram stores Computing Surveys 111 (1979) pp 39-65



 fisher81

 Fisher J A

 Trace scheduling a technique for global microcode compaction

 IEEE Trans on Computers C-307 (1981)_pp 478-490



 gh82

 Gross T R and Hennessy J_L

 Optimizing delayed branches

 Proc 15th Annual Workshop on Microprogramming (1982)

 pp 114-120



 hp03x

 Hennessy J_L and D A Patterson

 Computer Architecture A Quantitative Approach

 Third Edition

 Morgan Kaufman San_Francisco 2003



 km72

 Kuck D Y Muraoka and S Chen On the number of operations

 simultaneously executable in Fortran-like programs and their resulting

 speedup IEEE Transactions on Computers C-2112 (1972)

 pp 1293-1310



 lam88

 Lam M_S

 Software_pipelining an effective scheduling technique for VLIW

 machines

 Proc_ACM SIGPLAN 1988 Conference_on Programming_Language Design

 and Implementation_pp 318-328



 lamport74

 Lamport L The parallel_execution of DO loops Comm_ACM

 172 (1974) pp 83-93



 pd76

 Patel J H and E S Davidson

 Improving the throughput of a pipeline by insertion of delays

 Proc Third Annual Symposium_on Computer Architecture (1976)

 pp 159-164



 RG81

 Rau B R and C D Glaeser

 Some scheduling techniques and an easily schedulable horizontal

 architecture for high performance scientific computing

 Proc 14th Annual Workshop on Microprogramming (1981)_pp 183-198



 tokoro81

 Tokoro M E Tamura and T Takizuka

 Optimization of microprograms

 IEEE Trans on Computers C-307 (1981)_pp 491-504



 wood79

 Wood G

 Global optimization of microprograms through modular control

 constructs Proc 12th Annual Workshop in Microprogramming

 (1979)

 pp 1-6



 enumerate

 For detailed discussions of multiprocessor architectures we refer the

 reader to the text by Hennessy and Patterson hp03y



 Lamport lamport74x and

 Kuck Muraoka and Chen km72x introduced the concept of

 data-dependence analysis Early data-dependence tests used heuristics to

 prove a pair of references to be independent by determining if there

 are no solutions to Diophantine_equations and systems of real linear

 inequalities banerjee76 banerjee79 towle76

 Maydan Hennessy and Lam MH91 formulated the data-dependence test

 as integer_linear programming and showed that the problem can

 be_solved exactly and efficiently in practice The data-dependence

 analysis described_here is based_on work by

 Maydan Hennessy and Lam MH91 and

 Pugh and Wonnacott PW92

 which in turn use techniques of Fourier-Motzkin_elimination

 de73 and Shostak's algorithm shostak81



 The 70's and early 80's saw the use of loop

 transformations to improve vectorization and parallelization loop

 fusion ac72 loop fission ak81 stripmining

 loveman77 and loop interchange wolfe78 There were

 three major experimental parallelizervectorizing projects going on at

 the time Parafrase led by Kuck at the University of Illinois

 Urbana-Champaign pw86 the PFC project led by Kennedy at Rice

 University ak87 and the PTRAN project led by Allen at IBM

 Research ab88



 McKellar and Coffman mc69 first discussed using blocking

 to improve data_locality

 Lam Rothbert and Wolf lr91 provided the first

 in-depth empirical analysis of

 blocking on caches for modern architectures

 Wolf and Lam wl91 used

 linear-algebra techniques to compute data reuse in loops

 Sarkar and Gao sg91 introduced the

 optimization of array_contraction



 Lamport lamport74x was the first to model loops as iteration_spaces

 and used hyperplaning (a special_case of an affine transform) to find

 parallelism for multiprocessors Affine transforms have their root in

 systolic-array algorithm design kl78

 Intended as parallel

 algorithms directly implemented in VLSI systolic arrays require

 communication to be minimized along with parallelization

 Algebraic techniques were developed to map the computation onto space

 and time coordinates The concept of an affine schedule and the use

 of Farkas'_Lemma in affine transformations were_introduced by

 Feautrier feautrier92

 The affine-transformation algorithm described

 here is based_on work by

 Lim et_al ll97 lc99 ll01



 Porterfield porterfield89 proposed one of the first compiler

 algorithms to prefetch data

 Mowry Lam and Gupta ml92 applied reuse analysis to minimize

 the prefetch overhead and gain an overall performance improvement



 enumerate



 ak81

 Abu-Sufah W D J Kuck and D H Lawrie

 On the performance

 enhancement of paging systems through program analysis and

 transformations

 IEEE Trans on Computing C-305 (1981)_pp 341-356



 ab88

 Allen F E M Burke P Charles R Cytron and J Ferrante An

 overview of the PTRAN analysis system for multiprocessing J

 Parallel and Distributed Computing 55 (1988) pp 617-640



 ac72

 Allen F E and J Cocke

 A Catalogue of

 optimizing transformations in Design and Optimization of Compilers

 (R Rustin ed) pp 1-30 Prentice-Hall 1972



 ak87

 Allen R and K Kennedy Automatic translation of Fortran programs

 to vector form ACM Transactions on Programming_Languages and

 Systems 94 (1987) pp 491-542



 banerjee76

 Banerjee U

 Data_Dependence in Ordinary Programs

 Master's thesis Department of Computer_Science University of

 Illinois Urbana-Champaign 1976



 banerjee79

 Banerjee U

 Speedup of Ordinary Programs

 PhD thesis Department of Computer_Science University of

 Illinois Urbana-Champaign 1979



 de73

 Dantzig G and B C Eaves

 Fourier-Motzkin_elimination and its dual

 J Combinatorial Theory A(14) (1973)_pp 288-297



 feautrier92

 Feautrier P

 Some efficient solutions to the affine scheduling problem I One-dimensional time

 International J Parallel Programming 215 (1992)

 pp 313-348



 hp03y

 Hennessy J_L and D A Patterson

 Computer Architecture A Quantitative Approach

 Third Edition

 Morgan Kaufman San_Francisco 2003



 km72x

 Kuck D Y Muraoka and S Chen On the number of operations

 simultaneously executable in Fortran-like programs and their resulting

 speedup IEEE Transactions on Computers C-2112 (1972)

 pp 1293-1310



 kl78

 Kung H T and C E Leiserson

 Systolic arrays (for VLSI) in Duff I S and G W Stewart

 (eds) Sparse Matrix Proceedings pp 256-282 Society

 for Industrial and Applied Mathematics 1978



 lr91

 Lam M_S E_E Rothberg and M E Wolf The

 cache performance and optimization of blocked algorithms

 Proc Sixth International

 Conference_on Architectural Support for Programming

 Languages and Operating Systems (1991) pp 63-74



 lamport74x

 Lamport L The parallel_execution of DO loops Comm_ACM

 172 (1974) pp 83-93



 lc99

 Lim A W G I Cheong and M_S Lam An affine_partitioning

 algorithm to maximize parallelism and minimize_communication Proc 13th International_Conference on Supercomputing

 (1999) pp 228-237



 ll97

 Lim A W and M_S Lam

 Maximizing parallelism and minimizing synchronization with affine

 transforms

 Proc 24th ACM SIGPLAN-SIGACT Symposium_on Principles of

 Programming_Languages (1997) pp 201-214



 ll01

 Lim A W S-W Liao and M_S Lam

 Blocking and array_contraction across arbitrarily nested_loops using

 affine_partitioning

 Proc_ACM SIGPLAN Symposium_on Principles and Practice

 of Parallel Programming (2001) pp 103-112



 loveman77

 Loveman D B

 Program improvement by source-to-source transformation

 J_ACM 241 (1977) pp 121-145



 MH91

 Maydan D E J_L Hennessy and M_S Lam

 An efficient method for exact dependence analysis

 Proc_ACM SIGPLAN 1991 Conference_on Programming_Language Design

 and Implementation_pp 1-14



 mc69

 McKeller A C and E G Coffman

 The organization

 of matrices and matrix operations in a paged multiprogramming

 environment Comm_ACM 123 (1969) pp 153-165



 ml92

 Mowry T C_M S_Lam and A Gupta

 Design and evaluation of a compiler algorithm for prefetching

 Proc Fifth International_Conference on Architectural Support for

 Programming_Languages and Operating Systems (1992)

 pp 62-73



 pw86

 Padua D A and M J Wolfe

 Advanced compiler optimizations for supercomputers

 Comm_ACM 2912 (1986)

 pp 1184-1201



 porterfield89

 Porterfield A

 Software Methods for Improving Cache Performance on Supercomputer Applications

 PhD Thesis

 Department of Computer_Science

 Rice University 1989



 PW92

 Pugh W and D Wonnacott

 Eliminating false positives using the omega test

 Proc_ACM SIGPLAN 1992 Conference_on Programming_Language Design

 and Implementation_pp 140-151



 sg91

 Sarkar V and G Gao

 Optimization of array_accesses by collective loop transformations

 Proc 5th International_Conference on Supercomputing (1991)

 pp 194-205



 shostak81

 R Shostak Deciding linear_inequalities by computing loop

 residues

 J_ACM 284 (1981)_pp 769-779



 towle76

 Towle R A

 Control and Data_Dependence for Program Transformation

 PhD thesis Department of Computer_Science University of

 Illinois Urbana-Champaign 1976



 wl91

 Wolf M E and M_S Lam

 A data_locality optimizing algorithm

 Proc SIGPLAN 1991 Conference_on Programming_Language Design and

 Implementation_pp 30-44



 wolfe78

 Wolfe M J

 Techniques for Improving the Inherent Parallelism in Programs

 Master's thesis Department of Computer_Science University of

 Illinois Urbana-Champaign 1978



 enumerate

 Some of the basic concepts in interprocedural_analysis can be found in

 Allen74 banning79 barth78 and sp81x

 Callahan et_al cc86 describe an interprocedural constant-propagation

 algorithm



 Steensgaard steensgaard96 published the

 first scalable pointer-alias_analysis It is

 context-insensitive flow-insensitive and equivalence-based

 A context-insensitive

 version of the inclusion-based_points-to analysis was derived by

 Andersen and94

 Later Heintze and Tardieu ht01

 described an efficient algorithm for this analysis

 Fahndrich Rehof and Das fr00 presented a

 context-sensitive flow-insensitive

 equivalence-based analysis that scales to large_programs like gcc

 Notable among previous attempts to create a context-sensitive

 inclusion-based_points-to analysis is

 Emami Ghiya and Hendren eg94 which is a

 cloning-based context-sensitive flow-sensitive inclusion-based

 points-to algorithm



 Binary decision_diagrams (BDD's) first appeared in Bryant bryant86

 Their use for data-flow_analysis was by

 Ball and Rajamani br00 The

 application of BDD's to insensitive pointer_analysis is reported by

 Zhu zhu02 and Berndl et_al b03

 Whaley and Lam wl04 describe the first

 context-sensitive flow-insensitive inclusion-based algorithm that

 has_been shown to apply to real-life applications

 The paper describes a tool called bddbddb

 that automatically_translates analysis described in Datalog into BDD

 code Object-sensitivity was_introduced

 by Milanova Rountev and Ryder mr02



 For a discussion of Datalog see

 Ullman and Widom uw02

 Also see Lam et_al l05

 for a discussion of the connection of data-flow_analysis to Datalog



 The Metal code checker is described

 by Engler et_al ec00 and the PREfix

 checker was created

 by Bush Pincus and Sielaff bp00

 Ball and Rajamani br00 developed a program analysis engine

 called SLAM using model checking and symbolic execution

 to simulate all possible behaviors of a system

 Ball et_al b06 have created a static analysis

 tool called SDV based_on SLAM to find API usage errors in C

 device-driver programs by_applying BDD's to model checking



 Livshits and Lam ll05 describe_how

 context-sensitive_points-to analysis can be used to find SQL

 vulnerabilities in Java web applications

 Ruwase and Lam rl04 describe_how

 to keep_track of array extents and insert dynamic bounds checks

 automatically

 Rinard et_al rc04 describe_how to extend

 arrays dynamically to accommodate for the overflowed contents

 Avots et_al ad05

 extend the context-sensitive Java points-to_analysis to C and show

 how it can be used to reduce the cost of dynamic detection of buffer

 overflows



 enumerate



 Allen74

 Allen F E

 Interprocedural data flow analysis

 Proc IFIP Congress 1974 pp 398-402 North Holland Amsterdam 1974



 and94

 Andersen L Program Analysis and Specialization for the C

 Programming_Language PhD thesis DIKU Univ of Copenhagen

 Denmark 1994



 ad05

 Avots D M Dalton V B Livshits and M_S Lam

 Improving software

 security with a C pointer_analysis

 ICSE 2005 Proc

 27th International_Conference on Software Engineering

 pp 332-341



 br00

 Ball T and S K Rajamani A symbolic model checker for boolean

 programs Proc SPIN 2000 Workshop on Model Checking of

 Software pp 113-130



 b06

 Ball T E Bounimova B Cook V Levin J Lichtenberg C

 McGarvey B Ondrusek S Rajamani and A Ustuner

 Thorough static analysis of device drivers

 EuroSys (2006) pp 73-85



 banning79

 Banning J P An efficient way to find the side_effects of

 procedural calls and the aliases of variables

 Proc Sixth Annual Symposium_on Principles of Programming

 Languages (1979) pp 29-41



 barth78

 Barth J M A practical interprocedural data flow analysis

 algorithm Comm_ACM 219 (1978) pp 724-736



 b03

 Berndl M O Lohtak F Qian L Hendren and N Umanee Points-to

 analysis using BDD's

 Proc_ACM SIGPLAN 2003 Conference

 on Programming_Language Design and Implementation_pp 103-114



 bryant86

 Bryant R E Graph-based algorithms for Boolean function

 manipulation IEEE Trans on Computers C-358 (1986)

 pp 677-691



 bp00

 Bush W R J_D Pincus and D J Sielaff

 A static analyzer for finding dynamic_programming errors

 Software-Practice and Experience 307 (2000)

 pp 775-802



 cc86

 Callahan D K D Cooper K Kennedy and L Torczon

 Interprocedural constant_propagation Proc SIGPLAN 1986

 Symposium_on Compiler Construction SIGPLAN Notices 217 (1986)

 pp 152-161





 ec00

 Engler D B Chelf A Chou and S Hallem

 Checking system rules using system-specific programmer-written

 compiler extensions

 Proc Sixth USENIX Conference_on Operating Systems Design and

 Implementation (2000) pp 1-16



 eg94

 Emami M R Ghiya and L J Hendren

 Context-sensitive interprocedural points-to_analysis

 in the presence of function pointers

 Proc SIGPLAN Conference_on

 Programming_Language Design and Implementation (1994)

 pp 224-256



 fr00

 Fahndrich M J Rehof and M Das

 Scalable context-sensitive flow analysis using instantiation

 constraints

 Proc SIGPLAN Conference_on Programming_Language Design and

 Implementation (2000) pp 253-263



 ht01

 Heintze N and O Tardieu

 Ultra-fast aliasing analysis using CLA a million lines of C

 code in a second

 Proc of the SIGPLAN Conference_on Programming_Language Design

 and Implementation (2001) pp 254-263



 l05

 Lam M_S J Whaley V B Livshits M C Martin D Avots M Carbin

 and C Unkel Context-sensitive program analysis as database

 queries

 Proc 2005 ACM_Symposium on_Principles of Database Systems

 pp 1-12



 ll05

 Livshits V B and M_S Lam

 Finding security vulnerabilities in Java applications using static

 analysis

 Proc 14th USENIX Security Symposium (2005) pp 271-286



 mr02

 Milanova A A Rountev and B G Ryder

 Parameterized object sensitivity for points-to and side-effect

 analyses for Java

 Proc 2002 ACM SIGSOFT International Symposium_on

 Software Testing and Analysis

 pp 1-11



 rc04

 Rinard M C Cadar D Dumitran D Roy and T Leu

 A dynamic technique for eliminating buffer

 overflow vulnerabilities (and other memory errors)

 Proc 2004 Annual Computer Security Applications Conference

 pp 82-90



 rl04

 Ruwase O and M_S Lam A practical dynamic buffer overflow

 detector Proc 11th Annual Network and Distributed System

 Security Symposium (2004) pp 159-169



 sp81x

 Sharir M and A Pnueli

 Two approaches to interprocedural data flow

 analysis in S Muchnick and N Jones (eds) Program Flow Analysis

 Theory and Applications Chapter 7 pp 189-234 Prentice-Hall

 Upper Saddle River NJ 1981



 steensgaard96

 Steensgaard B Points-to analysis in linear time Twenty-Third ACM

 Symposium_on Principles of Programming_Languages (1996)



 uw02

 Ullman J_D and J Widom

 A First Course in Database Systems

 Prentice-Hall Upper Saddle River NJ 2002



 wl04

 Whaley J and M_S Lam

 Cloning-based context-sensitive_pointer alias_analysis using binary

 decision_diagrams

 Proc_ACM SIGPLAN 2004 Conference

 on Programming_Language Design and Implementation_pp 131-144



 zhu02

 Zhu J Symbolic Pointer_Analysis

 Proc International_Conference in Computer-Aided Design

 (2002) pp 150-157



 enumerate



 For the development of programming_languages that were created and

 in use by 1967 including Fortran Algol Lisp and Simula

 see wexelblat81 For languages that were created by 1982

 including C C Pascal and Smalltalk see bergin96



 The GNU Compiler Collection gcc is a popular source of

 open-source compilers for C C Fortran Java and other

 languages gcc Phoenix is a compiler-construction toolkit

 that provides an integrated framework for building the program

 analysis code_generation and code_optimization phases of

 compilers discussed in this_book phoenix



 For more information_about programming_language concepts we

 recommend ms06rs96 For more on computer

 architecture and how it impacts compiling we suggest

 hp04



 enumerate



 bergin96

 Bergin T J and R G Gibson History of Programming

 Languages ACM Press_New York 1996



 gcc

 httpgccgnuorg



 phoenix

 httpresearchmicrosoftcomphoenixdefaultaspx

 hp04

 Hennessy J_L and D A Patterson

 Computer Organization and Design The HardwareSoftware Interface

 Morgan-Kaufmann San_Francisco CA 2004



 ms06

 Scott M L

 Programming_Language Pragmatics second edition

 Morgan-Kaufmann San_Francisco CA 2006



 rs96

 Sethi R Programming_Languages Concepts and Constructs

 Addison-Wesley 1996



 wexelblat81

 Wexelblat R L History of Programming_Languages Academic

 Press_New York 1981



 enumerate

 This introductory chapter touches on a number of subjects that are

 treated in more_detail in the rest of the book Pointers to the

 literature appear in the chapters containing further material



 The context-free_grammar formalism originated with Chomsky

 chomsky56 as part of a study on natural language The

 idea also was used in the syntax description of two early

 languages Fortran by Backus backus59 and Algol_60 by Naur

 naur60-2 independently introduced a variant while working

 on a draft of Algol_60 The scholar Panini devised an equivalent

 syntactic notation to specify the rules of Sanskrit grammar

 between 400 BC and 200 BC inger67



 Syntax-directed definitions are a form of inductive definition in

 which the induction is on the syntactic_structure As such they

 have long been used informally in mathematics Their application

 to programming_languages came with the use of a grammar to

 structure the Algol_60 report Shortly thereafter

 Irons irons61-2 constructed a syntax-directed compiler



 Recursive-descent parsing has_been used since the early 1960s see

 for example Lucas lucas61 and Hoare hoare62b



 McCarthy mccarthy63 advocated that the translation of a

 language be based_on abstract_syntax In the same

 paper mccarthy63 p 24 he left the reader to convince

 himself that a tail-recursive formulation of the factorial

 function is equivalent to an iterative program



 enumerate



 backus59

 Backus JW The syntax and semantics of the proposed

 international algebraic language of the Zurich-ACM-GAMM

 Conference Proc Intl Conf Information Processing

 UNESCO Paris (1959) pp 125-132



 chomsky56

 Chomsky N Three models for the description of language

 IRE Trans on Information Theory IT-23 (1956)

 pp 113-124



 hoare62b

 Hoare C A R Report on the Elliott Algol translator Computer J 52 (1962) pp 127-129



 inger67

 Ingerman P Z Panini-Backus form suggested Comm_ACM

 103 (March 1967) p 137



 irons61-2

 Irons E_T A syntax directed compiler for Algol_60 Comm_ACM 41 (Jan 1961) pp 51-55



 lucas61

 Lucas P The structure of formula translators Elektronische Rechenanlagen 3 (1961) pp 159-166



 mccarthy63

 McCarthy J Towards a mathematical science of computation

 Information Processing 1962 North-Holland Amsterdam 1963

 pp 21-28



 naur60-2

 Naur P et_al Report on the algorithmic language ALGOL 60

 Comm_ACM 35 (May 1960) pp 299-314

 See also Comm_ACM 61 (Jan 1963) pp 1-17











 enumerate

 Regular_expressions were first developed by Kleene in the

 1950's kleene56

 Kleene was interested in describing the events that could be

 represented_by McCullough

 and Pitts' mp43 finite-automaton model of neural activity

 Since that time regular_expressions and finite_automata

 have become widely used in computer science



 Regular_expressions in various forms were used from the outset

 in many popular Unix utilities such_as

 awk ed egrep grep lex sed

 sh and vi

 The IEEE 1003 and ISOIEC 9945 standards documents for the

 Portable Operating System Interface (POSIX) define the

 POSIX extended regular_expressions which are similar to

 the original Unix regular_expressions with a few exceptions

 such_as mnemonic representations for character classes

 Many scripting languages such_as

 Perl Python and Tcl have adopted regular_expressions

 but often with incompatible extensions



 The familiar finite-automaton model and the minimization of finite

 automata as in Algorithm dfa-min-alg come from Huffman

 huff54 and Moore moore56

 Nondeterministic finite_automata were first proposed by Rabin and Scott

 rs59 the subset_construction of

 Algorithm subset-cons-alg showing the equivalence of

 deterministic and nondeterministic_finite automata is from there



 McNaughton and Yamada my60 first gave an algorithm to convert

 regular_expressions directly to deterministic_finite automata

 Algorithm re-dfa-alg described in

 Section fa-opt-sect was first used by Aho in

 creating the Unix regular-expression matching tool egrep

 This algorithm was also used in the regular-expression

 pattern_matching routines in awk awk

 The approach of using nondeterministic automata as an

 intermediary is due Thompson thom68 The latter paper also

 contains the algorithm for the direct simulation of

 nondeterministic_finite automata

 (Algorithm nfa-sim-alg) which was used by Thompson

 in the text editor QED



 Lesk developed the first version of Lex

 and then Lesk and Schmidt created a second version

 using Algorithm re-dfa-alg lex75

 Many variants of Lex have_been subsequently implemented

 The GNU version Flex can be downloaded

 along with documentation at flex

 Popular Java versions of Lex include

 JFlex jflex and JLex jlex



 The KMP algorithm discussed in the exercises to

 Section token-rec-sect just prior to Exercise kmp-begin-exer

 is from kmp77 Its generalization to many keywords appears

 in ac75 and was used by Aho in the first implementation

 of the Unix utility fgrep



 The theory of finite_automata and regular_expressions is covered in

 hmu01-3 A survey of string-matching techniques is in

 aho90



 enumerate



 aho90

 Aho_A V

 Algorithms for finding patterns in

 strings in

 Handbook of Theoretical Computer_Science (J van Leeuwen ed)

 Vol A Ch 5

 MIT Press Cambridge

 1990



 ac75

 Aho_A V and M J Corasick

 Efficient string matching an aid to bibliographic search

 Comm_ACM 186 (1975) pp 333-340



 awk

 Aho_A V B W Kernighan and P J Weinberger

 The AWK Programming_Language

 Addison-Wesley Boston MA 1988



 flex

 Flex home page httpwwwgnuorgsoftwareflex Free

 Software Foundation



 hmu01-3

 Hopcroft J E R Motwani and J_D Ullman

 Introduction to Automata Theory Languages and Computation

 Addison-Wesley Boston MA 2006



 huff54

 Huffman D A

 The synthesis of sequential machines

 J Franklin Inst 257 (1954) pp 3-4 161 190 275-303



 jflex

 JFlex home page httpjflexde



 jlex

 httpwwwcsprincetonedu appelmodernjavaJLex



 kleene56

 Kleene S_C

 Representation of events in nerve nets in sm56 pp 3-40



 lex75

 Lesk M E Lex - a lexical_analyzer generator Computing

 Science Tech Report 39 Bell_Laboratories Murray_Hill NJ

 1975 A similar document with the same title but with E Schmidt

 as a coauthor appears in Vol 2 of the Unix Programmer's

 Manual Bell laboratories Murray_Hill NJ 1975 see httpdinosaurcompilertoolsnetlexindexhtml



 kmp77

 Knuth D E J H Morris and V R Pratt

 Fast pattern_matching in strings

 SIAM J Computing 62 (1977) pp 323-350



 mp43

 McCullough W S and W Pitts

 A logical calculus of the ideas immanent in nervous activity Bull Math Biophysics 5 (1943) pp 115-133



 my60

 McNaughton R and H Yamada

 Regular_expressions and state graphs for automata IRE Trans

 on Electronic Computers EC-91 (1960) pp 38-47



 moore56

 Moore E F

 Gedanken experiments on sequential machines in sm56

 pp 129-153



 rs59

 Rabin M O and D Scott

 Finite automata and their decision problems

 IBM J Res and Devel 32 (1959) pp 114-125



 sm56

 Shannon C and J McCarthy (eds)

 Automata Studies Princeton Univ Press 1956



 thom68

 Thompson K

 Regular expression search algorithm

 Comm_ACM 116 (1968) pp 419-422



 enumerate

 The context-free_grammar formalism originated with Chomsky

 chomsky56 as part of a study on natural language The

 idea also was used in the syntax description of two early

 languages Fortran by Backus backus59 and Algol_60 by Naur

 naur60-2 The scholar Panini devised an equivalent

 syntactic notation to specify the rules of Sanskrit grammar

 between 400 BC and 200 BC inger67



 The phenomenon of ambiguity was observed first by Cantor

 can62 and Floyd floyd62 Chomsky Normal Form

 (Exercise chomsky-exer) is from chomsky59 The

 theory of context-free_grammars is summarized in hmu01



 Recursive-descent parsing was the method of choice for early

 compilers such_as hoare62b and compiler-writing systems

 such_as META schorre64 and TMG mcclure65 LL

 grammars were_introduced by Lewis and Stearns ls68

 Exercise birman-exer the linear-time simulation of

 recursive-descent is from bu73



 One of the earliest parsing techniques due to

 Floyd floyd63 involved the precedence of operators The

 idea was generalized to parts of the language that do_not involve

 operators by Wirth and Weber ww66 These techniques are

 rarely used today but can be seen as leading in a chain of

 improvements to LR_parsing





 LR_parsers were_introduced by Knuth knuth65 and the

 canonical-LR parsing_tables originated there This_approach was

 not considered practical because the parsing_tables were larger

 than the main memories of typical computers of the day until

 Korenjak korenjak69 gave a method for producing reasonably

 sized parsing_tables for typical programming_languages DeRemer

 developed the LALR deremer69 and SLR deremer71

 methods that are in use today The construction of LR_parsing

 tables for ambiguous_grammars came from aju75 and

 earley75



 Johnson's Yacc very quickly demonstrated the practicality of

 generating parsers with an LALR_parser generator for production

 compilers The manual for the Yacc parser_generator is found

 in yacc The open-source version Bison is

 described in bison A similar LALR-based parser_generator

 called CUP cup supports actions written in Java

 Top-down parser_generators incude Antlr antlr a

 recursive-descent_parser generator that accepts actions in C

 Java or C and LLGen llgen which is an

 LL(1)-based generator



 Dain dain91 gives a bibliography on syntax-error handling



 The general-purpose dynamic-programming parsing algorithm described in

 Exercise cyk-exer was invented independently by J Cocke

 (unpublished) by Younger younger67 and Kasami kasami65

 hence the CYK algorithm There is a more_complex general-purpose

 algorithm due to Earley earley70 that tabulates LR-items for

 each substring of the given input this algorithm while also

 in general is only on unambiguous_grammars



 enumerate



 aju75

 Aho_A V S_C Johnson and J_D Ullman

 Deterministic parsing of ambiguous_grammars

 Comm_ACM 188 (Aug 1975) pp 441-452



 backus59

 Backus JW The syntax and semantics of the proposed

 international algebraic language of the Zurich-ACM-GAMM

 Conference Proc Intl Conf Information Processing

 UNESCO Paris (1959) pp 125-132



 bu73

 Birman A and J_D Ullman

 Parsing algorithms with backtrack

 Information and Control 231 (1973)_pp 1-34



 can62

 Cantor D C On the ambiguity problem of Backus systems

 J_ACM 94 (1962) pp 477-479



 chomsky56

 Chomsky N Three models for the description of language

 IRE Trans on Information Theory IT-23 (1956)

 pp 113-124



 chomsky59

 Chomsky N

 On certain formal properties of grammars

 Information and Control 22 (1959) pp 137-167



 dain91

 Dain J Bibliography on Syntax Error Handling in Language

 Translation Systems 1991 Available from the compcompilers newsgroup see httpcompilersiecccomcomparcharticle91-04-050



 deremer69

 DeRemer F

 Practical Translators for LR() Languages

 PhD thesis MIT Cambridge MA 1969



 deremer71

 DeRemer F

 Simple LR() grammars

 Comm_ACM 147 (July 1971) pp 453-460



 bison

 Donnelly C and R Stallman Bison The YACC-compatible Parser

 Generator httpwwwgnuorgsoftwarebisonmanual



 earley70

 Earley J

 An efficient context-free parsing algorithm

 Comm_ACM 132 (Feb 1970) pp 94-102



 earley75

 Earley J Ambiguity and precedence in syntax description

 Acta Informatica 42 (1975) pp 183-192



 floyd62

 Floyd R W On ambiguity in phrase-structure languages

 Comm_ACM 510 (Oct 1962) pp 526-534



 floyd63

 Floyd R W

 Syntactic analysis and operator precedence

 J_ACM 103 (1963) pp 316-333



 llgen

 Grune D and C J H Jacobs A programmer-friendly LL(1) parser

 generator Software Practice and Experience 181

 (Jan 1988) pp 29-38 See also

 httpwwwcsvunl cerielLLgenhtml



 hoare62b

 Hoare C A R Report on the Elliott Algol translator Computer J 52 (1962) pp 127-129



 hmu01

 Hopcroft J E R Motwani and J_D Ullman Introduction

 to Automata Theory Languages and Computation Addison-Wesley

 Boston MA 2001



 cup

 Hudson S E et_al CUP LALR Parser_Generator in Java

 Available at httpwww2cstumeduprojectscup



 inger67

 Ingerman P Z Panini-Backus form suggested Comm_ACM

 103 (March 1967) p 137



 yacc

 Johnson S_C Yacc - Yet Another Compiler Compiler

 Computing Science Technical Report 32 Bell_Laboratories Murray

 Hill_NJ 1975 Available at httpdinosaurcompilertoolsnetyacc



 kasami65

 Kasami T

 An efficient recognition and syntax analysis algorithm for

 context-free languages AFCRL-65-758 Air Force Cambridge Research

 Laboratory Bedford MA 1965



 knuth65

 Knuth D E

 On the translation of languages from left to right

 Information and Control 86 (1965) pp 607-639



 korenjak69

 Korenjak A J

 A practical method for constructing LR() processors

 Comm_ACM 1211 (Nov 1969) pp 613-623



 ls68

 Lewis P M II and R E Stearns

 Syntax-directed transduction

 J_ACM 153 (1968) pp 465-488



 mcclure65

 McClure R_M

 TMG - a syntax-directed compiler

 Proc 20th ACM Natl Conf (1965) pp 262-274



 naur60-2

 Naur P et_al Report on the algorithmic language ALGOL 60

 Comm_ACM 35 (May 1960) pp 299-314 See also Comm_ACM 61 (Jan 1963) pp 1-17



 antlr

 Parr T ANTLR httpwwwantlrorg



 schorre64

 Schorre D V

 Meta-II a syntax-oriented compiler writing language

 Proc 19th ACM Natl Conf (1964) pp D13-1-D13-11



 ww66

 Wirth N and H Weber

 Euler a generalization of Algol and its formal definition Part I

 Comm_ACM 91 (Jan 1966) pp 13-23



 younger67

 Younger DH

 Recognition and parsing of context-free languages in time

 Information and Control 102 (1967) pp 189-208



 enumerate

 Syntax-directed definitions are a form of inductive definition in

 which the induction is on the syntactic_structure As such they

 have long been used informally in mathematics Their application

 to programming_languages came with the use of a grammar to

 structure the Algol_60 report



 The idea of a parser that calls for semantic_actions can be found in

 Samelson and Bauer sb60 and Brooker and

 Morris bm62 Irons irons61 constructed one of the

 first syntax-directed compilers using synthesized_attributes The

 class of L-attributed definitions comes_from lrs74









 Inherited_attributes dependency_graphs and a test for

 circularity of 's (that is whether or not there is some

 parse_tree with no order in which the attributes can be computed)

 are from Knuth knuth68 Jazayeri Ogden and

 Rounds jor75 showed that testing circularity requires

 exponential time as a function of the size of the



 Parser generators such_as Yacc yacc-5 (see also the

 bibliographic notes in Chapter parse-ch) support attribute

 evaluation during_parsing



 The survey by Paakki paakki95

 is a starting point for accessing the extensive literature on

 syntax-directed_definitions and translations



 enumerate



 bm62

 Brooker R A and D Morris A general translation program for phrase

 structure languages J_ACM 91 (1962) pp 1-10













 irons61

 Irons E_T A syntax directed compiler for Algol_60 Comm_ACM 41 (1961) pp 51-55



 jor75

 Jazayeri M W F Ogden and W C Rounds The intrinsic exponential

 complexity of the circularity problem for attribute grammars

 Comm_ACM 1812 (1975) pp 697-706



 yacc-5

 Johnson S_C Yacc - Yet Another Compiler Compiler

 Computing Science Technical Report 32 Bell_Laboratories Murray

 Hill_NJ 1975 Available at httpdinosaurcompilertoolsnetyacc



 knuth68

 Knuth DE Semantics of context-free languages Mathematical

 Systems Theory 22 (1968) pp 127-145 See also Mathematical Systems Theory 51 (1971) pp 95-96



 lrs74

 Lewis P M II D J Rosenkrantz and R E Stearns Attributed

 translations J Computer and System Sciences 93 (1974)

 pp 279-307



 paakki95

 Paakki J Attribute grammar paradigms - a high-level methodology in

 language implementation Computing Surveys 272 (1995)

 pp 196-255



 sb60

 Samelson K and F L Bauer Sequential formula translation Comm_ACM 32 (1960) pp 76-83



 enumerate

 Most of the techniques in this_chapter stem from the flurry of

 design and implementation activity around Algol_60

 Syntax-directed_translation into intermediate_code was well

 established by the time Pascal wirth71 and

 C johnson79 ritchie79 were created



 UNCOL (for Universal Compiler Oriented Language) is a mythical

 universal intermediate language sought since the mid 1950's

 Given an UNCOL compilers could be constructed by hooking a front

 end for a given source_language with a back end for a given target

 language strong58 The bootstrapping techniques given in

 the report strong58 are routinely used to retarget

 compilers



 The UNCOL ideal of mixing and matching front_ends with back ends

 has_been approached in a number of ways A retargetable compiler

 consists of one front_end that can be put together_with several

 back ends to implement a given language on several machines

 Neliac was an early example of a language with a retargetable

 compiler hhm60 written in its_own language Another

 approach is to retrofit a front_end for a new language onto an

 existing compiler Feldman feldman79b describes the

 addition of a Fortran 77 front_end to the C compilers

 johnson79 and ritchie79 GCC the GNU Compiler

 Collection gcc05 supports front_ends for C C

 Objective-C Fortran Java and Ada



 Value numbers and their implementation by hashing are from

 Ershov ershov58



 The use of type information to improve the security of Java

 bytecodes is described by Gosling gosling95



 Type_inference by using unification to solve sets of equations has

 been rediscovered several_times its application to ML is

 described by Milner milner78 See Pierce pierce02

 for a comprehensive treatment of types



 enumerate



 ershov58

 Ershov A P On programming of arithmetic_operations Comm_ACM 18 (1958) pp 3-6 See also Comm_ACM

 19 (1958) p 16



 feldman79b

 Feldman S I Implementation of a portable Fortran 77 compiler

 using modern tools ACM_SIGPLAN Notices 148 (1979)

 pp 98-106



 gcc05

 GCC home page httpgccgnuorg Free Software

 Foundation



 gosling95

 Gosling J Java intermediate bytecodes Proc_ACM

 SIGPLAN Workshop on Intermediate Representations (1995)

 pp 111-118











 hhm60

 Huskey H D M H Halstead and R McArthur Neliac - a

 dialect of Algol Comm_ACM 38 (1960) pp 463-468



 johnson79

 Johnson S_C A tour through the portable C compiler Bell

 Telephone Laboratories Inc Murray_Hill N J 1979



 milner78

 Milner R A theory of type polymorphism in programming J Computer and System Sciences 173 (1978) pp 348-375



 pierce02

 Pierce B C Types and Programming_Languages MIT Press

 Cambridge Mass 2002











 ritchie79

 Ritchie D M A tour through the UNIX C compiler Bell

 Telephone Laboratories Inc Murray_Hill N J 1979



 strong58

 Strong J J Wegstein A Tritter J Olsztyn O Mock and T

 Steel The problem of programming communication with changing

 machines a proposed solution Comm_ACM 18 (1958)

 pp 12-18 Part 2 19 (1958) pp 9-15 Report of the SHARE

 Ad-Hoc Committee on Universal Languages



 wirth71

 Wirth N The design of a Pascal compiler Software-Practice and Experience 14 (1971) pp 309-333



 enumerate

 In mathematical logic scope rules and parameter_passing by

 substitution date back to Frege frege79 Church's lambda

 calculus church41 uses lexical scope it has_been used as

 a model for studying programming_languages Algol_60 and its

 successors including C and Java use lexical scope Once

 introduced by the initial implementation of Lisp dynamic_scope

 became a feature of the language McCarthy mccarthy81

 gives the history



 Many of the concepts related to stack allocation were stimulated

 by blocks and recursion in Algol_60 The idea of a display for

 accessing nonlocals in a lexically scoped language is due to

 Dijkstra dijkstra60 A detailed description of stack

 allocation the use of a display and dynamic allocation of arrays

 appears in Randell and Russell randell64 Johnson and

 Ritchie johnson81 discuss the design of a calling_sequence

 that allows the number of arguments of a procedure to vary from

 call to call



 Garbage_collection has_been an active area of investigation see

 for example Wilson wilson92 Reference_counting dates_back

 to Collins collins60 Trace-based collection dates_back to

 McCarthy mccarthy60 who describes a mark-sweep algorithm

 for fixed-length cells The boundary-tag for managing free_space

 was designed by Knuth in 1962 and published in knuth68-7



 Algorithm algmod-mark-sweep is based_on

 Baker baker92 Algorithm algcopying-collector is

 based_on Cheney's cheney70 nonrecursive version of

 Fenichel and Yochelson's fenichel69 copying collector



 Incremental reachability analysis is explored by

 Dijkstra et_al dijkstra78 Lieberman and

 Hewitt lieberman83 present a generational collector as an

 extension of copying collection The train_algorithm began with

 Hudson and Moss hudson92



 enumerate



 baker92

 Baker H G Jr The treadmill real-time garbage_collection

 without motion sickness ACM_SIGPLAN Notices 273

 (Mar 1992) pp 66-70



 cheney70

 Cheney C J A nonrecursive list compacting algorithm Comm_ACM 1311 (Nov 1970) pp 677-678



 church41

 Church A The Calculi of Lambda Conversion Annals of

 Math Studies No 6 Princeton University Press Princeton N

 J 1941



 collins60

 Collins G E A method for overlapping and erasure of lists

 Comm_ACM 212 (Dec 1960) pp 655-657



 dijkstra60

 Dijkstra E W Recursive programming Numerische Math

 2 (1960) pp 312-318



 dijkstra78

 Dijkstra E W L Lamport A J Martin C S Scholten and E

 F M Steffens On-the-fly garbage_collection an_exercise in

 cooperation Comm_ACM 2111 (1978)

 pp 966-975



 fenichel69

 Fenichel R_R and J C Yochelson A Lisp garbage-collector

 for virtual-memory computer systems Comm_ACM 1211

 (1969) pp 611-612



 frege79

 Frege G Begriffsschrift a formula language modeled upon

 that of arithmetic for pure thought (1879) In J van

 Heijenoort From Frege to Harvard

 Univ Press Cambridge MA 1967



 hudson92

 Hudson R L and J E B Moss Incremental Collection of

 Mature Objects Proc Intl Workshop on Memory Management

 Lecture Notes In Computer_Science 637 (1992)

 pp 388-403



 johnson81

 Johnson S_C and D M Ritchie The C language calling

 sequence Computing Science Technical Report 102 Bell

 Laboratories_Murray Hill_NJ 1981



 knuth68-7

 Knuth D E Art of Computer Programming Volume 1

 Fundamental Algorithms Addison-Wesley Boston MA 1968



 lieberman83

 Lieberman H and C Hewitt A real-time garbage_collector based

 on the lifetimes of objects Comm_ACM 266 (June

 1983) pp 419-429



 mccarthy60

 McCarthy J Recursive functions of symbolic expressions and

 their computation by machine Comm_ACM 34 (Apr

 1960) pp 184-195



 mccarthy81

 McCarthy J History of Lisp See pp 173-185 in R L

 Wexelblat (ed) History of Programming_Languages Academic

 Press_New York 1981



 minsky63

 Minsky M A LISP garbage_collector algorithm using secondary

 storage A I Memo 58 MIT Project MAC Cambridge MA 1963



 randell64

 Randell B and L J Russell Algol_60 Implementation

 Academic Press_New York 1964



 wilson92

 Wilson P R Uniprocessor garbage_collection

 techniques



 center

 ftpftpcsutexasedupubgarbagebigsurvps

 center



 enumerate

 Many of the techniques covered in this_chapter have their origins

 in the earliest compilers Ershov's labeling algorithm appeared in

 1958 ersh58 Sethi and Ullman su70 used this

 labeling in an algorithm that they prove generated optimal_code

 for arithmetic_expressions Aho and Johnson aj76 used

 dynamic_programming to generate_optimal code for expression trees

 on machines







 Hennessy and Patterson hp03 has a good discussion on the

 evolution of and machine architectures and the

 tradeoffs involved in designing a good instruction set



 RISC architectures became popular after 1990 although their

 origins go back to computers like the CDC 6600 first delivered in

 1964 Many of the computers designed before 1990 were

 machines but most of the general-purpose computers installed

 after 1990 are still machines because they are based_on

 the Intel 80x86 architecture and its descendants such_as the

 Pentium The Burroughs B5000 delivered in 1963 was an early

 stack-based machine



 Many of the heuristics for code_generation

 proposed in this_chapter have_been used in various

 compilers

 Our strategy of allocating a fixed_number of registers

 to hold variables for the duration of a loop was

 used in the implementation of Fortran H by

 Lowry and Medlock lm69



 Efficient register_allocation techniques have also been

 studied from the time of the earliest compilers

 Graph coloring as a register-allocation technique

 was proposed by Cocke Ershov ersh71

 and Schwartz schw73

 Many variants of graph-coloring algorithms have

 been_proposed for register_allocation

 Our treatment of graph coloring follows Chaitin chai81

 chai82

 Chow and Hennessy describe their priority-based

 coloring algorithm for register_allocation in ch90

 See ct04 for a discussion of more_recent

 graph-splitting and rewriting techniques for register_allocation



 Lexical analyzer and parser_generators spurred the

 development of pattern-directed instruction

 selection

 Glanville and Graham gg78 used LR-parser

 generation techniques for automated instruction

 selection

 Table-driven code generators evolved into

 a variety of tree-pattern_matching code-generation tools

 pg88

 Aho Ganapathi and Tjiang agt89

 combined efficient tree-pattern_matching techniques

 with dynamic_programming in the code_generation tool twig

 Fraser Hanson and Proebsting fhp92 further refined

 these ideas in their simple efficient code-generator generator



 enumerate



 aj76

 Aho_A V and S_C Johnson Optimal code_generation for

 expression trees J_ACM 233 pp 488-501



 agt89

 Aho_A V M Ganapathi and S W K Tjiang Code generation

 using tree matching and dynamic_programming ACM Trans

 Programming_Languages and Systems 114 (1989)

 pp 491-516



 chai81

 Chaitin G J M A Auslander A K Chandra

 J Cocke M E Hopkins and P W Markstein

 Register_allocation via coloring

 Computer Languages 61 (1981)_pp 47-57



 chai82

 Chaitin G J

 Register_allocation and spilling via graph coloring

 ACM_SIGPLAN Notices 176 (1982) pp 201-207



 ch90

 Chow F and J_L Hennessy

 The priority-based coloring approach to register_allocation

 ACM Trans Programming_Languages and Systems 124

 (1990) pp 501-536



 ct04

 Cooper K D and L Torczon Engineering a Compiler Morgan

 Kaufmann San_Francisco CA 2004



 ersh58

 Ershov A P

 On programming of arithmetic_operations

 Comm_ACM 18 (1958) pp 3-6

 Also Comm_ACM 19 (1958) p 16



 ersh71

 Ershov A P

 The Alpha Automatic Programming System

 Academic Press_New York 1971



 fl91

 Fischer C N and R J LeBlanc

 Crafting a Compiler with C

 Benjamin-Cummings Redwood City CA 1991



 fhp92

 Fraser C W D R Hanson and T A Proebsting

 Engineering a simple efficient code_generator generator

 ACM Letters on Programming_Languages and Systems

 13 (1992) pp 213-226



 gg78

 Glanville R S and S L Graham A new method for compiler

 code_generation Conf Rec Fifth ACM_Symposium on

 Principles of Programming_Languages (1978) pp 231-240



 hp03

 Hennessy J_L and D A Patterson

 Computer Architecture A Quantitative Approach

 Third Edition

 Morgan Kaufman San_Francisco 2003



 lm69

 Lowry E S and C W Medlock Object code_optimization Comm_ACM 121 (1969) pp 13-22



 pg88

 Pelegri-Llopart E and S L Graham Optimal code_generation

 for expressions trees an application of BURS theory Conf Rec Fifteenth Annual ACM_Symposium on_Principles of

 Programming_Languages (1988) pp 294-308



 schw73

 Schwartz J T

 On Programming An Interim Report on the SETL Project

 Technical Report Courant Institute of Mathematical Sciences

 New_York 1973



 su70

 Sethi R and J_D Ullman The generation of optimal_code for

 arithmetic_expressions J_ACM 174 (1970)

 pp 715-728



 enumerate

 Two early compilers that did extensive code_optimization are

 Alpha ershov66 and Fortran H lm69x The fundamental treatise on

 techniques for loop optimization (eg code motion) is

 allen69 although earlier versions of some of these ideas appear in

 gear65 An informally distributed book cs70 was

 influential in disseminating code-optimization ideas



 The first description of the iterative_algorithm for data-flow_analysis

 is from the unpublished technical report of Vyssotsky and Wegner vw63

 The scientific study of data-flow_analysis is said to begin_with a pair

 of papers by Allen allen70 and Cocke cocke70



 The lattice-theoretic abstraction described_here is based_on the work of

 Kildall

 kildall73 These frameworks assumed distributivity which many

 frameworks do_not satisfy After a number of such frameworks came to

 light

 the monotonicity condition was embedded in the model by

 cc77 and ku77



 Partial-redundancy elimination was pioneered by mr79 The

 lazy-code-motion algorithm described in this_chapter is based_on kr92



 Dominators were first used in the compiler described

 in lm69 However the

 idea dates_back to prosser59



 The notion of reducible_flow

 graphs comes_from allen70

 The structure of these flow_graphs as presented here is from

 hu72 and hu74

 kpt73 and kos74

 first connected reducibility of flow_graphs to the common

 nested control-flow structures which explains why this class of flow_graphs

 is so common



 The definition of reducibility by -

 reduction as used in region-based_analysis is from ullman73

 The region-based approach was first used in a compiler described in

 wulf75



 The static single-assignment (SSA) form of intermediate_representation

 introduced in Section ssa-subsect incorporates both data flow

 and control_flow into its representation SSA facilitates the

 implementation of many optimizing transformations from a

 common framework cytron91



 enumerate



 allen69

 Allen F E Program optimization Annual Review in Automatic

 Programming 5 (1969) pp 239-307



 allen70

 Allen F E Control flow analysis ACM Sigplan Notices

 57 (1970) pp 1-19



 cocke70

 Cocke J

 Global common_subexpression elimination

 ACM_SIGPLAN Notices

 57 (1970) pp 20-24



 cs70

 Cocke J and J T Schwartz Programming_Languages and Their

 Compilers Preliminary Notes Courant Institute of Mathematical

 Sciences New_York Univ New_York 1970



 cc77

 Cousot P and R Cousot Abstract interpretation a unified lattice

 model for static analysis of programs by construction or approximation

 of fixpoints Fourth ACM_Symposium on_Principles of Programming

 Languages (1977) pp 238-252



 cytron91

 Cytron R J Ferrante B K Rosen M N Wegman and F K Zadeck

 Efficiently computing static single assignment form and the

 control dependence graph

 ACM Transactions on Programming_Languages and Systems

 134 (1991) pp 451-490



 ershov66

 Ershov A P Alpha - an automatic programming system of high

 efficiency J_ACM 131 (1966) pp 17-24



 gear65

 Gear C W High speed compilation of efficient object code Comm_ACM 88 (1965) pp 483-488



 hu72

 Hecht M_S and J_D Ullman Flow_graph reducibility SIAM J

 Computing 1 (1972) pp 188-202



 hu74

 Hecht M_S and J_D Ullman Characterizations of reducible_flow

 graphs J_ACM 21 (1974) pp 367-375



 ku77

 Kam J B and J_D Ullman Monotone data flow analysis frameworks

 Acta Informatica 73 (1977) pp 305-318



 kpt73

 Kasami T W W Peterson and N Tokura On the capabilities of

 while repeat and exit statements Comm_ACM 168 (1973)

 pp 503-512



 kildall73

 Kildall G A unified approach to global program optimization ACM_Symposium on_Principles of Programming_Languages (1973)

 pp 194-206



 kr92

 Knoop J Lazy code_motion Proc_ACM SIGPLAN 1992 conference on Programming_Language

 Design and Implementation

 pp 224-234





 kos74

 Kosaraju S R Analysis of structured programs J Computer

 and System Sciences

 93 (1974) pp 232-255



 lm69x

 Lowry E S and C W Medlock

 Object code_optimization

 Comm_ACM 121 (1969) pp 13-22



 mr79

 Morel E and C Renvoise Global optimization by suppression of

 partial redundancies Comm_ACM 22 (1979) pp 96-103



 prosser59

 Prosser R T Application of boolean matrices to the analysis of

 flow diagrams AFIPS Eastern Joint Computer Conference (1959)

 Spartan Books Baltimore MD pp 133-138



 ullman73

 Ullman J_D Fast algorithms for the elimination of common

 subexpressions Acta Informatica 2 (1973)_pp 191-213



 vw63

 Vyssotsky V and P Wegner A graph theoretical Fortran source

 language analyzer unpublished technical report Bell_Laboratories

 Murray_Hill NJ 1963



 wulf75

 Wulf W A R K Johnson C B Weinstock S O Hobbs and C_M

 Geschke The Design of an Optimizing Compiler Elsevier New_York

 1975



 enumerate

 Locality Optimizations

 ch11block



 The performance of a processor be it a part of a multiprocessor or

 not is highly sensitive to its cache behavior Misses in the cache

 can take tens of clock cycles so high cache-miss rates can lead to

 poor processor performance In the context of a multiprocessor with a

 common memory bus contention on the bus can further add to the

 penalty of poor data_locality



 As we_shall see even if we just wish to improve the locality of

 uniprocessors the affine-partitioning algorithm for parallelization

 is useful as a means of identifying opportunities for loop

 transformations In this_section we describe three techniques for

 improving data_locality in uniprocessors and multiprocessors



 enumerate

 We improve the temporal_locality of computed results by trying to use

 the results as_soon as they are generated We do so by dividing a

 computation into independent_partitions and executing all the

 dependent operations in each partition close_together



 Array contraction reduces the dimensions of an array and reduces

 the number of memory_locations accessed We can apply array

 contraction if only one location of the array is used at a given time



 Besides improving temporal_locality of computed results we also need to

 optimize for the spatial_locality of computed results and for both the

 temporal

 and spatial_locality of read-only data Instead of executing each

 partition one after the other we interleave a number of the

 partitions so that reuses among partitions occur close_together

 enumerate



 Temporal Locality of Computed Data



 The affine-partitioning algorithm pulls all the dependent operations

 together by executing these partitions serially we improve temporal

 locality of computed data Let_us return to the multigrid example

 discussed in Section secpsmoo Applying

 Algorithm_algnosync to parallelize the code in

 Fig_figpsmoo1 finds two degrees of parallelism The code in

 Fig figpsmoo2 contains two outer loops that iterate through the

 independent_partitions serially This transformed code has improved

 temporal_locality since computed results are used immediately in the

 same iteration



 Thus even if our goal is to optimize for sequential_execution it is

 profitable to use parallelization to find these related operations and

 place them together The algorithm we use here is similar to that of

 Algorithm_algpar which finds all the granularities of

 parallelism starting_with the outermost_loop As_discussed in

 Section ch11par the algorithm parallelizes

 strongly_connected components individually

 if we cannot find synchronization-free

 parallelism at each level This parallelization

 tends to increase communication

 Thus we combine separately parallelized strongly_connected components

 greedily if they share reuse



 Array Contraction



 The optimization of array_contraction provides another illustration of

 the tradeoff between storage and parallelism which was first introduced in the

 context of instruction-level_parallelism in

 Section secmem-trade-off Just as using more registers allows for

 more instruction-level_parallelism using more memory allows for more

 loop-level parallelism As shown in the multigrid example in

 Section secpsmoo expanding a temporary scalar_variable into an

 array allows different iterations to keep different instances of the

 temporary_variables and to execute at the same time Conversely when

 we have a sequential_execution that operates on one array_element at a

 time serially we can contract the array replace it with a scalar

 and have each iteration use the same_location



 In the transformed multigrid program shown in Fig figpsmoo2

 each iteration of the inner_loop produces and consumes a different

 element of and a row of If these

 arrays are not used outside of the code excerpt the iterations can

 serially reuse the same data storage instead of putting the values in

 different elements and rows respectively Figure figcontract

 shows the result of reducing the dimensionality of the arrays This

 code runs faster_than the original because it reads and writes less

 data Especially in the case when an array is reduced to a scalar

 variable we can allocate the variable to a register and eliminate the

 need to access memory altogether



 figurehtfb



 verbatim

 for_(j 2 j_jl j)

 for_(i 2 i il i)

 AP

 T 10(10 AP)

 D2 TAP

 DW12ji TDW12ji

 for (k3 k kl-1 k)

 AM AP

 AP

 T AP -AMDk-1

 Dk TAP

 DW1kji T(DW1kjiDW1k-1ji)





 for (kkl-1 k2 k-)

 DW1kji DW1kji DkDW1k1ji



 verbatim

 Code of Fig_figpsmoo1 after partitioning

 (Fig figpsmoo2) and array_contraction

 figcontract

 figure



 As less storage is used less parallelism is available Iterations in

 the transformed code in Fig figcontract now share data

 dependences and no_longer can be executed in parallel To parallelize

 the code on processors we can expand each of the scalar_variables

 by a factor of and have each processor access its_own private

 copy Thus the amount by which the storage is expanded is directly

 correlated to the amount of parallelism exploited



 There_are three reasons it is common to find opportunities for array

 contraction



 enumerate



 Higher-level programming_languages for scientific applications such

 as Matlab and Fortran 90 support array-level operations Each

 subexpression of array operations produces a temporary array

 Because the arrays can be large every array operation such_as a

 multiply or add would_require reading and writing many memory

 locations while requiring relatively few arithmetic_operations It is

 important that we reorder operations so that data is consumed

 as it is produced and that we contract these arrays into scalar_variables



 Supercomputers built in the 80's and 90's are all vector machines

 so many scientific applications developed then have_been

 optimized for such machines Even_though vectorizing compilers exist

 many programmers still write their code to operate_on vectors at a

 time The multigrid code example of this_chapter is an example of this

 style



 Opportunities for contraction are also introduced by the compiler As

 illustrated by variable in the multigrid example a compiler would

 expand arrays to improve parallelization We have to contract them

 when the space expansion is not necessary

 enumerate



 ex

 The array expression translates to



 verbatim

 for_(i0 in i) Ti Wi Xi

 for_(i0 in i) Zi Ti Yi

 verbatim

 Rewriting the code as



 verbatim

 for_(i0 in i) T Wi Xi Zi T Yi

 verbatim

 can speed it up considerably

 Of_course at the level of C code we would not even have to use

 the temporary but could write the assignment to as a single

 statement However here we are trying to model the intermediate-code

 level at which a vector processor would deal_with the operations

 ex



 alg

 algcontraction

 Array contraction



 A program transformed by Algorithm_algpar



 An_equivalent program with reduced array dimensions



 A dimension of an array can be contracted to a single element if



 enumerate



 Each independent

 partition uses only one element of the array



 The value of the element upon_entry to

 the partition is not used by the partition and



 The value of the element is

 not live_on exit from the partition



 enumerate



 Identify the contractable dimensions - those that satisfy the three

 conditions above - and replace them with

 a single element

 alg



 Algorithm algcontraction assumes that the program has first

 been transformed by Algorithm_algpar to pull all the dependent

 operations into a partition and execute the partitions sequentially

 It finds those array variables whose elements' live ranges in

 different iterations are disjoint If these variables are not live

 after the loop it contracts the array and has the processor operate

 on the same scalar location After array_contraction it may be

 necessary to selectively expand arrays to accommodate for parallelism

 and other locality optimizations



 The liveness_analysis required here is more_complex than that

 described in Section live-var-subsect If the array is declared

 as a global variable or if it is a parameter interprocedural

 analysis is required to ensure_that the value on exit is not used

 Furthermore we need to compute the liveness of individual array

 elements conservatively treating the array as a scalar would be too

 imprecise



 Partition Interleaving



 Different partitions in a loop often read the same data or read and

 write the same cache_lines In this and the next two sections

 we discuss_how to optimize for

 locality when reuse is found across partitions



 Reuse in Innermost Blocks



 We adopt the simple model that data can be found in the cache if it is

 reused within a small number of iterations If the innermost_loop has

 a large or unknown bound only reuse across iterations of the

 innermost_loop translates_into a locality benefit Blocking creates

 inner_loops with small known bounds allowing reuse within and across

 entire blocks of computation to be_exploited Thus blocking has the

 effect of capitalizing on more dimensions of reuse



 ex

 Consider the matrix-multiply code shown in Fig_mm-basic-fig and

 its blocked_version in Fig block-matrix-fig Matrix multiplication

 has reuse along every dimension of its three-dimensional iteration

 space In the original code the innermost_loop has iterations

 where is unknown and can be large Our simple model assumes that

 only the data reused across iterations in the innermost_loop is found

 in the cache



 In the blocked_version the three innermost_loops execute a

 three-dimensional block of computation with iterations on each

 side The block size is chosen by the compiler to be small enough

 so that all the cache_lines read and written within the block of

 computation fit into the cache Thus reused data across iterations in

 the third outermost_loop can be found in the cache

 ex



 We refer to the innermost set of loops with small known bounds as the

 innermost block It is desirable that the innermost block

 include all the dimensions of the iteration_space that carry reuse if

 possible Maximizing the lengths of each side of the block is not as

 important For the matrix-multiply example 3-dimensional blocking

 reduces the amount of data_accessed for each matrix by a factor of

 If reuse is present it is better to accommodate

 higher-dimensional blocks with shorter sides than lower-dimensional

 blocks with longer sides



 We can optimize locality of the innermost fully_permutable loop

 nest by blocking the subset of loops that share reuse We can

 generalize the notion of blocking to exploit reuses found among

 iterations of outer parallel loops also Observe that blocking

 primarily interleaves the execution of a small number of instances of

 the innermost_loop In matrix_multiplication each instance of the

 innermost_loop computes one element of the array answer there are

 of them Blocking interleaves the execution of a block of

 instances computing iterations from each instance at a time

 Similarly we can interleave iterations in parallel loops to take

 advantage of reuses between them



 We define two primitives below that can reduce the distance between

 reuses across different iterations We apply these primitives

 repeatedly starting from the outermost_loop until all the reuses are

 moved adjacent to each other in the innermost block



 Interleaving Inner Loops in a Parallel Loop



 Consider the case_where an outer parallelizable loop contains an inner

 loop To exploit reuse across iterations of the outer_loop we

 interleave the executions of a fixed_number of instances of the inner

 loop as shown in Fig it-interleaving-fig Creating

 two-dimensional inner blocks this transformation reduces the distance

 between reuse of consecutive_iterations of the outer_loop



 figurehtfb

 verbatim

 for_(i0 in i) for (ii0 iin ii4)

 for_(j0 jn_j) for_(j0 jn_j)

 S for (iii imin(n ii4) i)

 S



 verbatim

 (a) Source program (b) Transformed code

 Interleaving 4 instances of the inner_loop

 it-interleaving-fig

 figure



 The step that turns a loop

 verbatim

 for_(i0 in i)

 S

 verbatim

 into

 verbatim

 for (ii0 iin ii4)

 for (iii imin(n ii4) i)

 S

 verbatim

 is known_as stripmining In the case_where the outer_loop in

 Fig it-interleaving-fig has a small known bound we need not

 stripmine it but can simply permute the two loops in the original

 program



 Interleaving Statements in a Parallel Loop



 Consider the case_where a parallelizable loop contains a sequence of

 statements If some of these statements are

 loops themselves statements from consecutive_iterations may still be

 separated_by many operations We can exploit reuse between iterations

 by again interleaving their executions as shown in

 Fig stat-interleaving-fig This transformation distributes a stripmined loop across the statements Again if the

 outer_loop has a small fixed_number of iterations we need not

 stripmine the loop but simply distribute the original loop over all

 the statements



 figurehtfb

 verbatim

 for_(i0 in i) for (ii0 iin ii4)

 S1 for (iii imin(nii4) i)

 S2 S1

 for (iii imin(nii4) i)

 S2





 verbatim

 (a) Source program (b) Transformed code



 The statement-interleaving transformation

 stat-interleaving-fig

 figure



 We use to denote the execution of statement in

 iteration Instead of the original_sequential execution order

 shown in Fig stripmining-fig(a) the code executes in the order

 shown in Fig stripmining-fig(b)



 figurehtfb





 arrayllll

 s1(0) s2(0) sm(0)



 s1(1) s2(1) sm(1)



 s1(2) s2(2) sm(2)



 s1(3) s2(3) sm(3)



 s1(4) s2(4) sm(4)



 s1(5) s2(5) sm(5)



 s1(6) s2(6) sm(6)



 s1(7) s2(7) sm(7)





 array





 center

 (a) Original order

 center





 arrayllllllll

 s1(0) s1(1) s1(2) s1(3)



 s2(0) s2(1) s2(2) s2(3)







 sm(0) sm(1) sm(2) sm(3)



 s1(4) s1(5) s1(6) s1(7)



 s2(4) s2(5) s2(6) s2(7)







 sm(4) sm(5) sm(6) sm(7)





 array





 center

 (b) Transformed order

 center



 Distributing a stripmined loop

 stripmining-fig



 figure



 ex

 We_now return to the multigrid example and show_how we exploit reuse

 between iterations of outer parallel loops We observe that

 references and

 in the innermost_loops of the code in

 Fig figcontract have rather poor spatial_locality From reuse

 analysis as discussed in Section ch11reuse the loop with

 index carries spatial_locality and the loop with index

 carries group_reuse The loop with index is already the

 innermost_loop so we are_interested in interleaving operations on

 from a block of partitions with consecutive values



 We apply the transform to interleave statements in the loop to obtain

 the code in Fig figblock1 then apply the transform to

 interleave inner_loops to obtain the code in Fig figblock

 Notice_that as we interleave iterations from loop with index

 we need to expand variables into

 arrays that hold results at a time

 ex



 figurehtfb



 verbatim

 for_(j 2 j_jl j)

 for (ii 2 ii il iib)

 for_(i ii_i min(iib-1il)_i)

 ib i-ii1

 APib

 T 10(10 APib)

 D2ib TAPib

 DW12ji TDW12ji



 for_(i ii_i min(iib-1il)_i)

 for (k3 k kl-1 k)

 ib i-ii1

 AM APib

 APib

 T APib-AMDibk-1

 Dibk TAP

 DW1kji T(DW1kjiDW1k-1ji)





 for_(i ii_i min(iib-1il)_i)

 for (kkl-1 k2 k-)

 DW1kji DW1kji DiwkDW1k1ji

 Ends code to be executed by processor (ji)





 verbatim

 Excerpt of Fig_figpsmoo1 after partitioning

 array_contraction and blocking

 figblock1

 figure



 figurehtfb



 verbatim

 for_(j 2 j_jl j)

 for (ii 2 ii il iib)

 for_(i ii_i min(iib-1il)_i)

 ib i-ii1

 APib

 T 10(10 APib)

 D2ib TAPib

 DW12ji TDW12ji



 for (k3 k kl-1 k)

 for_(i ii_i min(iib-1il)_i)

 ib i-ii1

 AM APib

 APib

 T APib-AMDibk-1

 Dibk TAP

 DW1kji T(DW1kjiDW1k-1ji)





 for (kkl-1 k2 k-)

 for_(i ii_i min(iib-1il)_i)

 DW1kji DW1kji DiwkDW1k1ji

 Ends code to be executed by processor (ji)





 verbatim

 Excerpt of Fig_figpsmoo1 after partitioning

 array_contraction blocking and inner-loop interleaving

 figblock

 figure



 Putting it All_Together



 Algorithm alglocality optimizes locality for a uniprocessor

 and Algorithm algintegrated optimizes both parallelism and

 locality for a multiprocessor



 alg

 alglocality

 Optimize data_locality on a uniprocessor



 A program with affine array_accesses



 An_equivalent program that maximizes data_locality



 Do the following steps



 enumerate



 Apply_Algorithm algpar to optimize the temporal_locality

 of computed results



 Apply_Algorithm algcontraction to contract arrays where

 possible



 Determine the iteration subspace that may share the same data or cache

 lines using the technique described in Section ch11reuse For

 each statement identify those outer parallel loop dimensions that

 have data reuse



 For each outer parallel loop carrying reuse move a block of the

 iterations into the innermost block by_applying the interleaving

 primitives repeatedly



 Apply blocking to the subset of dimensions in the innermost fully

 permutable_loop nest that carries reuse



 Block outer fully_permutable loop_nest for higher levels of

 memory hierarchies such_as the third-level cache or the physical

 memory



 Expand scalars and arrays where necessary by the lengths of the blocks

 enumerate

 alg



 alg

 algintegrated

 Optimize parallelism and data_locality for multiprocessors



 A program with affine array_accesses



 An_equivalent program that maximizes parallelism and data_locality



 Do the following



 enumerate



 Use the Algorithm_algpar to parallelize the program and create

 an SPMD program



 Apply

 Algorithm alglocality to the SPMD program produced in Step 1

 to optimize its locality

 enumerate

 alg



 sexer

 Perform array_contraction on the following vector operations



 verbatim

 for_(i0 in i) Ti Ai_Bi

 for_(i0 in i) Di Ti Ci

 verbatim



 sexer



 exer

 Perform array_contraction on the following vector operations



 verbatim

 for_(i0 in i) Ti Ai_Bi

 for_(i0 in i) Si Ci Di

 for_(i0 in i) Ei Ti Si

 verbatim



 exer



 exer

 Stripmine the outer_loop



 verbatim

 for (in-1 i0 i-)

 for_(j0 jn_j)

 verbatim

 into strips of width 10

 exer

















































 Bottom-Up Parsing

 bottom-up-sect



 A bottom-up_parse corresponds to the construction of a parse_tree

 for an input_string beginning at the leaves (the bottom) and

 working up towards the root (the top) It is convenient to

 describe parsing as the process of building parse_trees although

 a parser may in fact carry out a translation instead of building

 an explicit tree The sequence of tree snapshots in

 Fig bottom-up-seq-fig illustrates a bottom-up_parse of the

 token_stream idid with_respect to

 the expression grammar_(expr-gram-display)



 figurehtfb

 center



 A bottom-up_parse for idid

 bottom-up-seq-figcenter

 figure



 This_section introduces a general style of bottom-up_parsing known

 as shift-reduce_parsing The largest class of grammars for which

 shift-reduce parsers can be built the LR_grammars will be

 discussed in Section_lr-parsers-sect Although it is too

 much work to build an LR_parser by hand tools called automatic

 parser_generators make it easy to construct efficient LR_parsers

 from suitable grammars The concepts in this_section are helpful

 for writing suitable grammars to make effective use of an LR

 parser_generator Algorithms for implementing parser_generators

 appear in Section_lr-parsers-sect



 Reductions

 We can think of bottom-up_parsing as the process of reducing a

 string to the start_symbol of the grammar At each

 reduction step a specific substring matching the right_side of a

 production is replaced_by the nonterminal on the left_side of that

 production



 The key decisions during bottom-up_parsing are about when to

 reduce and about what production to apply as the parse proceeds



 exreduction-ex

 The snapshots in Fig bottom-up-seq-fig illustrate a

 sequence of reductions the grammar is the expression grammar

 (expr-gram-display) The reductions will be discussed in

 terms of the sequence of strings

 disp

 id_id F_id T

 id T_F T E

 disp

 The strings in this sequence are formed_by looking only at the

 roots of the completed subtrees in a snapshot The sequence

 starts with the input_string idid

 The first reduction produces Fid by

 reducing the leftmost id to F using the production

 Fid The second reduction produces

 Tid by reducing F to T



 Now we have a choice between reducing the string T which

 is the right_side of ET and the

 string consisting of the second id which is the right_side

 of Fid Rather_than reduce T to

 E the second id is reduced to F resulting in

 the string TF This string then

 reduces to T The parse completes with the reduction of

 T to the start_symbol E

 ex



 By definition a reduction is the reverse of a step in a

 derivation - recall that in a derivation a nonterminal in a

 sentential_form is replaced_by the right_side of one of its

 productions The goal of bottom-up_parsing is therefore to

 construct a derivation in reverse The following derivation

 corresponds to the parse in Fig bottom-up-seq-fig



 disp

 tabularl_c l

 E_T



 T_F



 T id



 F_id



 id_id

 tabular

 disp



 This derivation is in fact a rightmost_derivation



 Handle Pruning



 Bottom-up parsing during a left-to-right_scan of the input

 constructs a rightmost_derivation in reverse Informally a

 handle is a substring that matches the right_side of a

 production and whose reduction represents one step along the

 reverse of a rightmost_derivation



 For_example adding subscripts to the tokens id for clarity

 the handles during the parse of

 according to the expression grammar_(expr-gram-display) are

 as in Fig handle-reductions-fig Although T is the

 right_side of the production ET the

 symbol T is not a handle in the sentential_form

 T If T were indeed replaced_by

 E we would get the string E

 which cannot be derived_from the start_symbol E

 Thus the leftmost substring that matches the right_side of some

 production need not be a handle



 figurehtfb

 center

 tabularrcl



 RIGHTSENTENTIALFORM HANDLE REDUCINGPRODUCTION



 -2pt

 Fid



 F_F TF



 T Fid



 TF TF ETF





 tabular

 Handles during a parse of

 handle-reductions-fig

 center

 figure



 Formally a handle of a right-sentential_form is a

 production and a position of where

 the string may be found and replaced_by to produce the

 previous right-sentential_form in a rightmost_derivation of

 That is if

 as in

 Fig handle-fig then in the position

 following is a handle of The string

 to the right of the handle contains only terminal_symbols For

 convenience we refer to the right_side rather_than

 as a handle Note we say a handle rather

 than the handle because the grammar could be ambiguous with

 more_than one rightmost_derivation of If a

 grammar is unambiguous then every right-sentential_form of the

 grammar has exactly one handle



 figurehtfb

 center



 A handle in the parse_tree for

 handle-fig

 center

 figure



 A rightmost_derivation in reverse can be obtained_by handle

 pruning That is we start with a string of terminals to be

 parsed If is a sentence of the grammar at hand then let

 where is the th right-sentential_form of

 some as yet unknown rightmost_derivation

 disp









 disp



 To reconstruct this derivation in reverse order we locate the

 handle in and replace by the left

 side of the relevant production to

 obtain the previous right-sentential_form Note

 that we do_not yet know how handles are to be found but we_shall

 see methods of doing_so shortly



 We then repeat this process That is we locate the handle

 in and reduce this handle to obtain

 the right-sentential_form If by continuing this

 process we produce a right-sentential_form consisting only of the

 start_symbol S then we halt and announce successful

 completion of parsing The reverse of the sequence of productions

 used in the reductions is a rightmost_derivation for the input

 string



 Shift-Reduce Parsing



 Shift-reduce parsing is a form of bottom-up_parsing in which a

 stack_holds grammar_symbols and an input_buffer holds the rest of

 the string to be_parsed As we_shall see the handle always

 appears at the top of the stack



 We use to mark the bottom of the stack and also the right_end

 of the input Initially the stack is empty and the string is

 on the input as_follows

 disp

 tabularp_p p

 STACK_INPUT





 tabular

 disp

 During a left-to-right_scan of the input_string the parser_shifts

 zero_or more input symbols onto the stack until it is ready to

 reduce a string of grammar_symbols on top of the stack It

 then reduces to the left_side of the appropriate

 production The parser repeats this cycle until it has detected an

 error or until the stack contains the start_symbol and the input

 is empty

 disp

 tabularp_p p

 STACK_INPUT





 tabular

 disp

 Upon entering this configuration the parser halts and announces

 successful_completion of parsing



 Figure shift-reduce-actions-fig steps through the actions a

 shift-reduce_parser might take in parsing the input_string

 according to the expression grammar

 (expr-gram-display)



 figurehtfb

 center

 tabularl r_l

 STACK_INPUT ACTION



 -2pt shift



 reduce by Fid



 F reduce by TF



 T shift



 T shift



 T reduce by Fid



 TF reduce by TTF



 T reduce by ET



 E accept





 tabular

 Configurations of a shift-reduce_parser on input

 shift-reduce-actions-fig

 center

 figure



 While the primary operations are shift and reduce there are

 actually four possible actions a shift-reduce_parser can make (1)

 shift (2) reduce (3) accept and (4) error



 enumerate





 Shift Shift the next_input symbol onto the top of the

 stack





 Reduce The right_end of the string to be reduced must_be

 at the top of the stack Locate the left end of the string within

 the stack and decide with what nonterminal to replace the string





 Accept Announce successful_completion of parsing





 Error Discover a syntax error and call an error_recovery

 routine

 enumerate



 The use of a stack in shift-reduce_parsing is justified by an

 important fact the handle will always eventually appear on top of

 the stack never inside This fact can be shown by considering the

 possible forms of two successive steps in any rightmost

 derivation Figure handle-proof-fig illustrates the two

 possible cases In case (1) is replaced_by and

 then the rightmost nonterminal in the right_side is

 replaced_by In case (2) is again expanded first

 but this time the right_side is a string of terminals only

 The next rightmost nonterminal will be somewhere to the left

 of



 figurehtfb

 center



 Cases for two successive steps of a rightmost_derivation

 handle-proof-figcenter

 figure



 In other_words

 disp

 (1)





 (2)





 disp



 Consider case (1) in reverse where a shift-reduce_parser has just

 reached the configuration

 disp

 tabularp_p p

 STACK_INPUT





 tabular

 disp

 The parser reduces the handle to to reach the

 configuration

 disp

 tabularp_p p



 tabular

 disp

 The parser can now shift the string onto the stack to reach

 the configuration

 disp

 tabularp_p p



 tabular

 disp

 with the handle on top of the stack and it gets

 reduced to



 In case (2) in configuration

 disp

 tabularp_p p



 tabular

 disp

 the handle is on top of the stack After reducing the

 handle to the parser can shift the string to

 get the next handle on top of the stack ready to be reduced

 to

 disp

 tabularp_p p



 tabular

 disp



 In both cases after making a reduction the parser had to shift

 zero_or more symbols to get the next handle onto the stack It

 never had to go into the stack to find the handle



 Items



 How does a shift-reduce_parser know when to shift and when to

 reduce For_example with stack_contents T and next

 input_symbol in Fig shift-reduce-actions-fig how

 does the parser know that T on the top of the stack is not a

 handle so the appropriate action is to shift and not to reduce

 T to E



 An LR_parser makes shift-reduce decisions by maintaining states to

 keep_track of where we are in a parse States will consist of

 sets of items



 An LR(0)_item (item for short) of a grammar is a

 production of with a dot at some position of the right_side

 Thus production XYZ yields the four

 items

 disp















 disp

 The production generates only one item

 An item can be represented_by a pair of

 integers the first giving the number of the production and the

 second the position of the dot



 Intuitively an item indicates how much of a production we have

 seen at a given point in the parsing process For_example the

 first item above indicates that we hope to see a string_derivable

 from XYZ next on the input The second item indicates that

 we have just seen on the input a string_derivable from X and

 that we hope next to see a string_derivable from YZ The

 last item indicates that we have_seen the right_side XYZ and

 that it may be time to reduce XYZ to A



 One collection of sets of LR(0)_items called the canonical

 LR(0)_collection provides the basis for constructing finite

 automata that are used to make parsing_decisions Such automata

 will be called LR(0)_automata The automaton for the

 expression grammar_(expr-gram-display) shown in

 Fig_lr-states-fig will serve as the running_example for

 discussing the canonical_LR(0) collection for a grammar





 figurehtfb

 center



 LR(0)_automaton for the expression grammar_(expr-gram-display)

 lr-states-fig

 center

 figure



 To construct the canonical_LR(0) collection for a grammar we

 define an augmented_grammar and two functions and





 If is a grammar with start_symbol then the

 augmented_grammar for is with a new start_symbol

 and production The purpose of this new

 starting production is to indicate to the parser when it should

 stop parsing and announce acceptance of the input That is

 acceptance occurs_when and only when the parser is about to reduce

 by



 If is a set of items for a grammar then is

 the set of items constructed from by the two rules

 enumerate



 Initially every item in is added to



 If is in and

 is a production then add the item

 to if it is not already

 there We apply this rule until_no more new items can be added to



 enumerate



 Intuitively in

 indicates that at some point in the parsing process we think we

 might next see a substring derivable_from as input The

 substring derivable_from will have a prefix derivable

 from by_applying one of the productions We therefore add

 items for all the productions that is if

 is a production we also include

 in



 exclosure-ex

 Consider the augmented expression grammar

 disp

 tabularl_c l

 E_E



 E_E T_T



 T_T F_F



 F ( E ) id



 tabular

 disp



 If is the set of one item E

 E then contains the items

 disp

 tabularl_c l

 E_E



 E_E T



 E_T



 T_T F



 T_F



 F ( E )



 F_id

 tabular

 disp

 This is the set of items in Fig_lr-states-fig



 Here EE is put in

 by rule (1) Since there is an E immediately to

 the right of a dot by rule_(2) we add the E-productions

 with dots at the left end that is E

 ET and E

 T Now there is a T immediately to the right

 of a dot so we add TT

 F and TF Next the

 F to the right of a dot forces F

 (E) and F

 id to be added No other items are put into

 by rule_(2)

 ex



 The function can be computed as in

 Fig closure-fig A convenient_way to implement the function

 is to keep a boolean array indexed by the

 nonterminals of such that is set to true if

 and when we add the items for each

 -production



 figurehtfb

 tabbing



 function



 set to



 do



 for each item

 in



 for each production of



 if is not in



 add to



 while (items are being added to )



 return





 tabbing

 Computation of closure-fig

 figure



 Note_that if one -production is added to the closure of

 with the dot at the left end then all -productions will be

 similarly added to the closure Hence it is not necessary in some

 circumstances actually to list the items

 added to by A list

 of the nonterminals whose productions were so added will

 suffice We divide all the sets of items of interest into two

 classes

 enumerate



 Kernel items which include the initial item

 and all items whose dots are not at the left

 end



 Nonkernel items which have their dots at the left end

 enumerate

 Moreover each set of items of interest is formed_by taking the

 closure of a set of kernel_items the items added in the closure

 can never be kernel_items of course Thus we can represent the

 sets of items we are really interested in with very little storage

 if we throw away all nonkernel items knowing that they could be

 regenerated by the closure process In Fig_lr-states-fig

 nonkernel items are in the shaded part of the box for a state



 The second useful function is where is

 a set of items and is a grammar symbol is defined

 to be the closure of the set of all items

 such that

 is in Intuitively the function is used to define the

 transitions in the LR(0)_automaton for a grammar The states of

 the automaton correspond to sets of items and

 specifies the transition from the state for under



 exgoto-ex

 If is the set of two items E

 E_EE

 T then contains the items

 tabbing

 E'

 E

 E_E T



 T TF



 T_F



 F (E)



 F_id

 tabbing



 We computed by_examining for items with

 immediately to the right of the dot E

 E is not such an item but

 EET is We moved

 the dot over the to get E

 ET and then took the closure of

 this set

 ex



 We are now_ready for the algorithm to construct

 the canonical_collection of sets of LR(0)_items for an augmented

 grammar - the algorithm is shown in

 Fig set-of-items-fig



 figurehtfb

 tabbing



 procedure



 set to ()



 do



 for each set of items in



 for each grammar symbol



 if is not empty and not in



 add to



 while (items are being added to )







 tabbing

 Computation of set-of-items-fig

 figure



 ex

 The canonical_collection of sets of LR(0)_items for grammar

 (expr-gram-display) and the function are shown in

 Fig_lr-states-fig The function is encoded by the

 transitions in the figure

 ex



 Viable Prefixes and Valid Items



 The central idea in the Simple LR_parsing method SLR

 method for short is to construct from the grammar a deterministic

 automaton called the LR(0)_automaton The states of the automaton

 correspond to sets of items from the canonical_LR(0) collection

 and the transitions are given by the function The LR(0)

 automaton for the expression grammar_(expr-gram-display)

 appears in Fig_lr-states-fig



 The start_state of the automaton corresponds to

 where is the start_symbol of the

 augmented_grammar All states are accepting_states We_say state

 to refer to the state corresponding to the set of items



 How can LR(0)_automata help with shift-reduce decisions Why does

 the approach work We first address parsing_decisions and then

 justify the approach by introducing viable_prefixes and valid

 items



 Shift-reduce decisions can be made as_follows Suppose that the

 string of grammar_symbols takes the LR(0)_automaton from

 the start_state 0 to some state Then with stack_contents

 (read from bottom to top) shift on next_input symbol

 if state has a transition on Otherwise we choose to

 reduce (LR parsers discussed in Section_lr-parsers-sect do

 not rescan the stack_contents for each parsing decision They look

 only at the top of the stack however they hold state numbers

 rather_than grammar_symbols on the stack)



 ex

 Figure viable-fig illustrates the repeated application of

 the automaton in Fig_lr-states-fig to the stack_contents at

 each step of a shift-reduce parse of

 The stack marker is not shown think of it as taking

 the automaton into the start_state 0



 figurehtfb

 center

 tabularr_l c_c l



 stack RESULTING NEXT NEXT



 LINE contents STATE SYMBOL STATE





 (1) 0 5



 (2) 5 none



 (3) F 3 none



 (4) T 2 7



 (5) T 7 5



 (6) T 5 none



 (7) TF 10 none



 (8) T 2 none



 (9) E 1 accept





 tabular

 Recognizing viable_prefixes during a parse of

 viable-fig

 center

 figure



 On line 1 with an empty stack the automaton stays in the start

 state 0 The next_input symbol is and there is a

 transition on id from state 0 to state 5 On line 2

 restarts in state 0 and goes to state 5 on stack_contents

 There_are no transitions out of state 5



 On line 5 with stack_contents T the automaton

 goes from the start_state 0 to 2 on T and from state 2 to 7

 on The next_input symbol is and state_7 has

 a transition on id to state 5 A shift_action is

 appropriate since T is a viable

 prefix

 ex



 Viable Prefixes The LR(0)_automaton for a grammar

 characterizes the strings of grammar_symbols that can appear on

 the stack of a shift-reduce_parser for the grammar The stack

 contents must_be a prefix of a right-sentential_form If the stack

 holds and the rest of the input is then a sequence

 of reductions will take to In terms of

 derivations and is a

 prefix of the right-sentential_form



 Not all prefixes of right-sentential_forms can appear on the

 stack however since the parser must not shift past the handle

 For_example suppose

 disp

 E F_id

 ( E ) id

 disp

 Then at various times during the parse the stack will hold

 ( (E (E) but it must not

 hold (E) since (

 E) is a handle which the parser must reduce to F

 before shifting



 The prefixes of right sentential_forms that can appear on the

 stack of a shift-reduce_parser are called viable_prefixes

 They are defined as_follows a viable_prefix is a prefix of a

 right-sentential_form that does_not continue past the right_end of

 the rightmost handle of that sentential_form By this definition

 it is always possible to add terminal_symbols to the end of a

 viable_prefix to obtain a right-sentential_form



 Valid Items SLR_parsing is based_on the fact that LR(0)

 automata recognize viable_prefixes We_say item

 is valid for a viable_prefix

 if there is a derivation



 In_general an item will be valid for many

 viable_prefixes



 The fact that is valid for

 tells_us a lot about_whether to shift or reduce

 when we find on the parsing stack In_particular

 if then it suggests that we have not_yet

 shifted the handle onto the stack so shift is our move If

 then it looks as if

 is the handle and we should reduce by this production

 Of_course two valid_items may tell_us to do different things for

 the same viable_prefix Some of these conflicts can be_resolved by

 looking_at the next_input symbol and others can be_resolved by

 the methods of Section ambig-gram-sect but we should not

 suppose that all parsing_action conflicts can be_resolved if the

 LR method is applied to an arbitrary grammar



 We can easily compute the set of valid_items for each viable

 prefix that can appear on the stack of an LR_parser In_fact it

 is a central theorem of LR_parsing theory that the set of valid

 items for a viable_prefix is exactly the set of items

 reached from the initial_state along a path labeled in

 the LR(0)_automaton for the grammar In essence the set of valid

 items embodies all the useful information that can be gleaned from

 the stack While we_shall not prove this theorem here we_shall

 give an example



 exvalid-items-ex

 Let_us consider the augmented expression grammar again whose sets

 of items and function are exhibited in

 Fig_lr-states-fig Clearly the string E

 T is a viable_prefix of the grammar The

 automaton of Fig_lr-states-fig will be in state after

 having read ET State

 contains the items

 disp

 tabularl_c l

 T TF



 F (E )



 F_id

 tabular

 disp

 which are precisely the items_valid for E

 T To_see this consider the following three rightmost

 derivations

 disp

 tabularc_c l c_c l c_c l



 E'_E E'_E E'_E





 ET

 ET

 E

 T





 ET

 F E

 TF E

 T

 F





 ET

 (E)

 ETid

 tabular

 disp

 The first derivation shows the validity of T

 TF the second the validity

 of F(E)

 and the third the validity of F

 id It can be shown that there are no other valid_items for

 ET and we leave a proof to the

 interested reader

 ex



 A nondeterministic_finite automaton for recognizing

 viable_prefixes can be constructed by treating the items

 themselves as states There is a transition from

 to

 labeled and there is a transition from

 to labeled

 Then for set of items (states of ) is exactly

 the - of a set of NFA_states defined in Section

 OLD36 Thus gives the transition from on symbol

 in the DFA constructed from by the subset_construction

 Viewed in this way the procedure ) in

 Fig set-of-items-fig is just the subset_construction itself

 applied to the NFA with items as states



 Conflicts During Shift-Reduce Parsing

 There_are context-free_grammars for which shift-reduce_parsing

 cannot be used Every shift-reduce_parser for such a grammar can

 reach a configuration in which the parser knowing the entire

 stack_contents and the next_input symbol cannot decide_whether to

 shift or to reduce (a shiftreduce conflict) or cannot

 decide which of several reductions to make (a reducereduce

 conflict) We_now give some examples of syntactic_constructs that

 give rise to such grammars Technically these grammars are not in

 the LR(k) class of grammars defined in Section

 lr-parsers-sect we refer to them as non-LR grammars The

 k in LR(k) refers to the number of symbols of

 lookahead on the input Grammars used in compiling usually fall in

 the LR(1) class with one symbol lookahead



 exnever-lr-ex

 An ambiguous_grammar can never be LR For_example consider the

 dangling-else_grammar (if-gram-display) of Section

 writing-grammars-sect



 disp

 tabularr_c l

 stmt if expr_then stmt



 if expr_then stmt_else stmt



 other

 tabular

 disp

 If we have a shift-reduce_parser in configuration

 disp

 tabularp_p p

 STACK_INPUT



 if expr_then stmt

 else

 tabular

 disp

 we cannot_tell whether if expr_then stmt

 is the handle no_matter what appears below it on the stack Here

 there is a shiftreduce_conflict Depending on what_follows the

 else on the input it might be correct to reduce if

 expr_then stmt to stmt or it might be

 correct to shift else and then to look for another

 stmt to complete the alternative if expr_then

 stmt_else stmt Thus we cannot_tell whether to

 shift or reduce in this case so the grammar is not LR(1) More

 generally no ambiguous_grammar as this one certainly is can be

 LR(k) for any k



 We should mention however that shift-reduce_parsing can be

 easily adapted to parse certain ambiguous_grammars such_as the

 if-then-else grammar above When we construct such a parser for a

 grammar containing the two productions above there will be a

 shiftreduce_conflict on else either shift or reduce by

 stmt if expr_then

 stmt If we resolve the conflict in favor of shifting the parser

 will behave naturally We discuss parsers for such ambiguous

 grammars in Section ambig-gram-sect

 ex



 Another common cause of non-LR-ness occurs_when we know we have a

 handle but the stack_contents and the next_input symbol are not

 sufficient to determine which production should be used in a

 reduction The next example_illustrates this situation



 exproc-array-ex

 Suppose we have a lexical_analyzer that returns token_id for

 all identifiers regardless of usage Suppose also that our

 language invokes procedures by giving their names with parameters

 surrounded_by parentheses and that arrays are referenced by the

 same syntax Since the translation of indices in array references

 and parameters in procedure_calls are different we_want to use

 different productions to generate lists of actual_parameters and

 indices Our grammar might therefore have (among others)

 productions such_as

 disp

 tabularr_r c_l

 (1) stmt id ( parameterlist )



 (2) stmt expr_expr



 (3) parameterlist parameterlist parameter



 (4) parameterlist parameter



 (5) parameter id



 (6) expr id ( exprlist )



 (7) expr id



 (8) exprlist exprlist_expr



 (9) exprlist_expr

 tabular

 disp



 A statement beginning with a(ij) would appear as the token

 stream id(id

 id) to the parser After shifting the first three

 tokens onto the stack a shift-reduce_parser would be in

 configuration



 disp

 tabularp_p p

 STACK_INPUT



 id ( id_id

 )

 tabular

 disp

 It is evident that the id on top of the stack must_be

 reduced but by which production The correct choice is production

 (5) if a is a procedure and production (7) if a is an

 array The stack does_not tell which information in the symbol

 table obtained from the declaration of a has to be used



 One_solution is to change the token_id in production (1) to

 procid and to use a more sophisticated lexical_analyzer that

 returns token procid when it recognizes an_identifier which

 is the name of a procedure Doing this would_require the lexical

 analyzer to consult the symbol_table before returning a token



 If we made this modification then on processing a(ij) the

 parser would be either in the configuration



 disp

 tabularp_p p

 STACK_INPUT



 procid ( id_id

 )

 tabular

 disp

 or in the configuration above In the former_case we choose

 reduction by production (5) in the latter case by production (7)

 Notice how the symbol third from the top of the stack determines

 the reduction to be made even_though it is not involved in the

 reduction Shift-reduce parsing can utilize information far down

 in the stack to guide the parse

 ex

 gcd

 Deciding if Accesses Touch a CommonElement

 Deciding if Accesses Touch a Common Element

 array-bounds-sect



 Our_goal is to determine given two accesses

 to the same array whether they ever touch a

 common element Deciding this question requires us to determine which

 elements of the array each access touches To decide this question we

 need to combine the bounds on the indexes of the loop_nest with the

 affine access functions that describe each access It is the purpose of

 this_section to develop the mechanics for this test

 Note_that the problem studied here is more_complex than what we did in

 Section ch11index There we looked only at the axes along which

 reuse occurs here we must examine the details of whether elements in

 common exist along those axes



 We start by asking whether two accesses have intersecting regions where

 they might access the same element(s) of an array In this analysis we

 are never certain that there are particular elements that are touched_by

 both accesses This analysis is sufficient for some optimizations For

 instance we may use it to select an ordering for the loops in a loop

 nest with the intent of maximizing reuse of a cache_line In this

 case we are happy if two accesses touch data in common because data

 reuse saves running_time If some

 expected reuse does_not actually occur there is no harm there

 was no better choice of loop ordering anyway



 However for some optimizations like parallelizing code or reordering

 accesses we need to

 be certain two accesses touch no data in common

 This sort of optimization requires more aggressive analysis of the

 elements actually touched

 Because array_elements have integer

 indexes and polyhedra have surfaces that may cut between integer grid

 points it is possible that no integer grid points lie in the intersection of

 two polyhedra even_though they have a nonempty intersection in the

 reals In_addition some access functions like

 Example bad-access-ex have holes in the set of array_elements

 that they access ie they access some but not all of the array

 elements that lie within the bounding polyhedron



 In Section fm-elim-subsect we introduce the

 Fourier-Motzkin_elimination algorithm to manipulate the bounding

 inequalities for a loop_nest

 This algorithm gives_us the best bounding polyhedron for the accesses

 that possibly occur during the execution of a loop_nest

 Then in Section data-dep-subsect we begin the study of finding

 the exact set of array_elements accessed in common by a pair of array

 accesses This problem is equivalent to linear-integer programming and

 is not tractable However in many common cases we can solve the

 problem with

 techniques for solving linear

 Diophantine (integer solutions only) equations



 Bounding the Array Regions Accessed

 fm-elim-subsect



 To_see the problem consider the loop_nest and array_access of

 Fig holes-fig

 In this code the array_elements

 accessed are for and no others

 The holes are caused by the nonunit

 coefficients in the affine functions a phenomenon that fortunately does

 not occur too often in

 practice



 figurehtfb



 verbatim

 for_(i 0 i 1 i)

 for_(j 0 j 3 j)

 A3i4j 0





 verbatim



 An array_access that causes holes

 holes-fig



 figure



 Instead of trying to find the exact set of array_elements accessed it

 is usually sufficient to find tight bounds on the region of the array

 accessed

 In the example above we see that is accessed only if and

 only if

 These bounds on the array index are really projections of the

 inequalities that define the polyhedron of the loop_indexes namely

 and



 Formally an array_access in a particular loop_nest can be described by

 the quadruple

 where B and b are the matrix and vector

 describing the loop_bounds as in Section matrix-bounds-subsect

 and F and f are the matrix and vector describing the access

 itself as in Section access-function-subsect As usual let i represent the vector of loop_indexes and as a new notation let a represent the vector of array indexes

 Assume i has dimension - that is the depth of the loop_nest is

 - and assume a has dimension that is the array's

 dimension is

 Then the set of the array_elements accessed is

 given by



 equation

 eqregion

 a in Ze there_exists i in Zd

 such that Bi b 0 and

 a Fi f



 equation

 We are_interested in finding the convex hull for the set of

 elements a that are

 accessed the convex hull of a set of points is the smallest

 convex_polyhedron that bounds all the points in the set



 The constraints and

 define a -dimensional polyhedron whose dimensions

 correspond to the components of the vectors

 i and a The convex hull of the array_elements accessed

 is the projection of the -dimensional polyhedron

 onto the -dimensional a variable space The projection of a

 polyhedron on a lower-dimensional space is intuitively the shadow cast

 by the object onto that space For_example when we project a

 3-dimensional object along the axis onto a 2-dimensional and

 plane we eliminate variable losing the height of the individual

 points and simply record the 2-dimensional footprint of the object in

 the - plane



 More_generally let be a -dimensional

 polyhedron Then the projection of onto the first of its

 dimensions is the set of points such that for

 some vector is in

 Projection of linear_inequalities can be computed using

 Fourier-Motzkin_elimination as_follows



 alg

 fm-elim-alg

 Fourier-Motzkin Elimination



 A polyhedron with variables

 That is is a set of linear constraints involving the variables



 One given variable is specified to be

 the variable to be_eliminated



 A polyhedron with variables

 (ie all the variables of except for ) that is the projection

 of onto dimensions other_than the th



 Let be all the constraints in involving

 Do the following



 enumerate



 add-constraints-item

 For every pair of a lower_bound and an upper_bound on in such_as



 arrayrl

 L c1 xm



 c2 xm U

 array



 create the new constraint



 c2L c1U



 Note_that and are integers but and may be

 expressions with variables other_than



 If integers and have a common factor divide both

 sides by that factor



 If the new constraint is not

 satisfiable then there is no solution to ie the polyhedra

 and are both empty spaces



 is the set of constraints plus all the constraints

 generated in Step_2



 enumerate

 Note incidentally that if has lower

 bounds and upper_bounds eliminating produces up to

 inequalities but no more

 alg



 The constraints added in Step (add-constraints-item)

 of Algorithm_fm-elim-alg

 correspond to the implications of

 constraints on the remaining variables in the system Therefore

 there is a solution in if and only if there_exists at_least one

 corresponding solution in Given a solution in the range on

 the corresponding can be found by_replacing all variables but

 in the constraints by their actual values



 To_find the region of the array accessed we simply eliminate all the loop_index

 variables from the system of constraints in Equation eqregion

 That is



 enumerate



 Replace all equalities by equivalent pairs of inequalities That is

 must_be replaced_by and



 For each loop_index variable in some order

 apply_Algorithm fm-elim-alg to

 eliminate that variable



 enumerate



 ex

 fm1-ex

 Let_us revisit the loop_nest



 verbatim

 for_(i 0 i 5 i)

 for_(j i_j 7 j)

 verbatim

 of Example ex2d whose polyhedron was shown in

 Fig loop-nest-fig However let_us consider a different array

 access Aii1

 As we saw in Example ex2d the loop-index bounds which we

 represent formally by the matrix-vector inequality

 is the set of four inequalities



 center



 center

 If we let the vector a be then the equation

 is





 arrayr

 x



 y



 array







 arrayrr

 1_0



 1_0



 array





 arrayr

 i



 j



 array







 arrayr

 0



 1



 array





 That is we have the equalities and which we_shall write

 as four equivalent inequalities



 center



 center



 We need to eliminate and to get constraints on and the

 coordinates of the array that are actually touched_by the array

 access Aii1 It is easier to eliminate first since there

 are only two constraints - one lower and one upper - that involve

 namely and Combining these we get

 We are now left with three lower_bounds on



 center



 and

 center

 and four upper_bounds on



 center



 and

 center

 The table of Fig 3-4-ineq-fig shows the

 consequences of pairing each lower_bound with each upper_bound



 figurehtfb



 center

 Lower Bound

 center

 center

 tabularl lccc









 Upper



 Bound







 tabular

 center



 Consequences of pairs of inequalities in Fourier-Motzkin

 elimination

 3-4-ineq-fig



 figure



 Some of the twelve consequents in Fig 3-4-ineq-fig are trivial

 and need not be preserved in the constraints on Examples

 include and Others can be_eliminated because they

 follow from others in the set for instance we can drop because

 is a stronger constraint The pair of inequalities

 and can be_combined into the equality

 Then given this equality we find that and are really

 the same inequality and one can be_eliminated The same holds for

 and Thus the entire set of constraints on the vector

 are



 center

 and

 center

 Put_another way the set of elements of array that are touched are



 ex



 ex

 fm2-ex

 Let_us consider

 the references in Example exaccesses supposing that all

 are accesses in

 a loop whose bounds are and

 We_shall represent the

 elements of the array accessed by a vector of indexes

 whose dimension depends_on the array

 The bounds are given in Fig figarraybounds



 figurehtb



 center

 tabularlcc

 Access Affine Expression

 Region Accessed







 Xi-1

















































 Yij



























































 Yjj1



























































 Y12



























































 Z1i2ij





























































 tabular

 center



 Bounds on array regions accessed by references in Example exaccesses

 figarraybounds

 figure



 The first access Xi-1 is easily seen to touch elements

 through and the second access accesses all elements in the

 square with corner The third access Yjj1 is quite similar to Example fm1-ex but with the indexes

 and interchanged The analysis is quite similar to what we saw

 there and the bounds are similar as_well The fourth access Y12 is again quite easy and tells_us is the only element

 accessed



 The final access Z1i2ij is the most complex However we

 can write down the inequalities and equalities as_follows The

 inequalities that come from the loop_bounds are



 center



 center

 The equalities that come from setting the vector equal

 to the affine_expression describing the

 access (shown in the second column of Fig figarraybounds)

 are



 center



 center

 Clearly is both the upper and lower_bound on That is

 all elements of accessed have first index 1

 Since and the upper and lower_bounds on

 are surely 1 and

 To obtain the bounds on we can start by_replacing by

 since That step eliminates as far as

 is concerned The best we can do to eliminate is to remember that

 Thus is replaced_by



 ex



 Solution Enumeration



 Projection can also be used to enumerate the points in a polyhedron

 As an example a sequential loop_nest is an enumeration of all the

 points in the iteration_space The bounds for each loop are expressed

 in terms of the values of the indexes for loops that surround the loop

 in question ie the outer loops Such bounds are said to be in triangular form



 alg

 algenumerate

 Putting loop_bounds in triangular form



 A convex_polyhedron over variables



 A lower_bound and an upper_bound for each

 expressed only in terms of the 's for



 The algorithm is described in Fig enumerate-fig

 alg



 figurehtfb



 center

 tabularl



 Use_Algorithm fm-elim-alg to find the bounds



 for (_)



 all the lower_bounds on in



 all the upper_bounds on in



 Use_Algorithm fm-elim-alg to eliminate from



 the constraints



 Call the resulting set of constraints







 Remove redundancies







 for (_)



 Remove any term in and implied by



 Add the remaining constraints of and on to







 tabular

 center



 Putting bounds in triangular form

 enumerate-fig



 figure



 ex

 Each of the accessed regions in Fig figarraybounds are

 in triangular form However these bounds are on the indexes of the

 array_elements rather_than the loop_indexes Note_that

 Algorithm_algenumerate applies

 to inequalities from any source

 ex



 Data_Dependence

 data-dep-subsect



 Parallelization or locality optimizations reorder the operations

 executed in the original_program As with all optimizations

 operations can be reordered only if the reordering does_not change the

 program's output

 We_say two operations are dependent if the external effect of the

 program depends_on the order in which these operations are executed

 In imperative programming where operations modify the memory

 state we can easily determine if operations are dependent by

 examining the memory_locations modified That is we run the operations

 in both orders and observe whether the net effect on any memory_location

 differs If so the operations are dependent if not they are

 independent(As always in code_optimization we are being

 conservative Even_though a memory_location has changed it is

 conceivable that this change has no observable effect on the output of

 the program However by assuming that any change to a memory_location

 leads to change in the output we have a simple test that avoids all

 program modifications that do change the output)



 In the present study we focus_on operations that are reads and writes

 of array_elements so these elements are the memory_locations of

 concern

 Two accesses whether read or write are

 clearly independent if they refer to two different locations In

 addition read operations do_not change the memory state therefore

 are also independent In other_words we say that two accesses

 are data_dependent if they refer to the same memory_location and

 one of them is a write operation To be_sure that the modified program

 does the same as the original the relative_execution ordering

 between every pair of data-dependent operations in the original

 program must_be preserved in the new program



 Recall from Section dd-3kinds-subsect that

 there are three flavors of data_dependence



 enumerate



 True dependence where a write is followed_by a read

 of the same_location



 Antidependence where a read is followed_by a write

 to the same_location



 Output dependence which is two writes to the same_location



 enumerate



 In the discussion_above data_dependence is defined for dynamic

 accesses

 We_say that a static_access in a

 program depends_on another as_long as there_exists a dynamic_instance of the

 first access that depends_on some instance of the second(Recall

 the difference_between static and dynamic_accesses A static_access is

 an array reference at a particular location in a program while a

 dynamic access is one execution of that reference)



 It is easy to see_how data_dependence can be used in

 parallelization For_example if no data_dependences are found in the

 accesses of a loop we can easily assign each iteration to a

 different processor Section ch11affine discusses_how we use

 this information systematically in parallelization



 Definition of Data_Dependence of Array Accesses



 Let_us consider

 two static accesses to the same array

 The first is represented_by access

 function and bounds



 and is in a -deep_loop nest

 the second is represented_by



 and is in a -deep_loop nest

 These accesses are data_dependent if



 enumerate



 At least_one of them is a

 write_reference and



 There exist vectors i in and in such

 that

 enumerate



 and



 enumerate



 enumerate



 Since a static_access normally embodies many dynamic_accesses it is

 also meaningful to ask if its dynamic_accesses may refer to the same

 memory_location To search for dependencies between instances of the

 same static_access we assume and

 augment the definition above with the additional_constraint that

 to rule out the trivial_solution



 Data_Dependence Versus Data Reuse

 We might superficially think that the question of testing data

 dependence is the same as discovering self-reuse and group-reuse as was

 discussed starting in Section ch11reuse It is true that

 instances of data_dependence imply reuse However there are several

 significant extensions required if we are to analyze data_dependence

 These are



 enumerate



 Data_dependence exists between dynamic_accesses not static accesses



 To_find instances of data_dependence we need to consider not only the

 access function but the loop_bounds



 To_find instances of data_dependence we need to worry_about holes

 and the particular array_elements that are accessed not just the bounds

 on the indexes of the array within which accesses occur



 enumerate



 ex

 Consider the following 1-deep loop_nest



 verbatim

 for_(i 1_i 10_i)

 Ai Ai-1



 verbatim

 This loop has two accesses Ai-1 and Ai the first is

 a read reference and the second a write To_find all the data

 dependences in this program we need to check if the write_reference

 shares a dependence with itself and with the read reference



 enumerate



 Data_dependence between Ai-1 and Ai Except for the

 first iteration each iteration reads the value written in the

 previous iteration Mathematically we know that there is a

 dependence because there exist_integers and such that



 center

 and

 center

 There_are nine solutions to the above system of

 constraints and so forth



 Data_dependence between Ai and Ai itself It is easy

 to see that different iterations in the loop write to different

 locations that is there are no data dependencies among the instances

 of the write_reference Ai

 Mathematically we know that there does_not exist a

 dependence because there do_not exist_integers and satisfying



 center

 and

 center

 Notice_that the third condition comes_from the requirement

 that the dependence be nontrivial - between different dynamic

 accesses The contradictory fourth condition comes_from the

 requirement that and are the same memory_location



 enumerate

 It is not necessary to consider data_dependences between the

 read reference Ai-1 and itself because any two read accesses

 are independent

 ex



 Linear Integer Programming



 Data_dependence requires finding whether there exist_integers that satisfy a

 system consisting of equalities and inequalities

 Equation eqregion expressed the equalities in terms of matrix

 F and vector f while the inequalities were represented_by

 matrix B and vector b However as we observed previously

 an equality can be replaced_by two inequalities and





 Thus data_dependence may be phrased as a search for

 integer_solutions that satisfy

 a set of linear_inequalities which is precisely the well-known

 problem of integer_linear programming Integer linear_programming is

 an NP-complete problem While no polynomial algorithm is known

 heuristics have_been developed to solve linear programs involving many

 variables and can be quite fast in many_cases Moreover the instances

 of linear integer programming we need to solve are quite specialized and

 relatively_simple in most cases The strategy we propose for testing

 the existence of integer_solutions to a set of equalities and

 inequalities can be outlined as_follows



 enumerate



 First considering only the equalities the portion

 in Equation (eqregion determine using the theory of

 linear_Diophantine equations whether there are any integer_solutions to

 the equalities We discuss Diophantine_equations in

 Section dioph-subsect If there are no integer_solutions to the

 equalities we are done Otherwise we use the equalities to substitute

 for some of the variables and thereby simplify the inequalities



 Use_Algorithm fm-elim-alg Fourier-Motzkin_elimination to

 determine_whether there is a nonempty polyhedron defined by the

 linear_inequalities If not then there are surely no integer

 solutions to the inequalities



 Determine if there are integer_solutions to the system of

 inequalities by a method to be discussed in Section rat-sol-subsect



 enumerate



 Linear Diophantine Equations

 dioph-subsect



 The first step in the algorithm outlined above is to check for the

 existence of integer_solutions to the equalities

 Equations with the stipulation that solutions must_be integers

 are known_as Diophantine_equations

 The following example shows_how the issue of integer_solutions arises



 ex

 exsimplegcd

 Consider the following code_fragment



 verbatim

 for_(i 1_i 10_i)

 A2i A2i1



 verbatim

 The write access A2i

 only touches even elements of while the read access A2i1 touches only odd elements

 This example is not as contrived as it may look

 An_example where even and odd numbers are treated differently is

 an array of complex numbers where the real and imaginary components

 are laid_out side by side



 If we simply do a bounds analysis on the two accesses as in

 Section fm-elim-subsect we find that the bounds on the write

 access are through while the bounds on the read access

 are through It appears as if the accesses do touch

 elements in common and thus are dependent accesses



 However when we try to solve for instances of the two accesses that

 actually touch the same element we find that we have a Diophantine

 equation with no solution Suppose there were integers and in

 the range 1 through 9 such that and are the same

 array_element Then



 2i 2i' 1



 There_are no integers and

 that can satisfy the above equation

 The proof is that if is an_integer then is even If is

 an_integer then is even so is odd

 No even number is also an odd number Therefore the equation has no

 integer_solutions and there is actually no dependence_between the read

 and write accesses

 ex



 In order to describe when there is a solution to a linear_Diophantine

 equation we need the concept of the greatest common divisor

 (GCD) of two or_more integers The GCD of integers

 denoted

 is the largest integer that evenly_divides all these integers

 GCD's can be computed efficiently by the well-known Euclidean

 algorithm (see the box on the subject)



 ex

 because and

 each have remainder 0 yet any integer larger_than 6 must leave a

 nonzero remainder when dividing at_least one of 24 36 and 54 For

 instance 12 divides 24 and 36 evenly but not 54

 ex



 The importance of the GCD is in the following theorem



 th

 dioph-th

 The linear_Diophantine equation



 a1 x1 a2 x2 an xn c



 has an_integer solution for if and only if

 divides

 th



 ex

 gcd-ex

 We observed in Example exsimplegcd that the linear

 Diophantine_equation has no solution We can write this

 equation as



 2i -2i' 1



 Now and 2 does_not divide 1 evenly Thus there is no

 solution



 For another example consider



 24x36y54z30



 Since and there is a solution in integers

 for and

 One_solution is and but there is an infinity of

 other solutions

 ex





 The Euclidean Algorithm

 The Euclidean_algorithm for finding works as_follows

 First assume that and are positive integers and Note

 that the GCD of negative numbers or the GCD of a negative and a

 positive number is the same as the GCD of their absolute values so we

 can assume all integers are positive



 If

 then If let be the remainder of If

 then evenly_divides so Otherwise

 compute this result will also be



 To_compute for use the Euclidean

 algorithm to compute

 Then recursively compute





 Rational Solutions to Linear Programs

 rat-sol-subsect



 The second step of our algorithm for detecting data_dependences is to

 solve the inequalities without the constraint that the variables have

 integer values This problem is the classical linear-programming

 problem

 Two accesses do_not have a data_dependence if the regions of data

 accessed do_not overlap Let_us first show an example to illustrate

 this case



 ex

 Consider the following simple loop



 verbatim

 for_(i 1_i 10_i)

 Ai Ai10



 verbatim

 The elements_touched by Ai are while the

 elements_touched

 by Ai10 are The ranges do_not

 overlap and therefore there are no data_dependences

 More formally we need to show that there are no two dynamic_accesses

 and with and If

 there were such integers and then we could substitute

 for and get the four constraints on and

 However implies which contradicts

 Thus no such integers and exist

 ex



 In_general

 we use use the Fourier-Motzkin_elimination algorithm to determine if

 there is a rational (not-necessarily-integer) solution to a set of

 inequalities

 More_specifically use

 Algorithm_algenumerate to eliminate

 one variable from the system at a time until either the reduced answer

 has no solution or until all the variables are eliminated



 Note_that Algorithm_algenumerate creates upper and lower_bounds

 for each variable that depend only on variables that are eliminated

 later It is then possible to back substitute and get integer bounds for

 each variable

 That is since the last variable to be_eliminated can have bounds that

 depend_on no variable it already has integer upper and lower_bounds

 The penultimate variable to be_eliminated can have bounds that depend

 only on the last variable and its bounds are integers so we can

 substitute for the last variable to get integer bounds on the

 penultimate variable Then the third-from-last variable's bounds which

 depend only on the last two variables can have its bounds turned_into

 integers and so on



 ex

 Let_us look_at the last set of bounds in Fig figarraybounds

 which were (both a lower and upper bound) and

 Recall that represents a constant in this

 example but let_us substitute 100 for to give us the triangular

 set of inequalities



 center

 tabularl_l













 tabular

 center



 As promised the bounds on involve only integers

 It happens that the bounds on involve only integers although they

 might have involved However the bounds on do indeed

 involve lower numbered variables in this case only although

 might also have appeared Since and we already

 have determined that 1 is a lower_bound on the integer lower

 bound_on is or Similarly the upper

 bound_on is 1 plus twice the upper_bound on or



 ex



 Integer Programming

 int-prog-subsect



 Finding a rational_solution to a set of inequalities does_not imply

 that there_exists an_integer solution Thus the third_step in cases

 where there are rational solutions to the linear-programming problem is

 to check_whether any integer_solutions exist The next example both

 illustrates the essence of the problem and suggests what the general

 rule is



 ex

 fm-integer-ex

 Suppose we have determined the following bounds on variable



 arrayrl

 7 2i



 3i 11

 array



 It is easy to see that that is

 although there are rational solutions like there

 is no integer_solution Applying Fourier-Motzkin to eliminate the

 variable reduces the system to



 arrayrl

 21 22

 array



 which obviously is satisfiable proving that there_exists a rational

 solution The Fourier-Motzkin_elimination drops the fact that we are

 seeking a value between 21 and 22 that is a multiple of 6



 Put_another way if has to be an_integer we may as_well raise 7 in

 the lower_bound to the next higher multiple of 2 that is

 Likewise we may as_well lower 11 in the upper_bound

 to the next lower multiple of 3 that is

 If we try to eliminate from the resulting

 inequalities



 arrayrl

 8 2i



 3i 9

 array



 we get for which we know no solution exists

 ex



 It is relatively rare that a set of inequalities due to loop_bounds

 has rational solutions but no integer_solutions It is thus useful to

 find a way to know for certain that an_integer solution already

 exists

 In_general the Fourier-Motzkin_elimination step involves

 a lower_bound and an upper_bound on some variable such_as



 arrayrl

 l cl v



 cu v u



 array



 Each of and are integers and we_want to know not

 only if some rational exists (which is true if and only if

 ) but whether an_integer value of exists

 As in Example fm-integer-ex we can replace the lower_bound by

 and replace the upper_bound by

 The justification is that if is an_integer we lose

 no solutions by raising to the next multiple of (if is not

 already a multiple of ) or by

 lowering to the next multiple of (if is not already such a

 multiple)



 Thus we may change the inequalities to



 arrayrl

 cllcl cl v



 cu v cuucu



 array



 These have an_integer solution if they have any solution at all And

 they have a solution if and only if the inequality that we get by

 eliminating has a solution that is



 lclucu



 Putting it All_Together



 We_now have all the tools we need to determine in the general case

 whether or not there are integer_solutions to a set of equalities and

 inequalities In_particular we put these tools together in the

 following algorithm to test for data_dependence



 alg

 dd-alg

 Data_dependence analysis



 A set of equalities and inequalities representing one or_more array

 accesses The equalities represent the accesses themselves and the

 inequalities represent the loop_bounds



 A decision whether or not there is a data_dependence among any of the

 dynamic_accesses represented_by the given static accesses and loop

 bounds The existence of a data_dependence is equivalent to the

 existence of an_integer solution to the given equalities and

 inequalities



 Perform the following steps



 enumerate



 Attempt to solve the given equalities using a standard method such_as

 Gaussian_elimination However since we_want only integer_solutions

 every time a linear equation is constructed apply

 Theorem dioph-th to rule out if possible the existence of an

 integer_solution If we can rule out such solutions then answer

 no otherwise proceed to the next step



 fm-step

 Apply Fourier-Motzkin_elimination to the system of linear

 inequalities If there does_not exist a real solution then there is

 no integer_solution so answer no Otherwise proceed to the next

 step



 Here we have determined a

 real solution using Fourier-Motzkin_elimination Let be the last

 variable eliminated

 enumerate

 Test if there are integer_solutions for whose

 bounds are numbers only using the test of

 Section int-prog-subsect If not then there is no integer

 solution so answer no

 Otherwise choose any integer

 solution for the last variable say Try to back substitute

 as in Section rat-sol-subsect to get numerical bounds for all the

 variables taking_advantage of the triangular form of the upper and

 lower_bounds as per Algorithm_algenumerate

 Test as in Section int-prog-subsect to see

 if there is an_integer solution for all the variables If so answer

 yes

 If there is no solution with

 then we modify the constraints so we are forced to

 search for another integer_solution for the variable

 Create two problems adding the upper_bound

 in the first and the lower_bound in the other

 The original linear

 program has an_integer solution if and only if there is a solution in either

 of the subproblems

 Note_that while we technically have to begin each subproblem at

 step (fm-step) usually that step will tell_us immediately that

 one of the subproblems has no rational solutions so we do_not have to

 (in a series of recursive branches) create a subproblem for each

 possible integer value of that lies within its original bounds

 enumerate



 enumerate

 alg



 Summary



 We have shown that information that a programmer can glean easily from

 array references is equivalent to certain standard mathematical concepts

 Given an access function





 enumerate



 The dimension of the data region accessed is given by the rank of the

 matrix_F The dimension of the space of accesses to the same

 location is given by the nullity of F Iterations whose

 differences belong to the null_space of F refer to the same

 array_elements



 Iterations that share self-temporal_reuse of an access are separated

 by vectors in the null_space of F Self-spatial reuse can be

 computed similarly by asking when two iterations use the same row

 rather_than the same element Two accesses

 and share easily exploitable

 locality Reuse is found between instances along the d

 direction where d is the particular solution to the equation





 The region of an array accessed can be bounded by

 projecting away the index variables from the linear constraints



 Bi b 0 and

 a Fi f





 The data_dependence problem - whether two references can refer to the

 same_location - is equivalent to integer_linear programming Two

 access functions share a data_dependence if there are integer-valued

 vectors i and such that

 and





 enumerate



 hexer

 In the box on the Euclidean_algorithm we made a number of assertions

 without proof

 Prove each of the following



 itemize



 a)

 The Euclidean_algorithm as stated always works In_particular

 where is the nonzero remainder of



 b)

 for





 c)

 The GCD is really a function of sets of integers ie order doesn't

 matter Show the commutative law for GCD

 Then show the more difficult statement the associative law for

 GCD

 Finally show that together these laws imply that the GCD of a set of

 integers is the same regardless of the order in which the GCD's of

 pairs of integers are computed



 itemize

 hexer



 hexer

 Find another solution to the second Diophantine_equation in

 Example gcd-ex

 hexer

 Input Buffering

 buffer-sect



 Before discussing the problem of recognizing

 lexemes in the input let_us examine some ways that the simple

 but important task of reading the source_program can be speeded

 This task is made difficult by the fact that we often have to look one

 or_more characters beyond the next lexeme before we can be_sure we have

 the right lexeme

 The box on Tricky Problems When Recognizing Tokens in

 Section lex-role-sect gave an extreme example but there are many

 situations_where we need to look_at least_one additional character

 ahead

 For_instance we cannot be_sure we've seen the end of an_identifier

 until we see a character that is not a letter or digit and therefore

 is not part of the lexeme for id

 In C single-character operators like or could

 also be the beginning of a two-character operator like

 or

 Thus we_shall introduce a two-buffer scheme that handles large

 lookaheads safely

 We then consider an improvement involving sentinels that saves time

 checking for the ends of buffers



 Buffer Pairs

 buffer-pair-subsect



 Because of the amount of time taken to process characters

 and the large_number of characters that must_be processed during the

 compilation of a large source_program specialized buffering techniques

 have_been developed to reduce the amount of overhead required to process

 a single input_character

 An_important scheme involves two buffers that are alternately reloaded

 as suggested in Fig buffer1-fig



 figurehtfb

 fileuullmanalsuch3figsbuffer1eps

 Using a pair of input buffers

 buffer1-fig

 figure



 Each buffer is of the same size and is usually the size of a

 disk block eg 4096 bytes

 Using one system read command we can read characters into a buffer

 rather_than using one system call per character

 If fewer_than characters remain in the input file then a special

 character represented_by eof marks the end of the source file

 and is different from any possible character of the source_program



 Two pointers to the input are maintained



 enumerate



 Pointer lexemeBegin marks the beginning

 of the current lexeme whose extent we are attempting to determine



 Pointer forward scans ahead until a pattern match is found the

 exact strategy whereby this determination is made will be covered in the

 balance of this_chapter



 enumerate

 Once the next lexeme is determined forward is set to the

 character at its right_end

 Then after the lexeme is recorded as an attribute

 value of a token returned to the parser lexemeBegin

 is set to the character immediately_after the lexeme just found

 In Fig buffer1-fig we see forward has passed the end of

 the next lexeme (the Fortran exponentiation

 operator) and must_be retracted one position to its

 left



 Advancing forward requires that we first test whether we have

 reached the end of one of the buffers and if so we must reload the

 other buffer from the input and move forward to the beginning of

 the newly loaded buffer

 As_long as we never need to look so_far ahead of the actual lexeme that

 the sum of the lexeme's length plus the distance we look ahead

 is greater_than we_shall never overwrite

 the lexeme in its buffer before determining it



 Can We Run Out of Buffer Space

 In most modern languages lexemes are short and one or two characters

 of lookahead is sufficient

 Thus a buffer size in the thousands is ample and the double-buffer

 scheme of Section buffer-pair-subsect works without problem

 However there are some risks

 For_example if character_strings can be very long extending over many

 lines then we could face the possibility that a lexeme is longer than



 To_avoid problems with long character_strings we can treat_them as a

 concatenation of components one from each line over which the string is

 written

 For_instance in Java it is conventional to represent long strings by

 writing a piece on each line and concatenating pieces with a

 operator at the end of each piece



 A more difficult problem occurs_when arbitrarily long lookahead may be

 needed

 For_example some languages_like PLI do_not treat keywords as reserved that is you can use identifiers with the same name as a

 keyword like DECLARE

 If the lexical_analyzer is presented with text of a PLI program that

 begins

 DECLARE ( ARG1 ARG2

 it cannot be_sure whether DECLARE is a keyword and ARG1 and

 so on are variables being declared or whether DECLARE is a

 procedure name with its arguments

 For this reason modern languages tend to reserve their keywords

 However if not one can treat a keyword like DECLARE as an

 ambiguous identifier and let the parser resolve the issue perhaps in

 conjunction with symbol-table lookup





 Sentinels

 sentinel-subsect



 If we use the scheme of Section buffer-pair-subsect as described

 we must check each time we advance forward that we have not

 moved off one of the buffers if we do then we must also reload the

 other buffer

 Thus for each character read we make two tests one for the end of the

 buffer and one to determine what character is read (the latter may be a

 multiway branch)

 We can combine the buffer-end test with the test for the current

 character if we extend each buffer to hold

 a sentinel character at the end

 The sentinel is a special character that cannot be part of the source

 program and a natural choice is the character eof



 figurehtfb

 fileuullmanalsuch3figsbuffer2eps

 Sentinels at the end of each buffer

 buffer2-fig

 figure



 Figure buffer2-fig shows the same arrangement as

 Fig buffer1-fig but with the sentinels added

 Note_that eof retains its use as a marker for the end of the

 entire input

 Any eof that appears other_than at the end of a buffer means that

 the input is at an end

 Figure sentinel-code-fig summarizes the algorithm for advancing

 forward

 Notice how the first test which can be part of a multiway branch based

 on the character pointed to by forward is the only test we make

 except in the case_where we actually are at the end of a buffer or the

 end of the input



 figurehtfb



 center

 tabularl

 switch ( forward )



 case eof



 if (forward is at end of first buffer )



 reload second buffer



 forward beginning of second buffer







 else if (forward is at end of second buffer )



 reload first buffer



 forward beginning of first buffer







 else eof within a buffer marks the end of input



 terminate lexical analysis



 break



 Cases for the other characters







 tabular

 center



 Lookahead code with sentinels

 sentinel-code-fig



 figure



 Implementing Multiway Branches

 We might imagine that the switch in Fig sentinel-code-fig

 requires many steps to execute and that placing the case eof

 first is not a wise choice

 Actually it doesn't matter in what order we list the cases for each

 character

 In_practice a

 multiway branch depending_on the input_character is

 made in one step by jumping to an address found in an array of

 addresses indexed by characters



 Bottom-Up Parsing

 bottom-up-sect



 A bottom-up_parse corresponds to the construction of a parse_tree

 for an input_string beginning at the leaves (the bottom) and

 working up towards the root (the top) It is convenient to

 describe parsing as the process of building parse_trees although

 a front_end may in fact carry out a translation directly without

 building an explicit tree The sequence of tree snapshots in

 Fig bottom-up-seq-fig illustrates a bottom-up_parse of the

 token_stream with_respect to the expression

 grammar_(expr-gram-display)



 figurehtfb

 center



 A bottom-up_parse for idid

 bottom-up-seq-figcenter

 figure



 This_section introduces a general style of bottom-up_parsing known

 as shift-reduce_parsing The largest class of grammars for which

 shift-reduce parsers can be built the LR_grammars will be

 discussed in Sections slr-sect and

 lr-parsers-sect Although it is too

 much work to build an LR_parser by hand tools called automatic

 parser_generators make it easy to construct efficient LR_parsers

 from suitable grammars The concepts in this_section are helpful

 for writing suitable grammars to make effective use of an LR

 parser_generator Algorithms for implementing parser_generators

 appear in Section_lr-parsers-sect



 Reductions

 We can think of bottom-up_parsing as the process of reducing a

 string to the start_symbol of the grammar At each reduction step a specific substring matching the body of a

 production is replaced_by the nonterminal at the head of that

 production



 The key decisions during bottom-up_parsing are about when to

 reduce and about what production to apply as the parse proceeds



 exreduction-ex

 The snapshots in Fig bottom-up-seq-fig illustrate a

 sequence of reductions the grammar is the expression grammar

 (expr-gram-display) The reductions will be discussed in

 terms of the sequence of strings



 center



 center

 The strings in this sequence are formed from the roots of all the

 subtrees in the snapshots The sequence starts with the input_string

 The first reduction produces by

 reducing the leftmost to using the production

 The second reduction produces by

 reducing to



 Now we have a choice between reducing the string which

 is the body of and the

 string consisting of the second which is the body

 of Rather_than reduce T to

 E the second is reduced to resulting in

 the string This string then

 reduces to The parse completes with the reduction of

 to the start_symbol

 ex



 By definition a reduction is the reverse of a step in a

 derivation (recall that in a derivation a nonterminal in a

 sentential_form is replaced_by the body of one of its

 productions) The goal of bottom-up_parsing is therefore to

 construct a derivation in reverse The following

 corresponds to the parse in Fig bottom-up-seq-fig



 center



 center

 This derivation is in fact a rightmost_derivation



 Handle Pruning

 handle-prune-subsect



 Bottom-up parsing during a left-to-right_scan of the input

 constructs a rightmost_derivation in reverse Informally a

 handle is a substring that matches the body of a

 production and whose reduction represents one step along the

 reverse of a rightmost_derivation



 For_example adding subscripts to the tokens id for clarity

 the handles during the parse of

 according to the expression grammar_(expr-gram-display) are

 as in Fig handle-reductions-fig Although is the

 body of the production the

 symbol is not a handle in the sentential_form

 If were indeed replaced_by

 we would get the string

 which cannot be derived_from the start_symbol

 Thus the leftmost substring that matches the body of some

 production need not be a handle



 figurehtfb

 center

 tabularrcl

 RIGHTSENTENTIALFORM HANDLE REDUCINGPRODUCTION



 -2pt





















 tabular

 Handles during a parse of

 handle-reductions-fig

 center

 figure



 Formally if as in

 Fig handle-fig then production in the

 position following is a handle of

 Alternatively a handle of a right-sentential_form is a

 production and a position of where

 the string may be found such that replacing at

 that position by produces the previous right-sentential_form

 in a rightmost_derivation of



















 Notice_that the string

 to the right of the handle must contain only terminal_symbols For

 convenience we refer to the body rather_than

 as a handle Note we say a handle rather

 than the handle because the grammar could be ambiguous with

 more_than one rightmost_derivation of If a

 grammar is unambiguous then every right-sentential_form of the

 grammar has exactly one handle



 figurehtfb

 center



 A handle in the parse_tree for

 handle-fig

 center

 figure



 A rightmost_derivation in reverse can be obtained_by handle

 pruning That is we start with a string of terminals to be

 parsed If is a sentence of the grammar at hand then let

 where is the th right-sentential_form of

 some as yet unknown rightmost_derivation



 center





 center



 To reconstruct this derivation in reverse order we locate the

 handle in and replace by the

 head of the relevant production to

 obtain the previous right-sentential_form Note

 that we do_not yet know how handles are to be found but we_shall

 see methods of doing_so shortly



 We then repeat this process That is we locate the handle

 in and reduce this handle to obtain

 the right-sentential_form If by continuing this

 process we produce a right-sentential_form consisting only of the

 start_symbol then we halt and announce successful

 completion of parsing The reverse of the sequence of productions

 used in the reductions is a rightmost_derivation for the input

 string



 Shift-Reduce Parsing

 shift-reduce-subsect



 Shift-reduce parsing is a form of bottom-up_parsing in which a

 stack_holds grammar_symbols and an input_buffer holds the rest of

 the string to be_parsed As we_shall see the handle always

 appears at the top of the stack just_before it is identified as the

 handle



 We use to mark the bottom of the stack and also the right_end

 of the input Conventionally when discussing bottom-up_parsing we show

 the top of the stack on the right rather_than on the left as we did for

 top-down_parsing

 Initially the stack is empty and the string is

 on the input as_follows



 center

 tabularp_p p

 STACK_INPUT





 tabular

 center

 During a left-to-right_scan of the input_string the parser_shifts

 zero_or more input symbols onto the stack until it is ready to

 reduce a string of grammar_symbols on top of the stack It

 then reduces to the head of the appropriate

 production The parser repeats this cycle until it has detected an

 error or until the stack contains the start_symbol and the input

 is empty



 center

 tabularp_p p

 STACK_INPUT





 tabular

 center

 Upon entering this configuration the parser halts and announces

 successful_completion of parsing Figure

 shift-reduce-actions-fig steps through the actions a

 shift-reduce_parser might take in parsing the input_string

 according to the expression grammar

 (expr-gram-display)



 figurehtfb

 center

 tabularl r_l

 STACK_INPUT ACTION



 height 12pt depth 0pt width 0pt shift



 reduce by



 reduce by



 shift



 shift



 reduce by



 reduce by



 reduce by



 accept



 tabular

 Configurations of a shift-reduce_parser on input

 shift-reduce-actions-fig

 center

 figure



 While the primary operations are shift and reduce there are

 actually four possible actions a shift-reduce_parser can make (1)

 shift (2) reduce (3) accept and (4) error



 enumerate



 Shift Shift the next_input symbol onto the top of the

 stack



 Reduce The right_end of the string to be reduced must_be

 at the top of the stack Locate the left end of the string within

 the stack and decide with what nonterminal to replace the string



 Accept Announce successful_completion of parsing



 Error Discover a syntax error and call an error_recovery

 routine

 enumerate



 The use of a stack in shift-reduce_parsing is justified by an

 important fact the handle will always eventually appear on top of

 the stack never inside This fact can be shown by considering the

 possible forms of two successive steps in any rightmost

 derivation Figure handle-proof-fig illustrates the two

 possible cases In case (1) is replaced_by and

 then the rightmost nonterminal in the body is

 replaced_by In case (2) is again expanded first

 but this time the body is a string of terminals only The next

 rightmost nonterminal will be somewhere to the left of



 figurehtfb

 center



 Cases for two successive steps of a rightmost_derivation

 handle-proof-figcenter

 figure



 In other_words



 center

 tabularl_l

 (1)





 (2)



 tabular

 center

 Consider case (1) in reverse where a shift-reduce_parser has just

 reached the configuration



 center

 tabularp_p p

 STACK_INPUT





 tabular

 center

 The parser reduces the handle to to reach the

 configuration



 center

 tabularp_p p



 tabular

 center

 The parser can now shift the string onto the stack

 by a sequence of zero_or more shift moves to reach

 the configuration



 center

 tabularp_p p



 tabular

 center

 with the handle on top of the stack and it gets

 reduced to



 Now_consider case (2) In configuration



 center

 tabularp_p p



 tabular

 center

 the handle is on top of the stack After reducing the

 handle to the parser can shift the string to

 get the next handle on top of the stack ready to be reduced

 to



 center

 tabularp_p p



 tabular

 center

 In both cases after making a reduction the parser had to shift

 zero_or more symbols to get the next handle onto the stack It

 never had to go into the stack to find the handle



 Conflicts During Shift-Reduce Parsing

 sr-conflict-subsect



 There_are context-free_grammars for which shift-reduce_parsing

 cannot be used Every shift-reduce_parser for such a grammar can

 reach a configuration in which the parser knowing the entire

 stack and also the next_input symbols cannot decide_whether to

 shift or to reduce (a shiftreduce conflict) or cannot

 decide which of several reductions to make (a reducereduce

 conflict) We_now give some examples of syntactic_constructs that

 give rise to such grammars Technically these grammars are not in

 the LR(k) class of grammars defined in Section

 lr-parsers-sect we refer to them as non-LR grammars The

 k in LR(k) refers to the number of symbols of

 lookahead on the input Grammars used in compiling usually fall in

 the LR(1) class with one symbol of lookahead at most



 ex

 never-lr-ex An ambiguous_grammar can never be LR For

 example consider the dangling-else_grammar

 (if-gram-display) of Section writing-grammars-sect



 center

 tabularr_c l

 stmt if expr_then stmt



 if expr_then stmt_else stmt



 other

 tabular

 center

 If we have a shift-reduce_parser in configuration

 center

 tabularp_p p

 STACK_INPUT



 if expr_then stmt_else

 tabular

 center

 we cannot_tell whether if expr_then stmt

 is the handle no_matter what appears below it on the stack Here

 there is a shiftreduce_conflict Depending on what_follows the

 else on the input it might be correct to reduce if

 expr_then stmt to stmt or it might be

 correct to shift else and then to look for another stmt to complete the alternative if expr_then

 stmt_else stmt



 Note_that shift-reduce_parsing can be adapted to parse certain

 ambiguous_grammars such_as the if-then-else grammar above If we

 resolve the shiftreduce_conflict on else in favor of

 shifting the parser will behave as we expect associating each

 else with the previous unmatched then We discuss

 parsers for such ambiguous_grammars in Section

 ambig-gram-sect

 ex



 Another common setting for conflicts occurs_when we know we have a

 handle but the stack_contents and the next_input symbol are

 insufficient to determine which production should be used in a

 reduction The next example_illustrates this situation



 ex

 proc-array-ex Suppose we have a lexical_analyzer that

 returns the token_name id for all names regardless of their

 type Suppose also that our language invokes procedures by giving

 their names with parameters surrounded_by parentheses and that

 arrays are referenced by the same syntax Since the translation of

 indices in array references and parameters in procedure_calls are

 different we_want to use different productions to generate lists

 of actual_parameters and indices Our grammar might therefore have

 (among others) productions such_as those in

 Fig proc-array-fig



 figurehtfb



 center

 tabularr_r c_l

 (1) stmt id ( parameterlist )



 (2) stmt expr_expr



 (3) parameterlist parameterlist parameter



 (4) parameterlist parameter



 (5) parameter id



 (6) expr id ( exprlist )



 (7) expr id



 (8) exprlist exprlist_expr



 (9) exprlist_expr

 tabular

 center



 Productions involving procedure_calls and array

 references proc-array-fig



 figure



 A statement beginning with p(ij) would appear as the token

 stream to the parser After shifting the

 first three tokens onto the stack a shift-reduce_parser would be

 in configuration



 center

 tabularp_p p

 STACK_INPUT



 id ( id_id

 )

 tabular

 center

 It is evident that the id on top of the stack must_be

 reduced but by which production The correct choice is production

 (5) if p is a procedure but production (7) if p is an

 array The stack does_not tell which information in the symbol

 table obtained from the declaration of p must_be used



 One_solution is to change the token_id in production (1) to

 procid and to use a more sophisticated lexical_analyzer that

 returns the token_name procid when it recognizes a lexeme that

 is the name of a procedure Doing_so would_require the lexical

 analyzer to consult the symbol_table before returning a token



 If we made this modification then on processing p(ij) the

 parser would be either in the configuration



 center

 tabularp_p p

 STACK_INPUT



 procid ( id_id

 )

 tabular

 center

 or in the configuration above In the former_case we choose

 reduction by production (5) in the latter case by production (7)

 Notice how the symbol third from the top of the stack determines

 the reduction to be made even_though it is not involved in the

 reduction Shift-reduce parsing can utilize information far down

 in the stack to guide the parse

 ex





 exer

 handle-exer

 For the grammar of

 Exercise more-cfgs-exer(a) indicate the handle in each of the

 following right-sentential_forms



 itemize



 a)

 b)



 itemize

 exer



 exer

 handle2-exer

 Repeat_Exercise handle-exer for the grammar

 of Exercise_cfg-exer and the

 following right-sentential_forms



 itemize



 a)

 b)

 c)



 itemize

 exer



 exer

 Give bottom-up parses for the following input strings and grammars



 itemize



 a) The input 000111 according to the grammar of

 Exercise handle-exer



 b) The input according to the grammar of

 Exercise handle2-exer



 itemize

 exer

 Context-Insensitive InterproceduralAnalysis

 Context-Insensitive Interprocedural_Analysis

 virt-meth-sect



 We_now consider method invocations We first explain how

 points-to_analysis can be used to compute a precise call_graph which

 is useful in computing precise points-to_results We then

 formalize on-the-fly call-graph discovery and show_how Datalog can

 be used to describe the analysis succinctly



 Effects of a Method Invocation



 The effects of a method call such_as x yn(z) in Java on the

 points-to_relations can be computed as_follows

 enumerate

 Determine the type of the receiver_object which is the object that

 points to Suppose its type is Let be the method_named

 in the narrowest superclass of that has a method_named

 Note_that in general which method is invoked can only be determined

 dynamically



 The formal_parameters of are assigned the objects_pointed to by

 the actual_parameters The actual_parameters include not just the

 parameters passed in directly but also the receiver_object itself Every

 method invocation assigns the receiver_object to the this

 variable(Remember that variables are distinguished by the

 method to which they belong so there is not just one variable named

 this but rather one such variable for each method in the

 program) We refer to the this variables as the 0th formal

 parameters of methods

 In x yn(z) there are two formal_parameters the

 object pointed to by is assigned to variable this and the

 object pointed to by is assigned to the first declared formal_parameter

 of



 The returned object of is assigned to the left-hand-side variable

 of the assignment_statement

 enumerate



 In context-insensitive_analysis parameters and returned values are

 modeled by copy statements The interesting question that

 remains is how to determine the type of the receiver_object We can

 conservatively determine the type according to the declaration of the

 variable for example if the declared variable has type then

 only methods named in subtypes of can be invoked Unfortunately

 if the declared variable has type Object then all methods

 with name are all potential targets In real-life programs that

 use object hierarchies extensively and include many large libraries

 such an approach can result in many spurious call_targets making the

 analysis both slow and imprecise



 We need to know what the variables can point to

 in order to compute the call

 targets but unless we know the call_targets we cannot find out what

 all the variables can point to This recursive relationship requires

 that we discover the call_targets on the fly as we compute the

 points-to set The analysis continues until_no new call_targets and

 no new points-to_relations are found



 figurehtfb



 verbatim

 class T

 1) g_T n() return_new R()



 class S extends T

 2) h_T n() return_new S()



 class R extends S

 3) i T n() return_new R()





 main ()

 4) j T a new_T()

 5) a an()



 verbatim



 A virtual_method invocation

 virt-meth-code-fig

 figure



 ex

 virt-meth-ex

 In the code in Fig_virt-meth-code-fig is a subtype of

 which itself is a subtype of Using only the declared type

 information an() may invoke any of the three declared methods with

 name since and are both subtypes of 's declared type

 Furthermore it appears that

 may point to objects and after line_(5)



 By analyzing the points-to relationships we first determine that

 can point to an object of type Thus the method declared in

 line_(1) is a call target Analyzing line_(1) we determine that

 also can point to an object of type Thus the method

 declared in line_(3) may also be a call target and can now also

 point to another object of type Since there are no more new

 call_targets the analysis terminates without analyzing the method

 declared in line (2) and without concluding that can point to

 ex



 Call_Graph Discovery in Datalog

 cg-discovery-sect



 To formulate the Datalog_rules for context-insensitive interprocedural

 analysis we introduce three EDB_predicates each of which is

 obtainable easily from the source code



 enumerate



 says is the th actual_parameter used in

 call_site



 says_that is th formal_parameter declared in method



 says_that is the method called when is invoked on

 a receiver_object of type ( stands_for class_hierarchy analysis)



 enumerate



 Each edge of the call_graph is represented_by an IDB_predicate

 As we discover more call-graph edges more points-to

 relations are created as the parameters are passed in and returned

 values are passed out This effect is summarized by the rules shown

 in Figure cgdiscovery-fig



 figurehtfb



 centertabularr_r c_l

 1) -



















 2) -















 tabularcenter



 Datalog_program for call-graph discovery

 cgdiscovery-fig



 figure



 The first rule computes the call

 target of the call_site That is

 says_that there is a

 call_site labeled that invokes method_named on the receiver_object

 pointed to by The subgoals say that if can point to heap

 object which is allocated as type and is the method used

 when is invoked on objects of type then call_site may

 invoke method



 The second rule_says that if site can call method then

 each formal_parameter of can point to whatever the corresponding

 actual_parameter of the call can point to

 The rule for handling returned values is left as

 an_exercise



 Combining these two rules with those explained in

 Section andersen-sect create a context-insensitive_points-to

 analysis that uses a call_graph that is computed on the fly This

 analysis has the side_effect of creating a call_graph using a

 context-insensitive and flow-insensitive points-to_analysis This

 call_graph is significantly more accurate than one computed based only

 on type declarations and syntactic analysis



 Dynamic Loading and Reflection



 Languages like Java allow dynamic loading of classes It is

 impossible to analyze all the possible code executed by a program and

 hence impossible to provide any conservative approximation of call

 graphs or pointer aliases statically Static analysis can only

 provide an approximation based_on the code analyzed Remember

 that all the analyses described_here can be applied at

 the Java bytecode level and thus it is not necessary to examine the

 source code This option is especially significant because Java programs

 tend to use many libraries



 Even_if we assume that all the code to be executed is analyzed there

 is one more complication that makes conservative analysis impossible

 reflection Reflection allows a program to determine dynamically the

 types of objects to be created the names of methods invoked as_well

 as the names of the fields accessed The type method and field

 names can be computed or derived_from user input so in general the

 only possible approximation is to assume the universe



 ex

 reflection-ex

 The code below shows a common use of reflection

 verbatim

 1) String className

 2) Class c ClassforName(className)

 3) Object o cnewInstance()

 4) T t (T) o

 verbatim



 The forName method in the Class library takes a string

 containing the class name and returns the class The method newInstance returns an instance of that class Instead of leaving

 the object with type Object this object is cast to a

 superclass of all the expected classes

 ex



 While many large Java applications use reflection they tend to use

 common idioms such_as the one shown in Example reflection-ex

 As_long as the application does_not redefine the class loader we can

 tell the class of the object if we know the value of className

 If the value of className is defined in the program because

 strings are immutable in Java knowing what className points to will

 provide the name of the class This technique is another use of points-to

 analysis If the value of className is based_on user input

 then the points-to_analysis can help locate where the value is entered

 and the developer may be_able to limit the scope of its value



 Similarly we can exploit the typecast statement line_(4) in Example

 reflection-ex to approximate the type of dynamically created

 objects Assuming that the typecast exception handler has not been

 redefined the object must belong to a subclass of the class



 exer

 virt-meth-exer

 For the code of Fig_virt-meth-code-fig

 itemize

 a) Construct the EDB relations and



 b) Make all possible inferences of and facts

 itemize

 exer



 hexer

 How_would you add to the EDB_predicates and rules of

 Section cg-discovery-sect additional predicates and rules to take

 into_account the fact that if a method call returns an object then the

 variable to which the result of the call is assigned can point to whatever

 the variable holding the return value can point to

 hexer





 hexer

 Extend the rules of Fig_andersen-types-dl-fig to include a

 restriction on rule_(3) Note_that you must define suitable EDB_predicates

 to reflect the declarations of types for fields in the code

 hexer



 Call_Graph Discovery

 virt-meth-sect



 To this point we have assumed that call_sites lead to a particular

 procedure and that this procedure may be determined by_looking at the

 program Thus in Example andersen-ex we were_able to determine that

 the function called by main was the particular function with that

 name that was shown We were_able to use the rule for copy statements

 to reflect the call and return because we knew how the formal and actual

 parameters linked up



 When pointers to functions (in C) or virtual methods (in Java) are in play

 we have more difficulties In this_section we_shall consider the case of

 virtual methods and extend the rules of Section andersen-sect to

 limit the number of different methods that must_be considered with each

 virtual_method invocation



 The Problem of Virtual Method Invocation



 Consider an assignment involving a method call x ym() as in Java

 If we do_not know the type of then there may be several different

 methods named that this use of the identifier may refer to

 If we are to be conservative about finding the objects that variables and

 fields of heap_objects may point to we have to assume that any of these

 methods may be called here As a consequence we have to assume that the

 following are part of the code and therefore are reflected by EDB facts



 enumerate



 For each method_named there is a copy_statement from to this

 the variable that represents the object to which the method is

 applied(Remember that variables are distinguished by the method to

 which they belong so there is not just one variable named this but

 rather one such variable for each method in the program)



 For each method_named there is a copy_statement from the variable

 holding the return value of that method to



 enumerate



 figurehtfb



 verbatim

 1) t m()

 2) t a thisf

 3) return a





 4) s m()

 5) return this







 6) h t b new t

 7) g s c new s

 8) bf c

 9) t d bm()

 verbatim



 A virtual_method invocation

 virt-meth-code-fig



 figure



 ex

 virt-meth-ex

 Examine the code in Fig_virt-meth-code-fig We see two methods named

 one that returns an object of type and the other that returns an

 object of type We may assume that is a subtype of but it

 doesn't matter for this example Following the two method definitions we

 see four later statements numbered (6) through (9)



 If we follow the policy that the method invocation at line_(9) must_be

 treated_as if both methods named are called then we make a number of

 inferences First lines (6) and (7) tell_us that and

 Line (8) says_that Now line_(9) says_that

 is copied to the this variables of both methods To distinguish them

 we_shall refer to them as thisT and thisS for the methods that

 return types and respectively Thus the copy rule plus the fact

 tells_us that and



 The statement (2) in the first method together_with tells

 us Then the fact that is returned at line_(3) together

 with the assignment of the return value to at line_(9) says

 However lines (5) and (9) together_with the fact let_us

 conclude that is also true This conclusion is not valid since

 is evidently of type and the second method could not be invoked at

 line_(9) However without considering the types of variables we cannot

 tell which method is invoked

 ex



 Using Type Inference_Rules



 Our task is to determine all the types (classes) that a variable could

 belong to Once we have that information we can use the policy of the

 programming_language regarding method dispatch to determine which methods

 might be called at a call_site For_example the Java (and standard rule

 for object-oriented languages) is to treat a method applied to an object

 that may be of type as a call to the method_named in the narrowest

 supertype of that has a method_named



 Key to our analysis are three EDB_predicates that reflect important type

 information in the code being analyzed We_shall use the following



 enumerate



 says_that variable is declared to have type



 says_that heap_object is declared to have type



 means that an object of type can be assigned to a

 variable with the type This information is generally gathered from the

 declaration of subtypes in the program but also_incorporates information

 about the predefined classes of the language is always

 true



 enumerate



 ex

 Line (7) in Fig_virt-meth-code-fig tells_us two facts

 First because is declared to be a reference to an object of type

 Also since any object_created at line_(7) must_be of

 type

 ex



 We can modify the rules from Fig_andersen-dl-fig to allow inferences

 only if the variable assigned gets a heap_object of an assignable type

 The rules are shown in Fig_andersen-types-dl-fig



 figurehtfb



 centertabularr_r c_l

 1) - new



 2) -



















 3) -











 4) -























 tabularcenter



 Adding type restrictions to insensitive pointer_analysis

 andersen-types-dl-fig



 figure



 The first modification is to rule_(2) The last three subgoals say that we

 can only conclude that can point to if there are types and

 that variable and heap_object may respectively have

 such that objects of type can be assigned to variables that are

 references to type

 A similar additional_restriction has_been added to rule (4) Notice_that

 there is no additional_restriction in rule_(3) because our approach does

 not deal_with the types of fields in heap_objects However since these

 fields are normally declared to have a type we could extract additional EDB

 information from the code and have a restriction for rule_(3) as_well This

 extension is left as an_exercise



 ex

 Let_us assume that in Fig_virt-meth-code-fig the method of lines

 (1) through (3) is a method for the class In_particular the type of

 thisT is Also the method in lines_(4) and (5) is for the

 subtype so the type of thisS is

 Then for the code of Fig_virt-meth-code-fig the additional EDB_predicates

 have the following facts



 centertabularl_l l













 tabularcenter

 Notice in particular that is false



 Let_us consider rule_(2) of Fig_andersen-types-dl-fig

 and in particular the possibility that in line_(9)

 refers to the method of lines_(4) and (5) If so then there is a copy

 from to thisS That is rule_(2) must_be satisfied with

 and Since is false but is true we

 must have in the body of rule_(2) Since is false but

 is true we must also have in the body Likewise we are

 forced to have in the body because only is

 true

 But now the last subgoal is false That

 is the rules with type constraints of Fig_andersen-types-dl-fig do

 not allow_us to assume that the second method is called at line_(9) of

 Fig_virt-meth-code-fig As a result we cannot infer

 using the rules that take type information into_account We have in

 effect resolved the question of which is referred to at line_(9)

 ex



 figurehtfb



 verbatim

 t m()

 return this





 s m()

 s a thisf

 return a





 r m()

 h r b new r

 rf this

 return b







 i t c new t

 j s d new s

 cf d

 t e cm()

 verbatim



 Code for Exercise virt-meth-exer

 virt-meth-exer-fig



 figure



 exer

 virt-meth-exer

 In the code of Fig virt-meth-exer-fig assume that is a

 subtype of which is a subtype of



 itemize

 a) Construct the EDB relations and



 b) Make all possible inferences of and facts

 itemize

 exer



 hexer

 Extend the rules of Fig_andersen-types-dl-fig to include a

 restriction on rule_(3) Note_that you must define suitable EDB_predicates

 to reflect the declarations of types for fields in the code

 hexer

 Call_Graph Discovery

 seccg



 The call_graph generated using class_hierarchy analysis can have many

 spurious call_targets which can lead to many spurious points-to

 relationslhotak03 We can get more_precise results by

 creating the call_graph on the fly using points-to_relations As the

 algorithm generates points-to_results they are used to identify the

 receiver types of the methods invoked and to bind calls to target

 methods and as call_graph edges are discovered we use them to find

 more points-to_relations The algorithm_converges when no new call

 targets and no new pointer relations are found



 figurehtb

 alg Context-insensitive_points-to analysis that computes

 call_graph on the fly

 algci-fly

 alg

 Domains



 Domains_from Algorithm_algci-types plus

 tabbing

 123456123456712345678













 tabbing

 Relations



 Relations from Algorithm_algci-types with the modification

 that is now a computed relation plus

 arraylll

 input (typeT nameN targetM)



 input (invokeI paramZ varV)



 input (methodM paramZ varV)



 input 0 (invokeI targetM)



 input (methodM invokeI)





 output (invokeI targetM)

 array



 Rules



 Rules from Algorithm_algci-types plus

 equationarraylcl

 (im) - 0(im) IE0rule



 (im2) - (m1in) (i0v)



 (vh) (ht) (tnm2) IErule



 (v1v2) - (im) (mzv1)



 (izv2) Arule

 equationarray

 figure



 Modifying Algorithm_algci-types to discover call graphs on the fly

 is simple Instead of an input relation computed from a

 given call_graph we derive it from method

 invocation statements and points-to_relations





 description





 I is the domain of invocation_sites in the program An

 invocation_site is a method invocation of the form

 r pm(p p)

 Note_that





 N is the domain of method names used in invocations In an

 invocation

 r pn(p p)

 n is the method name





 M is the domain of implemented methods in the program It does

 not include abstract or interface methods





 Z is the domain used for numbering parameters





 T N M

 encodes virtual_method dispatch information from the class

 hierarchy means that is the target of

 dispatching the method name on type







 I Z V encodes the actual_parameters

 for invocation_sites

 means that is passed as parameter

 number at invocation_site





 M Z V encodes formal_parameters for methods

 means that formal_parameter of

 method is represented_by variable





 I M are the initial invocation_edges They

 record the invocation_edges whose targets are statically

 bound In Java some calls are static or non-virtual Additionally

 local type analysis combined with analysis of the class_hierarchy

 allows_us to determine that some calls have a single targetDean1995OOP

 means that invocation_site can be analyzed

 statically to call method





 M I N represents invocation_sites

 means that method contains an

 invocation_site with virtual_method name

 Non-virtual invocation_sites are given a special null method

 name which does_not appear in the relation













 I M is an output relation encoding all

 invocation_edges means that invocation_site calls

 method

 description



 The rules in Algorithm algci-fly compute the relation

 used in Algorithm_algci-types

 Rules (IE0rule) and (IErule) find the invocation_edges

 with the former handling statically bound targets and the latter

 handling virtual calls Rule (IErule) matches invocation_sites

 with the type of the this pointer and the class_hierarchy

 information to find the possible target methods If an invocation

 site with method name is invoked on variable and can

 point to and has type and invoking on type leads

 to method then is a possible target of invocation



 Rule (Arule) handles parameter passing(We

 also match thread objects to their corresponding run() methods

 even_though the edges do_not explicitly appear in the call graph) If

 invocation_site has a target method variable is passed

 as argument number and the formal_parameter of method is

 then the points-to set of includes the points-to set of

 Return values are handled in a likewise

 manner only the inclusion relation is in the opposite direction

 We see that as the discovery of more variable points-to ()

 can create

 more invocation_edges () which in turn can create more

 assignments () and more points-to_relations

 The algorithm_converges when all the relations stabilize

 Parallelism and Locality











 Many applications in scientific engineering and commercial

 applications have an insatiable need for computational cycles Such

 examples include weather prediction protein folding for designing

 drugs fluid dynamics for designing aero-propulsion systems quantum

 chromodynamics (QCD) to study the strong interactions of high-energy

 physics and data mining in database applications One_way to speed

 up a computation is to divide it into components and execute them in

 parallel on different machines



 Using parallelism to provide performance is particularly attractive

 from a hardware perspective It is easier and cheaper to double the

 potential computation power of a machine by replicating a set of

 hardware units than to make the original machine run twice as fast

 Parallelism is a scalable technique in that it is relatively easier to

 build large-scale parallel machines Unfortunately it is not easy to

 create applications that can use processor-level parallelism

 effectively



 First a pre-requisite to efficient parallel_execution is that an

 application be divisible into many units that can be computed

 simultaneously Suppose we can parallelize only half of the

 operations in a computation The computation can only double in speed

 regardless of how many machines we have In_general if the fraction

 of the program parallelized is the program will take at_least

 of the original_program execution time achieving a speed_up of

 at most This observation is known_as the Amdahl's Law As

 illustrated by Figure figAmdahl we can obtain scalable speed

 up on a program only if nearly all the computation can execute in

 parallel

 figurehtb

 Amdahl's Law

 figAmdahl

 figure



 Second parallelizing even 100 of the computation does_not

 necessarily guarantee a speed_up Overheads incurred in parallel

 execution can cause a parallel program to run even slower than a

 sequential program Modern machines require upwards of hundreds of

 machine cycles to communicate even just a word of data Unless the

 computation can be partitioned_into large mostly independent chunks

 of computation the overhead can overwhelm the benefit of parallel

 execution Thus besides finding parallelism we also need to improve

 data_locality on each processor to speed_up the execution of

 individual processors and to minimize the need for interprocessor

 communication



 Parallelization is not a universally applicable technique While

 there are some embarassingly parallel applications such_as web

 servers that respond to large_numbers of independent queries most

 general-purpose symbolic computations have complex control and data

 dependences that render them difficult or impossible to parallelize

 Fortunately many time-consuming scientific and engineering

 applications with their simpler control structures and large data

 sets can be_parallelized Techniques that can automatically find

 parallelism in numerical applications and improve their data_locality

 have_been developed



 A parallelizing compiler takes a sequential program and automatically

 produces code to be executed on the target parallel machine

 Automating parallelization has the following benefits

 enumerate



 Encapsulation of expertise Programmers often do_not have the

 expertise required to optimize a program for a parallel machine Much

 of the effort on parallelizing compiler research has gone into

 understanding what makes a parallel program run correctly In_fact

 this research process has also generated valuable information on how

 to build effective parallel machines Encapsulating the know-how in a

 compiler allows the knowledge be brought to bear on all programs



 Ease of programming Even_if a programmer knows how to produce

 efficient parallel code the parallelization process is both tedious

 and error-prone It is easy to introduce mistakes into the code that

 are hard to find because a parallel program's behavior may depend_on

 the relative timing between the machines Automatic parallelization

 makes it easier for the programmers and eliminates a major source of

 errors



 Machine independence Different kinds of parallel machines may

 require the code to be optimized different_ways automatic

 parallelization allows the same code be run efficiently on different

 architectures



 Legacy software An automatic parallelizer can be applied to

 existing codes written in conventional programming_languages This is

 especially important since original developers of many existing

 applications are no_longer available and it is difficult for new

 developers to fully understand the dependences in a program

 Automatic parallelization makes it possible to capitalize on the large

 existing software base

 enumerate



 Automatic parallelization has_been shown to be useful for the class of

 applications that hold their data in arrays and access them in the

 program using affine_expressions of outer_loop indices An

 affine_expression of variables are of the

 form

 where are constants By understanding these affine_expressions

 which succinctly capture the relations between operations and data

 accessed a compiler can optimize the data_locality of the code and

 generate efficient parallel code The principles behind these

 techniques can also be applied to other application domains



 Unless we know what is desired of the output of an parallelizer we

 would not know how to develop an effective parallelizer Thus we

 start this_chapter by discussing what it takes to write an efficient

 parallel program in Section ch10intro This_chapter focuses on

 optimizing programs with affine array indices We describe the kind

 of information we can obtain with affine accesses in

 Section ch10index We introduce the algorithms for managing

 simple forms of parallelization in Section ch10affine

 Section ch10sync addresses how we can automatically generate

 pipelined code and Section ch10block discusses locality

 optimizations

















 Optimizing for Parallelism and Locality



 An optimizing_compiler for a multiprocessor must deal_with an entirely

 new set of issues that we did_not encounter when we studied code

 optimization for single-processor machines in Chapter_code-op-ch

 When our program executes on a multiprocessor we have the opportunity

 to speed execution by dividing the work among many processors operating

 in parallel However all of the work on data-flow_analysis assumes

 that we don't care how a given statement is reached and in fact these

 Chapter_code-op-ch

 techniques take_advantage of the fact that we don't distinguish_among

 different executions of the same statement



 In parallel programming the opposite is true We need to find

 opportunities for dividing different executions of the same statement

 among different_processors

 In order to do so without_introducing changes in what the program does

 we need to be very careful to determine when one instance of a statement

 depends_on another instance of that statement or another statement

 Especially important is the matter of array_accesses For_example the

 statement Ai 0 in a loop often can be executed in parallel

 with portions of the array that correspond to

 different values of the index

 divided among different_processors On the other_hand if there is

 another statement Aj 1 being executed we need to worry_about

 whether could ever be the same as and if so in which order do

 we execute the instances of these two statements that have a common

 value of the array index



 The techniques of this_chapter are applicable not only to increasing

 parallelism in programs but also to increasing locality Programs run

 faster even on a uniprocessor if the data they access has a higher

 chance of being in the cache To improve this probability we need to

 transform programs so they use data shortly after they create it



 In this_chapter we develop the theory of affine array_accesses and

 transformations of those accesses The material relies heavily on

 linear_algebra which will be introduced as needed

 We begin_with an overview of issues in parallel computation in

 Section ch11intro

 Then Section ch11mm is an extended concrete example - matrix

 multiplication - that shows_how the techniques to be developed in this

 section can support both parallelism and locality (reuse of data in fast

 memory such_as caches)



 Section it-space-sect introduces the loop_nest and the linear

 algebra that goes along with it We may view each index in a set of

 nested_loops as one dimension of an iteration_space

 The region of this space

 covered by a loop_nest depends_on the upper and lower limits on each

 loop Understanding the algebra of iteration_spaces is important for

 reordering loops in a nest and such loop reordering is often important

 for improving the parallelism andor locality of a loop_nest



 Section ch11index introduces another linear

 space for the indexes of a

 multidimensional array Then Section array-bounds-sect combines

 the two previous sections to answer the important question of whether

 two array_accesses found within a loop_nest can ever refer to the same

 array_element Answering this question requires that we look not only

 at the array_accesses themselves but at the shape of the iteration

 space for the loop_nest in which those accesses are embedded



 The final sections apply these preliminaries to increase both

 the parallelism and the data_locality of programs that have nested_loops and

 array_accesses Section ch11affine discusses

 synchronization-free_parallelism where we simply partition the work

 of a loop_nest into independent pieces based_on the value of one or_more

 loop_indexes and assign different pieces to different_processors Of

 course in order to make this transformation we have to be_sure that

 there are no constraints on timing across the different pieces -

 constraints that are

 caused by both pieces accessing the same array_element Thanks

 to the previous Section array-bounds-sect we have the mechanics to

 make that determination



 Then in Section ch11sync we consider techniques for

 parallelizing code even_though there are constraints on the order of

 execution of certain statements that are executed by different

 processors The key technique called pipelining is introduced and

 we show_how to maximize the degree of pipelining allowed by a program

 Finally in Section ch11block we consider blocking another

 technique for parallelizing code by partitioning the range of

 one or_more loop_indexes into smaller pieces Blocking not only

 supports parallel_execution under the right circumstances but maximizes

 data_locality - that is the ability of the underlying hardware to

 keep needed data in fast memory such_as caches





















 Interprocedural_Analysis

 ptr-ch



 In this_chapter we motivate the importance of interprocedural_analysis

 by discussing a number of important optimization problems that

 cannot be_solved with intraprocedural analysis

 We begin by describing the common forms of interprocedural

 analysis and explaining the difficulties in their implementation

 We then describe applications for interprocedural_analysis

 For widely used programming_languages like C and Java

 pointer_alias analysis is key to any interprocedural_analysis

 Thus for much of the chapter we discuss techniques needed

 to compute pointer aliases To start we present Datalog

 a notation that greatly hides the complexity of an efficient pointer_analysis

 We then describe an algorithm for pointer_analysis and show_how we

 use the abstraction of binary_decision diagrams (BDD's)

 to implement the algorithm efficiently



 Most compiler optimizations including those described in

 Chapters code-op-ch inst-para-ch and

 affine-ch are performed on

 procedures one at a time We refer to such analyses as intraprocedural These analyses conservatively assume that

 procedures invoked may alter the state of all the variables visible to

 the procedures and that they may create all possible side_effects such

 as modifying any of the variables visible to the procedure or

 generating exceptions that cause the unwinding of the call stack

 Intraprocedural analysis is thus relatively_simple albeit imprecise

 Some optimizations do_not need interprocedural_analysis

 while others may yield almost no useful information without it



 An interprocedural_analysis operates across an entire program flowing

 information from the caller to its callees and vice_versa

 One relatively_simple but useful technique is to inline

 procedures that is to replace a procedure invocation by the body

 of the procedure itself with suitable modifications to account for

 parameter_passing and the return value

 This method is applicable only if we know

 the target of the procedure call



 If procedures are

 invoked indirectly through a pointer or via the method-dispatch

 mechanism prevalent in object-oriented_programming

 analysis of the program's pointers or references can

 in some_cases determine the targets of the indirect invocations

 If there is a unique target inlining can be applied Even_if a

 unique target is determined for each procedure invocation inlining

 must_be applied judiciously In_general it is not

 possible to inline recursive procedures directly and even without

 recursion inlining can expand the code size exponentially





















 3in1

 1

 1

 1

 A Complete Front End

 together-sect



 The complete compiler_front end in this appendix is based_on the

 informally described simple compiler of

 Sections postfix-sect-syntree-sect The main

 difference from Chapter simple-ch is that the front_end

 generates jumping_code for boolean_expressions as in

 Section control-3code-sect We begin_with the syntax of the

 source_language described by a grammar that needs to be adapted

 for top-down_parsing



 The Java code for the translator consists of five packages main lexer symbols parser and inter

 Package inter contains classes for the language_constructs

 in the abstract_syntax Since the code for the parser interacts

 with the rest of the packages it will be discussed last Each

 package is stored as a separate directory with a file per class



 Going into the parser the source_program consists of a stream of

 tokens so object-orientation has little to do with the code for

 the parser Coming out of the parser the source_program consists

 of a syntax_tree with constructs or nodes implemented as objects

 These objects deal_with all of the following construct a

 syntax-tree_node check types and generate three-address

 intermediate_code (see package inter)



 Object-Oriented Versus Phase-Oriented With an

 object-oriented approach all the code for a construct is

 collected in the class for the construct Alternatively with a

 phase-oriented approach the code is grouped by phase so a type

 checking procedure would have a case for each construct and a

 code_generation procedure would have a case for each construct

 and so on



 The tradeoff is that an object-oriented approach makes it easier

 to change or add a construct such_as for statements and a

 phase-oriented approach makes it easier to change or add a phase

 such_as type_checking With objects a new construct can be added

 by writing a self-contained class but a change to a phase such

 as inserting code for coercions requires changes across all the

 affected classes With phases a new construct can result in

 changes across the procedures for the phases





















 Finding Linearly Independent Solutions

 time-math-ch



 alg

 figtime-math

 Finds a maximal_set of linearly_independent solutions for

 and expresses them as rows of matrix



 An matrix



 A matrix of linearly_independent solutions to







 The algorithm is shown in pseudocode below

 Note_that denotes the th_row of matrix denotes

 rows through of matrix and denotes the

 rectangle of matrix in rows through and columns through



 alg



 tabularl





 1



 1



 an -by- identity matrix







 while_( true )







 1 Make into a diagonal

 matrix with



 positive diagonal entries

 and



 are solutions











 while_( there_exists such that



 and are both 0 )



 Move pivot to by row and column



 interchange



 Interchange row with row in



 if_( )















 for ( to )



 if_( and )



































 tabular



 tabularl

 2 Find a solution besides It must_be a



 nonnegative

 combination of





 Find such that







 if_( there_exists a nontrivial solution say

 )







 NoMoreSoln false







 else are the only solutions



 NoMoreSoln true







 3 Make



 if_( NoMoreSoln )

 Move solutions to



 for ( to )



 Interchange rows and in and











 else Use row addition to find more solutions







 for ( to )



 if_( there_exists such that )



 if_( there_exists such that )





 for ( to )



 if_( )























 else



 for ( to step -1 )



 if_( )







 Interchange with



 Interchange with











 tabular



 tabularl

 4 Make



 for ( to )



 for ( 1 to )



 if_( )



 Pick an such that and























 5 If necessary repeat with rows



 if_( NoMoreSoln or or )



 Remove rows to from



 return







 else







 for ( to 1 step -1 )



 if_( there is no such that )







 Interchange column with in



























 tabular







 theorem

 Algorithm alglegal finds a maximal_set of linearly_independent

 solutions to where has_rank



 thmlegal

 theorem

 proof



 Let be a linear_combination of the rows of matrix





 As_discussed in Algorithm algfp

 is a solution to

 if and only if are non-negative



 The rows in are linearly_independent Since elementary row

 operations do_not change the number of linearly_independent rows in a

 matrix transforming to contain as many rows with non-negative

 as possible we find a maximal_set of linearly

 independent_solutions

 of the rows of matrix

 say is a solution to

 if and only if are non-negative

 proof



 1

 1

 1

 id1

 1

 Introduction

 intro-bs-ch

 ch1



 Programming languages are notations for describing computations to

 people and to machines The world as we know it depends_on

 programming_languages because all the software running on all the

 computers was written in some programming_language But before a

 program can be run it first must_be translated_into a form in

 which it can be executed by a computer



 The software systems that do this translation are called compilers



 This book is about how to design and implement compilers We_shall

 discover that a few basic ideas can be used to construct

 translators for a wide variety of languages and machines Besides

 compilers the principles and techniques for compiler design are

 applicable to so many other domains that they are likely to be

 reused many_times in the career of a computer scientist The study

 of compiler writing touches upon programming_languages machine

 architecture language theory algorithms and software

 engineering



 In this preliminary chapter we introduce the different forms of

 language translators give a high_level overview of the structure

 of a typical compiler and discuss the trends in programming

 languages and machine architecture that are shaping compilers We

 include some observations on the relationship_between compiler

 design and computer-science theory and an outline of the

 applications of compiler technology that go beyond compilation We

 end with a brief outline of key programming-language concepts that

 will be needed for our study of compilers



















 A Simple One-Pass Compiler

 intro-ch



 equation

 ite-eq

 arrayc

 If and are statements and is an expression then



 if then else is a statement



 array

 equation





 expr

 optexpr

 factor

 match

 stmt

 stmts

 term

 id

 env



 -





 FIRST

 A Simple Compiler

 simple-ch



 This_chapter introduces a number of basic compiling techniques and

 illustrates_how they can be_combined to develop a working

 translator We start small with a one-page analyzer for lists of

 digits and build_up to a translator for program_fragments

 consisting of statements For_example from the statement in

 Fig quick-src-fig the translator produces the

 representation in Fig quick-quad-fig

 Such a representation of a program is called intermediate_code

 since it is an important step in translating the original_program

 (source code) to its final machine-executable form (object

 code)



 figuretfb

 verbatim



 int i int j float a float v float_x



 while_( true )

 do i_i1 while_( ai v )

 do j_j-1 while_( aj v )

 if_( i_j ) break

 x_ai ai_aj aj_x





 verbatim



 A statement to be translated quick-src-fig

 figure



 figuretfb

 center

 tabularl_l

 1 ifFalse true goto 18



 2 i i 1



 3 t1 a i



 4 t2 t1 v



 5 ifTrue t2 goto 2



 6 j j - 1



 7 t3 a j



 8 t4 t3 v



 9 ifTrue t4 goto 6



 10 t5 i_j



 11 ifFalse t5 goto 13



 12 goto 18



 13 x a i



 14 t6 a j



 15 a i t6



 16 a j x



 17 goto 1



 18



 tabular

 center

 Intermediate_code for the program_fragment in

 Fig quick-src-fig quick-quad-fig

 figure



 The working translator is written in Java The use of Java is

 convenient but not essential In_fact many of the ideas in this

 chapter predate both Java and C























 Lexical Analysis

 lexan-ch



 In this_chapter we show_how to construct a lexical_analyzer

 To implement a lexical_analyzer by hand it helps to

 start with a diagram or other description for the lexemes

 of each token

 We can then write code to identify each occurrence of each lexeme on the

 input and to return information_about the token identified



 We can also produce a lexical_analyzer automatically by specifying the lexeme

 patterns to a lexical-analyzer generator and compiling those

 patterns into code that functions as a lexical_analyzer

 This_approach makes it easier to modify a lexical_analyzer since we

 have only to rewrite the affected patterns not the entire program

 It also speeds up the process of implementing the lexical_analyzer

 since the programmer specifies the software at the very high_level of

 patterns and relies on the generator to produce the detailed code

 We_shall introduce in Section lex-sect

 a lexical-analyzer generator called

 Lex (or Flex in a more_recent embodiment)



 We begin the study of lexical-analyzer generators by introducing

 regular_expressions a convenient notation for specifying lexeme patterns

 We show_how this notation can be transformed first into nondeterministic

 automata and then into deterministic automata

 The latter two notations can be used as input to a driver that is

 code which simulates these automata and uses them as a guide to

 determining the next token

 This driver and the specification of the automaton form the nucleus of

 the lexical_analyzer



























 FIRST

 FOLLOW

 CLOSURE

 GOTO

 ACTION





 to 0pt6pt

 to 0pt6pt





 to 0pt7pt

 lm

 rm

 lm

 rm

 1

 1





 0pt6pt1



 1

 1

 1

 Syntax Analysis

 parse-ch



 This_chapter is devoted to parsing methods that are

 typically used in compilers We first present the basic concepts

 then techniques suitable for hand implementation and finally

 algorithms that have_been used in automated tools Since programs

 may contain syntactic_errors we discuss extensions of the parsing

 methods for recovery from common errors



 By design every programming_language has precise rules that prescribe

 the syntactic_structure of well-formed programs

 In C for example a program is made up of functions a function out of

 declarations and statements a statement out of expressions and

 so on

 The syntax of programming_language constructs can be specified_by

 context-free_grammars or BNF (Backus-Naur Form) notation introduced

 in Section syntax-sect

 Grammars offer significant benefits for both

 language designers and compiler writers



 itemize



 A grammar gives a precise yet easy-to-understand syntactic

 specification of a programming_language



 From certain classes of grammars we can construct

 automatically an efficient parser that determines the syntactic

 structure of a source_program As a side benefit the

 parser-construction process can reveal syntactic ambiguities and

 trouble spots that might have slipped through the initial design

 phase of a language



 The structure imparted to a language by a properly designed

 grammar is useful for translating source programs into correct

 object code and for detecting errors



 A grammar allows a language to be evolved or developed

 iteratively by_adding new constructs to perform new tasks These

 new constructs can be integrated more easily into an

 implementation that follows the grammatical_structure of the

 language

 itemize

























 1

 1

 1





 to 0pt6pt

 to 0pt6pt





 to 0pt7pt

 lm

 rm

 lm

 rm

 1

 1



 val

 SDD

 SDT



 height 1 depth 5pt width 0pt



 Syntax-Directed Translation

 sdt-ch



 This_chapter develops the theme of Section trans-sect the

 translation of languages guided by context-free_grammars The

 translation techniques in this_chapter will be applied in

 Chapter_inter-ch to type_checking and intermediate-code

 generation The techniques are also useful for implementing little

 languages for specialized tasks this_chapter includes an example

 from typesetting



 We associate information with a language construct by_attaching

 attributes to the grammar symbol(s) representing the

 construct as discussed in Section synth-attr-subsect A

 syntax-directed_definition specifies the values of attributes by

 associating semantic_rules with the grammar productions For

 example an infix-to-postfix translator might have a production

 and rule



 equationeqsdd-post

 arrayc c

 production semantic_rule



 E E1 T Ecode

 E1code Tcode ''

 array

 equation

 This production has two nonterminals and the

 subscript in distinguishes the occurrence of in the

 production_body from the occurrence of as the head Both

 and have a string-valued_attribute code The semantic

 rule specifies that the string is formed_by

 concatenating and the

 character While the rule makes it explicit that the

 translation of is built up from the translations of

 and it may be inefficient to implement the

 translation directly by manipulating strings



 From Section sdt-intro-subsect a syntax-directed

 translation_scheme embeds program_fragments called semantic

 actions within production_bodies as in



 equationeqsdt-post

 E E1 T print ''

 equation

 By convention semantic_actions are enclosed within

 curly_braces (If curly_braces occur as grammar_symbols we

 enclose them within single quotes as in and ) The

 position of a semantic_action in a production_body determines the

 order in which the action is executed In

 production (eqsdt-post) the action occurs at the end

 after all the grammar_symbols in general semantic_actions may

 occur at any position in a production_body













 Between the two notations syntax-directed_definitions can be more readable

 and hence more useful for specifications

 However translation_schemes can

 be more_efficient and hence more useful for implementations



 The most general approach to syntax-directed_translation is to

 construct a parse_tree or a syntax_tree and then to compute the

 values of attributes at the nodes of the tree

 by visiting the nodes of the tree In many_cases

 translation can be done during_parsing without building an

 explicit tree We_shall therefore study a class of

 syntax-directed translations called L-attributed translations (L for

 left-to-right) which encompass virtually all translations that

 can be performed during_parsing We also study a smaller class

 called S-attributed translations (S for synthesized) which can be

 performed easily in connection_with a bottom-up_parse







































 1

 1

 1



 DAG

 SDD

 SDT

 val



 Intermediate-Code Generation

 inter-ch



 In the analysis-synthesis model of a compiler the front_end

 analyzes a source_program and creates an intermediate

 representation from which the back end generates

 target code Ideally details of the source_language are confined

 to the front_end and details of the target_machine

 to the back end

 With a suitably defined intermediate_representation

 a compiler for language and machine can

 then be built by combining the front_end for language with the

 back end for machine

 This_approach to creating suite of compilers can save a considerable

 amount of effort compilers can be built by writing just

 front_ends and back ends



 This_chapter deals_with intermediate_representations

 static type_checking and intermediate_code generation

 For_simplicity we assume that a compiler_front end is

 organized as in Fig inter-role-fig where parsing static

 checking and intermediate-code_generation are done sequentially

 sometimes they can be_combined and folded into parsing

 We_shall use the syntax-directed formalisms of Chapters simple-ch and

 sdt-ch to specify checking and translation Many of the

 translation_schemes can be_implemented during either bottom-up or

 top-down_parsing using the techniques of Chapter_sdt-ch

 All schemes can be_implemented by creating a syntax_tree

 and then walking the tree



 figurehtbf



 Logical structure of a compiler_front

 endinter-role-fig

 figure



 Static checking includes type_checking which ensures that

 operators are applied to compatible operands It also includes any

 syntactic checks that remain after parsing For_example static

 checking assures that a break-statement in C is enclosed within a

 while- for- or switch-statement an error is reported if such an

 enclosing statement does_not exist



 The approach in this_chapter can be used for a wide range of

 intermediate_representations including syntax_trees and

 three-address_code both of which were_introduced in

 Section syntree-sect The term three-address_code comes

 from instructions of the general form with

 three addresses two for the operands and and one for the

 result



 In the process of translating a program in a given source_language

 into code for a given target_machine a compiler may construct a

 sequence of intermediate_representations as in

 Fig icodes-fig High-level representations are close to the

 source_language and low-level representations are close to the

 target_machine Syntax trees are high_level they depict the

 natural hierarchical_structure of the source_program and are well

 suited to tasks like static type_checking



 figurehtfb







 A compiler might use a sequence of intermediate

 representations icodes-fig



 figure



 A low-level representation is suitable for machine-dependent tasks

 like register_allocation and instruction_selection Three-address

 code can range from high- to low-level depending_on the choice of

 operators For expressions the differences between syntax_trees

 and three-address_code are superficial as we_shall see in

 Section triples-subsect For looping statements

 for example a syntax_tree represents the components of a statement

 whereas three-address_code contains labels and jump instructions

 to represent the flow of control as in machine language



 The choice or design of an intermediate_representation varies from

 compiler to compiler An intermediate_representation may either be

 an actual language or it may consist of internal data_structures

 that are shared by phases of the compiler C is a programming

 language yet it is often used as an intermediate form

 because it is flexible it compiles into efficient machine code

 and its compilers are widely available The original C compiler

 consisted of a front_end that generated C treating a C compiler

 as a back end



























 3in1

 1

 1

 1





 Run-Time Environments

 run-time-ch



 A compiler must accurately implement the abstractions embodied in

 the source-language definition These abstractions typically

 include the concepts we discussed in Section plbasics-sect

 such_as names scopes bindings data types operators

 procedures parameters and flow-of-control constructs The

 compiler must cooperate with the operating_system and other

 systems software to support these abstractions on the target

 machine



 To do so the compiler creates and manages a run-time

 environment in which it assumes its target programs are being

 executed This environment deals_with a variety of issues such_as

 the layout and allocation of storage locations for the objects

 named in the source_program the mechanisms used by the target

 program to access variables the linkages between procedures the

 mechanisms for passing parameters and the interfaces to the

 operating_system inputoutput devices and other programs



 The two themes in this_chapter are the allocation of storage

 locations and access to variables and data We_shall discuss

 memory_management in some detail including stack allocation heap

 management and garbage_collection In the next chapter we

 present techniques for generating target code for many common

 language_constructs





























 1

 1

 1



 DAG

 RISC





 CISC



 Code_Generation

 codegen-ch



 The final phase in our compiler model is the code_generator It

 takes as input the intermediate_representation (IR) produced_by

 the front_end of the compiler along with relevant symbol_table

 information and produces as output a semantically equivalent

 target program as shown in Fig codegen-role-fig



 The requirements imposed on a code_generator are severe The

 target program must preserve the semantic meaning of the source

 program and be of high quality that is it must make effective

 use of the available resources of the target_machine Moreover

 the code_generator itself must run efficiently



 figure



 Position of code_generator codegen-role-fig

 figure



 The challenge is that mathematically the problem of generating

 an optimal target program for a given source_program is

 undecidable many of the subproblems encountered in code

 generation such_as register_allocation are computationally

 intractable In_practice we must_be content with heuristic

 techniques that generate good but not_necessarily optimal_code

 Fortunately heuristics have matured enough that a carefully

 designed code_generator can produce code that is several_times

 faster_than code produced_by a naive one



 Compilers that need to produce efficient target programs include

 an optimization_phase prior to code_generation The optimizer maps

 the IR into IR from which more_efficient code can be generated In

 general the code-optimization and code-generation phases of a

 compiler often referred to as the back end may make

 multiple passes_over the IR before generating the target program

 Code optimization is discussed in detail in

 Chapter_code-op-ch The techniques presented in this_chapter

 can be used whether or not an optimization_phase occurs before

 code_generation



 A code_generator has three primary tasks instruction_selection

 register_allocation and assignment and instruction ordering The

 importance of these tasks is outlined in

 Section issues-sect Instruction selection involves choosing

 appropriate target-machine_instructions to implement the IR

 statements Register_allocation and assignment involves deciding

 what values to keep in which registers Instruction ordering

 involves deciding in what order to schedule the execution of

 instructions



 This_chapter presents algorithms that code generators can use to

 translate the IR into a sequence of target language instructions

 for simple register machines The algorithms will be illustrated

 by using the machine_model in Section target-sect

 Chapter inst-para-ch covers the problem of code_generation

 for complex modern machines that support a great deal of

 parallelism within a single instruction



 After discussing the broad issues in the design of

 a code_generator we show what kind of target

 code a compiler needs to generate to support the

 abstractions embodied in a typical source_language

 In Section rt-storage-sect we outline implementations of

 static and stack allocation of data areas and show_how names in

 the IR can be converted into addresses in the target code



 Many code generators partition IR instructions into basic

 blocks which consist of sequences of instructions that are

 always executed together The partitioning of the IR into basic

 blocks is the subject of Section bb-sect The following

 section presents simple local transformations that can be used to

 transform basic_blocks into modified basic_blocks from which more

 efficient code can be generated These transformations are a

 rudimentary form of code_optimization although the deeper theory

 of code_optimization will not be taken up until

 Chapter_code-op-ch An_example of a useful local

 transformation is the discovery of common_subexpressions at the

 level of intermediate_code and the resultant replacement of

 arithmetic_operations by simpler copy operations



 Section_simple-cg-sect presents a simple code-generation

 algorithm that generates code for each statement in turn keeping

 operands in registers as_long as possible The output of this kind

 of code_generator can be readily improved by peephole optimization

 techniques such_as those discussed in the following

 Section peephole-sect



 The remaining sections explore instruction_selection and register

 allocation































 OUT

 IN

 OUT

 IN

 MFP

 MOP

 IDEAL

 1

 1

 1



 Machine-Independent Optimizations

 code-op-ch



 High-level language_constructs can

 introduce substantial run-time overhead if we naively translate each

 construct independently into machine code This_chapter discusses_how

 to eliminate many of these inefficiencies

 Elimination of unnecessary instructions in object code or the replacement of

 one sequence of instructions by a faster sequence of instructions that does

 the same_thing is usually called code_improvement or code

 optimization



 Local code_optimization

 (code improvement

 within a basic block) was_introduced in Section bb-opt-sect

 This_chapter deals_with global code_optimization

 where improvements take into_account what_happens across basic

 blocks

 We begin in Section_secopt-sources with a discussion of the

 principal opportunities for code_improvement



 Most_global optimizations are based_on data-flow

 analyses which are algorithms to gather information

 about a program The results of data-flow_analyses all have

 the same form for each instruction in the program they

 specify some property that must hold every time that instruction is

 executed The analyses differ in the properties they compute For

 example a constant-propagation analysis computes for each

 point in the program and for each variable used by the program whether

 that variable has a unique constant value at that point

 This information may be used to

 replace variable references by constant values for instance

 As_another example a liveness_analysis determines for

 each point in the program whether the value held by a particular

 variable at that point is sure to be

 overwritten_before it is read If so we do

 not need to preserve that value either in a register or in a memory

 location



 We introduce data-flow_analysis in Section_secdf-intro including

 several important_examples of the kind of information we gather globally

 and then use to improve the code

 Section secdf-foundation introduces the general_idea of a

 data-flow_framework of which the data-flow_analyses in

 Section_secdf-intro are

 special_cases

 We can use essentially

 the same algorithms for all these instances of data-flow

 analysis and we can

 measure the performance of these algorithms and show their correctness

 on all instances as_well

 Section_const-prop-sect is an example of the general framework

 that does more_powerful analysis than the earlier examples

 Then in Section_secpre we consider a powerful technique called

 partial_redundancy elimination for

 optimizing the placement of each expression evaluation in the program

 The solution to this problem requires the solution of

 a variety of different data-flow_problems



 In Section loops-sect we take_up the discovery and analysis of

 loops in programs

 The identification of loops leads to another family of algorithms for

 solving_data-flow problems that is based_on the hierarchical_structure

 of the loops of a well-formed (reducible) program

 This_approach to data-flow_analysis is covered in

 Section_region-sect

 Finally Section secinduction uses hierarchical analysis to

 eliminate

 induction_variables (essentially variables that count the number of

 iterations around a loop)

 This code_improvement is one of the most_important we can make for

 programs_written in commonly_used programming_languages























 Conclusion

 secconclusion























 This_paper shows that by using_BDDs it is possible to obtain

 efficient implementations of context-sensitive analyses using an

 extremely simple technique We clone all the methods in a call_graph

 one per context of interest and simply_apply a context-insensitive

 analysis over the cloned graph to get context-sensitive_results By

 numbering similar contexts contiguously the BDD is able to handle the

 exponential_blowup of contexts by exploiting their commonalities We

 showed that this approach can be applied to type_inference thread

 escape_analysis and even fully context-sensitive_points-to analysis on

 large_programs



 This_paper shows that we can create efficient BDD-based analyses

 easily By keeping data and analysis results as relations we can

 express queries and analyses in terms of Datalog The system we

 have developed automatically converts Datalog_programs into

 BDD implementations that are even more_efficient than those we have

 painstakingly hand-tuned



 Context-sensitive pointer_analysis is the cornerstone of deep program

 analysis for modern programming_languages By combining (1)

 context-sensitive_points-to results (2) a simple approach to context

 sensitivity and (3) a simple logic-programming based query framework

 we believe we have made it much_easier to create advanced program

 analyses





 In this_paper we presented a context-sensitive_inclusion-based

 pointer_alias analysis that scales to hundreds of thousands of Java

 bytecodes Our technique is based_on cloning the call_paths in the

 original_program and applying a simple context-insensitive algorithm

 to the expanded_call graph As a program gets large there is an

 exponential_blowup in the number of paths however we can represent

 the large_number of contexts efficiently using_BDDs and the set-based

 semantics of BDD_operations allow_us to compute the results for all

 contexts in parallel In essence the BDD data_structure automatically

 exploits the commonalities between different_contexts leading to

 an efficient_representation and an efficient algorithm



 We have implemented this technique on top of a BDD-based

 deductive database system that automatically_translates our

 declarative specifications into highly_efficient BDD code

 allows_us to abstract away the implementation details and reduce the

 analysis to just a few lines automatically handles the

 optimizations and transformations With we were_able to

 quickly and easily develop efficient implementations of other

 context-sensitive analyses and queries Our context-sensitive

 points-to_analysis can analyze even the largest of our benchmarks

 in under 19 minutes





 Our_approach to represent all analysis constraints and computed

 relations as BDDs and handle context_sensitivity by cloning has two

 important advantages (1) the results are readily accessible with BDD

 queries (2) it is easy to develop and implement new algorithms

 These advantages are significant in enabling researchers to create

 more and better techniques We hope to have demonstrated this point

 by developing two context-sensitive_pointer alias_analyses and four

 meaningful context-sensitive analyses that use the pointer_alias

 analysis results and applying them to twenty real large_programs





 Our implementation is available publicly at

 httpjoeqsourceforgenet We will post the information collected

 on the applications on their Sourceforce site when this_paper is

 published



 Code-Scheduling Constraints

 secilpconstraints



 Code scheduling is a form of program optimization that applies to the

 machine code that is produced_by the code_generator

 Code scheduling is subject to three kinds of constraints



 enumerate



 Control-dependence constraints All the operations executed in

 the original_program must_be executed in the optimized one



 Data-dependence constraints The operations in the optimized

 program must produce the same results as the corresponding ones in

 the original_program



 Resource constraints The schedule must not oversubscribe

 the resources on the machine



 enumerate



 These scheduling constraints guarantee that the optimized program

 produces the same results as the original However because code scheduling

 changes the order in which the operations execute the state of the

 memory at any one point may not match any of the memory states in a

 sequential_execution This situation

 is a problem if a program's execution is

 interrupted by for example a thrown exception or a user-inserted

 breakpoint Optimized programs are therefore harder to debug Note

 that this problem is not specific to code scheduling but applies to

 all other optimizations including partial-redundancy_elimination

 (Section secpre) and

 register_allocation (Section ra-sect)



 Data_Dependence

 dd-3kinds-subsect



 It is easy to see that if we change the execution order of two

 operations that do_not touch any of the same variables we cannot possibly

 affect their results In_fact even if these two

 operations read the same variable we can still permute

 their execution Only if an operation writes to a variable

 read or written by another can changing their execution order

 alter their results Such pairs of operations are said to share a

 data_dependence and their relative_execution order must_be

 preserved

 There_are three flavors of data_dependence



 enumerate



 True dependence read after write If a write is followed_by a read

 of the same_location the read depends_on the value written such

 a dependence is known_as a true dependence



 Antidependence write after read If a read is followed_by a write

 to the same_location we say that there is an antidependence from the

 read to the write The write does_not depend_on the read per se but

 if the write happens before the read then the read operation will

 pick up the wrong value Antidependence is a byproduct of

 imperative programming where the same memory_locations are used to

 store different values It is not a true dependence and

 potentially can be_eliminated by storing the values in different

 locations



 Output dependence write after write Two writes to the same_location

 share an output dependence If the dependence is violated the value of

 the memory_location written will have the wrong value after both

 operations are performed



 enumerate

 Antidependence and output dependences are referred to as storage-related

 dependences These are not true dependences and can be_eliminated

 by using different locations to store different values Note_that data

 dependences apply to both memory accesses and register accesses



 Finding Dependences Among Memory Accesses



 To check if two memory accesses share a data_dependence we only need to

 tell if they can refer to the same_location we do_not need to know

 which location is being accessed

 For_example we can tell that addresses given by a pointer and an

 offset from the same pointer cannot refer to the same_location

 even_though we may not know what points to

 Data_dependence

 is generally undecidable at_compile time The compiler must

 assume that operations may refer to the same_location unless it can

 prove otherwise



 ex

 Given the code sequence



 center

 tabularl_l l_l

 1) a 1



 2) p 2



 3) x a



 tabular

 center

 unless the compiler knows that cannot possibly point to

 it must conclude that the three operations need to execute

 serially There is an output dependence flowing from statement (1) to

 statement (2) and there are two true dependences flowing from

 statements (1) and (2) to statement (3)

 ex



 Data-dependence analysis is highly sensitive to the programming

 language used in the program For type-unsafe languages_like C and C

 where a pointer can be cast to point to any kind of object

 sophisticated analysis is necessary to prove independence between any

 pair of pointer-based memory accesses Even local or global scalar

 variables can be accessed indirectly unless we can prove that their

 addresses have not been stored anywhere by any instruction

 in the program In type-safe

 languages_like Java objects of different types are necessarily

 distinct from each other Similarly local primitive variables on the

 stack cannot be aliased with accesses through other names



 A correct discovery of data_dependences requires a number of different

 forms of analysis We_shall focus_on the major questions that must_be

 resolved if the compiler is to detect all the dependences that exist in

 a program and how to use this information in code scheduling

 Later chapters show_how these analyses are performed



 Array Data-Dependence Analysis



 Array data_dependence is the

 problem of disambiguating between the values of indexes in

 array-element accesses For_example the loop



 verbatim

 for_(i 0 i_n i)

 A2i A2i1

 verbatim

 copies odd elements in the array to the even elements just

 preceding them Because all the read and written locations in the

 loop are distinct from each other there are no dependences_between

 the accesses and all the iterations in the loop can execute in

 parallel Array data-dependence analysis often referred to simply as

 data-dependence analysis is very important for the optimization

 of numerical applications This topic will be discussed in detail in

 Section ch11datadep



 Pointer-Alias Analysis



 We_say that two pointers are aliased if they can refer to the same object Pointer-alias analysis

 is difficult because there are many potentially aliased pointers in a

 program and they can each point to an_unbounded number of dynamic objects

 over time To get any precision pointer-alias_analysis must_be

 applied across all the functions in a program This topic is

 discussed starting in Section andersen-sect



 Interprocedural_Analysis



 For languages that pass parameters by

 reference interprocedural_analysis is needed to determine if the same

 variable is passed as two or_more different arguments Such aliases

 can create dependences_between seemingly distinct parameters

 Similarly global variables can be used as parameters and thus create

 dependences_between parameter accesses and global variable accesses

 Interprocedural analysis discussed in Chapter ptr-ch is

 necessary to determine these aliases



 Tradeoff Between Register Usage and Parallelism

 secmem-trade-off



 In this_chapter we_shall assume that the machine-independent

 intermediate_representation of the source_program uses an

 unbounded_number of pseudoregisters to represent variables

 that can be allocated to registers These variables include scalar

 variables in the source_program that cannot be referred to by any

 other names as_well as temporary_variables that are generated_by the

 compiler to hold the

 partial results in expressions Unlike memory_locations registers

 are uniquely named

 Thus precise data-dependence constraints can be

 generated for register accesses easily



 The unbounded_number of pseudoregisters used in the intermediate

 representation must eventually be mapped to the small number of

 physical registers available on the target_machine Mapping several

 pseudoregisters to the same physical register has the unfortunate

 side_effect of creating artificial storage dependences that constrain

 instruction-level_parallelism Conversely executing instructions in

 parallel creates the need for more storage to hold the values being

 computed simultaneously Thus the goal of minimizing the number of

 registers used conflicts directly with the goal of maximizing

 instruction-level_parallelism Examples exsimple-reg

 and exreg-instr below illustrate this classic trade-off between

 storage and parallelism



 Hardware Register Renaming

 Instruction-level parallelism was first used in computer architectures

 as a means to speed_up ordinary sequential machine code Compilers at

 the time were not aware of the instruction-level_parallelism in the

 machine and were designed to optimize the use of registers They

 deliberately reordered instructions to minimize the number of

 registers used and as a result also minimized the amount of

 parallelism available Example exreg-instr illustrates_how

 minimizing register usage in the computation of expression trees also

 limits its parallelism



 There was so little parallelism left in the sequential code that

 computer architects invented the concept of hardware register renaming

 to undo the effects of register optimization in compilers

 Hardware register renaming

 dynamically changes the assignment of registers as the program_runs

 It interprets the machine code stores values intended for the same

 register in different internal registers and updates all their uses

 to refer to the right registers accordingly



 Since the artificial register-dependence constraints were_introduced by

 the compiler in the first place they can be_eliminated by using a

 register-allocation algorithm that is cognizant of instruction-level

 parallelism Hardware register renaming is still useful in the case

 when a machine's instruction set can only refer to a small number of

 registers This capability allows an implementation of the architecture to

 map the small number of architectural registers in the

 code to a much larger number of internal registers dynamically



 ex

 exsimple-reg

 The code below copies the values of variables in locations a and c

 to variables in locations b and d respectively

 using pseudoregisters

 t1 and t2



 verbatim

 LD t1 a t1 a

 ST b t1 b t1

 LD t2 c t2 c

 ST d t2 d t2

 verbatim

 If all the memory_locations accessed are known to be distinct from

 each other then the copies can proceed in parallel However if t1 and t2 are assigned the same register so as to minimize the

 number of registers used the copies are necessarily serialized

 ex



 figurehtb

 fileuullmanalsuch11figsmem-tradeeps

 Expression tree in Example reg-instr-ex

 figexpr11

 figure



 ex

 reg-instr-ex

 exreg-instr

 Traditional register-allocation techniques aim to minimize the number

 of registers used when performing a computation Consider the

 expression



 verbatim

 (a_b) c (d e)

 verbatim

 shown as a syntax_tree in Fig figexpr11 It is possible to perform

 this computation using three registers as illustrated by the

 machine code in Fig mem-trade2-fig



 figurehtfb

 center

 tabular l_l

 LD r1 a r1 a



 LD_r2 b r2 b



 ADD_r1 r1_r2 r1 r1r2



 LD_r2 c r2 c



 ADD_r1 r1_r2 r1 r1r2



 LD_r2 d r2 d



 LD r3 e r3 e



 ADD r2_r2 r3 r2 r2r3



 ADD_r1 r1_r2 r1 r1r2



 tabular

 center



 Machine code for expression of Fig figexpr11

 mem-trade2-fig



 figure



 The reuse of registers however serializes the computation The

 only operations allowed to execute in parallel are the loads of

 the values in locations a and b and the loads of

 the values in locations d and e It thus takes a

 total of 7 steps to complete the computation in parallel



 Had we used different

 registers for every partial sum the expression could be evaluated

 in 4 steps which is the height of the

 expression tree in Fig figexpr11

 The parallel computation is suggested by Fig mem-trade3-fig

 ex



 figurehtfb



 center

 tabularlllll

 r1 a r2 b r3 c r4 d r5 e



 r6 r1r2 r7 r4r5



 r8 r6r3



 r9 r8r7



 tabular

 center



 Parallel evaluation of the expression of Fig figexpr11

 mem-trade3-fig



 figure



 Phase Ordering Between Register_Allocation and Code

 Scheduling



 If registers are allocated before scheduling the resulting code tends

 to have many storage dependences that limit code scheduling On the

 other_hand if code is scheduled before register_allocation the

 schedule created may require so many registers that register spilling

 (storing the contents of a register in a memory_location so the

 register can be used for some other purpose)

 may negate the advantages of instruction-level_parallelism Should a

 compiler allocate registers first before it schedules the code Or

 should it be the other way round Or do we need to address these two

 problems at the same time



 To answer the questions above we must consider the characteristics of

 the programs being compiled Many nonnumeric applications do_not

 have that much available parallelism It suffices to dedicate a small

 number of registers for holding temporary results in expressions We

 can first apply a coloring algorithm as in Section coloring-subsect

 to allocate registers for all the

 nontemporary variables then schedule the code and finally

 assign registers to

 the temporary_variables



 This_approach does_not work for numeric applications where there are

 many more large expressions We can use a hierarchical approach where

 code is optimized inside out starting_with the innermost_loops

 Instructions are first scheduled assuming that every pseudoregister

 will be allocated its_own physical register Register_allocation is

 applied after scheduling and spill code is added where necessary and

 the code is then rescheduled This process is repeated for the code

 in the outer loops When several inner_loops are considered together

 in a common outer_loop the same variable may have_been assigned

 different registers

 We can change the register assignment to avoid

 having to copy the values from one register to another In

 Section secsp we_shall discuss the interaction between

 register_allocation and scheduling further in the context of a specific

 scheduling algorithm



 Control Dependence

 seccontdep



 Scheduling operations within a basic_block is relatively_easy because

 all the instructions are guaranteed to execute once control_flow

 reaches the beginning of the block Instructions in a basic_block can

 be reordered arbitrarily as_long as all the data_dependences are

 satisfied Unfortunately basic_blocks especially in nonnumeric

 programs are typically very small on average there are only about

 five instructions in a basic_block In_addition operations in the

 same block are often highly related and thus have little parallelism

 Exploiting parallelism across basic_blocks is therefore crucial



 An optimized program must execute all the operations in the original

 program It can execute more instructions than the original as_long

 as the extra instructions do_not change_what the program does

 Why would executing extra

 instructions speed_up a program's execution If we know that an

 instruction is likely to be executed and an idle resource is available

 to perform the operation for free we can execute the instruction speculatively The program_runs faster when the speculation turns_out

 to be correct



 An instruction is said to be control-dependent on

 instruction if the outcome of determines whether is to be

 executed The notion of control dependence corresponds to the concept

 of nesting levels in block-structured programs Specifically in

 the if-else statement



 verbatim

 if (c) s1 else s2

 verbatim

 s1 and s2 are control dependent on Similarly

 in the while-statement



 verbatim

 while (c) s

 verbatim

 the body is control dependent on



 ex

 excontdep

 In the code_fragment



 verbatim

 if (a t)

 b aa

 d ac

 verbatim

 the statements b aa and d ac have

 no data_dependence with any other part of the fragment The

 statement b aa depends_on the comparison The

 statement d ac however does_not depend_on the comparison

 and can be executed any

 time Assuming that the multiplication does_not cause any

 side_effects it can be performed speculatively as_long as

 is written only after is found to be greater_than

 ex



 Speculative Execution Support

 spec-exec-subsect



 Memory loads are one type of instruction that can benefit greatly

 from speculative execution Memory loads are quite common of course

 They have relatively long execution latencies addresses used

 in the loads are commonly available in advance and the result can be

 stored in a new_temporary variable without destroying the value of

 any other variable

 Unfortunately memory loads can raise exceptions if their addresses

 are illegal so speculatively accessing illegal addresses may

 cause a correct program to halt unexpectedly Besides mispredicted

 memory loads can cause extra cache_misses and page faults which are

 extremely costly



 ex

 In the fragment



 verbatim

 if_(p null)

 q p

 verbatim

 dereferencing speculatively will cause this correct program to

 halt in error if is null

 ex



 Many high-performance processors provide special features to support

 speculative memory accesses We mention the most_important ones next



 Prefetching



 The prefetch instruction was

 invented to bring data from memory to the cache before it is used A

 prefetch instruction indicates to the processor that the program

 is likely to use a particular memory word in the near future If the

 location specified is invalid or if accessing it causes a page fault

 the processor can simply ignore the operation Otherwise the

 processor will bring the data from memory to the cache if it is not

 already there



 Poison Bits



 Another architectural feature called poison bits was invented to

 allow speculative load of data from memory into the register file

 Each register on the machine is augmented with a poison bit If

 illegal memory is accessed or the accessed page is not in memory the

 processor does_not raise the exception immediately but instead just

 sets the poison bit of the destination register An exception is raised

 only if the contents of the register with a marked poison bit

 are used



 Predicated Execution



 Because branches are expensive and mispredicted branches are even

 more so (see_Section secarch) predicated instructions were

 invented to reduce the number of branches in a program A predicated

 instruction is like a normal instruction but has an extra predicate

 operand to guard its execution the instruction is executed only if

 the predicate is found to be true



 As an example a conditional move instruction CMOVZ R2R3R1 has

 the semantics that the contents of register R3 are moved to

 register R2 only if register_R1 is zero Code such

 as



 verbatim

 if (a 0)

 b cd

 verbatim

 can be_implemented with two machine_instructions assuming that

 and are allocated to registers R1

 R2 R4 R5 respectively as_follows



 verbatim

 ADD_R3 R4 R5

 CMOVZ R2 R3 R1

 verbatim

 This conversion replaces a series of instructions sharing a control

 dependence with instructions sharing only data

 dependences These instructions can then be_combined with adjacent

 basic_blocks to create a larger basic_block More_importantly with

 this code the processor does_not have a chance to mispredict thus

 guaranteeing that the instruction pipeline will run smoothly



 Predicated execution does come with a cost Predicated instructions

 are fetched and decoded even_though they may not be executed in the

 end Static schedulers must reserve all the resources needed for

 their execution and ensure_that all the potential data_dependences are

 satisfied Predicated execution should not be used aggressively

 unless the machine has many more resources than can possibly be used

 otherwise



 Dynamically Scheduled Machines

 The instruction set of a statically scheduled machine explicitly

 defines what can execute in parallel

 However recall from Section pipe-exec-subsect that some machine

 architectures allow the decision to be made at_run time

 about what can be executed in parallel

 With dynamic scheduling the

 same machine code can be run on different members of the same family

 (machines that implement the same instruction set) that have varying

 amounts of parallel-execution support In_fact machine-code

 compatibility is one of the major advantages of dynamically scheduled

 machines



 Static schedulers implemented in the compiler by software can help

 dynamic schedulers (implemented in the machine's hardware)

 better utilize machine resources To build a static

 scheduler for a dynamically scheduled

 machine

 we can use almost the same scheduling

 algorithm as for statically scheduled machines except that no-op instructions left in the schedule need not be generated

 explicitly The matter is discussed further in

 Section secdyn-inter



 A Basic Machine Model



 Many machines can be represented using the following simple model A

 machine consists of



 enumerate



 A set of operation types such_as loads stores

 arithmetic_operations and so on



 A vector representing hardware resources

 where is

 the number of units available of the th kind of resource Examples

 of typical resource types include memory access units ALU's and

 floating-point functional units



 enumerate



 Each operation has a set of input operands a set of output operands

 and a resource requirement Associated with each input operand is

 an input latency indicating when the input value must_be available

 (relative to the start of the operation) Typical

 input operands have zero latency meaning that the values are needed

 immediately at the clock when the operation is issued Similarly

 associated_with each output operand is an output latency which

 indicates when the result is available relative to the start of the

 operation



 Resource usage for each machine operation type is modeled by a

 two-dimensional resource-reservation_table The width of the

 table is the number of kinds of resources in the machine and its length

 is the duration over which resources are used by the operation

 Entry

 is the number of units of the th resource used by an

 operation of type clocks after it is issued For notational

 simplicity we assume if refers to a nonexistent entry in

 the table (ie is greater_than the number of clocks it takes to

 execute the operation)

 Of_course for any and must_be less_than or

 equal to the number of resources of type that the machine

 has



 Typical machine operations occupy only one_unit of resource at the

 time an operation is issued Some operations may use more_than one

 functional unit For_example a multiply-and-add operation may use a

 multiplier in the first clock and an adder in the second Some

 operations such_as a divide may need to occupy a resource for

 several clocks Fully pipelined operations are those that can

 be issued every clock even_though their results are not available

 until some number of clocks later We need not model the resources of

 every stage of a pipeline explicitly one single unit to represent the

 first stage will do Any operation occupying the first stage of a

 pipeline is guaranteed the right to proceed to subsequent stages in

 subsequent clocks





 TODO LATER



 Other Scheduling Constraints



 We_now discuss some common architectural features found in machines

 with instruction-level_parallelism In some_cases they can be

 handled with the basic model described above in some_cases we use

 separate passes to handle them before or after scheduling



 While we tend to focus_on the number of functional units on a

 processor they are relatively_small and inexpensive in comparison

 with the rest of the support to keep these functional units occupied

 In_particular it is desirable to have a large central register file

 with enough ports and interconnectivity so that every functional unit

 can use any of the registers as operands and write its result to any

 register This however is costly both in terms of the size of the

 data path as_well as the encoding space required in the instruction

 word As a result architectures often impose restrictions on what

 what can be executed in parallel For_example only certain

 combinations of parallel operations are allowed or instead of a

 single register file there may be smaller disjoint ones each feeding

 to a subset of the functional units All these restrictions reduce

 the generality of the machine and also increase the complexity of

 scheduling As the instruction set becomes less orthogonal it is

 harder for the scheduler to exploit the full potential of the

 machine



 description

 Instruction Formats In a VLIW machine all the operations to

 be executed in a single cycle are specified in a single instruction

 word For_example the Multiflow Trace computer has instruction words

 with 1024 bits which can issue 28 operations in a single clock It

 is common for the scheduler not to be_able to find useful work for

 each of these operation slots If the instruction word has a fixed

 length where a field is dedicated for each possible operation then

 the compiler would end up issuing many nop's in those fields

 This not only leads to larger code sizes but also poor utilization of

 the instruction fetch bandwidth which is one of the most precious

 resources on the processor



 One technique adopted for example in the Intel's IA64 architecture

 is to use a variable instruction length Extra bits are dedicated to

 indicate the number of operations to be issued in parallel thus

 eliminating the need for wasted bits to encode nop's The

 scheduler can simply assume a naive horizontal format and a simple

 postpass can encode the code accordingly



 Another_approach is to have a small discrete number of formats such

 as one for specifying single operations and one for specifying three

 operations The compiler can choose to use wide instruction words

 only when there is sufficient parallelism or only in innermost_loops

 Often simple strategies that use a postpass after code scheduling to

 select the format suffice If a postpass approach is infeasible then

 we may wish to model the instruction format like a resource constraint

 and only operations that are compatible can be scheduled in the same

 clock cycle



 Code Alignment How fast can an instruction word be fetched

 from memory may depend_on its alignment in memory If the instruction

 word straddles two cache_lines it make take longer to fetch the

 instruction into the processor



 Register Banks



 Interconnectivity

 description





 figurehtfb



 center

 tabularr_l l_l

 1) a b



 2) c_d



 3) b_c



 4) d a



 5) c_d



 6) a b



 tabular

 center



 A sequence of assignments exhibiting data_dependences

 data-dep-exer-fig



 figure



 exer

 The assignments in Fig data-dep-exer-fig have certain

 dependences For each of the following pairs of statements classify

 the dependence as true dependence antidependence

 output dependence or no dependence (ie the

 instructions can appear in either order)



 itemize



 a) Statements (1) and (4)

 b) Statements (3) and (5)

 c) Statements (1) and (6)

 d) Statements (3) and (6)

 e) Statements (4) and (6)



 itemize

 exer



 sexer

 par-reg-exer

 Evaluate the expression

 exactly as parenthesized (ie do_not use the commutative or

 associative laws to reorder the additions) Give register-level machine

 code to provide the maximum possible parallelism

 sexer



 exer

 Repeat_Exercise par-reg-exer for the following expressions



 itemize

 a)

 b)

 itemize

 If instead of maximizing the parallelism we minimized the number of

 registers how many steps would the computation take How many steps do

 we save by using maximal parallelism



 exer



 exer

 The expression of Exercise par-reg-exer can be executed by the

 sequence of instructions shown in Fig data-dep2-exer-fig

 If we have as much parallelism as we need how many steps are needed to

 execute the instructions

 exer



 figurehtfb



 center

 tabularr_l l

 1) LD r1 u r1 u



 2) LD_r2 v r2 v



 3) ADD_r1 r1_r2 r1_r1 r2



 4) LD_r2 w r2 w



 5) LD r3 x r3 x



 6) ADD r2_r2 r3 r2_r2 r3



 7) ADD_r1 r1_r2 r1_r1 r2



 8) LD_r2 y r2 y



 9) LD r3 z r3 z



 10) ADD r2_r2 r3 r2_r2 r3



 11) ADD_r1 r1_r2 r1_r1 r2



 tabular

 center



 Minimal-register implementation of an arithmetic expression

 data-dep2-exer-fig



 figure



 hexer

 Translate the code_fragment discussed in Example excontdep using

 the CMOVZ conditional copy instruction of

 Section spec-exec-subsect What are the data_dependences in your

 machine code

 hexer

 Synchronization Between Parallel Loops

 ch11const-sync



 Most programs have no parallelism if we do_not allow processors to

 perform any synchronizations But adding even a small constant number of

 synchronization operations to a program can expose more

 parallelism We_shall first discuss parallelism made possible by a

 constant number of synchronizations in this_section and the general

 case_where we embed synchronization operations in loops in the next



 A Constant Number of Synchronizations



 Programs with no_synchronization-free parallelism may contain a

 sequence of loops some of which are parallelizable if they are

 considered independently We can parallelize such loops by

 introducing synchronization_barriers before and after their execution

 Example_exadi illustrates the point



 figurehtfb

 verbatim

 for_(i 1_i n_i)

 for_(j 0 j_n j)

 Xij f(Xij Xi-1j)

 for_(i 0 i_n i)

 for_(j 1_j n_j)

 Xij g(Xij Xij-1)

 verbatim



 Two sequential loop_nests

 adi-fig



 figure

 ex

 exadi

 In Fig adi-fig is a program representative of an ADI

 (Alternating Direction Implicit) integration algorithm There is no

 synchronization-free_parallelism Dependences in the first loop_nest

 require that each processor works on a column of array however

 dependences in the second loop_nest require that each processor works

 on a row of array For there to be no communication the entire

 array has to reside on the same processor hence there is no

 parallelism We observe however that both loops are independently

 parallelizable



 One_way to parallelize the code is to have different_processors work

 on different columns of the array in the first loop synchronize and

 wait for all processors to finish and then operate_on the

 individual rows In this way all the computation in the algorithm

 can be_parallelized with the introduction of just

 one synchronization operation

 However we note_that while only one synchronization is

 performed this parallelization requires almost all the data in matrix

 to be transferred between processors It is possible to reduce

 the amount of communication by introducing more synchronizations

 which we_shall discuss in Section ch11par

 ex



 It may appear that this approach is applicable only to programs consisting

 of a sequence of loop_nests However

 we can create additional opportunities for

 the optimization through code transforms We can apply loop fission

 to decompose loops in the original_program into several

 smaller loops which can then be_parallelized individually by

 separating them with barriers We illustrate this technique with

 Example_exscc2



 ex

 exscc2

 Consider the following loop



 verbatim

 for (i1 in i)

 Xi Yi Zi (s1)

 WAi Xi (s2)



 verbatim

 Without knowledge of the values in array we must assume that the

 access in statement may write to any of the elements of

 Thus the instances of must_be executed sequentially in the

 order they are executed in the original_program



 There is no_synchronization-free parallelism and

 Algorithm_algnosync will simply assign all the computation to

 the same processor

 However at the least instances of statement

 can be executed in parallel We can parallelize part of this code by

 having different_processors perform difference instances of statement Then

 in a separate sequential loop one processor say numbered 0 executes

 as in the SPMD code shown in Fig one-synch-fig

 ex



 figurehtfb

 verbatim

 Xp Yp Zp (s1)

 synchronization_barrier

 if_(p 0)

 for (i1 in i)

 WAi Xi (s2)

 verbatim



 SPMD code for the loop in Example_exscc2 with

 being a variable holding the processor ID

 one-synch-fig

 figure



 Program-Dependence Graphs



 To_find all the parallelism made possible by a constant number of

 synchronizations we can apply fission to the original_program

 greedily

 Break up loops into as many separate loops as possible

 and then parallelize each loop independently



 To expose all the opportunities for loop fission we use the

 abstraction of a program-dependence graph (PDG) A program

 dependence graph of a program is a graph whose nodes are the

 assignment statements of the program and whose edges capture the data

 dependences and the directions of the data_dependence between

 statements An edge from statement to statement exists

 whenever some dynamic_instance of shares a data_dependence with

 a later dynamic_instance of



 To construct the PDG for a program we first find the data_dependences

 between every pair of (not_necessarily distinct) static accesses in every

 pair of (not_necessarily distinct) statements Suppose we determine

 that there is a dependence_between access in statement and

 access in statement Recall that an instance of a

 statement is specified_by an index vector

 where is the loop_index of the th

 outermost_loop in which the statement is embedded



 enumerate

 If there_exists a data-dependent pair of instances of

 and of and is executed before

 in the original_program written

 then there is an edge from to



 Similarly if there_exists a data-dependent pair of instances

 of and of and

 then there is an edge from to



 enumerate

 Note_that it is possible for a data_dependence between two statements

 and to generate both an edge from to and an

 edge from back to



 In the special_case where statements and are not distinct

 if and only if

 ( is lexicographically less_than

 ) In the general case and may be different

 statements possibly belonging to different loop_nests



 ex

 For the program of Example_exscc2 there are no dependences among

 the instances of statement However the th instance of

 statement must follow the th instance of statement

 Worse since the reference may write any element of array

 the th instance of depends_on all previous instances of

 That is statement depends_on itself

 The PDG for the program of Example_exscc2 is shown

 in Fig scc2-fig Note_that there is one cycle in the graph containing

 only

 ex



 figurehtfb

 fileuullmanalsuch11figsscc2eps

 Program-dependence graph for the program of

 Example_exscc2

 scc2-fig

 figure



 The program-dependence graph makes it easy to determine if we can

 split statements in a loop Statements connected in a cycle in a

 PDG cannot be split If is a dependence_between

 two statements in a

 cycle then some instance of must execute before some instance

 of and vice_versa Note_that this mutual dependence occurs

 only if and

 are embedded in some common loop Because of the mutual

 dependence we cannot execute all instances of one statement before

 the other and therefore loop fission is not allowed On the other

 hand if the dependence

 is unidirectional we can split up the loop

 and execute all the instances of first then

 those of



 figurehtfb



 verbatim

 for_(i 0 i_n i)

 Zi Zi Wi (s1)

 for_(j i_j n_j)

 Xij YijYij (s2)

 Zj Zj Xij (s3)





 verbatim



 center

 (a) A program

 center



 fileuullmanalsuch11figsscc3eps



 center

 (b) Its dependence graph

 center



 Program and dependence graph for

 Example exconst-sync

 figconst-sync



 figure



 ex

 exconst-sync

 Figure figconst-sync(b) shows the program-dependence graph

 for the program of Fig figconst-sync(a)

 Statements and belong to a cycle in the graph and therefore

 cannot be placed in separate loops We can however split statement

 out and execute all its instances before executing the rest of

 the computation as in Fig const-sync2-fig

 The first loop is parallelizable but the second is not We can

 parallelize the first loop by placing barriers before and after its

 parallel_execution

 ex



 figurehtfb



 verbatim

 for_(i 0 i_n i)

 for_(j i_j n_j)

 Xij YijYij (s2)

 for_(i 0 i_n i)

 Zi Zi Wi (s1)

 for_(j i_j n_j)

 Zj Zj Xij (s3)



 verbatim



 Grouping strongly_connected components of a loop_nest

 const-sync2-fig



 figure



 Hierarchical Time



 While the relation

 can be very_hard to compute in general there is a

 family of programs to which the optimizations of this_section are

 commonly applied and for which there is a straightforward way to

 compute dependencies

 Assume that the program is

 block structured consisting of loops and simple arithmetic_operations

 and no other control constructs A statement in the program is either

 an assignment_statement a sequence of statements or a loop construct

 whose body is a statement The control structure thus represents a

 hierarchy At the top of the hierarchy is the node representing the

 statement of the whole program An assignment_statement is a leaf

 node If a statement is a sequence then its_children are the

 statements within the sequence laid_out from left to right according

 to their lexical order If a statement is a loop then its_children are

 the components of the loop body which is typically a sequence of one or

 more statements



 figurehtfb



 verbatim

 s0

 L1 for_(i 0 )

 s1

 L2 for_(j 0 )

 s2

 s3



 L3 for (k 0 )

 s4

 s5



 verbatim



 A hierarchically structured program

 hier-source-fig



 figure



 ex

 exhier

 The hierarchical_structure of the program in Fig hier-source-fig

 is shown in

 Fig fighier

 The hierarchical nature of the execution sequence is highlighted in

 Fig fighier1 The single instance of precedes all

 other operations because it is the first statement executed Next we

 execute all instructions from the first iteration of the outer_loop

 before those in the second iteration and so forth For all dynamic

 instances whose loop_index has value 0 the statements

 and are executed in lexical order We can repeat

 the same argument to generate the rest of the execution order

 ex



 figurehtfb



 fileuullmanalsuch11figsprog-hiereps



 Hierarchical structure of the program in

 Example exhier

 fighier

 figure



 figurehtfb



 arrayrlllll

 1s0



 2L1 i0 s1



 3 L2 j0 s2



 4 s3



 5 j1 s2



 6 s3



 7



 8 L3 k0 s4



 9 k1 s4



 10



 11 s5



 12 i1 s1



 13



 array



 Execution order of the program in Example exhier

 fighier1

 figure



 We can resolve the ordering of two instances from two different

 statements in a hierarchical manner If the statements share common

 loops we compare the values of their common loop_indexes starting

 with the outermost_loop As soon_as we find a difference_between

 their index values the difference determines the ordering Only if

 the index values for the outer loops are the same do we need to

 compare the indexes of the next inner_loop This process

 is analogous to how

 we would compare time expressed in terms of hours minutes and

 seconds To compare two times we first compare the hours and only

 if they refer to the same hour would we compare the minutes and so

 forth If the index values are the same for all common loops then we

 resolve the order based_on their relative lexical placement

 Thus the execution order for the simple nested-loop programs we have

 been discussing is often referred to as hierarchical time



 Let be a statement_nested in a -deep_loop and in

 a -deep_loop sharing common (outer) loops note and

 certainly Suppose

 is an instance of and

 is an instance of



 if

 and only if either



 enumerate



 or



 and appears

 lexically before



 enumerate



 The predicate can be

 written as a disjunction of linear_inequalities



 (i1 j1) (i1 j1 i2 j2) (i1 j1 id-1 jd-1 id jd)





 A PDG edge from to exists as_long as the

 data-dependence condition and one of the disjunctive clauses can be

 made true

 simultaneously Thus we may need to solve up to or

 linear integer programs depending_on whether appears lexically

 before to determine the existence of one edge



 The Parallelization Algorithm



 We_now present a simple algorithm that first splits up the

 computation into as many different loops as possible then

 parallelizes them independently



 alg

 alg1sync

 Maximize the degree of parallelism allowed by

 synchronizations



 A program with array_accesses



 SPMD code with a constant number of synchronization_barriers



 enumerate

 Construct the program-dependence graph and partition the statements

 into strongly_connected components (SCC's) Recall from

 Section cyc-dep-subsect that a strongly_connected component is a

 maximal subgraph of the original whose every node in the subgraph can

 reach every other node



 Transform the code to execute SCC's in a topological_order by_applying

 fission if necessary



 Apply_Algorithm algnosync to each SCC to find all of its

 synchronization-free_parallelism Barriers are inserted before and

 after each parallelized SCC



 enumerate

 alg



 While Algorithm_alg1sync finds all degrees of parallelism with

 synchronizations it has a number of weaknesses First it may

 introduce unnecessary synchronizations As a matter of fact if we

 apply this algorithm to a program that can be_parallelized without

 synchronization the algorithm will parallelize each statement

 independently and introduce a synchronization_barrier between the

 parallel loops executing each statement Second while there may only

 be a constant number of synchronizations the parallelization scheme

 may transfer a lot of data among processors with each

 synchronization In some_cases the cost of communication makes the

 parallelism too_expensive and we may even be better off executing the

 program sequentially on a uniprocessor In the following sections we

 shall next take_up ways to increase data_locality and thus reduce the

 amount of communication



 sexer

 sync1-exer

 Apply_Algorithm alg1sync to the code of

 Fig sync1-exer-fig

 sexer



 figurehtfb



 verbatim

 for_(i0 i100_i)

 Ai Ai Xi (s1)

 for_(i0 i100_i)

 for_(j0 j100_j)

 Bij Yij Ai Aj (s2)

 verbatim



 Code for Exercise sync1-exer

 sync1-exer-fig



 figure



 exer

 sync2-exer

 Apply_Algorithm alg1sync to the code of

 Fig sync2-exer-fig

 exer



 figurehtfb



 verbatim

 for_(i0 i100_i)

 Ai Ai Xi (s1)

 for_(i0 i100_i)

 Bi Bi Ai (s2)

 for_(j0 j100_j)

 Cj Yj Bj (s3)



 verbatim



 Code for Exercise sync2-exer

 sync2-exer-fig



 figure



 exer

 sync3-exer

 Apply_Algorithm alg1sync to the code of

 Fig sync3-exer-fig

 exer



 figurehtfb



 verbatim

 for_(i0 i100_i)

 Ai Ai Xi (s1)

 for_(i0 i100_i)

 for_(j0 j100_j)

 Bj Ai Yj (s2)

 Ci Bi Zi (s3)

 for_(j0 j100_j)

 Dij Ai Bj (s4)



 verbatim



 Code for Exercise sync3-exer

 sync3-exer-fig



 figure



 Constant Propagation



 All the data-flow_schemas discussed in Section are

 actually simple examples of distributive frameworks with finite_height

 Thus the iterative Algorithm applies to them in

 either its forward or backward version and produces the solution

 in each case

 In this_section we_shall examine in detail a useful data-flow

 framework with more interesting properties



 Recall that constant_propagation or constant_folding

 replaces expressions that

 evaluate to the same constant every time

 they are executed by that constant

 The constant-propagation framework described below

 is different from all the data-flow

 problems discussed so_far in that







 a)

 it has an_unbounded set of possible

 data-flow_values even for a fixed flow_graph and



 b)

 it is not_distributive





 Constant propagation is a forward_data-flow problem The semilattice

 representing the data-flow_values and the family of transfer_functions

 are presented next



 Data-Flow Values for the Constant-PropagationFramework



 The set of data-flow_values is a product_lattice with one component

 for

 each variable in a program

 The lattice for a single variable consists of the following







 All constants appropriate for the type of the variable



 The value which stands_for not-a-constant A variable is

 mapped to this value if it is determined not to have a constant

 value The variable may have_been assigned an input value or derived_from a

 variable that is not a constant or assigned different constants along

 different paths that lead to the same program point



 The value which stands_for undefined

 A variable is assigned this value if nothing may yet be asserted

 presumably no definition of the variable has

 been discovered to reach the point in question





 Note_that and are not the same they are essentially

 opposites

 says we have_seen so many_ways a variable could be defined

 that we know it is not constant

 says we have_seen so little about the variable that we

 cannot say anything at all



 The semilattice for a typical integer-valued variable is shown in

 Fig Here the top_element is

 and the bottom_element is That is the greatest

 value in the partial_order is and the least is



 The constant

 values are unordered but they are all

 less_than and greater_than

 As_discussed in Section the

 meet of two values is their greatest_lower bound Thus

 for all values







 For any constant





 and given two distinct constants and







 figureuullmanalsuch9figsconstant-latticeeps

 Semilattice representing the possible values of a single

 integer variable



 A data-flow value for this framework

 is a map from each variable in the program to one of

 the values in the constant semilattice The value of a variable

 in a map is denoted_by



 The Meet for the Constant-Propagation Framework



 The semilattice of

 data-flow_values

 is simply the product of the semilattices like

 Fig one

 for each variable Thus if and only if

 for all variables we have



 Put_another way if for

 all variables



 Transfer_Functions for the Constant-PropagationFramework



 We assume in the following that a basic_block contains only one

 statement Transfer_functions for basic_blocks containing

 several statements can be constructed by_composing the functions

 corresponding to individual_statements

 The set consists of certain

 transfer_functions that accept a map of variables to values in the

 constant lattice

 and return another such map



 contains the identity_function which takes a map as input and

 returns the same map as output

 also contains

 the constant transfer_function for the entry_node

 This transfer_function given any input map

 returns a map where for all variables

 This boundary_condition makes_sense because before executing any

 program statements there are no definitions for any variables



 In_general

 let be the transfer_function of statement and let and

 represent data-flow_values

 such that

 We_shall describe in terms of the relationship_between and









 If is not an assignment_statement then is simply the

 identity_function



 If is an assignment to variable then

 for all variables

 and is defined as_follows





 If the right-hand-side (RHS) of the statement is a constant then





 If the RHS is of the form thenAs usual

 represents a generic_operator not_necessarily addition





















 If the RHS is any other expression (eg a function call or assignment

 through a pointer) then







 Monotonicity of the Constant-PropagationFramework



 Let_us show that the constant_propagation framework is monotone First

 we can consider the effect of a function on a single variable

 In all

 but case 2(b) either does_not change the value of or

 it changes the map to return a constant or In these cases

 must surely be

 monotone



 For case 2(b) the effect of is tabulated in

 Fig The first and second columns represent

 the possible input values of and the last

 represents the output value of The values are

 ordered from the greatest to the smallest in each column or subcolumn

 To show that the function is monotone we check that for each

 possible input value of the value of

 does_not get bigger as the value of gets smaller

 For_example in

 the case_where

 has a constant value as the value of varies from

 to to the value of varies from

 to and then to respectively

 We can repeat this procedure for all the possible values of

 Because of symmetry we do_not even need to repeat the procedure for

 the second operand before we conclude that the output value cannot get

 larger as the input gets smaller











 The constant-propagation transfer_function for x_yz





 Nondistributivity of the Constant-PropagationFramework



 The constant-propagation framework as defined is monotone but not

 distributive That is the iterative solution

 is safe but may be smaller_than the

 solution An_example will prove that the

 framework is not_distributive





 figureuullmanalsuch9figsnon-distributiveeps

 An_example demonstrating that the constant_propagation

 framework is not_distributive





 In the program in Fig and

 are set to 2 and 3 in block and to 3 and 2 respectively in

 block We know that regardless of which path is taken the

 value of at the end of block is 5 The iterative

 algorithm does_not discover this fact

 however Rather it applies

 the meet_operator at the entry of getting as the values

 of and Since adding two yields a the

 output produced_by Algorithm is that at the exit

 of the program This result is safe but imprecise

 Algorithm is

 imprecise because it does_not keep_track of the correlation that

 whenever is 2 is 3 and vice_versa

 It is possible but significantly more_expensive to use a more_complex

 framework that tracks all the possible equalities that hold among

 pairs of expressions_involving the variables in the program this

 approach is discussed in Exercise



 Theoretically we can attribute this loss of precision to the

 nondistributivity of the constant_propagation framework

 Let and be the transfer_functions representing blocks

 and respectively

 As shown in Fig









 rendering the framework nondistributive









 Example of nondistributive transfer_functions



 Interpretation of the Results



 The value is used in the iterative_algorithm for two purposes

 to initialize the entry_node and to initialize

 the interior_points of the program before the iterations The meaning

 is slightly_different in the two cases The first says_that variables

 are undefined at the beginning of the program execution the second says_that

 for lack of information at the beginning of the iterative process we

 approximate the solution with the top_element At the end of

 the iterative process the variables at the exit of the entry

 node will still hold the value since

 never changes



 It is possible that may

 show up at some other program points When they do it means that no

 definitions have_been observed for that variable along any of the

 paths_leading up to that program point Notice_that with the

 way we define the meet_operator as_long as there_exists a path that

 defines a variable reaching a program point the variable will not have an

 value If all the definitions_reaching a

 program point have the same constant value

 the variable is considered a constant

 even_though it may not be defined along some program path



 By assuming that the program is correct the algorithm can find more

 constants than it otherwise would That is the algorithm

 conveniently chooses some values for those possibly

 undefined_variables in order to make the program more_efficient This change

 is legal

 in most programming_languages since undefined_variables are

 allowed to take on any value If the language semantics requires that

 all undefined_variables be given some specific value then we must

 change our problem formulation accordingly And if instead we

 are_interested in finding possibly undefined_variables in a program

 we can formulate a different data-flow_analysis to

 provide that result (see Exercise )





 figureuullmanalsuch9figscorr-patheps

 Meet of and a constant





 In Fig the values of are 10 and

 at the exit of basic_blocks and respectively Since

 the value of is 10 on entry to block

 Thus block where is used can

 be optimized by_replacing by 10

 Had the path executed

 been

 the value of reaching basic_block

 would have_been undefined So it appears incorrect to replace the

 use of by 10



 However if it is impossible for predicate to be false while is

 true

 then this execution

 path never occurs While the programmer may be aware of that

 fact it may well be beyond the capability of any data-flow

 analysis to determine

 Thus if we assume that the program is correct and that all the variables

 are defined before they are used it is indeed correct that the value

 of at the beginning of basic_block can only be 10

 And if the program is incorrect to begin_with then

 choosing 10 as the value

 of cannot be worse than allowing to assume some random

 value





 Suppose we_wish to detect all possibility of a variable being uninitialized

 along any path to a point where it is used How_would you modify the

 framework of this_section to detect such situations





 An interesting and powerful data-flow-analysis framework is obtained_by

 imagining the domain to be all possible partitions of expressions so

 that two expressions are in the same class if and only if they are

 certain to have the same value along any path to the point in question

 To_avoid having to list an infinity of expressions we can represent

 by listing only the minimal pairs of equivalent expressions For

 example if we execute the statements





 then the minimal set of equivalences is

 From these follow other equivalences such_as and

 but there is no need to list these explicitly







 a)

 What is the appropriate meet_operator for this framework



 b)

 Give a data_structure to represent domain values and an algorithm to

 implement the meet_operator



 c)

 What are the appropriate functions to associate with statements

 Explain the effect that a statement such_as a bc should have on

 a partition of expressions (ie on a value in )



 d)

 Is this framework monotone Distributive



 Context Sensitivity

 seccontext

















 Our_approach to context_sensitivity is to create the abstraction of a

 context-sensitive call_graph where each method is cloned by its

 number of contexts and have each cloned method call the

 corresponding cloned target In this way a context-insensitive

 algorithm can be used to generate context-sensitive_results by simply

 applying to the cloned methods and their invocation_edges



 There_are many different_ways in which we can define context

 sensitivity We have created two different pointer_alias analyses

 one based_on call_paths and the other on receiver objects The former

 illustrates the simpler case_where the context-sensitive call_graph is

 available prior to context-sensitive_points-to analysis The latter

 requires the context-sensitive call_graph be generated on the fly as

 the context-sensitive_points-to results are produced



 Call Paths

 contextpathbased



 We start by presenting the context-sensitive_points-to algorithm

 assuming that a context-sensitive call_graph is pre-computed ahead of

 time We then define what we mean by call-path based sensitivity and

 how we generate the context-sensitive call_graph used in the

 context-sensitive_points-to analysis



 Context-Sensitive_Pointer Analysis with a Pre-computed

 Context-Sensitive Call_Graph



 A clone of method is identified by a context Each

 cloned method has its_own cloned variables cloned

 invocation_sites each of which is connected by its_own

 invocation_edge to the cloned target



 Besides cloning methods we can also clone the heap_objects to

 increase the precision of the analysis For each invocation_site

 in context given an invocation_edge the returned

 object is given context



 alg Context-sensitive points-to_analysis with a pre-computed

 call_graph

 algcs

 alg



 description

 Context C C is the domain of all contexts

 description





 description

 Context-sensitive invocation_edges C_I

 C_M

 iff invocation_site in context calls method in context

 description





 description

 Context-sensitive arguments and return values C

 V_C V

 if variable in context includes the

 points-to set of variable in context due to parameter

 passing



 Context-sensitive variable points-to C_V

 C_H if variable

 in context can point to heap_object in context



 Context-sensitive heap points-to C_H

 F C_H

 if field of heap_object

 of context can point to heap_object of context

 description



















 equation

 (ci i cm m) (m z v1) formal (i z v2) actual

 (ci v1 cm v2)

 cArule

 equation



 equation

 (v h) vP0 (ch h cm m)

 (ch v cm h)

 crule0

 equation



 equation



 arrayc

 (cv1_v1 cv2 v2) (cv2_v2 ch h)



 (v1 h) array



 (cv1_v1 ch h)

 crule1

 equation



 equation



 arrayc

 (v1_f v2) S (cv1_v1 ch1 h1)



 (cv2_v2 ch2_h2) array



 (ch1 h1 f ch2_h2)

 crule2

 equation



 equation



 arrayc

 (v1_f v2) L (cv1_v1 ch1 h1)



 (ch1 h1 f ch2_h2) (v2 h2) array



 (cv2_v2 ch2_h2)

 crule3

 equation

 Rule cArule interprets the context-sensitive_invocation edges

 to find the bindings between actual and formal_parameters The rest

 of the rules are the context-sensitive counterpart to those found in

 Algorithm_algci-types with contexts added to qualify variables

 and heap_objects



 Call-Path-Based Abstraction

 contextcallpath



 A commonly_used abstraction in context_sensitivity is the notion of a

 call path which is a sequence of callers and their invocation_sites

 from the entry point of the program leading up to a method being

 executed A call path is thus a sequence of invocation_edges

 such that is an

 invocation_site in an entry method typically main and is

 an invocation_site in method for all (Other

 entry methods in typical_programs are static class initializers

 object finalizers and thread run methods)



 There_are many possible call_paths in a program unbounded in the

 presence of recursion and exponentially_many with_respect to the

 number of methods in a program otherwise One common approach to

 limiting the size of a call path is to limit it to some constant

 number of closest callees We choose to be fully context-sensitive

 for programs without recursion that is we create a clone for each

 acyclic_path that can reach a method In the presence of recursion

 we collapse the recursive_cycles That is nodes in the same strongly

 connected_components are placed in the same equivalence_class We

 create a clone for method for each acyclic_path of connected

 components reaching method This notion is most similar to that

 proposed by Emami Under this definition of context_sensitivity large_programs can have

 many_contexts As an example one program with 2412 methods has contexts

 If we arbitrarily assign integers to different_contexts in the program

 the representation of the context-sensitive_invocation edges is

 prohibitively large More_importantly the results among different

 contexts for the same method have many similarities We must_be able

 to take_advantage of this common structure for the results to scale

 reasonably For these reasons we number the contexts for each method

 consecutively If is the number of contexts for method then

 its contexts are numbered 0 through If a caller has

 contexts then its callee has at_least contexts We choose a

 numbering such that a caller's contexts differ from the matching

 callee's at an invocation_site by the same offset



 alg Generating context-sensitive_invocation edges from a

 call_graph

 algcontext

 alg

 A multigraph_where is a

 set of vertices is a set of invocation_sites

 is a set of edges such that iff





 A relation that maps a

 calling_context and a given invocation to the callee's context and vertex



 Let be a

 graph where entire strongly_connected components in are reduced to a

 single vertex That is let be the strongly_connected

 component representing vertex in



 Es (s(v1)i s(v2)) (v1 i v2) E s(v1)

 s(v2)



 All vertices within a strongly_connected component have the same

 number of contexts A strongly_connected component has as many

 contexts as the sum of all its predecessor vertices' contexts Let

 be the number of contexts given to a strongly_connected

 component



 s



 arrayll

 1 (s'is) Es



 (s'is) Es s' otherwise

 array







 We define a total order

 for all edges to the same component





 (s'is)



 arrayll

 0 s' s



 (sis) v (s'is) s otherwise

 array









 R (c i c (s(v')is(v)) v)

 (v' i v) E 0 c v'-1







 Let_us discuss the cost of creating such a BDD to represent the

 context-sensitive_invocation edges For each invocation_site we have

 to create a large_number of relations that map a large_number of

 contexts to a large_number of contexts all separated_by a common

 offset Creating a set of consecutive numbers using available BDD

 operations is expensive It is simple to construct a BDD directly to

 represent a set of consecutive numbers For_example a boolean

 function that accepts 4-bit numbers in the range 013 is



 f(b3b2b1b0)

 b3 b2 b1



 We have added an operator called range to directly such a BDD

 to represent a range of numbers The definition of the operator is

 simple though tedious and is omitted here due to space constraints

 It suffices to say that range runs in O() BDD_operations where

 is the number of bits in the domain Once such a BDD is created

 efficient add operation exists to generate the corresponding contexts

 offset by a constant





 Given a domain a low value and a high value VarRange generates a

 BDD which is the disjunction of the values from low to high

 inclusive The algorithm works by counting down from the high-order

 bits of the number setting bits until it finds a situation where the

 remainder of the bits can be marked as don't-care It then unions

 the BDD and narrows the range The algorithm_terminates when the

 range narrows to zero VarRange runs in O() BDD_operations where

 is the number of bits in the domain Due to space constraints we

 have omitted the pseudocode for the algorithm and will include that in

 the final copy Show an example





 Context-Sensitive Invocation Edges



 For non-allocation sites we compute the context-sensitive_invocation

 edges directly by_applying Algorithm_algcontext to the

 call_multigraph where is the set of

 methods and if and only if





 Allocation sites are treated differently from the rest of invocation

 sites because the contexts of their target are used to name heap

 objects As points-to_analysis is highly sensitive to the number of

 objects in the system we cannot afford to use the full call path

 sensitivity for allocation_sites On the other_hand the convention

 of simply using the allocation_site as identification is too

 imprecise For programs use the same method to allocate all objects

 of the same class such is the case of the factory design pattern

 this naming scheme degenerates to a simple type analysis To gain

 better precision we consider methods that return freshly allocated

 objects as a factory method and name heap_objects by the call

 paths through factory_methods To improve the scalability we only

 do this for objects that have fields that can hold more_than one type

 of object as determined by our context-insensitive_analysis (see

 Section queriesqueries)



 We compute the context-sensitive factory invocation relation

 by_applying Algorithm_algcontext to the factory multigraph

 where is the set of methods is

 the invocation_sites and if and only if

 are factory_methods



 The relations between an allocation_site for a particular context and

 the context of the allocated object are represented explicitly This

 is feasible because the number of allocation_sites is relatively

 small The quad iff

 enumerate

 is the context representing an acyclic call path in call

 graph CG terminating in a method with allocation_site and



 is the context representing the longest suffix of factory

 methods in in factory graph FG and is an edge in CG

 enumerate



 Object Sensitivity

 contextobjsens



 Context information can also be computed on the fly based_on the

 points-to_information in the program We give an example of this in

 our formulation of object sensitivitymilanova02parameterized





 Object sensitivity provides a form of context_sensitivity whereby the

 context is determined by the identity of the this pointer upon

 which the method is invokedmilanova02parameterized

 Object sensitivity lends itself well to object-oriented languages

 because of the way in which objects are used Milanova presented many examples of how object sensitivity improves precision

 when the program uses encapsulation inheritance or generic data

 structuresmilanova02parameterized



 We explore a more_precise form of object sensitivity than in the

 previous work - our analysis distinguishes the identities of objects

 based_on their nested creation context rather_than simply the

 identity of their creator At a call_site the context of the method

 invoked is the context of the receiver_object The context-sensitive

 call_graph depends_on the contexts of the receiver_object and must_be

 computed on the fly as the points-to_results are computed However

 we can compute all the possible heap contexts a priori



 We define an allocation multigraph

 for a program where is the set of types and

 if and only if is a site in an instance method in type

 that allocates an object of type (When computing AG we ignore exception objects Strings and objects with no

 reference fields) We define a special global type for static

 methods if and only if is a site in a static

 method that allocates an object of type We can compute the

 context-sensitive allocation edges by_applying

 Algorithm_algcontext to



 alg Object-sensitive points-to_analysis with the

 context-sensitive call_graph computed on the fly



 algos

 alg







 description

 Context-sensitive allocation edges C_I

 C T

 iff invocation_site invoked on receiver_object with context

 allocates object with type in context

 description

























 equation



 (m' i n) mI (i m)

 (i_0 v)

 (cv v ch h)





 (cv i ch m)

 OScallrule

 equation



 equation



 arrayc

 (v h) vP0

 (h t)



 (m h n) mI

 (m 0 v')



 (cv' v' ch' h')

 (h' t') hT

 (c' t' c t) IEa

 array





 (cv' v c h)

 OSrule0

 equation





 Algorithm algos is similar to Algorithm_algcs except

 that (1) Rule OScallrule is added to create context-sensitive

 invocation_edges on the fly based_on the context of the receiver

 object at the invocation_site and (2) Rule OSrule0 replaces

 Rule crule0 to create the contexts for the heap_objects based_on

 the context-sensitive allocation edges



 Context Sensitive Points-To

 seccontext



 A context-insensitive or monomorphic analysis produces just one

 set of results for each method regardless how many_ways a method may

 be invoked This leads to imprecision because information from

 different calling_contexts must_be merged so information

 along one calling_context can propagate to other calling_contexts A

 context-sensitive or polymorphic analysis avoids this

 imprecision by allowing different_contexts to have different

 results



 We can make a context-sensitive_version of a context-insensitive

 analysis as_follows We make

 a clone of a method for each path through the call_graph linking each

 call_site to its_own unique clone We then run the original

 context-insensitive_analysis over the exploded_call graph However this

 technique can require an exponential (and in the presence of cycles

 potentially unbounded) number of clones to be created



 It has_been observed that different_contexts of the same method often

 have many similarities For_example parameters to the same method

 often have the same types or similar aliases This observation led to

 the concept of partial transfer_functions (PTF) where summaries for

 each input pattern are created on the fly as they are

 discoveredWilsonThesisRWilson95

 However PTFs are notoriously difficult to implement and get correct

 as the programmer must explicitly calculate the input patterns and

 manage the summaries Furthermore the technique has not been shown

 to scale to very_large programs



 Our_approach is to allow the exponential explosion to occur and rely

 on the underlying BDD representation to find and exploit the

 commonalities_across contexts BDDs can express large sets of

 redundant data in an efficient manner Contexts with identical

 information will automatically be shared at the data_structure level

 Furthermore because BDDs operate down at the bit level it can even

 exploit commonalities between contexts with different information

 BDD_operations operate_on entire relations at a time rather_than one

 tuple at a time Thus the cost of BDD_operations depends_on the size

 and shape of the BDD relations which depends greatly on the variable

 ordering rather_than the number of tuples in a relation Also due

 to caching in BDD packages identical subproblems only have to be

 computed once Thus with the right variable_ordering the results

 for all contexts can be computed very efficiently





 Numbering Call Paths

 contextpathnumbering



 figuretb





 counting4

 Example of path numbering The graph on the left is the

 original graph Nodes and are in a cycle and therefore

 are placed in one equivalence_class Each edge is marked with path

 numbers at the source and target of the edge The graph on the right

 is the graph with all of the paths expanded

 figcounting

 figure

 figurehtb

 center

 tabularcc

 Call paths Reduced call_paths



 reaching reaching



























 tabular

 center

 The six contexts of function in

 Example excallpath

 figm6

 figure



 We implement cloning by giving each call path to a method a

 unique context number Variables and invocation_sites of a clone of a

 method invocation are given the same context number An invocation

 edge in a cloned_call graph links a call_site in one context with a

 context of the target method



 A call path is a sequence of invocation_edges

 such that is an invocation_site in an

 entry method typically main(Other entry methods in

 typical_programs are static class initializers object finalizers and

 thread run methods) and is an invocation_site in

 method for all



 For programs without recursion every call path to a method defines a

 context for that method To handle recursive programs which have an

 unbounded_number of call_paths we first find the strongly_connected

 components (SCCs) in a call_graph By eliminating all method

 invocations whose caller and callee belong to the same SCC from the

 call_paths we get a finite set of reduced_call paths Each

 reduced_call path to an SCC defines a context for the methods in the

 SCC Thus information from different paths_leading to the SCCs are

 kept separate but the methods within the SCC invoked with the same

 incoming call path are analyzed context-insensitively



 ex

 excallpath

 Figure figcounting(a) shows a small call_graph with just six

 methods and a set of invocation_edges Each invocation_edge has a

 name being one of through its source is labeled by the

 context number of the caller and its sink by the context number of the

 callee The numbers will be explained in Example exnumbering

 Methods and belong to a strongly_connected component so

 invocations along edges and are eliminated in the computation

 of reduced_call graphs While there are infinitely many call_paths

 reaching method there are only six reduced_call paths_reaching

 as shown in Figure figm6 Thus has six clones

 one for each reduced_call path

 ex





 Under this definition of context_sensitivity large_programs can have

 many_contexts For_example pmd from our test programs has 1971

 methods and contexts In the BDD representation we give

 each reduced_call path reaching a method a distinct context

 number It is important to find a context_numbering scheme that

 allows the BDDs to share commonalities_across contexts

 Algorithm_algcontext shows one such scheme



 alg Generating context-sensitive_invocation edges from a

 call_graph

 algcontext

 alg

 A call_multigraph



 Context-sensitive invocation_edges

 C_I C_M where

 C is the domain of context_numbers

 means that

 invocation_site in context calls method in context

 enumerate

 A method with clones will be given numbers

 Nodes with no predecessors are given a singleton context numbered 1



 Find strongly_connected components in the input call_graph

 The th clone of a method always calls the th clone of another

 method belonging to the same component



 Collapse all methods in a strongly_connected component to a

 single_node to get an acyclic reduced graph



 For each node in the reduced graph in topological_order

 tabbing

 1212341234 Set the counts of contexts created to 0



 For each incoming edge



 If the predecessor of the edge has contexts



 create clones of node



 Add tuple to for







 tabbing

 enumerate





 ex

 exnumbering

 We_now show the results of applying_Algorithm algcontext to

 Example excallpath the root node is given context

 number 1 We_shall visit the invocation

 edges from left to right Nodes and being members of a

 strongly_connected component are represented as one node The

 strongly_connected component is reached by two edges from Since

 has only one context we create two clones one reached by each

 edge For method the predecessor on each of the two incoming

 edges has two contexts thus has four clones Method has

 two clones one for each clone that invokes Finally method

 has six clones Clones 1-4 of method invoke clones 1-4

 and clones 1-2 of method call clones 5-6 respectively The

 cloned graph is shown in Figure figcounting(b)

 ex



 The numbering_scheme used in Algorithm_algcontext plays up the

 strengths of BDDs Each method is assigned a contiguous range of

 contexts which can be represented efficiently in BDDs The contexts

 of callees can be computed simply by_adding a constant to the contexts

 of the callers this operation is also cheap in BDDs Because the

 information for contexts that share common tail sequences are likely

 to be similar this numbering allows the BDD data_structure to share

 effectively across common contexts For_example the

 sequentially-numbered clones 1 and 2 of both have a common tail

 sequence Because of this the contexts are likely to be similar

 and therefore the BDD can take_advantage of the redundancies



 To optimize the creation of the cloned invocation graph we have

 defined a new primitive that creates a BDD representation of

 contiguous ranges of numbers in O() operations where is the

 number of bits in the domain In essence the algorithm creates one

 BDD to represent numbers below the upper_bound and one to represent

 numbers above the lower_bound and computes the conjunction of these

 two BDDs





 It is simple to construct a BDD to represent contiguous numbers

 for example a boolean

 function that accepts 4-bit numbers in the range from 0 to 13 is

 simply



 f(b3b2b1b0)

 b3 b2 b1



 However no primitive for constructing contiguous ranges exists so we

 have added an operator for this purpose called range The

 definition of the operator is simple though tedious and is omitted

 here due to space constraints It suffices to say that range

 runs in O() BDD_operations where is the number of bits in the

 domain Once such a BDD is created we can use an efficient_BDD add

 operation to generate the corresponding numbers offset by a constant



 Context-Sensitive_Pointer Analysis with a Pre-computed Call_Graph

 contextcspa



 We are now_ready to present our context-sensitive_pointer analysis

 We assume the presence of a pre-computed_call graph created for

 example by using a context-insensitive_points-to analysis

 (Algorithm algci-fly) We apply_Algorithm algcontext to

 the call_graph to generate the context-sensitive_invocation edges

 Once that is created we can simply_apply

 a context-insensitive_points-to analysis on the exploded_call graph to

 get context-sensitive_results

 We keep the results separate for each clone by

 adding a context number to methods variables invocation_sites

 points-to_relations etc





 In this_section we present the context-sensitive_pointer analysis for

 pre-computed_call graphs A clone of method is identified by a

 context Each cloned method has its_own cloned

 variables cloned invocation_sites each of which is

 connected by its_own invocation_edge to the cloned

 target





 figurehtb

 alg Context-sensitive points-to_analysis with a pre-computed

 call_graph

 algcs

 alg

 Domains



 Domains_from Algorithm_algci-types plus

 tabbing

 12123456712345678C 9223372036854775808

 tabbing

 Relations



 Relations from Algorithm_algci-types plus

 3em

 arraylll

 input (callerC invokeI calleeC tgtM)



 (destcC destV srccC srcV)



 output (contextC variableV heapH)

 array

 Rules

 0em

 equationarraylcl

 (vh)_- (vtv)_(hth) (tvth) cvPfilterrule



 (cvh) -_0(vh) (ch) crule0



 (c1v1h) - (c1v1c2v2)



 (c2v2h) (v1h) crule1



 (h1fh2) -_(v1fv2)



 (cv1h1) (cv2h2) crule2



 (cv2h2) -_(v1fv2) (cv1h1)



 (h1fh2)

 (v2h2) crule3



 2l(c1v1c2v2) equationarray

 -55ex

 equationarraylcl

 - (c2ic1m) (mzv1)



 (izv2) cArule

 equationarray

 figure





 description



 C is the domain of context_numbers Our BDD_library uses signed 64-bit

 integers to represent domains so the size is limited to







 C_I C_M is the set of

 context-sensitive_invocation edges

 means that invocation_site in context calls method

 in context This relation is computed using

 Algorithm_algcontext







 C_V C_V is the

 context-sensitive_version of the relation

 means variable in context includes the

 points-to set of variable in context due to parameter

 passing Again return values are handled analogously







 C_V H is the context-sensitive

 version of the variable points-to_relation ()

 means variable

 in context can point to heap_object

 description



 Rule (cArule) interprets the context-sensitive_invocation edges

 to find the bindings between actual and formal_parameters The rest

 of the rules are the context-sensitive counterparts to those found in

 Algorithm_algci-types





 CHECK THE PARAGRAPH

 Algorithm_algcs refers to allocated_objects simply by its

 allocation_site We can improve the precision of the algorithm by

 naming the heap allocation_sites by their call_paths For programs

 that use the same method to allocate all objects of the same class

 such is the case of the factory design pattern this naming scheme

 degenerates to a simple type analysis To gain better precision we

 consider methods that return freshly allocated_objects as a factory method and name heap_objects by the call_paths through

 factory_methods For optimization we only do this for objects that

 have fields that can hold more_than one type of object as determined

 by our context-insensitive_analysis (see

 Section queriesqueries)





 Algorithm_algcs takes advantage of a pre-computed_call graph to

 create an efficient context_numbering scheme for the contexts We can

 compute the call_graph on the fly while enjoying the benefit of the

 numbering_scheme by numbering all the possible contexts with a

 conservative call_graph and delaying the generation of the invocation

 edges only if warranted by the points-to_results We can reduce the

 iterations necessary by exploiting the fact that many of the

 invocation_sites of a call_graph created by a context-insensitive

 analysis have single targets Such an algorithm has an execution time

 similar to Algorithm_algcs but is of primarily academic interest

 as the call_graph rarely improves due to the extra precision from

 context-sensitive_points-to information





 Besides cloning methods we can also clone the heap_objects to

 increase the precision of the analysis For each invocation_site

 in context given an invocation_edge the returned

 object is given context





 Context-Sensitive_Pointer Analysis with Call_Graph Discovery



 In a similar_manner as before we can further improve the precision of

 the analysis by using the context-sensitive_points-to results to

 determine the context-sensitive call_graph on-the-fly Since it is

 critical that the context_numbering allows commonalities of the

 results be_exploited we use Algorithm_algcontext to first

 compute the context_numbers for a conservative call_graph

 (Algorithmalgcs-fly) determines which of the edges are actually

 present on the fly and creates only the necessary parameter bindings



 figurehtb

 alg Context-sensitive points-to_analysis with call_graph discovery

 algcs-fly

 alg

 Domains



 Domains_from Algorithm_algcs





 Relations



 Relations from Algorithm_algcs plus

 arraylll

 (callerC invokeI calleeC tgtM)



 array

 Rules



 Rules from Algorithm_algcs except Rule (cArule) plus

 0em

 equationarraylcl

 (ciicmm) - 0(im) (ciicmm) cIE0rule



 (ciicmm2) - (m1in) (i0v)



 (civh) (ht)



 (tnm2) (ciicmm2) cIErule



 2l(civ1cmv2)

 equationarray

 -55ex

 equationarraylcl

 - (ciicmm) (mzv1)



 (izv2) cArule2

 equationarray

 figure



 description



 Numbering of invocation_edges C_I

 C_M iff

 in the invocation_edge from to context matches

 context according to the context-sensitive_points-to information

 description



 Rule (cIE0rule) adds context_numbers to call_graph edges that

 are known statically Rule (cIErule) matches invocations to

 target methods in a context-sensitive manner Notice_that it uses the

 context-sensitive_points-to information to discover call_targets on a

 per-context basis Thus the call_graph edges themselves are

 context-sensitive Rule (cArule2) creates the effects of

 parameter_passing due to the context-sensitive call_graph edges



 alg Generating context-sensitive_invocation edges from a

 call_graph

 algcontext

 alg

 A multigraph_where is a

 set of vertices is a set of invocation_sites

 is a set of edges such that iff



 A relation that maps a

 calling_context and a given invocation to the callee's context and vertex



 Let be a

 graph where entire strongly_connected components in are reduced to a

 single vertex That is let be the strongly_connected

 component representing vertex in



 Es (s(v1)i s(v2)) E(v1 i v2) s(v1)

 s(v2)



 All vertices within a strongly_connected component have the same

 number of contexts A strongly_connected component has as many

 contexts as the sum of all its predecessor vertices' contexts Let

 be the number of contexts given to a strongly_connected

 component



 s



 arrayll

 1 Es(s'is)



 Es(s'is) s' otherwise

 array







 We define a total order

 for all edges to the same component





 (s'is)



 arrayll

 1 s' s



 (sis) v (s'is) s otherwise

 array









 R (c i c (s(v')is(v)) v)

 E(v' i v) 1 c v'









 Control Flow

 control-3code-sect



 The translation of statements such_as if-else-statements and

 while-statements is tied to the translation of boolean

 expressions In programming_languages boolean_expressions

 are often used to



 enumerate



 Alter the flow of control Boolean expressions are

 used as conditional expressions in statements that alter the flow

 of control The value of such boolean_expressions is implicit in a

 position reached in a program For_example in

 the expression must_be true if statement is reached



 Compute logical values A boolean_expression can

 represent true or false as values Such boolean

 expressions can be evaluated in analogy to arithmetic_expressions

 using three-address_instructions with logical operators



 enumerate



 The intended use of boolean_expressions is determined by its

 syntactic context For_example an expression following the

 keyword if is used to alter the flow of control while an

 expression on the right_side of an assignment is used to denote a

 logical value Such syntactic contexts can be specified in a

 number of ways we may use two different nonterminals use

 inherited_attributes or set a flag during_parsing Alternatively

 we may build a syntax_tree and invoke different procedures for the

 two different uses of boolean_expressions



 This_section concentrates on the use of boolean_expressions to

 alter the flow of control For clarity we introduce a new

 nonterminal for this purpose In

 Section bool-val-subsect we consider how a compiler can

 allow boolean_expressions to represent logical values



 Boolean Expressions



 Boolean expressions are composed of the boolean operators

 (which we denote and using the C convention for the

 operators AND OR and NOT respectively)

 applied to elements that are boolean

 variables or relational expressions

 Relational expressions are of the form where and

 are arithmetic_expressions In this_section we consider

 boolean_expressions generated_by the following grammar



 center





 center



 We use the attribute relop to indicate which of the six comparison

 operators or

 is represented_by rel As is customary we assume

 that and are left-associative and that has lowest precedence then then



 Given the expression if we determine that

 is true then we can conclude that the entire expression is

 true without_having to evaluate Similarly given

 if is false then the entire expression is

 false



 The semantic definition of the programming_language determines

 whether all parts of a boolean_expression must_be evaluated If

 the language definition permits (or requires) portions of a

 boolean_expression to go unevaluated then the compiler can

 optimize the evaluation of boolean_expressions by computing only

 enough of an expression to determine its value Thus in an

 expression such_as neither_nor is

 necessarily evaluated fully If either or is an

 expression with side_effects (eg it contains a function that

 changes a global variable) then an unexpected answer may be

 obtained



 Short-Circuit Code



 In short-circuit (or jumping) code the boolean

 operators and translate into jumps

 The operators themselves do_not appear in the code instead the

 value of a boolean_expression is represented_by a position in the

 code sequence



 ex

 bool-jump-ex1

 The statement



 center

 if_( x 100 x_200 x_y ) x 0

 center

 might be translated_into the code of Fig jump-code-fig In

 this translation the boolean_expression is true if control

 reaches label If the expression is false control goes

 immediately to skipping and the assignment x 0

 ex



 figurehtfb



 center

 tabularl_l

 if x 100 goto



 ifFalse_x 200_goto



 ifFalse_x y goto



 x 0





 tabular

 center



 Jumping code

 jump-code-fig



 figure



 Flow-of-Control Statements



 We_now consider the translation of boolean_expressions into

 three-address_code in the context of statements such_as those

 generated_by the following grammar



 center

 tabularr_c l

 if_( )



 if_( ) else



 while_( )



 tabular

 center

 In these productions nonterminal represents a boolean

 expression and nonterminal represents a statement



 This grammar generalizes the running_example of while expressions that

 we introduced in Example while-ex As in that example both

 and have a synthesized_attribute code which gives the

 translation into three-address_instructions For_simplicity we

 build_up the translations and as

 strings using syntax-directed_definitions The semantic_rules

 defining the attributes could be_implemented instead by

 building_up syntax_trees and then emitting code during a tree

 traversal or by any of the approaches outlined in

 Section l-att-sect



 The translation of

 consists of followed_by as

 illustrated in Fig code-layout-fig(a) Within

 are jumps based_on the value of If is

 true control_flows to the first instruction of

 and if is false control_flows to the instruction immediately

 following



 figurehtfb





 Code for if- if-else- and while-statementscode-layout-fig

 figure



 The labels for the jumps in and

 are managed using inherited_attributes With a boolean_expression

 we associate two labels the label to which

 control_flows if is true and the label to

 which control_flows if is false With a statement we

 associate an inherited_attribute denoting a label

 for the instruction immediately_after the code for In some

 cases the instruction immediately_following is a

 jump to some label A jump to a jump to from

 within is avoided using



 The syntax-directed_definition in

 Fig flow-stmt-sdd-bool-3code-sdd produces

 three-address_code for boolean_expressions in the context of if-

 if-else- and while-statements



 figurehtfb

 center

 tabularl_l Production_Semantic Rules



















































































































 tabular

 center

 Syntax-directed definition for flow-of-control

 statementsflow-stmt-sdd

 figure



 We assume that

 creates a new_label each time it is called and that





 attaches label to

 the next

 three-address_instruction to be generated(If implemented

 literally the semantic_rules will generate lots of labels and may

 attach more_than one label to a three-address_instruction The

 backpatching approach of Section backpatch-sect creates

 labels only when they are needed Alternatively unnecessary labels can

 be_eliminated during a subsequent optimization

 phase)



 A program consists of a statement generated_by The

 semantic_rules associated_with this production initialize

 to a new_label consists of

 followed_by the new_label Token

 in the production is a

 placeholder for assignment statements The translation of

 assignments is as discussed in Section expr-3code-sect for

 this discussion of control_flow is simply





 In translating

 the semantic_rules in Fig_flow-stmt-sdd create a new_label

 and attach it to the first three-address

 instruction generated for the statement as illustrated in

 Fig code-layout-fig(a)

 Thus jumps to

 within the code for will go to the code for Further by

 setting to we ensure_that

 control will skip the code for if evaluates to false



 In translating the if-else-statement

 the code

 for the boolean_expression has jumps out of it to the first

 instruction of the code for if is true and to the

 first instruction of the code for if is false as

 illustrated in Fig code-layout-fig(b) Further control

 flows from both and to the three-address_instruction

 immediately_following the code for - its label is given by

 the inherited_attribute An explicit

 appears after the code for to

 skip_over the code for No goto is needed after

 since is the same as



 The code for is formed from

 and as shown in

 Fig code-layout-fig(c) We use a local variable

 to hold a new_label attached to the first

 instruction for this while-statement which is also the first

 instruction for We use a variable rather_than an attribute

 because is local to the semantic_rules for this

 production The inherited label marks the

 instruction that control must flow to if is false hence

 is set to be A new_label

 is attached to the first instruction for

 the code for generates a jump to this label if is true

 After the code for we place the instruction

 which causes a jump back to the

 beginning of the code for the boolean_expression Note_that

 is set to this label so jumps

 from within can go directly to



 The code for consists of the code for

 followed_by the code for The semantic_rules manage the

 labels the first instruction after the code for is the

 beginning of the code for and the instruction after the

 code for is also the instruction after the code for



 We discuss the translation of flow-of-control_statements further

 in Section backpatch-sect

 There we_shall see an alternative method

 called backpatching which emits code for statements in one pass



 Control-Flow Translation of Boolean Expressions



 The semantic_rules for boolean_expressions in

 Fig bool-3code-sdd complement the semantic_rules for

 statements in Fig_flow-stmt-sdd As in the code layout of

 Fig code-layout-fig a boolean_expression is translated

 into three-address_instructions that evaluate using

 conditional and unconditional_jumps to one of two labels

 if is true and if is

 false



 figurehtfb

 center

 tabularl_l

 Production

 Semantic_Rules

































































































 tabular

 center

 Generating three-address_code for booleans

 bool-3code-sdd

 figure



 The fourth production in Fig bool-3code-sdd

 is translated directly into a comparison

 three-address_instruction with jumps to the appropriate places

 For_instance

 of the form translates_into



 centertabularl

 if a b goto



 goto

 tabular

 center



 The remaining productions for are translated as_follows



 enumerate



 Suppose is of the form If is

 true then we immediately know that itself is true so

 is the same as If is false

 then must_be evaluated so we make be the

 label of the first instruction in the code for The true and

 false_exits of are the same as the true and false_exits of

 respectively



 The translation of is similar



 No code is needed for an expression of the form

 just interchange the true and false_exits of to get

 the true and false_exits of



 The constants true and false translate into

 jumps to and respectively



 enumerate



 ex

 bool-jump-ex2

 Consider_again the following statement from

 Example bool-jump-ex1



 equationbool-eq

 if( x 100 x_200 x_y ) x 0

 equation

 Using the syntax-directed_definitions in

 Figs flow-stmt-sdd and bool-3code-sdd we would obtain the

 code in Fig bool-trans-fig



 figurehtfb



 centertabularl_l

 if x 100 goto



 goto



 if x_200 goto



 goto



 if x_y goto



 goto



 x 0





 tabular

 center



 Control-flow translation of a simple if-statement

 bool-trans-fig



 figure



 The statement (bool-eq) constitutes a program generated_by

 from Fig_flow-stmt-sdd The semantic_rules

 for the production generate a new_label for the instruction

 after the code for Statement has the form

 where is x 0 so the rules in

 Fig_flow-stmt-sdd generate a new_label and attach it

 to the first (and only in this case) instruction in

 which is x 0



 Since has lower precedence_than the boolean

 expression in (bool-eq) has the form

 where is Following the rules in

 Fig bool-3code-sdd

 is the label of the assignment

 x 0 is a new_label

 attached to the first instruction in the code for





 Note_that the code generated is not optimal in that the

 translation has three more instructions (goto's) than the code in

 Example bool-jump-ex1 The instruction

 is redundant since is the

 label of the very next instruction The two

 instructions can be_eliminated by using

 ifFalse instead of if instructions as in

 Example bool-jump-ex1

 ex



 Avoiding Redundant Gotos

 avoid-goto-subsect



 In Example bool-jump-ex2 the comparison

 translates_into the code_fragment



 center

 tabularl_l

 if x_200 goto



 goto





 tabular

 center

 Instead consider the instruction



 center

 tabularl_l

 ifFalse_x 200_goto





 tabular

 center

 This ifFalse instruction takes advantage of the natural flow

 from one instruction to the next in sequence so control simply

 falls through to label if

 thereby avoiding a jump



 In the code layouts for if- and while-statements in

 Fig code-layout-fig the code for statement

 immediately follows the code for the boolean_expression By

 using a special_label fall (ie don't generate any jump)

 we can adapt the semantic_rules

 in Fig_flow-stmt-sdd and bool-3code-sdd to allow control

 to fall_through from the code for to the code for

 The new rules for

 in Fig_flow-stmt-sdd

 set to fall



 center

 tabularr_c l

 fall









 tabular

 center

 Similarly the rules for if-else- and while-statements also set

 to fall



 We_now adapt the semantic_rules for boolean_expressions to allow

 control to fall_through whenever possible The new rules for

 in Fig fall-rel-fig generate two

 instructions as in Fig bool-3code-sdd if both

 and are explicit labels that is

 neither equals fall Otherwise if is an

 explicit label then must_be fall so they

 generate an if instruction that lets control fall_through

 if the condition is false Conversely if is an

 explicit label then they generate an ifFalse instruction

 In the remaining case both and

 are fall so no jump in generated(In C and Java

 expressions may contain assignments within them so code must_be

 generated for the subexpressions and even if both

 and are fall If desired

 dead_code can be_eliminated during an optimization phase)



 figurehtfb

 center

 tabularr_c l





























 tabular

 center



 Semantic rules for



 fall-rel-fig



 figure



 In the new rules for in

 Fig fall-or-fig note_that the meaning of label fall

 for is different from its meaning for Suppose

 is fall ie control falls through if

 evaluates to true Although evaluates to true if

 does must ensure_that control jumps over the

 code for to get to the next instruction after



 figure

 center

 tabularr_c l





 fall



















 tabular

 center

 Semantic rules for

 fall-or-fig

 figure



 On the other_hand if evaluates to false the truth-value of is

 determined by the value of so the rules in

 Fig fall-or-fig ensure_that corresponds

 to control falling through from to the code for



 The semantic_rules for

 are similar to those in Fig fall-or-fig

 We leave them as an

 exercise



 exbool-jump-ex3

 With the new rules using the special_label fall the

 program (bool-eq) from Example bool-jump-ex2



 center

 if( x 100 x_200 x_y ) x 0

 center

 translates_into the code of Fig if-fall-fig



 figurehtfb



 centertabularl_l

 if x 100 goto



 ifFalse_x 200_goto



 ifFalse_x y goto



 x 0





 tabular

 center



 If-statement translated using the fall-through technique

 if-fall-fig



 figure



 As in Example bool-jump-ex2 the rules for

 create label

 The difference from Example bool-jump-ex2 is that the

 inherited_attribute is fall when the

 semantic_rules for are applied

 ( is ) The rules in

 Fig fall-or-fig create a new_label to allow a

 jump over the code for if evaluates to true Thus

 is and is fall since must_be evaluated if is false



 The production that generates

 is therefore reached with

 and

 With these inherited labels the rules in Fig fall-rel-fig

 therefore generate a single instruction



 ex



 Boolean Values and Jumping Code

 bool-val-subsect



 The focus in this_section has_been on the use of boolean

 expressions to alter the flow of control in statements A boolean

 expression may also be evaluated for its value as in

 assignment statements such_as x true or x ab



 A clean way of handling both roles of boolean_expressions is to

 first build a syntax_tree for expressions using either of the

 following approaches



 enumerate



 Use two passes Construct a complete syntax_tree for the

 input and then walk the tree in depth-first_order computing the

 translations specified_by the semantic_rules



 Use one pass for statements but two passes for expressions

 With this approach we would translate in

 before is

 examined The translation of however would be done by

 building its syntax_tree and then walking the tree



 enumerate



 The following grammar has a single nonterminal for

 expressions

 center

 tabularr_c l











 tabular

 center

 Nonterminal governs the flow of control in

 The same nonterminal

 denotes a value in

 and



 We can handle these two roles of expressions by using separate

 code-generation functions Suppose that attribute denotes

 the syntax-tree_node for an expression and that nodes are

 objects Let method generate jumping_code at an

 expression node and let method generate code to

 compute the value of the node into a temporary



 When appears in

 method is

 called at node The implementation of is based

 on the rules for boolean_expressions in Fig bool-3code-sdd

 Specifically jumping_code is generated_by calling

 where is a new_label for the first

 instruction of and is the label





 When appears in

 method is called at node If has the

 form the method call

 generates code as discussed in Section expr-3code-sect If

 has the form we first generate

 jumping_code for and then assign true or false to a new

 temporary t at the true and false_exits respectively from

 the jumping_code



 For_example the assignment



 can be_implemented by the code in Fig jump-asgn-fig



 figurehtfb



 center

 tt

 tabularl_l

 ifFalse a b goto



 ifFalse c_d goto



 t true



 goto



 t false



 x t

 tabular

 tt

 center



 Translating a boolean assignment by computing the value of a

 temporary

 jump-asgn-fig



 figure



 exer

 Add rules to the syntax-directed_definition of Fig_flow-stmt-sdd

 for the following control-flow constructs



 itemize

 a) A repeat-statement repeat while

 b) A for-loop for

 itemize

 exer



 exer

 Modern machines try to execute many instructions at the same time

 including branching instructions Thus there is a severe cost if

 the machine speculatively follows one branch when control

 actually goes another_way (all the speculative work is thrown

 away) It is therefore desirable to minimize the number of

 branches Notice_that the implementation of a while-loop in

 Fig code-layout-fig(c) has two branches per interation one

 to enter the body from the condition and the other to jump

 back to the code for As a result it is usually preferable

 to implement while as if it were if repeat until Show what the code

 layout looks like for this translation and revise the rule for

 while-loops in Fig_flow-stmt-sdd

 exer



 hexer

 Suppose that there were an exclusive-or operator (true if and only

 if exactly one of its two arguments is true) in C Write the rule for

 this operator in the style of Fig bool-3code-sdd

 hexer



 exer

 Translate the following statements using the goto-avoiding translation

 scheme of Section avoid-goto-subsect



 itemize

 a) if (ab cd ef) x 1

 b) if (ab cd ef) x 1

 c) if (ab cd ef) x 1

 itemize

 exer



 exer

 short3-scheme-exer

 Give a translation_scheme based_on the syntax-directed_definition

 in Figs flow-stmt-sdd and bool-3code-sdd

 exer



 exer

 short2-sdd-exer

 Adapt the semantic_rules in

 Figs flow-stmt-sdd and bool-3code-sdd to allow control to

 fall_through using rules like the ones in

 Figs fall-rel-fig and fall-or-fig

 exer



 hexer

 short3-sdd-exer

 The semantic_rules for statements in

 Exercise short2-sdd-exer generate unnecessary labels Modify

 the rules for statements in Fig_flow-stmt-sdd to create

 labels as needed using a special_label deferred to mean

 that a label has not_yet been created Your rules must generate

 code similar to that in Example bool-jump-ex1

 hexer



 vhexer

 Section avoid-goto-subsect talks about using fall-through

 code to minimize the number of jumps in the generated intermediate

 code However it does_not take_advantage of the option to replace

 a condition by its complement eg replace if ab goto goto by if ab goto goto Develop a SDD that does take

 advantage of this option when needed

 vhexer

 Context-Sensitive_Pointer Analysis

 csens-sect



 As_discussed in Section secipa-intro-cs context_sensitivity

 can improve greatly the precision of interprocedural_analysis We

 talked about two approaches to interprocedural_analysis one based_on

 cloning (Section ipa-intro-cloning) and one on summaries

 (Section ipa-intro-summary) Which one should we use



 There_are several difficulties in computing the summaries of points-to

 information First the summaries are large Each method's summary

 must include the effect of all the updates that the function and all

 its callees can make in terms of the incoming parameters That is a

 method can change the points-to sets of all data reachable through

 static variables incoming parameters and all objects_created by the

 method and its callees While complicated schemes have_been proposed

 there is no known solution that can scale to large_programs Even_if

 the summaries can be computed in a bottom-up_pass computing the

 points-to sets for all the exponentially_many contexts in a typical

 top-down_pass presents an even greater problem Such information is

 necessary for global queries like finding all points in the code that touch a

 certain object



 In this_section we discuss a cloning-based context-sensitive

 analysis A cloning-based analysis simply clones the methods one for

 each context of interest

 We then apply the

 context-insensitive_analysis to the cloned_call graph While this

 approach

 seems simple the devil is in the details of handling the large_number of clones

 How many_contexts are there Even_if we use the idea of collapsing

 all recursive_cycles as discussed in Section ipa-call-strings

 it is not uncommon to find contexts in a Java application

 Representing the results of these many_contexts is the challenge



 We separate the discussion of context_sensitivity into two_parts



 enumerate



 How to handle context_sensitivity logically This part is easy

 because we simply_apply the context-insensitive algorithm to the

 cloned_call graph



 How to represent the exponentially_many contexts One_way is

 to represent the information as binary_decision diagrams (BDD's) a

 highly-optimized data_structure that has_been used for many other

 applications

 enumerate



 This_approach to context_sensitivity is an excellent example of

 the importance of abstraction As we are going to show

 we eliminate algorithmic complexity by leveraging the years of work that

 went into the BDD abstraction We can specify a context-sensitive

 points-to_analysis in just a few lines of Datalog which in turn takes

 advantage of many thousands of lines of existing

 code for BDD manipulation This_approach has several important

 advantages First it makes possible the easy expression of further

 analyses that use the results of the points-to_analysis After all

 the points-to_results on their own are not interesting Second it

 makes it much_easier to write the analysis correctly as it leverages

 many lines of well-debugged code



 Contexts and Call Strings

 context-call-subsect



 The context-sensitive_points-to analysis described below assumes that

 a call_graph has_been already_computed This step helps make possible a

 compact representation of the many calling_contexts To get the call

 graph we first run a context-insensitive_points-to analysis that

 computes the call_graph on the fly as discussed in

 Section virt-meth-sect We_now describe_how to create a cloned

 call_graph



 A context is a representation of the call string that forms the history

 of the active function calls Another way to look_at the context is that it

 is a summary of the sequence of calls whose activation_records are currently on the

 run-time_stack If there are no recursive functions on the stack then the

 call string - the sequence of locations from which the calls on the stack

 were made - is a complete representation It is also an

 acceptable representation in the sense

 that there is only a finite number of different_contexts although that number

 may be exponential in the number of functions in the program



 However if there are recursive functions in the program then the number of

 possible call strings is infinite and we cannot allow all possible call

 strings to represent distinct contexts There_are various ways we can limit

 the number of distinct contexts For_example we can write a regular

 expression that describes all possible call strings and convert that

 regular_expression to a deterministic_finite automaton using the methods of

 Section re-fa-sect The contexts can then be identified with the

 states of this automaton



 Here we_shall adopt a simpler scheme that captures the history of

 nonrecursive calls but considers recursive_calls to be too hard to

 unravel We begin by finding all the mutually_recursive sets of functions

 in the program The process is simple and will not be elaborated in detail

 here Think of a graph whose nodes are the functions with an edge from

 to if function calls function The strongly_connected components

 (SCC's) of this

 graph are the sets of mutually_recursive functions As a common special

 case a function that calls itself but is not in an SCC

 with any other function is an SCC by itself The nonrecursive

 functions are also SCC's by themselves Call an SCC nontrivial if it

 either has more_than one member (the mutually_recursive case) or it has a

 single recursive member The SCC's that are single nonrecursive functions

 are trivial SCC's



 Our modification of the rule that any call string is a context is as

 follows Given a call string delete the occurrence of a call_site if



 enumerate



 is in a function



 Function is called at site ( is possible)



 and are in the same strong component (ie and are mutually

 recursive or and is recursive)



 enumerate

 The result is that when a member of a nontrivial SCC is called the call

 site for that call becomes part of the context but calls within to

 other functions in the same SCC are not part of the context Finally when

 a call outside is made we record that call_site as part of the

 context



 figurehtfb



 verbatim

 void p()

 h_T a new_T()

 s1 T b aq()

 s2 bs()





 T q()

 s3 T c thisr()

 i T d new_T()

 s4 dt()

 return d





 T r()

 s5 T e thisq()

 s6 es()

 return e





 void s()

 s7 T f thist()

 s8 f ft()





 T t()

 j T g new_T()

 return g



 verbatim



 Methods and call_sites for a running_example

 five-fns-fig



 figure



 ex

 context-ex

 In Fig five-fns-fig is a sketch of five methods with some call

 sites and calls among them An examination of the calls shows that and

 are mutually_recursive However and are not recursive at

 all Thus our contexts will be lists of all the call_sites except s3

 and s5 where the recursive_calls between and take place



 Let_us consider all the ways we could get from to that is all the

 contexts in which calls to occur



 enumerate



 could call at s2 and then could call at either s7 or s8 Thus two possible call strings are

 and



 could call at s1 Then and could call each other

 recursively some number of times We could break the cycle

 enumerate

 At s4 where is called directly by

 This choice leads to only one context

 At s6 where calls Here we can reach either by the call

 at s7 or the call at s8 That gives_us two more contexts

 and

 enumerate



 enumerate

 There_are thus five different_contexts in which can be called Notice

 that all these contexts omit the recursive call_sites s3 and s5 For_example the context actually represents the

 infinite set of call strings

 for all

 ex



 We_now describe_how we derive the cloned_call graph Each cloned

 method is identified by the method in the program and a context

 Edges can be derived by_adding the corresponding contexts to

 each of the edges in the original call_graph Recall that there is an

 edge in the original call_graph linking call_site with method

 if the predicate is true To add contexts to identify

 the methods in the cloned_call graph we can define a corresponding

 predicate such that is true if the call

 site in context calls the context of method



 Adding Context to Datalog Rules



 To_find context-sensitive_points-to relations we can simply_apply the

 same context-insensitive_points-to analysis to the cloned_call graph

 Since a method in the cloned_call graph is represented_by the original

 method and its context we revise all the Datalog_rules accordingly

 For_simplicity the rules below do_not include the type restriction

 and the 's are any new variables



 figurehtfb

 centertabularr_r c_l



 1) - new













 2) -











 3) -















 4) -















 5) -

















 tabularcenter

 Datalog_program for context-sensitive_points-to analysis

 csens-fig

 figure



 An additional argument representing the context must_be given to the

 IDB_predicate says_that variable in context

 can point to heap_object All the rules are self-explanatory

 perhaps with the exception of Rule 5 Rule 5 says_that if the call

 site in context calls method of context then the

 formal_parameters in method of context can point to the

 objects_pointed to by the corresponding actual_parameters in context





 Additional Observations About Sensitivity



 What we have described is one formulation of context_sensitivity that

 has_been shown to be practical enough to handle many large real-life

 Java programs using the tricks described briefly in the next section

 Nonetheless this algorithm cannot yet handle the largest of Java

 applications



 The heap_objects in this formulation are named by their call_site but

 without context_sensitivity That simplification can cause problems

 Consider the object-factory idiom where

 all objects of the same type are allocated by the same routine

 The current scheme would make all objects of that class share the same

 name It is relatively_simple to handle such cases by essentially

 inlining the allocation code In_general it is desirable to increase

 the context_sensitivity in the naming of objects While it is easy to

 add context_sensitivity of objects

 to the Datalog formulation getting the analysis to scale to

 large_programs is another matter



 Another important form of sensitivity is object sensitivity An

 object-sensitive technique can distinguish_between methods invoked on

 different receiver objects Consider the scenario of a call_site in a

 calling_context where a variable is found to point to two different

 receiver objects of the same class Their fields

 may point to different objects Without distinguishing between the

 objects a copy among fields of the this object reference will create

 spurious relationships unless we separate the analysis according to the

 receiver objects Object sensitivity is more useful than context

 sensitivity for some analyses





 exer

 Add context to the analysis of the code in Fig andersen2-fig Does

 context let_us avoid any of the invalid inferences

 exer





 figurehtfb



 verbatim

 void p()

 h_T a new_T()

 i T b new_T()

 c1 T c aq(b)





 T q(T y)

 j T d new_T()

 c2 d thisq(d)

 c3 d dq(y)

 c4 d dr()

 return d





 T r()

 return this



 verbatim



 Code for Exercises csens1-exer and csens2-exer

 csens-exer-fig



 figure



 exer

 csens1-exer

 What are all the contexts that would be distinguished if we apply the

 methods of this_section to the code in Fig csens-exer-fig

 exer



 hexer

 csens2-exer

 Perform a context sensitive analysis of the code in

 Fig csens-exer-fig

 hexer



 hexer

 Extend the Datalog_rules of this_section to incorporate the type and

 subtype information following the approach of

 Section virt-meth-sect

 hexer

 Context-Sensitive Points-to_Analysis



 There_are many kinds of pointer_alias analysis Points to is one of

 them and probably one of the easiest ones



 Background

 Different approaches

 - elimination-based algorithms - solve a much harder problem

 - partial transfer_functions

 - invocation graph based



 Invocation-graph based

 - may be large in contexts but answers are relatively_small



 Approach is to create an exploded_call graph and use our

 context-insensitive algorithm



 Definition of context-sensitive paths

 How to number the contexts

 What are the properties

 What is it a good idea

 gcd

 Array Data-Dependence Analysis

 ch11datadep



 Parallelization or locality optimizations frequently

 reorder the operations

 executed in the original_program As with all optimizations

 operations can be reordered only if the reordering does_not change the

 program's output

 Since we cannot in general understand deeply what a program does code

 optimization generally

 adopts a simpler conservative test

 for when we can be_sure that the program output is

 not affected we check that the operations on any memory_location are

 done in the same order in the original and modified programs

 In the present study we focus_on

 array_accesses so the array_elements are the memory_locations of

 concern



 Two accesses whether read or write are

 clearly independent (can be reordered)

 if they refer to two different locations In

 addition read operations do_not change the memory state and therefore

 are also independent Following Section ch11reuse

 we say that two accesses

 are data_dependent if they refer to the same memory_location and

 at_least one of them is a write operation To be_sure that the modified program

 does the same as the original the relative_execution ordering

 between every pair of data-dependent operations in the original

 program must_be preserved in the new program



 Recall from Section dd-3kinds-subsect that

 there are three flavors of data_dependence



 enumerate



 True dependence where a write is followed_by a read

 of the same_location



 Antidependence where a read is followed_by a write

 to the same_location



 Output dependence which is two writes to the same_location



 enumerate



 In the discussion_above data_dependence is defined for dynamic

 accesses

 We_say that a static_access in a

 program depends_on another as_long as there_exists a dynamic_instance of the

 first access that depends_on some instance of the second(Recall

 the difference_between static and dynamic_accesses A static_access is

 an array reference at a particular location in a program while a

 dynamic access is one execution of that reference)



 It is easy to see_how data_dependence can be used in

 parallelization For_example if no data_dependences are found in the

 accesses of a loop we can easily assign each iteration to a

 different processor Section ch11affine discusses_how we use

 this information systematically in parallelization



 Definition of Data_Dependence of Array Accesses



 Let_us consider

 two static accesses to the same array in possibly different loops

 The first is represented_by access

 function and bounds



 and is in a -deep_loop nest

 the second is represented_by



 and is in a -deep_loop nest

 These accesses are data_dependent if



 enumerate



 At least_one of them is a

 write_reference and



 There exist vectors i in and in such

 that

 enumerate



 and



 enumerate



 enumerate



 Since a static_access normally embodies many dynamic_accesses it is

 also meaningful to ask if its dynamic_accesses may refer to the same

 memory_location To search for dependencies between instances of the

 same static_access we assume and

 augment the definition above with the additional_constraint that

 to rule out the trivial_solution



 ex

 Consider the following 1-deep loop_nest



 verbatim

 for_(i 1_i 10_i)

 Zi Zi-1



 verbatim

 This loop has two accesses and the first is

 a read reference and the second a write To_find all the data

 dependences in this program we need to check if the write_reference

 shares a dependence with itself and with the read reference



 enumerate



 Data_dependence between and Except for the

 first iteration each iteration reads the value written in the

 previous iteration Mathematically we know that there is a

 dependence because there exist_integers and such that



 center

 and

 center

 There_are nine solutions to the above system of

 constraints and so forth



 Data_dependence between and itself It is easy

 to see that different iterations in the loop write to different

 locations that is there are no data dependencies among the instances

 of the write_reference

 Mathematically we know that there does_not exist a

 dependence because there do_not exist_integers and satisfying



 center

 and

 center

 Notice_that the third condition

 comes_from the

 requirement that and are the same memory_location

 The contradictory fourth condition

 comes_from the requirement

 that the dependence be nontrivial - between different dynamic

 accesses



 enumerate

 It is not necessary to consider data_dependences between the

 read reference and itself because any two read accesses

 are independent

 ex



 Integer Linear Programming



 Data_dependence requires finding whether there exist_integers that satisfy a

 system consisting of equalities and inequalities The equalities are

 derived_from the matrices and vectors representing the accesses the

 inequalities are derived_from the loop_bounds Equalities can be

 expressed_as inequalities

 an equality can be replaced_by two inequalities and





 Thus data_dependence may be phrased as a search for integer_solutions

 that satisfy a set of linear_inequalities which is precisely the

 well-known problem of integer_linear programming Integer

 linear_programming is an NP-complete problem While no polynomial

 algorithm is known heuristics have_been developed to solve linear

 programs involving many variables and they can be quite fast in many_cases

 Unfortunately such standard heuristics are inappropriate for data_dependence

 analysis where the challenge is to solve many small and simple

 integer_linear programs rather_than large complicated integer_linear

 programs



 The data_dependence analysis algorithm consists of three_parts



 enumerate



 Apply the GCD (Greatest Common Divisor)

 test which checks if there is an_integer solution to the

 equalities using the theory of linear_Diophantine equations If

 there are no integer_solutions then there are no data_dependences

 Otherwise we use the equalities to substitute for some of the

 variables thereby obtaining simpler inequalities



 Use a set of simple heuristics to handle the large_numbers of typical

 inequalities



 In the rare case_where the heuristics do_not work we use a linear

 integer programming solver that uses a branch-and-bound approach

 based_on Fourier-Motzkin_elimination

 enumerate



 The GCD Test

 dioph-subsect



 The first subproblem is to check for the existence of integer

 solutions to the equalities Equations with the stipulation that

 solutions must_be integers are known_as Diophantine_equations

 The following example shows_how the issue of integer_solutions arises

 it also demonstrates that even_though many of our examples involve a

 single loop_nest at a time the data_dependence formulation applies

 to accesses in possibly different loops



 ex

 exsimplegcd

 Consider the following code_fragment



 verbatim

 for_(i 1_i 10_i)

 Z2i 10



 for_(j 1_j 10 j)

 Z2j1 20



 verbatim

 The access

 only touches even elements of while access

 touches only odd elements Clearly these two accesses

 share no data_dependence regardless of the loop_bounds We can

 execute iterations in the second loop before the first or interleave

 the iterations

 This example is not as contrived as it may look

 An_example where even and odd numbers are treated differently is

 an array of complex numbers where the real and imaginary components

 are laid_out side by side



 To prove the absence of data_dependences in this example we reason as

 follows

 Suppose there were integers and such that and

 are the same array_element We get the Diophantine

 equation



 2i 2j 1



 There_are no integers and

 that can satisfy the above equation

 The proof is that if is an_integer then is even If is

 an_integer then is even so is odd

 No even number is also an odd number Therefore

 the equationhas no

 integer_solutions and thus there is no dependence_between the two

 accesses and

 ex



 To describe when there is a solution to a linear_Diophantine

 equation we need the concept of the greatest common divisor

 (GCD) of two or_more integers The GCD of integers

 denoted

 is the largest integer that evenly_divides all these integers

 GCD's can be computed efficiently by the well-known Euclidean

 algorithm (see the box on the subject)



 ex

 because and

 each have remainder 0 yet any integer larger_than 6 must leave a

 nonzero remainder when dividing at_least one of 24 36 and 54 For

 instance 12 divides 24 and 36 evenly but not 54

 ex



 The importance of the GCD is in the following theorem



 th

 dioph-th

 The linear_Diophantine equation



 a1 x1 a2 x2 an xn c



 has an_integer solution for if and only if

 divides

 th



 ex

 gcd-ex

 We observed in Example exsimplegcd that the linear

 Diophantine_equation has no solution We can write this

 equation as



 2i -2j 1



 Now and 2 does_not divide 1 evenly Thus there is no

 solution



 For another example consider the equation



 24x36y54z30



 Since and there is a solution in integers

 for and

 One_solution is and but there are an infinity of

 other solutions

 ex





 The Euclidean Algorithm

 The Euclidean_algorithm for finding works as_follows

 First assume that and are positive integers and Note

 that the GCD of negative numbers or the GCD of a negative and a

 positive number is the same as the GCD of their absolute values so we

 can assume all integers are positive



 If

 then If let be the remainder of If

 then evenly_divides so Otherwise

 compute this result will also be



 To_compute for use the Euclidean

 algorithm to compute

 Then recursively compute





 The first step to the data_dependence problem is to use a standard

 method such_as Gaussian_elimination to solve the given equalities

 Every time a linear equation is constructed apply

 Theorem dioph-th to rule out if possible the existence of an

 integer_solution If we can rule out such solutions then answer

 no Otherwise we use the solution of the equations to reduce the

 number of variables in the inequalities



 ex

 Consider the two equalities



 center

 tabularl_l l

 0



 5



 tabular

 center

 Looking at each equality by itself it appears there might be a

 solution For the first equality divides 0 and for

 the second equality divides 5

 However if we use the first equality to solve for and

 substitute for in the second equality we get This

 Diophantine_equation has no solution since does_not

 divide 5 evenly

 ex



 Heuristics for Solving Integer Linear Programs

 heur-ilp-subsect



 The data_dependence problem requires many simple integer_linear

 programs be_solved We_now discuss several techniques to handle

 simple inequalities and a technique to take_advantage of the

 similarity found in data_dependence analysis



 Independent-Variables Test



 Many of the integer_linear programs from data_dependence consist of

 inequalities that involve only one unknown The programs can be

 solved simply by testing if there are integers between the constant

 upper_bounds and constant lower_bounds independently



 ex

 Consider the nested loop



 verbatim

 for_(i 0 i 10_i)

 for_(j 0 j 10 j)

 Zij Zj10i11

 verbatim

 To_find if there is a data_dependence between and

 we ask if there exist_integers and

 such that



 arrayc

 0 iji'j' 10



 i j' 10



 j_i' 11



 array





 The GCD test applied to the two equalities

 above will determine that there may be an_integer solution

 The integer_solutions to the equalities are expressed by



 i t1 j t2 i' t2 - 11 j' t1 - 10



 for any integers and

 Substituting the variables and into the linear_inequalities

 we get



 arrayrcl

 0 t1 10



 0 t2 10



 0 t2 - 11 10



 0 t1 - 10 10



 array



 Thus combining the lower_bounds from the last two inequalities with the

 upper_bounds from the first two we deduce



 arrayrcl

 10 t1 10



 11 t2 10



 array



 Since the lower_bound on is greater_than its upper_bound there

 is no integer_solution and hence no data_dependence This example

 shows that even if there are equalities involving several variables

 the GCD test may still create linear_inequalities that involve one

 variable at a time

 ex



 Acyclic Test



 Another simple heuristic is to find if there_exists a variable that is

 bounded below or above by a constant In certain circumstances

 we can safely

 replace the variable by the constant the simplified inequalities

 have a solution if and only if the original inequalities have a

 solution Specifically suppose every lower_bound on

 is of the form



 center

 for some

 center

 while the upper_bounds on are all of the form



 ci vi c0 c1 v1 ci-1 vi-1

 ci1vi1 cn vn



 where is nonnegative

 Then we can replace variable by its smallest possible

 integer value If there is no such lower_bound we simply replace

 with

 Similarly if

 every constraint involving can be_expressed in the two forms

 above but with the directions of the inequalities reversed

 then we can replace variable with the largest possible

 integer value or by

 if there is no constant upper

 bound This step can be repeated to simplify the

 inequalities and in some_cases determine if there is a solution



 ex

 Consider the following inequalities



 arrayrrcll

 1 v1 v2 10



 0 v3 4



 v2 v1



 v1 v3 4



 array



 Variable is bounded from below by and from above by

 However is bounded from below only by the constant 1 and

 is bounded from above only by the constant 4 Thus replacing

 by 1 and by 4 in the inequalities we obtain



 arrayrrcll

 1 v1 10



 1 v1



 v1 8



 array



 which can now be_solved easily with the independent-variables test

 ex



 The Loop-Residue Test



 Let_us now consider the case_where every variable is bounded from

 below and above by other variables It is commonly the case in data

 dependence analysis that constraints have the form

 which can be_solved using a simplified version of the loop-residue_test

 due to Shostak A set of these constraints can be

 represented_by a directed graph whose nodes are labeled with variables

 There is an edge from to labeled whenever there is a

 constraint



 We define the weight of a path to be

 the sum of the labels of all the edges along the path Each path in the

 graph represents a combination of the constraints in the system That

 is we can infer that whenever

 there_exists a path from to with

 weight A cycle in the graph with weight represents the

 constraint for each node on the cycle

 If we can find a negatively weighted

 cycle in the graph then we can infer which is impossible

 In this case we can conclude that there is no solution and thus no dependence



 We can also incorporate into the loop-residue_test constraints of the

 form and for variable and constant We

 introduce into the system of inequalities a new dummy variable

 which is added to each constant upper and lower_bound Of_course

 must have value 0 but since the loop-residue_test only looks for

 cycles the actual values of the variables never becomes significant

 To handle constant bounds we replace



 center

 tabularl

 by



 by



 tabular

 center



 ex

 exloopresidue

 Consider the inequalities



 arrayrrcll

 1 v1 v2 10



 0 v3 4



 v2 v1



 2v1 2v3 - 7



 array



 The constant upper and lower_bounds on become and

 the constant bounds on and are handled

 similarly

 Then converting the last constraint to we can create

 the graph shown in

 Fig figloopresidue The cycle has

 weight so there is no solution to this set of inequalities

 ex



 figurehtfb



 fileuullmanalsuch11figsloopresidueeps

 Graph for the constraints of Example exloopresidue

 figloopresidue

 figure



 Memoization



 Often similar data_dependence problems are solved repeatedly

 because simple access patterns are repeated

 throughout the program One important technique to speed_up data

 dependence processing is to use memoization Memoization tabulates

 the results to the problems as they are generated The table of stored

 solutions is

 consulted as each problem is presented the problem needs to be_solved

 only if the result to the problem cannot be found in the table



 Solving General Integer Linear Programs

 rat-sol-subsect



 We_now describe a general approach to solving the integer_linear programming

 problem The problem is NP-complete our algorithm uses a

 branch-and-bound approach that can take an exponential amount of

 time in the worst_case

 However it is rare that the heuristics of

 Section heur-ilp-subsect cannot resolve

 the problem and even if we do need to apply the algorithm of this

 section

 it seldom needs to perform the branch-and-bound step



 The approach is to first check for the existence of rational solutions

 to the inequalities This problem is the classical linear-programming

 problem If there is no rational_solution to the inequalities then

 the regions of data touched_by the accesses in question do_not

 overlap and there surely is no data_dependence If there is a rational

 solution we first try to prove that there is an_integer solution

 which is commonly the case Failing that we then split the

 polyhedron bounded by the inequalities into two smaller problems and

 recurse



 ex

 Consider the following simple loop



 verbatim

 for_(i 1_i 10_i)

 Zi Zi10

 verbatim

 The elements_touched by access are while the

 elements_touched

 by are The ranges do_not

 overlap and therefore there are no data_dependences

 More formally we need to show that there are no two dynamic_accesses

 and with and If

 there were such integers and then we could substitute

 for and get the four constraints on and

 However implies which contradicts

 Thus no such integers and exist

 ex



 Algorithm algintegersol describes how to determine

 if an_integer solution can be found for a set of linear_inequalities

 based_on the Fourier-Motzkin_elimination algorithm



 alg

 algintegersol

 Branch-and-bound solution to integer_linear programming problems



 A convex_polyhedron over variables



 yes if has an_integer solution no otherwise



 The algorithm is shown in Fig figintegersol

 alg



 figurehtb

 center

 tabularl_l

 1) apply_Algorithm fm-elim-alg to to project_away variables



 in that order



 2) let be the polyhedron after projecting away for







 3) if is empty return no



 There is no rational_solution if which

 involves



 only constants has unsatisfiable constraints



 4) for ()



 5) if_( does_not include an_integer value) break



 6) pick an_integer in the middle of the range for

 in



 7) modify by_replacing by



 8)



 9) if return yes



 10) if return no



 11) let the lower and upper_bounds on in be



 and respectively



 12) recursively apply this algorithm to and







 13) if (either returns yes) return yes else

 return no



 tabular

 center



 Finding an_integer solution in inequalities

 figintegersol

 figure



 Lines (1) through (3) attempt to find a rational_solution to the

 inequalities If there is no rational_solution there is no integer

 solution If a rational_solution is found this means that the

 inequalities define a nonempty polyhedron It is relatively rare for

 such a polyhedron not to include any integer_solutions - for that to

 happen the polyhedron must_be relatively thin along some dimension

 and fit between integer points



 Thus lines_(4) through (9) try to check quickly if there is an

 integer_solution Each step of the Fourier-Motzkin_elimination

 algorithm produces a polyhedron with one fewer dimension than the

 previous one We consider the polyhedra in reverse order

 We start with the polyhedron with one variable and assign to that

 variable an_integer solution roughly in the middle of the range of

 possible values if possible We then substitute the value for the

 variable in all other polyhedra decreasing their unknown variables by

 one We repeat the same process until we have processed all the

 polyhedra in which case an_integer solution is found or we have found a

 variable for which there is no integer_solution



 If we cannot find an_integer value for even the first variable there

 is no integer_solution (line 10) Otherwise all we know is that

 there is no integer_solution including the combination of specific integers

 we have picked so_far and the result is inconclusive Lines (11) through

 (13) represent the branch-and-bound step If variable is found

 to have a rational but not integer_solution we split the polyhedron

 into two with the first requiring that must_be an_integer

 smaller_than the rational_solution found and the second requiring

 that must_be an_integer greater_than the rational_solution

 found If neither has a solution then there is no dependence



 Summary



 We have shown that essential pieces of

 information that a compiler can glean from

 array references are equivalent to certain standard mathematical concepts

 Given an access function





 enumerate



 The dimension of the data region accessed is given by the rank of the

 matrix_F The dimension of the space of accesses to the same

 location is given by the nullity of F Iterations whose

 differences belong to the null_space of F refer to the same

 array_elements



 Iterations that share self-temporal_reuse of an access are separated

 by vectors in the null_space of F Self-spatial reuse can be

 computed similarly by asking when two iterations use the same row

 rather_than the same element Two accesses

 and share easily exploitable

 locality along the d

 direction if d is the particular solution to the equation

 In_particular if d is the

 direction corresponding to the innermost_loop ie the vector

 then there is spatial_locality if the array is

 stored in row-major form



 The data_dependence problem - whether two references can refer to the

 same_location - is equivalent to integer_linear programming Two

 access functions share a data_dependence if there are integer-valued

 vectors i and such that

 and





 enumerate



 exer

 Find the GCD's of the following sets of integers



 itemize

 a)

 b)

 c)

 itemize

 exer



 sexer

 For the following loop



 verbatim

 for_(i 0 i 10_i)

 Ai A10-i

 verbatim

 indicate all the



 itemize



 a) True dependences (write followed_by read of the same location)

 b) Antidependences (read followed_by write to the same location)

 c) Output dependences (write followed_by another write to the

 same location)



 itemize

 sexer



 hexer

 In the box on the Euclidean_algorithm we made a number of assertions

 without proof

 Prove each of the following



 itemize



 a)

 The Euclidean_algorithm as stated always works In_particular

 where is the nonzero remainder of



 b)





 c)

 for





 d)

 The GCD is really a function on sets of integers ie order doesn't

 matter Show the commutative law for GCD

 Then show the more difficult statement the associative law for

 GCD

 Finally show that together these laws imply that the GCD of a set of

 integers is the same regardless of the order in which the GCD's of

 pairs of integers are computed



 e)

 If and are sets of integers then





 itemize

 hexer



 hexer

 Find another solution to the second Diophantine_equation in

 Example gcd-ex

 hexer



 exer

 Apply the independent-variables test in the following situation The

 loop_nest is



 verbatim

 for_(i0 i100_i)

 for_(j0 j100_j)

 for (k0 k100 k)

 verbatim

 and inside the nest is an assignment involving array_accesses

 Determine if there are any data_dependences due to

 each of the following statements



 itemize



 a) Aijk Ai100j100k100

 b) Aijk Aj100k100i100

 c) Aijk Aj-50k-50i-50

 d) Aijk Ai99k100j



 itemize

 exer



 sexer

 In the two constraints



 arrayrrcll

 1 x y-100



 3 x 2y-50



 array



 eliminate by_replacing it by a constant lower_bound on

 sexer



 sexer

 Apply the loop-residue_test to the following set of constraints



 arrayl_l

 0x99 yx-50



 0y99 zy-60



 0z99



 array



 sexer



 exer

 Apply the loop-residue_test to the following set of constraints



 arrayl_l

 0x99 yx-50



 0y99 zy40



 0z99 xz20



 array



 exer



 exer

 Apply the loop-residue_test to the following set of constraints



 arrayl_l

 0x99 yx-100



 0y99 zy60



 0z99 xz50



 array



 exer

 IN

 OUT

 def

 A Logical Representation of Data Flow

 datalog-sect



 To this point our representation of data-flow_problems and solutions can

 be termed set-theoretic That is we represent information as sets

 and compute results using operators like union and intersection For

 instance when we introduced the reaching-definitions_problem in

 Section secrd-df we computed and

 for a block and we described these as sets

 of definitions We represented the contents of the block by its gen

 and kill_sets



 To cope with the complexity of interprocedural_analysis we now

 introduce a more general and succinct notation based_on logic

 Instead of saying something_like definition is in we

 shall use a notation like to mean the same_thing Doing_so

 allows_us to express succinct rules about inferring program

 facts

 It also allows_us to implement these rules efficiently in

 a way that generalizes the bit-vector approach to set-theoretic

 operations

 Finally the logical approach allows_us to combine what appear to be

 several independent analyses into one integrated algorithm For

 example in Section_secpre we described partial-redundancy

 elimination by a sequence of four data-flow_analyses and two other

 intermediate steps In the logical notation all these steps could be

 combined_into one collection of logical rules that are solved simultaneously



 Introduction to Datalog



 Datalog is a language that uses a Prolog-like notation but whose

 semantics is far simpler than that of Prolog

 To_begin the elements of Datalog are atoms of the form

 Here



 enumerate



 is a predicate - a symbol that represents a type of

 statement such_as a definition_reaches the beginning of a block



 are terms such_as variables or constants We_shall

 also allow simple expressions as arguments of a

 predicate(Formally such terms are built from function symbols

 and complicate the implementation of Datalog considerably However we

 shall use only a few operators such_as addition or subtraction of

 constants in contexts that do_not complicate matters)



 enumerate



 A ground atom is a predicate with only constants as arguments

 Every ground atom asserts a particular fact and its value is either

 true or false It is often convenient to represent a predicate by a

 relation or table of its true ground_atoms Each ground atom is

 represented_by a single row or tuple of the relation

 The columns of the relation are named by attributes

 and each tuple has a component for each attribute The attributes

 correspond to the components of the ground_atoms represented_by the

 relation Any ground atom in the relation is true and ground_atoms not

 in the relation are false



 ex

 atom-ex

 Let_us suppose the predicate means definition_reaches

 the beginning of block Then we might suppose that for a

 particular flow_graph is

 true as are and We might also

 suppose that for this flow_graph all other facts are false Then

 the relation in Fig atom-rel-fig represents the value of this

 predicate for this flow_graph



 figurehtfb



 center

 tabularll

















 tabular

 center



 Representing the value of a predicate by a relation

 atom-rel-fig



 figure



 The attributes of the relation are and The three tuples of the

 relation are and

 ex



 We_shall also see at times an atom that is really a comparison between

 variables and constants An_example would be or In

 these examples the predicate is really the comparison operator That

 is we can think of as if it were written in predicate form

 There is an important difference_between comparison

 predicates and others however A comparison predicate has its standard

 interpretation while an ordinary predicate like means only what it

 is defined to mean by a Datalog_program (described next)



 A literal is either an atom or a negated atom We indicate

 negation with the word NOT in front of the atom Thus NOT is an assertion that definition does_not reach the

 beginning of block



 Datalog Rules



 Rules are a way of expressing logical inferences In Datalog_rules

 also serve to suggest how a computation of the true facts should be

 carried_out The form of a rule is



 center

 -

 center

 The components are as_follows



 itemize



 is an atom and are literals (atoms possibly

 negated)



 is the head and form the body of

 the rule



 Each of the 's is sometimes_called a subgoal of the rule



 itemize



 We should read the - symbol as if

 The meaning of a rule is the head is true if the body is true

 More_precisely we apply a rule to a given set of ground_atoms as

 follows

 Consider all possible substitutions of constants for the variables of

 the rule If a substitution makes every subgoal of the body true

 (assuming that all and only the given ground_atoms are true)

 then we can infer that the head with this substitution of constants for

 variables is a true fact Substitutions that do_not make all subgoals

 true give us no information the head may or may not be true



 A Datalog_program is a collection of rules

 This program is applied to data that is to a set of ground_atoms

 for some of the predicates The result of the program is the set of

 ground_atoms inferred by_applying the rules until_no more inferences can

 be made



 Datalog Conventions

 We_shall use the following conventions for Datalog_programs



 enumerate



 Variables begin_with a capital letter



 All other elements begin_with lowercase letters or other symbols such

 as digits These elements include predicates and constants that are arguments

 of predicates



 enumerate



 ex

 path-ex

 A simple example of a Datalog_program is the computation of paths in a

 graph given its (directed) edges That is there is one predicate

 that means there is an edge from node to node

 Another predicate means that there is a path from to

 The rules defining paths are



 center

 tabularr_r c_l

 1) -



 2) -



 tabular

 center

 The first rule_says that a single edge is a path That is whenever we

 replace variable by a constant and variable by a constant

 and is true (ie there is an edge from node to node

 ) then is also true (ie there is a path from to

 )

 The second rule_says that if there is a path from some node to some

 node and there is also a path from to node then there is a

 path from to This rule expresses transitive_closure Note

 that any path can be formed_by taking the edges along the path and

 applying the transitive_closure rule repeatedly



 For_instance suppose that the following facts (ground atoms) are true

 and Then we can use the first

 rule with three different substitutions to infer

 and As an example substituting and

 instantiates the first rule to be

 Since is true we infer



 With these three facts we can use the second rule several_times

 If we substitute and we instantiate the rule to be

 Since both subgoals of

 the body have_been inferred they are known to be true so we may infer

 the head Then the substitution and

 lets_us infer the head that is there is a path from node 1

 to node 4

 ex



 Intensional and Extensional Predicates



 It is conventional in Datalog_programs to distinguish predicates as

 follows



 enumerate



 EDB or extensional database predicates are those that are defined

 a-priori That is their true facts are either given in a relation or

 table or they are given by the meaning of the predicate (as would be

 the case for a comparison predicate eg)



 IDB or intensional database predicates are defined only by the

 rules



 enumerate

 A predicate must_be IDB or EDB and it can be only one of these As a

 result any predicate that appears in the head of one or_more rules must

 be an IDB_predicate Predicates appearing in the body can be either IDB

 or EDB For_instance in Example path-ex is an EDB

 predicate and is an IDB_predicate Recall that we were given

 some facts such_as but the facts were

 inferred by the rules



 When Datalog_programs are used to express data-flow algorithms the EDB

 predicates are computed from the flow_graph itself IDB_predicates are

 then expressed by rules and the data-flow_problem is solved by

 inferring all possible IDB facts from the rules and the given EDB

 facts



 ex

 drd-ex

 Let_us consider how reaching_definitions might be_expressed in Datalog

 First it makes_sense to think on a statement level rather_than a block

 level that is the construction of gen and kill_sets from a basic_block

 will be integrated with the computation of the reaching_definitions

 themselves Thus the block suggested in Fig drd1-fig is

 typical Notice_that we identify points within the block numbered

 if is the number of statements in the block The

 th definition is at point and there is no definition at

 point 0



 figurehtfb



 center

 tabularl_l l

 x_yz



 p u



 x v







 tabular

 center



 A basic_block with points between statements

 drd1-fig



 figure



 A point in the program must_be represented_by a pair where

 is a block name and is an_integer between 0 and the number of

 statements in block Our formulation requires two EDB_predicates



 enumerate



 is true if and only if the th statement in block

 may define variable For_instance in Fig drd1-fig

 is true is true and

 is true for every possible variable that may point to at that

 point For the moment we_shall assume that can be any variable of

 the type that points to



 is true if and only if block is a successor of block

 in the flow_graph and has statements That is control can

 flow from the point of to the point 0 of For_instance

 suppose that is a predecessor of block in

 Fig drd1-fig and has 5 statements Then

 is true



 enumerate



 There is one IDB_predicate It is intended to be true

 if and only if the definition of variable at the th statement of

 block reaches the point in block

 The rules defining predicate are in Fig drd2-fig



 figurehtfb



 center

 tabularr_r c_l

 1) -







 2) -















 3) -







 tabular

 center



 Rules for predicate

 drd2-fig



 figure



 Rule (1) says_that if the th statement of block defines then

 that definition of reaches the th point of (ie the point

 immediately_after the statement) This rule corresponds to the concept

 of gen in our earlier set-theoretic formulation of reaching

 definitions



 Rule (2) represents the idea that a definition passes_through a

 statement unless it is killed and the only way to kill a definition

 is to redefine its variable with 100 certainty In detail rule_(2)

 says_that the definition of variable from the th statement of

 block reaches the point of block if



 itemize



 a)

 it reaches the

 previous point of and



 b)

 there is at_least one variable other_than

 that may be defined

 at the th statement of



 itemize



 Finally rule_(3) expresses the flow of control in the graph It says

 that the definition of at the th statement of block reaches

 the point 0 of if there is some block with statements such

 that the definition of reaches the end of and

 is a successor of

 ex



 The EDB_predicate from Example drd-ex clearly

 can be read off the flow_graph We can obtain from the flow

 graph as_well if we are conservative and assume a pointer can point

 anywhere If we_want to limit the range of a pointer to variables of

 the appropriate type then we can obtain type information from the

 symbol_table and use a smaller relation

 An option is to make an IDB_predicate and define it by rules

 These rules will use more primitive EDB_predicates which can themselves

 be determined from the flow_graph and symbol_table



 ex

 drd2-ex

 Suppose we introduce two new EDB_predicates



 enumerate



 is true whenever the th statement of block has

 on the left Note_that can be a variable or a simple expression

 with an l-value like



 is true if the type of is Again can be any expression

 with an l-value and can be any expression for a legal type



 enumerate

 Then we can write rules for making it an IDB_predicate

 Figure drd3-fig is an expansion of Fig drd2-fig with two

 of the possible rules for Rule (4) says_that the th

 statement of block defines if is assigned by the th

 statement Rule (5) says_that can also be defined by the th

 statement of block if that statement assigns to and is any

 of the variables of the type that points to Other kinds of

 assignments would need other rules for



 figurehtfb



 centertabularr_r c_l

 1) -







 2) -















 3) -











 4) -







 5) -











 tabularcenter



 Rules for predicates and

 drd3-fig



 figure



 As an example of how we would make inferences using the rules of

 Fig drd3-fig let_us re-examine the block of

 Fig drd1-fig The first statement assigns a value to variable

 so the fact would be in the EDB The third

 statement also assigns to so is another EDB fact

 The second statement assigns indirectly through so a third EDB fact

 is

 Rule (4) then allows_us to infer and



 Suppose that is of type pointer-to-integer (int) and and

 are integers Then we may use rule (5) with

 int and equal to either or to infer

 and Similarly we can infer the same about any other

 variable whose type is integer or coerceable to an_integer

 ex



 Execution of Datalog Programs

 datalog-eval-subsect



 Every set of Datalog_rules defines relations for its IDB_predicates as

 a function of the relations that are given for its EDB_predicates

 Start with the assumption that the IDB relations are empty (ie the

 IDB_predicates are false for all possible arguments)

 Then repeatedly

 apply the rules inferring new facts whenever the rules require us to do

 so When the process converges we are done and the resulting IDB

 relations form the output of the program This process is formalized in

 the next algorithm which is similar to the iterative_algorithms

 discussed in Chapter_code-op-ch



 alg

 naive-eval-alg

 Simple evaluation of Datalog_programs



 A Datalog_program and sets of facts for each EDB_predicate



 Sets of facts for each IDB_predicate



 For each predicate in the program let be the

 relation of facts that are true for that predicate

 If is an EDB_predicate then is the set of facts given for

 that predicate If is an IDB_predicate we_shall compute

 Execute the algorithm in Fig naive-eval-fig

 alg



 figurehtfb



 center

 tabularl

 for_(each IDB_predicate )







 while_(changes to any_occur)



 consider all possible substitutions of constants for



 variables in all the rules



 determine for each substitution whether all the



 subgoals of the body are true using the current



 's to determine truth of EDB and IDB_predicates



 if (a substitution makes the body of a rule true)



 add the head to if is the head predicate







 tabular

 center



 Evaluation of Datalog_programs

 naive-eval-fig



 figure



 ex

 naive-eval-ex

 The program in Example path-ex computes paths in a graph

 To apply_Algorithm naive-eval-alg we start with EDB_predicate

 holding all the edges of the graph and with the relation for

 empty On the first round rule_(2) yields nothing since there

 are no facts But rule (1) causes all the facts to become

 facts as_well That is after the first round we know

 if and only if there is an edge from to



 On the second round rule (1) yields no new paths facts because the EDB

 relation never changes However now rule_(2) lets_us put

 together two paths of length 1 to make paths of length 2 That is

 after the second round is true if and only if there is a

 path of length 1 or 2 from to Similarly on the third round

 we can combine paths of length 2 or less to discover all paths of length

 4 or less On the fourth round we discover paths of length up to to 8

 and in general after the th round is true if and only

 if there is a path from to of length or less

 ex



 Incremental Evaluation of Datalog Programs



 There is an efficiency enhancement of Algorithm naive-eval-alg

 possible Observe that a new IDB fact can only be discovered on round

 if it is the result of substituting constants in a rule such that

 at_least one of the subgoals becomes a fact that was just discovered on

 round The proof of that claim is that if all the facts among the

 subgoals were known at round then the new fact would have

 been discovered when we made the same substitution of constants on round





 To take_advantage of this observation introduce for each IDB_predicate

 a predicate that will hold only the newly discovered -facts

 from the previous round Each rule that has one or_more IDB_predicates

 among its subgoals is replaced_by a collection of rules Each rule in

 the collection is formed_by replacing exactly one occurrence of some IDB

 predicate in the body by

 Finally for all rules we replace the head

 predicate by The resulting rules are said to be in incremental form



 The relations for each IDB_predicate accumulates all the -facts

 as in Algorithm naive-eval-alg In one round we



 enumerate



 Apply the rules to evaluate the predicates



 Then subtract from to make_sure the facts in are

 truly new



 Add the facts in to



 Set all the relations to for the next round



 enumerate

 These ideas will be formalized in Algorithm seminaive-alg

 However first we_shall give an example



 Incremental Evaluation of Sets

 It is also possible to solve set-theoretic data-flow_problems

 incrementally For_example in reaching_definitions a definition can

 only be newly discovered to be in on the th round if it was just

 discovered to be in for some predecessor of The

 reason we do_not generally try to solve such data-flow_problems

 incrementally is that the bit-vector implementation of sets is so

 efficient It is generally easier to fly through the complete vectors

 than to decide_whether a fact is new or not



 ex

 seminaive-ex

 Consider the Datalog_program in Example path-ex again

 The incremental form of the rules is given in

 Fig seminaive1-fig Rule (1) does_not change except in the

 head because it has no IDB subgoals in the body However rule_(2)

 with two IDB subgoals becomes two different rules In each rule one

 of the occurrences of in the body is replaced_by

 Together these rules enforce the idea that at_least one of the two

 paths concatenated by the rule must have_been discovered on the previous

 round

 ex



 figurehtfb



 center

 tabularr_r c_l

 1) -







 2a) -











 2b) -







 tabular

 center



 Incremental rules for the path Datalog_program

 seminaive1-fig



 figure



 alg

 seminaive-alg

 Incremental evaluation of Datalog_programs



 A Datalog_program and sets of facts for each EDB_predicate



 Sets of facts for each IDB_predicate



 For each predicate in the program let be the

 relation of facts that are true for that predicate

 If is an EDB_predicate then is the set of facts given for

 that predicate If is an IDB_predicate we_shall compute

 In_addition for each IDB_predicate let be a relation of

 new facts for predicate



 enumerate



 Modify the rules into the incremental form described above



 Execute the algorithm in Fig seminaive2-fig



 enumerate

 alg



 figurehtfb



 center

 tabularl

 for_(each IDB_predicate )















 repeat



 consider all possible substitutions of constants for



 variables in all the rules



 determine for each substitution whether all the



 subgoals of the body are true using the current



 's and 's to determine truth of EDB



 and IDB_predicates



 if (a substitution makes the body of a rule true)



 add the head to where is the head



 predicate



 for_(each predicate )















 until (all 's are empty)



 tabular

 center



 Evaluation of Datalog_programs

 seminaive2-fig



 figure



 Problematic Datalog Rules



 There_are certain Datalog_rules or programs that technically have

 no meaning and should not be used The two most_important risks are



 enumerate



 Unsafe rules those that have a variable in the head that does

 not appear in the body in a way that constrains that variable to take on

 only values that appear in the EDB



 Unstratified programs sets of rules that have a recursion

 involving a negation



 enumerate

 We_shall elaborate on each of these risks



 Rule Safety



 Any variable that appears in the head of a rule must also appear in the

 body Moreover that appearance must_be in a subgoal that is an

 ordinary IDB or EDB atom It is not acceptable if the variable appears

 only in a negated atom or only in a comparison operator The_reason

 for this policy is to avoid rules that let_us infer an_infinite number

 of facts



 ex

 safety-ex

 The rule



 center

 tabularl

 - NOT



 tabular

 center

 is unsafe for two_reasons Variable appears only in the negated

 subgoal and the comparison

 appears only in the comparison

 The consequence is that is true for an_infinite number of pairs

 as_long as is false and is anything other_than

 ex



 Stratified Datalog



 In order for a program to make sense recursion and negation must_be

 separated The formal requirement is as_follows We must_be able to

 divide the IDB_predicates into strata so that if there is a rule

 with head predicate and a subgoal of the form NOT

 then is either EDB or an IDB_predicate in a lower stratum than

 As_long as this rule is satisfied we can evaluate the strata lowest

 first by Algorithm naive-eval-alg or seminaive-alg and

 then treat the relations for the IDB_predicates of that strata as if

 they_were EDB for the computation of higher strata However if we

 violate this rule then the iterative_algorithm may fail to converge as

 the next example shows



 ex

 stratified-ex

 Consider the Datalog_program consisting of the one rule



 center

 tabularl

 - NOT



 tabular

 center

 Suppose is an EDB_predicate and only is true Is

 true



 This program is not stratified Whatever stratum we put in its

 rule has a subgoal that is negated and has an IDB_predicate (namely

 itself) that is surely not in a lower stratum than



 If we apply the iterative_algorithm we start with so

 initially the answer is no is not true

 However the first iteration lets_us infer since both and

 NOT are true But then the second iteration tells_us

 is false

 That is substituting 1 for in the rule does_not allow

 us to infer since subgoal NOT is false

 Similarly the third iteration says is true the fourth says it

 is false and so on We_conclude that this unstratified program is

 meaningless and do_not consider it a valid program

 ex



 hexer

 dl-rd-exer

 In this problem we_shall consider a reaching-definitions

 data-flow_analysis that is simpler than that in

 Example drd-ex Assume that each statement by itself is a block

 and initially assume that each statement defines exactly one variable

 The EDB_predicate means that statement is a predecessor

 of statement The EDB_predicate means that the

 variable defined by statement is We_shall use IDB_predicates

 and to mean that definition_reaches the

 beginning or end of statement respectively Note_that a definition

 is really a statement number Fig dl-rd-exer-fig

 is a Datalog_program that expresses

 the usual algorithm for computing reaching_definitions



 figurehtfb



 center

 tabularr_r c_l

 1) -







 2) -



 3) - NOT







 4) -



 tabular

 center



 Datalog_program for a simple reaching-definitions analysis

 dl-rd-exer-fig



 figure



 Notice_that rule (1) says_that a statement kills itself but rule_(2)

 assures that a statement is in its_own out set anyway Rule (3) is

 the normal transfer_function and rule (4) allows confluence since

 can have several predecessors



 Your problem is to modify the rules to handle the common case_where a

 definition is ambiguous eg an assignment through a pointer In this

 situation may be true for several different 's and

 one A definition is best represented_by a pair where

 is a statement and is one of the variables that may be defined at

 As a result and become three-argument predicates

 eg means that the (possible) definition of at

 statement reaches the beginning of statement

 hexer



 exer

 Write a Datalog_program analogous to Fig dl-rd-exer-fig to

 compute available_expressions In_addition to predicate defines

 use a predicate that says statement causes expression

 to be evaluated Here is the operator in the expression

 eg

 exer



 exer

 Write a Datalog_program analogous to Fig dl-rd-exer-fig to

 compute live_variables In_addition to predicate defines

 assume a predicate that says statement uses variable

 exer



 exer

 In Section_secpre we defined a data-flow calculation that

 involved six concepts anticipated available earliest postponable

 latest and used Suppose we had written a Datalog_program to define

 each of these in terms of EDB concepts derivable_from the program (eg

 gen and kill information) and others of these six concepts Which of

 the six depend_on which others Which of these dependences are negated

 Would the resulting Datalog_program be stratified

 exer



 exer

 Suppose that the EDB_predicate consists of the following

 facts



 center

 tabularl_l l









 tabular

 center



 itemize



 a) Simulate the Datalog_program of Example path-ex on this

 data using the simple evaluation strategy of Algorithm naive-eval-alg

 Show the facts discovered at each round



 b) Simulate the Datalog_program of Fig seminaive1-fig on this

 data as part of the incremental

 evaluation strategy of Algorithm seminaive-alg

 Show the facts discovered at each round



 itemize

 exer



 exer

 The following rule



 center

 tabularl

 - NOT



 tabular

 center

 is part of a larger Datalog_program



 itemize



 a) Identify the head body and subgoals of this rule

 b) Which predicates are certainly IDB_predicates of program

 c) Which predicates are certainly EDB_predicates of

 d)_Is the rule safe

 e) Is stratified



 itemize

 exer



 exer

 Convert the rules of Fig drd2-fig to incremental form

 exer

 Types and Declarations

 decl-sect



 The applications of types can be grouped under checking and

 translation



 itemize



 Type_checking uses logical rules to reason_about the

 behavior of a program at_run time Specifically it ensures that

 the types of the operands match the type expected by an operator

 For_example the operator in Java expects its two

 operands to be booleans the result is also of type boolean



 Translation Applications From the type of a name a

 compiler can determine the storage that will be needed for that

 name at_run time Type information is also needed to calculate

 the address denoted_by an array reference to insert explicit type

 conversions and to choose the right version of an arithmetic

 operator among other things



 itemize



 In this_section we examine types and storage layout for names

 declared within a procedure or a class The actual storage for a

 procedure call or an object is allocated at_run time when the

 procedure is called or the object is created As we examine local

 declarations at_compile time we can however lay out relative_addresses where the relative_address of a name or a

 component of a data_structure is an offset from the start of a

 data area



 Type Expressions

 type-expr-subsect



 Types have structure which we_shall represent using type

 expressions a type expression is either a basic type or is

 formed_by applying an operator called a type_constructor to

 a type expression The sets of basic types and constructors depend

 on the language to be checked



 ex

 type-expr-ex The array type int23 can be read as

 array of 2 arrays of 3 integers each and written as a type

 expression

 This type is represented_by the tree in Fig type-expr-fig

 The operator array takes two parameters a number and a

 type

 ex



 figurehtbf

 Type

 expression for int23type-expr-fig

 figure



 We_shall use the following definition of type expressions



 itemize



 A basic type is a type expression Typical basic types for a

 language include boolean char integer float and void

 the latter denotes the absence of a value



 A type name is a type expression



 A type expression can be formed_by applying the array

 type_constructor to a number and a type expression



 A record is a data_structure with named fields A type

 expression can be formed_by applying the record type

 constructor to the field names and their types Record types will

 be_implemented in Section record-types-subsect by_applying

 the constructor record to a symbol_table containing entries

 for the fields



 A type expression can be formed_by using the type

 constructor for function types We write

 for function from type to type Function types will be

 useful when type_checking is discussed in

 Section type-check-sect



 If and are type expressions then their Cartesian

 product is a type expression Products are introduced

 for completeness they can be used to represent a list or tuple of

 types (eg for function parameters) We assume that

 associates to the left and that it has higher_precedence than





 Type expressions may contain variables whose values are type

 expressions Compiler-generated type variables will be used in

 Section poly-subsect



 itemize



 Type Names

 and Recursive Types

 Once a class is defined its name can be used

 as a type name in C or Java for example consider Node in the

 program_fragment



 center

 tabularl

 public_class Node 123 125







 public Node n

 tabular

 center



 Names can be used to define recursive types which are needed for

 data_structures such_as linked_lists The pseudocode for a list

 element



 center

 class Cell 123 int info Cell next

 125

 center

 defines the recursive type Cell as a class that

 contains a field info and a field next of type Cell Similar recursive types can be defined in C using records

 and pointers

 The techniques in this_chapter carry over to recursive types



 A convenient_way to represent a type expression is to use a graph

 The value-number method of Section val-num-subsect can be

 used to construct a DAG for a type expression with interior

 nodes for type constructors and leaves for basic types type

 names and type variables for example see the tree in

 Fig type-expr-fig(Since type names denote type

 expressions they can set up implicit cycles see the box on

 Type Names and Recursive Types If edges to type names are

 redirected to the type expressions denoted_by the names then the

 resulting graph can have cycles due to recursive types)



 Type Equivalence



 When are two type expressions equivalent Many type-checking

 rules have the form if two type expressions are equal

 then return a certain type else error Potential

 ambiguities arise when names are given to type expressions and the

 names are then used in subsequent type expressions The key issue

 is whether a name in a type expression stands_for itself or

 whether it is an abbreviation for another type expression



 When type expressions are represented_by graphs two types are

 structurally equivalent if and only if one of the following

 conditions is true



 itemize

 They are the same basic type



 They are formed_by applying the same constructor to

 structurally equivalent types



 One is a type name that denotes the other

 itemize

 If type names are treated_as standing for themselves then the

 first two conditions in the above definition lead to name

 equivalence of type expressions



 Name-equivalent expressions are assigned the same value number if

 we use Algorithm val-num-alg Structural equivalence can be

 tested using the unification algorithm in

 Section unify-subsect



 Declarations

 decl-subsect



 We_shall study types and declarations using a simplified grammar

 that declares just one name at a time declarations with lists of

 names can be handled as discussed in Example sdd-type-ex

 The grammar is

 center

 tabularr_c l





 record ''''



 int float



 num

 tabular

 center



 The fragment of the above grammar that deals_with basic and array

 types was used to illustrate inherited_attributes in

 Section array-type-subsect The difference in this_section

 is that we consider storage layout as_well as types



 Nonterminal generates a sequence of declarations

 Nonterminal generates basic array or record types

 Nonterminal generates one of the basic types int and

 float Nonterminal for component generates strings of

 zero_or more integers each integer surrounded_by brackets An

 array type consists of a basic type specified_by followed_by

 array components specified_by nonterminal A record type (the second

 production for )

 is a sequence of declarations for the fields of the record all

 surrounded_by curly_braces





 Storage Layout for Local Names



 From the type of a name we can determine the amount of storage

 that will be needed for the name at_run time At compile_time we

 can use these amounts to assign each name a relative_address The

 type and relative_address are saved in the symbol-table_entry for

 the name Data of varying length such_as strings or data whose

 size cannot be determined until run_time such_as dynamic arrays

 is handled by reserving a known fixed amount of storage for a

 pointer to the data Run-time storage management is discussed in

 Chapter_run-time-ch



 Suppose that storage comes in blocks of contiguous bytes where a

 byte is the smallest unit of addressable memory Typically a byte

 is eight bits and some number of bytes form a machine word

 Multibyte objects are stored in consecutive bytes and given the

 address of the first byte



 Address Alignment

 The

 storage layout for data objects is strongly influenced by the

 addressing constraints of the target_machine For_example

 instructions to add integers may expect integers to be aligned that is placed_at certain positions in memory such_as

 an address divisible by 4 Although an array of ten characters

 needs only enough bytes to hold ten characters a compiler may

 therefore allocate 12 bytes - the next multiple of 4 -

 leaving 2 bytes unused Space left

 unused due to alignment considerations is referred to as padding When space is at a premium a compiler may pack

 data so that no padding is left additional instructions may then

 need to be executed at_run time to position packed data so that it

 can be operated on as if it were properly aligned



 The width of a type is the number of storage units needed

 for objects of that type A basic type such_as a character

 integer or float requires an integral number of bytes For easy

 access storage for aggregates such_as arrays and classes is

 allocated in one contiguous block of bytes(Storage

 allocation for pointers in C and C is simpler if all pointers

 have the same width The_reason is that the storage for a pointer

 may need to be allocated before we learn the type of the objects

 it can point to)



 The translation_scheme () in Fig type-width-fig

 computes types and their widths for basic and array types record

 types will be discussed in Section record-types-subsect The

 uses synthesized_attributes type and width for

 each nonterminal and two variables and to pass type and

 width information from a node in a parse_tree to the node for

 the production In a syntax-directed

 definition and would be inherited_attributes for



 figurehtfb

 center

 tabularr_c l_l l























 num







 tabular

 center

 Computing types and their widths type-width-fig

 figure



 The body of the -production consists of nonterminal an

 action and nonterminal which appears on the next line The

 action between and sets to and to

 If then

 is set to integer and is set to 4 the

 width of an_integer Similarly if then

 is float and is 8 the width

 of a float



 The productions for determine_whether generates a basic

 type or an array type If then becomes

 and becomes



 Otherwise specifies an array component The action for

 forms by

 applying the type_constructor array to the operands

 and For_instance the

 result of applying array might be a tree structure such_as

 Fig type-expr-fig



 The width of an array is obtained_by multiplying the width of an

 element by the number of elements in the array If addresses of

 consecutive integers differ by 4 then address_calculations for an

 array of integers will include multiplications by 4 Such

 multiplications provide opportunities for optimization so it is

 helpful for the front_end to make them explicit In this_chapter

 we ignore other machine dependencies such_as the alignment of data

 objects on word boundaries



 ex

 The parse_tree for the type int23 is shown by dotted

 lines in Fig type-sdt-fig The solid lines show_how the

 type and width are passed from down the chain of 's through

 variables and and then back up the chain as synthesized

 attributes type and width The variables and

 are assigned the values of and

 respectively before the subtree with the nodes is examined

 The values of and are used at the node for

 to start the evaluation of the synthesized_attributes up

 the chain of nodes

 ex



 figurehtfb

 array(3integer)

 array(2array(3integer))





 Syntax-directed_translation of array types

 type-sdt-fig

 figure



 Sequences of Declarations



 Languages such_as C and Java allow all the declarations in a

 single procedure to be processed as a group The declarations may

 be distributed within a Java procedure but they can still be

 processed when the procedure is analyzed Therefore we can use a

 variable say to keep_track of the next available

 relative_address



 The translation_scheme of Fig decl-types-fig deals_with a

 sequence of declarations of the form where

 generates a type as in Fig type-width-fig Before the first

 declaration is considered is set to 0 As each

 new name is seen is entered into the symbol_table with

 its relative_address set to the current value of

 which is then incremented by the width of the type of



 figurehtfb

 center

 tabularr_c l_l l

 offset 0























 tabular

 center

 Computing the relative_addresses of declared names

 decl-types-fig

 figure



 The semantic_action within the production

 creates a symbol-table_entry by

 executing



 Here top denotes the current symbol_table The method

 creates a symbol-table_entry for

 with type and relative

 address in its data area



 The initialization of in Fig decl-types-fig

 is more evident if the first production appears on one line as

 equationmarker-eq

 P offset 0 D

 equation

 Nonterminals generating called marker_nonterminals can

 be used to rewrite productions so that all actions appear at the

 ends of right_sides see Section l-att-lr-subsect Using a

 marker_nonterminal (marker-eq) can be restated as

 center

 tabularr_c l_l







 tabular

 center



 Fields in Records and Classes

 record-types-subsect



 The translation of declarations in Fig decl-types-fig

 carries over to fields in records and classes Record types can be

 added to the grammar in Fig type-width-fig by_adding the

 following production



 center

 record ''''

 center



 The fields in this record type are specified_by the

 sequence of declarations generated_by The approach of

 Fig decl-types-fig can be used to determine the types and

 relative_addresses of fields provided we are careful about two

 things



 itemize

 The field names within a record must_be distinct that is a

 name may appear at most once in the declarations generated_by



 The offset or relative_address for a field name is relative

 to the data area for that record

 itemize



 ex

 The use of a name for a field within a record does_not

 conflict with other uses of the name outside the record Thus the

 three uses of x in the following declarations are distinct

 and do_not conflict with each other

 center

 tabularl

 'float x'



 'record float_x float y p'



 'record int tag float_x float y q'

 tabular

 center



 A subsequent assignment

 sets

 variable x to the sum of the fields named x in the

 records p and q Note_that the relative_address of

 x in p differs_from the relative_address of x in

 q

 ex



 For_convenience record types will encode both the types and

 relative_addresses of their fields using a symbol_table for the

 record type A record type has the form where

 record is the type_constructor and is a symbol-table object that

 holds information_about the fields of this record type



 The translation_scheme in

 Fig record-types-fig consists of a single production to be

 added to the productions for in Fig type-width-fig

 This production has two semantic_actions The embedded action

 before saves the existing symbol_table denoted_by top

 and sets top to a fresh symbol_table It also saves the

 current offset and sets offset to 0 The

 declarations generated_by will result in types and relative

 addresses being put in the fresh symbol_table The action after

 creates a record type using top before restoring the

 saved symbol_table and offset



 figurehtfb

 center

 tabularr_c l_l l

 record ''







 ''







 tabular

 center

 Handling of field names in records

 record-types-fig

 figure



 For concreteness the actions in Fig record-types-fig give

 pseudocode for a specific implementation Let class_Env

 implement symbol_tables The call

 pushes the current symbol

 table denoted_by top onto a stack Variable top is

 then set to a new symbol_table Similarly offset is pushed

 onto a stack called Stack Variable offset is then

 set to 0



 After the declarations in have_been translated the symbol

 table top holds the types and relative_addresses of the

 fields in this record Further offset gives the storage

 needed for all the fields The second action sets

 to and to offset Variables top and offset are then restored

 to their pushed values to complete the translation of this record

 type



 This discussion of storage for record types carries over to

 classes since no storage is reserved for methods See

 Exercise class-layout-exer



 exer Determine the types and relative_addresses for the

 identifiers in the following sequence of declarations

 center

 tabularl

 'float x'



 'record float_x float y p'



 'record int tag float_x float y q'

 tabular

 center

 exer



 hexer

 class-layout-exer Extend the handling of field names in

 Fig record-types-fig to classes and single-inheritance

 class hierarchies



 itemize



 a)_Give an implementation of class_Env that allows

 linked symbol_tables so that a subclass can either redefine a

 field name or refer directly to a field name in a superclass



 b)_Give a translation_scheme that allocates a contiguous

 data area for the fields in a class including inherited fields

 Inherited fields must maintain the relative_addresses they_were

 assigned in the layout for the superclass



 itemize



 hexer

 Introduction to Data-Flow_Analysis

 secdf-intro



 All the optimizations introduced in Section_secopt-sources can

 benefit from data-flow_analysis Data-flow_analysis is a

 technique that derives information related to the flow

 of data along control paths For_example one way to implement global

 common-subexpression_elimination requires us to determine_whether two textually

 identical expressions evaluate to the same value along any possible

 execution_path of the program

 As_another example if the result

 of an assignment is not used along any subsequent execution_path

 then we can eliminate the

 assignment as dead_code

 These and many other important questions can be addressed

 by data-flow_analysis



 The Data-Flow Abstraction



 A program execution can be_viewed as a series of transformations of the

 program state which consists of the values of the program

 counter and all the variables in the program including those associated

 with stack frames below the top of the run-time_stack Each execution of

 an intermediate-code

 statement transforms an input state to a new output state The input

 state is associated_with the program point before the statement

 and the output state is associated_with the program point after the

 statement



 When we analyze the behavior of a program we must consider all the

 possible sequences of program points (paths) through a flow_graph

 that the program execution can take

 We then extract out of the possible program states at each point the

 information we need for the particular data-flow_analysis problem we aim

 to solve

 In more_complex analyses we must consider paths that jump among the

 flow_graphs for various procedures as calls and returns are

 executed

 However to begin our study we_shall concentrate_on the paths through a

 single flow_graph for a single procedure



 Let_us see what the flow_graph tells_us about the possible_execution

 paths



 itemize



 Within one basic_block the program point after a statement is the same

 as the program point before the next statement



 If there is an edge from basic_block to basic_block then the

 program point after the last statement of

 may be followed immediately by

 the program point before the first statement of





 itemize

 Thus we may define

 an execution_path (or just path)

 from to to be a sequence of points

 such that for each

 between 1 and either



 enumerate



 is the point_immediately preceding a statement and

 is the point_immediately following that

 statement in the same block or



 is the end of some block and

 is the beginning

 of a successor block



 enumerate



 In_general there is an_infinite

 number of possible_execution paths through a program

 and there is no finite upper_bound on the length of an execution_path

 Program analyses summarize all the possible program states that can

 occur at a point in

 the program with a finite set of facts As_discussed in

 Section secprog-abs different analyses may choose to abstract

 out different information



 ex

 Even the simple program in Fig figdf-abs describes an

 unbounded_number of execution_paths

 Not entering the loop at all

 the shortest complete

 execution_path consists of the program points

 The next shortest path executes one iteration of the loop

 and consists of the points



 We know that for example

 the first time program point_(5) is executed

 the value of a is 1 due to definition We_say that

 reaches point_(5) in the first iteration In subsequent

 iterations reaches point_(5) and the value of a is 243



 figurehtb

 figureuullmanalsuch9figsdf-abseps

 Example program illustrating the data-flow_abstraction

 figdf-abs

 figure



 In_general it is not possible to

 keep_track of all the program states for all

 possible paths In data-flow_analysis we do_not distinguish

 among the paths taken to reach a program point

 Moreover we do_not keep_track of entire states rather we abstract_out

 certain details keeping only the data we need for the purpose of the

 analysis

 Two examples will illustrate_how the same program states may lead to

 different information abstracted at a point



 enumerate



 To help users debug their programs we may wish

 to find out what are all the values a variable may take on at a program

 point

 and where these values may be defined For_instance we may

 summarize all the program states at point_(5) by

 saying that the value of a is one of and that it

 may be defined by one of The definitions that

 may reach a program point along some path are known_as reaching_definitions



 Suppose instead we are_interested in implementing constant_folding

 If a use of the variable is reached by only one definition and that

 definition assigns a constant to then we can simply replace by

 the constant

 If on the other_hand several definitions of may

 reach a single program point then we cannot perform constant_folding on



 Thus for constant_folding we_wish to find those definitions that

 that are the unique definition of their variable

 to reach a given program point no_matter which execution_path is

 taken For point_(5) of Fig figdf-abs there is no definition

 that must_be the definition of a at that point so this set is

 empty for a at point_(5)

 Even_if a variable has a unique definition at a point that definition

 must assign a constant to the variable

 Thus we may simply describe certain

 variables as not a constant instead of collecting all their

 possible values or all its possible definitions



 enumerate

 Thus we see that the same information may be

 summarized differently depending_on the purpose of the analysis

 ex



 The Data-Flow_Analysis Schema

 secdf-schema



 In each application of

 data-flow_analysis we associate with every program point a data-flow value that represents an abstraction of the set of all

 possible program states that can be observed for that

 point For_example the domain of the data-flow_values

 for reaching_definitions

 are subsets of definitions in the program and they represent

 all the possible definitions that may reach a program point As

 discussed_above the choice of abstraction depends_on the goal of the

 analysis to be efficient we only keep_track of information that is

 relevant



 We denote the data-flow_values before and after each statement as

 and respectively The data-flow_problem is

 generally defined to be finding a solution to a set of constraints on

 and for all statements There_are two sets of

 constraints those based_on the semantics of the statements and those

 based_on the flow of control



 Transfer_Functions



 The data-flow_values before and after a statement are constrained by

 the semantics of the statement For_example suppose our data-flow

 analysis involves determining the constant value of variables at points

 If variable a has

 value before executing statement b a then both a and

 b will have the value after the statement This relationship

 between the data-flow_values before and after the assignment_statement is

 known_as a transfer_function



 Transfer

 functions come in two flavors information may propagate

 forward along execution_paths or it may flow

 backwards up the execution_paths In a forward-flow problem the

 transfer_function of a statement which we_shall usually

 denote takes the data-flow

 value before the statement and produces a new data-flow value after the

 statement That is



 s_fs(s)



 Conversely in a backward-flow problem

 the transfer_function for statement

 converts a data-flow value after the statement to a new

 data-flow value before the statement That is



 s_fs(s)





 Control-Flow Constraints



 The second set of constraints on data-flow_values

 is derived_from the flow of control

 Within a basic_block control_flow is simple

 If a block consists of statements in that

 order then the control-flow value out of is the same as the

 control-flow value into

 That is



 center

 for all

 center



 However control-flow edges between basic_blocks

 create more_complex constraints between the last statement of

 one basic_block and the first statement of the following block

 For_example if we are_interested in collecting

 all the definitions that may reach a program point then the set of

 definitions_reaching the leader statement of a basic_block is the union of

 the definitions after the last statements of each of the predecessor

 blocks



 Data Flow Schemas on Basic_Blocks



 While a data-flow_schema technically involves data-flow_values at each

 point in the program we can save time and space by recognizing that

 what goes on inside a block is usually quite_simple control_flows from

 the beginning to the end of the block

 Thus we can restate the schema in terms of data-flow_values entering and

 leaving the blocks

 We denote the data-flow_values immediately_before

 and immediately

 after each basic_block by and respectively

 The constraints involving and can be derived_from

 those involving

 and for the various statements as_follows



 Suppose block consists of statements in that

 order

 If is the first statement of basic_block then

 Similarly if is the last statement of basic_block

 then The transfer_function of a basic_block

 which we denote can be derived by_composing the transfer_functions of the

 statements in the block That is let be the

 transfer_function of statement

 Then

 The relationship_between the beginning and end of the block is



 B_fB(B)





 The constraints

 due to control_flow between basic_blocks can easily be_rewritten by

 substituting and for and

 respectively

 For_instance if data-flow_values are information_about the sets of

 constants that may be assigned to a variable then



 B_P a predecessor of B_P





 When the data-flow is backwards the equations are similar but with the

 roles of the 's and 's reversed

 That is



 B_fB(B)





 B_S a successor of B

 S





 To summarize in data-flow_analysis we typically create for each

 basic_block two data-flow variables representing the start and end

 points of the block There_are two sets of constraints In the

 first variables of the same basic_block are constrained by a transfer

 function representing the semantics of the statements in the block

 The second set of constraints reflects the control_flow between basic

 blocks



 Unlike linear arithmetic equations the data-flow_equations do_not

 usually have a unique solution

 Our_goal is to find the most_precise solution that satisfies

 these two sets of constraints

 That is we need a solution that encourages valid code improvements but

 does_not justify unsafe transformations - those that change_what the

 program computes

 This issue is discussed briefly here in the box on Conservatism and

 more extensively in Section df-semantics-subsect

 In the following subsections we discuss some of the most_important

 examples of problems that can be_solved by data-flow_analysis



 Reaching_Definitions

 secrd-df



 Reaching_definitions is one of the most

 common and useful data-flow_schemas

 By knowing where in the program each variable x may have_been defined

 when control_reaches each point we can tell many things about x

 For just two examples

 a compiler then knows whether x is a constant at point and a

 debugger can tell_whether it is possible for x to be an undefined

 variable should x be used at



 We_say a definition_reaches a point if there is a path

 from the point_immediately following to such that is not

 killed along that path

 We kill a definition of a variable x if

 there is any other definition of x anywhere

 along the pathfootnoteNote that the path may have loops so we

 could come to another occurrence of along the path which does_not

 kill footnote

 Intuitively if a definition of some

 variable x reaches point then might be the place at

 which the value of x used at might last have_been defined



 A definition of a variable x is a statement that assigns or may

 assign a value to x As_discussed in

 Section secdata-acc parameters array_accesses and indirect

 references may have aliases and it is not easy to tell if a statement

 is referring to a particular variable x Program analysis

 must_be conservative if we do_not know whether a statement is assigning a

 value of x we must assume that it may assign to it that

 is variable x after statement may have either its original

 value before or the new value created by For the sake of

 simplicity the rest of the chapter assumes that we

 are dealing only with variables that have no_aliases This class of

 variables includes

 all local_scalar variables in most languages in the case of C and

 C local_variables whose addresses have_been taken are

 excluded as discussed in Section secdata-acc



 Conservatism in Data-Flow_Analysis

 Since all data-flow_schemas compute approximations to the ground truth

 (as_defined by all possible_execution paths of the program) we are

 obliged to assure that any errors are in the safe_direction

 A policy decision is safe (or conservative)

 if it never allows_us to

 change_what the program computes

 Safe policies may unfortunately cause_us to miss some code

 improvements that would retain the meaning of the program but in

 essentially all code-optimizations there is no safe policy that misses

 nothing

 It would generally be unacceptable to use an unsafe policy - one that

 sped up the code at the expense of changing what the program computes



 Thus when designing a data-flow_schema we must_be conscious of how the

 information will be used and make_sure that any approximations we make

 are in the conservative or safe_direction

 Each schema and application must_be considered independently

 For_instance if we use reaching_definitions for constant_folding it is

 safe to think a definition_reaches when it doesn't (we_might think x

 is not a constant when in fact it is and could have_been folded) but

 not safe to think a definition doesn't reach when it does (we_might

 replace x by a constant when the program would at times have a value

 for x other_than that constant)



 ex

 Shown in Fig_figreach-def is a flow_graph with seven

 definitions Let_us focus_on the definitions_reaching block

 All the definitions in block

 reach the beginning of block The

 definition j_j-1 in block also reaches the

 beginning of block because no other definitions of j can

 be found in the loop leading back to This definition however

 kills the definition j_n preventing it from reaching

 or The statement i_i1 in does_not reach

 the beginning of though because the variable of i is

 always redefined by i u3 Finally the definition

 a u2 also reaches the beginning of block

 ex



 figurehtb

 figureuullmanalsuch9figsreach-defeps

 Flow_graph for illustrating reaching_definitions

 figreach-def

 figure



 By defining reaching_definitions as we have we sometimes allow

 inaccuracies

 However they are all in the safe or conservative direction

 For_example

 notice our_assumption that all edges

 of a flow_graph can be traversed

 This assumption may not be true in practice

 For_example for no values of

 a

 and

 b

 can control actually reach statement 2

 in the following program_fragment



 center

 tabularl

 if_( a b ) statement 1

 else if_( a b ) statement 2

 tabular

 center



 To decide in general whether each path in a flow_graph can

 be taken is an undecidable problem

 Thus we simply assume that every branch in the program can be taken

 In most applications of reaching_definitions it

 is conservative to assume that a

 definition can reach a point even if it might not

 Thus we may allow paths that are never be traversed in any execution

 of the program and we may allow definitions to pass

 through ambiguous definitions of the same variable safely



 Transfer Equations for Reaching_Definitions



 We_shall now set up the constraints for the reaching

 definitions problem We start by_examining the details of a single

 statement Consider a definition



 center

 u vw

 center

 Here and frequently in what_follows

 is used as a generic binary_operator



 This statement generates a definition

 and kills all the other definitions in the program that

 define variable u

 while leaving the remaining incoming definitions unaffected

 The transfer_function of definition thus can be_expressed as



 equation

 gen-kill-eq

 fd(x) gend (x_- killd)

 equation

 where the set of definitions generated_by the statement

 and is the set of all other definitions of u in the program



 As_discussed in Section secdf-schema the transfer_function of a

 basic_block can be found by_composing the transfer_functions of the

 statements contained therein

 The composition of functions of the form (gen-kill-eq) which we

 shall_refer to as - form is also of

 that form as we can see as_follows

 Suppose there are two functions and



 Then



 align

 f2(f1(x))

 gen2(gen1(x-kill1)-kill2)



 (gen2(gen1-kill2))(x-(kill1kill2))



 align



 This rule extends to a block consisting of any number of statements

 Suppose block has statements with transfer_functions

 for

 Then the transfer_function for block may be written as



 fB(x) genB (x_- killB)



 where



 killB kill1kill2killn



 and

 align

 genB

 genn(genn-1-killn)(genn-2-killn-1-killn)



 (gen1-kill2-kill3-align



 Thus like a statement a basic_block also generates a set of

 definitions and kills a set of definitions The set contains

 all the definitions inside the block

 that are visible immediately_after the block - we refer to

 them as downwards-exposed A definition is downwards-exposed in

 a basic_block only if it is not_killed by a

 subsequent definition to the same variable inside the same basic

 block A basic block's set is simply the union of all the

 definitions killed by the individual_statements Notice_that a

 definition may appear in both the and set of a basic

 block

 If so the fact that it is in takes_precedence

 because in - form the set is applied before the

 set



 ex

 gen-kill-ex

 The set for the following basic_block



 center

 tabularr_l

 a 3



 a 4



 tabular

 center



 is since is not downwards-exposed The set

 contains both and since kills and vice_versa

 Nonetheless since the subtraction of the set precedes

 the union operation with the set the result of the transfer

 function for this block always includes definition

 ex



 Control-Flow Equations



 Next we consider the set of constraints derived_from the control_flow

 between basic_blocks Since a definition_reaches a program point as

 long_as there_exists at_least one path along which the definition

 reaches

 whenever there is a control-flow edge

 from to

 However since a definition cannot reach a point unless there is a path

 along which it reaches

 needs to be no larger_than the

 union of the reaching_definitions of all the predecessor blocks

 That is it is safe to assume



 B_P a predecessor of B_P





 We refer to union as the meet_operator for

 reaching_definitions

 In any data-flow_schema the meet_operator is the one

 we use to create a summary of the

 contributions from different paths at the confluence of those

 paths



 Iterative_Algorithm for Reaching_Definitions



 We assume that every control-flow_graph has two empty basic_blocks an

 entry_node which represents the starting point of the graph

 and an exit node to which all exits_out of the graph go

 Since no definitions reach the beginning of the graph

 the transfer_function for the entry block is a simple

 constant function that returns as an answer That is





 The reaching_definitions problem is defined by the following

 equations





 entry

 For all basic_blocks other_than entry



 B genB (B_- killB)





 B_P a predecessor of B_P



 These recurrences can be_solved using

 the following algorithm



 alg

 algreaching-definitions

 Reaching_definitions



 A flow_graph for which and have_been computed for

 each block



 and the set of definitions

 reaching the entry and exit of each block of the flow_graph



 We use an iterative_approach in which we start with the estimate

 for all and converge to the

 desired values of and

 As we must iterate_until the 's (and hence the 's) converge

 we could use a boolean variable to record on each pass_through the

 blocks whether any has changed

 However in this and in similar algorithms described later we assume

 that the exact mechanism for keeping_track of changes is understood and

 we elide those details



 The algorithm is sketched in Fig_figreach-def-alg

 The first two lines initialize certain data-flow

 valuesfootnoteThe observant reader will notice that we could

 easily combine lines_(1) and (2) However in similar data-flow

 algorithms it may be necessary to initialize the entry or

 exit node differently from the way we initialize the other nodes

 Thus we follow a pattern in all iterative_algorithms of applying a

 boundary_condition like line_(1) separately from the initialization

 of line (2)footnote

 Line (3) starts the loop in which we iterate_until convergence and the

 inner_loop of lines_(4) through_(6) applies the data-flow_equations to

 every block other_than the entry

 alg



 figurehtb

 center

 tabularr_l

 1)



 2) for ( each basic_block other_than entry )





 3)_while ( changes to any occur )



 4) for ( each basic_block other_than entry )





 5)





 6)











 tabular

 center



 Iterative_algorithm to compute reaching_definitions

 figreach-def-alg



 figure



 Intuitively Algorithm_algreaching-definitions

 propagates definitions as far as they

 will go without being killed thus

 simulating all possible_executions

 of the program

 Algorithm_algreaching-definitions will_eventually halt

 because

 never decreases in size for any once a definition is

 added it stays there forever

 (The proof of this fact is left as an inductive exercise)

 Since the set of all definitions is finite there must eventually

 be a pass of the while-loop during which nothing is added to any

 and the algorithm then terminates

 We are safe terminating then because if the 's have not changed

 the 's will not change on the next pass

 And if the 's do_not change the 's cannot so on

 all subsequent_passes there can be no changes



 The number of nodes in the flow_graph is

 an upper_bound on the number of times

 around the while-loop

 The_reason is that if a definition_reaches a point

 it can do so along a cycle-free_path and the number

 of nodes in a flow_graph is an upper_bound on the number

 of nodes in a cycle-free_path

 Each time around the while-loop the definition progresses

 by at_least one node along the path in question



 In_fact if we properly order the blocks in the for-loop of line

 (5) there is empirical evidence that the average_number

 of iterations of the while-loop is under 5

 (see_Section secconvergence-speed)

 Since sets of definitions can be represented_by bit_vectors

 and the operations on these sets can be_implemented by

 logical operations on the bit_vectors

 Algorithm_algreaching-definitions is surprisingly efficient in practice



 ex

 exreaching-definitions

 We_shall represent the seven_definitions

 in the flow_graph of Fig_figreach-def

 by

 bit_vectors where bit from the left represents

 definition

 The union of sets is computed by_taking the logical_OR of the

 corresponding bit_vectors and the difference of two sets

 is computed by

 complementing the bit_vector of and then taking the logical

 AND of the result and the bit_vector for



 Shown in

 the table in Fig_figreach-def-comp are the values taken on by

 the and sets in Algorithm_algreaching-definitions

 The initial

 values indicated by a superscript 0 as in are assigned by

 the loop of line (2)

 of Fig_figreach-def-alg to be the empty_set represented_by bit

 vector 000_0000

 The values of subsequent_passes of the algorithm are also indicated by

 superscripts and

 labeled and for the first pass and

 and for the second



 figurehtb

 center

 tabularc_c c_c c_c

 Block



 000_0000 000_0000 111_0000 000_0000 111_0000



 000_0000 111_0000 001_1100 111 0111_001 1110



 000_0000 001_1100 000_1110 001_1110 000_1110



 000_0000 001_1110 001_0111 001_1110 001_0111



 000_0000 001_0111 001_0111 001_0111 001_0111



 tabular

 center

 Computation of and

 figreach-def-comp

 figure



 Suppose the for-loop of lines_(4) through_(6) is executed with



 B B1_B2 B3 B4 exit



 in that order

 With since

 is the empty_set and is

 This value differs_from the previous value so we

 now know there is a change on the first round (and will proceed to a

 second round)



 Then we consider and compute



 align

 B21 B11 B40



 111_0000 000_0000 111_0000



 B21 genB2 (B21 - killB2)



 000 1100 (111 0000 - 110 0001) 001_1100



 align



 This computation is summarized in Fig_figreach-def-comp

 For_instance at the end of the first pass

 reflecting the fact that is generated

 and that and reach

 and are not_killed in

 There_are no changes in any of the

 sets after the second pass so the algorithm_terminates

 ex



 Live-Variable Analysis

 live-var-subsect



 Some code improving transformations

 depend_on information computed in the direction

 opposite to the flow of control in a program

 we_shall examine one such example now

 In live-variable_analysis

 we_wish to know for variable

 x and

 point whether the value of

 x at

 could be used along some path in the flow

 graph starting_at

 If so we say

 x is

 live

 at otherwise

 x is

 dead

 at



 An_important use for live-variable information

 is register_allocation

 After a value is computed in a register and

 presumably used within a block it is not

 necessary to store that value if it is dead at

 the end of the block

 Also if all registers are full and we need another

 register we should favor using a register with a dead

 value since that value does_not have to be stored



 Here we define the data-flow_equations directly in terms of

 and which represent the set of variables live at

 the points immediately_before and after block respectively

 These_equations can also be derived by first defining the transfer

 functions of individual_statements and composing them to create the

 transfer_function of a basic_block

 Define



 enumerate



 is the set of variables

 definitely assigned values in prior to any use of that

 variable in and



 is the

 set of variables whose values may be

 used in prior to any definition of the variable



 enumerate

 That is any variable in must_be considered live_on entrance to

 block while definitions of variables

 in definitely have no use

 past the beginning of

 In effect membership in kills any opportunity

 for a variable to be live because of paths that begin at



 Thus the equations relating and

 to the

 unknowns and are





 exit

 and for all basic_blocks other_than exit



 align

 B useB ( B - defB)



 B_S a successor of B_S



 align

 The first equation specifies the boundary_condition which is that no

 variables are live_on exit from the program The second

 equation_says that

 a variable is live_coming into a block if

 either it is used before redefinition in the block or it is live

 coming_out of the block and is not redefined in the block

 The third equation_says that a

 variable is live_coming out of a block

 if and only if it is live_coming into one of its successors



 The relationship_between the equations for liveness and

 the reaching-definitions equations

 should be noticed



 itemize



 Both sets of equations have union as the meet_operator

 The_reason is that in each data-flow_schema we propagate information

 along paths and we care only about_whether any path with desired

 properties exist rather_than whether_something is true along all

 paths



 However information flow for liveness travels backward opposite to

 the direction of control_flow because in this problem we_want to make

 sure that the use of a variable at a point

 is transmitted to all points prior to in an execution_path so that

 we may know at the prior point that will have its value used



 itemize



 To solve a backward

 problem instead of initializing we initialize



 Sets and have their roles interchanged and and

 substitute for and respectively

 As for reaching_definitions the solution to the liveness equations

 is not_necessarily

 unique and we_want the solution with the smallest sets of

 live_variables

 The algorithm used is

 essentially a backwards version of Algorithm_algreaching-definitions



 alg

 algliveness

 Live variable analysis



 A flow_graph with def and computed for each block



 and the set of variables

 live_on entry and exit of each block of the flow_graph



 Execute the program in Fig figliveness

 alg



 figurehtb

 center

 tabularl





 for ( each basic_block other_than exit )





 while_( changes to any occur )



 for ( each basic_block other_than exit )



















 tabular

 center



 Iterative_algorithm to compute live_variables

 figliveness



 figure



 Available_Expressions

 ae-subsect



 An expression is

 available

 at a point if every_path from

 the entry_node to evaluates

 and after the last

 such evaluation prior to reaching there are no_subsequent

 assignments to or footnoteNote

 that as usual in this_chapter we use the operator as a

 generic_operator not_necessarily standing for additionfootnote

 For the available-expressions data-flow_schema we say that a block

 kills

 expression

 if it assigns (or may assign)

 or and does_not

 subsequently recompute

 A block

 generates

 expression

 if it definitely evaluates

 and

 does_not subsequently redefine or



 Note_that the notion of killing or generating an available

 expression is not exactly the same as that for reaching

 definitions

 Nevertheless these notions of kill and generate behave

 essentially

 as they do for reaching_definitions



 The primary use of available-expressions information is for

 detecting global_common subexpressions

 For_example in Fig figcse(a) the expression

 4i

 in block will be a common_subexpression if

 4i

 is available at the entry point of block

 It will be available if

 i

 is not assigned a new value in block or if as

 in Fig figcse(b)

 4i

 is recomputed after

 i

 is assigned in



 figurehtb

 figureuullmanalsuch9figscseeps

 Potential common_subexpressions across blocks

 figcse

 figure



 We can compute the set of generated expressions for each point in a block

 working from beginning to end of the block

 At the point prior to the block no expressions are generated

 If at point set of expressions is available

 and is the point after with statement x_yz between

 them then we form the set of expressions

 available at by the following two steps



 enumerate



 Add to the expression



 Delete from any expression involving variable



 enumerate



 Note the steps must_be done in the correct order as could be

 the same as or

 After we reach the end of the block is the set of generated

 expressions for the block

 The set of killed expressions is all expressions say

 such that either or is defined in the block and

 is not generated_by the block

 Note_that could be both generated and killed but the -

 form of transfer_function causes generation to take precedence



 ex

 exavail-exp

 Consider the four statements of Fig figavail-exp-comp

 After the first bc is available

 After the second statement

 a-d becomes available but bc is no_longer

 available because b has_been redefined

 The third statement

 does_not make bc available again because the value

 of c is immediately changed

 After the last statement a-d is no_longer available because

 d has changed

 Thus no expressions are generated and all expressions_involving

 a b_c or d are killed

 ex



 figurehtb

 center

 tabularc_c

 Statement Available_Expressions







 a bc



 bc



 b a-d



 a-d



 c bc



 a-d



 d a-d







 tabular

 center



 Computation of available_expressions

 figavail-exp-comp



 figure



 We can find available_expressions in a manner reminiscent of the

 way reaching_definitions are computed

 Suppose is the universal_set of all expressions

 appearing on the right of one or_more statements of the program

 For each block let be the set of expressions in

 that are available at the point just_before the beginning of

 Let be the same for the point following the end

 of

 Define to be the expressions generated_by

 and to be the set of expressions in killed

 in

 Note_that and can all be

 represented_by bit_vectors

 The following equations relate the unknowns and to

 each other and the known quantities and



 entry

 For all basic_blocks other_than entry



 B egenB (B_- ekillB)





 B_P a predecessor of B_P





 The above equations look almost identical to the equations for

 reaching_definitions

 Like reaching_definitions the boundary_condition is

 because

 at the exit of the entry_node there are no available

 expressions

 The most_important difference is that the meet_operator

 is intersection rather_than union

 This operator is the proper one because an expression

 is available at the beginning of a block only if it is available

 at the end of all its_predecessors

 In_contrast a definition_reaches the beginning of a block

 whenever it reaches the end of any one or_more of its_predecessors



 The use of rather_than makes the available-expression

 equations behave

 differently from those of reaching_definitions

 While neither set has a unique solution for reaching_definitions it is the

 solution with the smallest sets

 that corresponds to the definition of reaching and we

 obtained that solution by starting_with the assumption

 that nothing reached anywhere and building

 up to the solution

 In that way we never assumed that a definition

 could reach a point unless

 an actual path propagating to could be found

 In_contrast for available expression equations we_want the solution

 with the

 largest sets of available_expressions

 so we start with an approximation that is too large and work down



 It may not be obvious that by starting_with the assumption

 everything (ie the set ) is available everywhere except at the

 end of

 the entry block

 and eliminating

 only those expressions for which we can discover a path along which it

 is not available we do reach a set of truly_available expressions

 In the case of available_expressions it is conservative to produce a subset

 of the exact set of available_expressions

 The argument for subsets being conservative is that our intended use of the

 information is to replace the computation of an available expression

 by a previously_computed value

 Not knowing an expression is available only inhibits

 us from improving the code while believing an expression is available

 when it is not could cause_us to change_what the program computes



 figurehtb

 figureuullmanalsuch9figsavail-expeps

 Initializing the sets to is too

 restrictive

 figavail-exp-ex

 figure



 ex

 exavail-exp-comp

 We_shall concentrate_on a single block in Fig figavail-exp-ex

 to illustrate the effect of the initial approximation of

 on

 Let and abbreviate and

 respectively

 The data-flow_equations for block are



 B2 B1_B2





 B2 G (B2 -_K)



 These_equations may be_rewritten as recurrences

 with and being the th approximations of

 and respectively

 Ij1 B1 Oj

 Oj1 G (Ij1 -_K)



 Starting with



 we get

 However if we start with

 then we get

 as we should

 Intuitively the solution obtained starting_with



 is more desirable because it correctly reflects the fact

 that expressions in

 that are not_killed by are available at

 the end of

 ex



 alg

 algavailable-expr

 Available expressions



 A flow_graph with and

 computed for each block

 The initial block is



 and the set of

 expressions available at

 the entry and exit of each block of the flow_graph





 Execute the algorithm of Fig figavail-exp-alg

 The explanation of the steps is similar to that for Fig_figreach-def-alg

 alg



 figurehtb

 center

 tabularl





 for ( each basic_block other_than entry)





 while_( changes to any occur )



 for ( each basic_block other_than entry)



















 tabular

 center

 Iterative_algorithm to compute available_expressions

 figavail-exp-alg

 figure





 Definition-Use Chains

 A calculation done in virtually the same manner as

 live-variable_analysis is

 definition-use chaining

 (du-chaining)

 We_say a variable is

 used

 at statement if its -value may be required

 For_example

 b

 and

 c

 (but_not

 a

 are used in

 each of the statements

 a bc

 and

 ab c

 The du-chaining

 problem is to compute for a point the set of uses

 of a variable say

 x

 such that there is a path from

 to that does_not redefine

 x



 The data-flow_values in the du-chaining_problem are sets of pairs

 such that is a statement that uses variable

 As with live_variables

 if we can compute the set of uses reachable from

 the end of block then we can compute

 the definitions reached from any point within block by

 scanning the portion of block that follows

 In_particular if there is a definition of variable

 x

 in the block we can determine the

 du-chain

 for that definition the list of all possible uses of that definition



 The equations for computing du-chaining information look

 exactly like those for liveness with substitution for and

 In place of we have the set of pairs_such

 that is a statement in which uses variable

 x

 and such

 that no prior definition of

 x

 occurs in

 Instead of we have

 the set of pairs_such

 that is a statement which uses

 x

 is not in and

 has a definition of x

 These_equations are solved by the obvious analog of

 Algorithm_algliveness





 Summary



 In the above we have discussed three instances of data-flow_problems reaching

 definitions live_variables and available_expressions

 As summarized in Fig figframework

 the definition of each problem

 is given by the domain of the data-flow_values

 the direction of the data flow the

 family of transfer_functions the boundary_condition and the meet

 operator The last_row shows the initial values used in the iterative

 algorithm These values are chosen so that the iterative_algorithm

 will find the most_precise

 solution to the equations This choice is not strictly a part of

 the definition of the data-flow_problem since it is an artifact needed

 for the iterative_algorithm There_are other ways of solving the

 problem For_example we saw how the transfer_function of a basic

 block can be derived by_composing the transfer_functions of the individual

 statements in the block a similar compositional approach may be used

 to compute a transfer_function for the entire procedure or transfer

 functions from the entry of the procedure to any program point





 figurehtb

 center

 tabularl_l l_l

 Reaching_Definitions Live Variables Available_Expressions



 Domain_Sets of definitions Sets of variables Sets of expressions





 Direction_Forwards Backwards_Forwards























 Transfer







 function



 Boundary









 Meet_()





 Initialize



 tabular

 center

 Summary of three data-flow_problems

 figframework

 figure



 Why the Available-Expressions Algorithm Works

 We need to explain why starting all 's except that for the entry

 block with the set of all expressions leads to a conservative

 solution to the data-flow_equations that is all expressions found to

 be available really are available

 First because intersection is the meet_operation in this data-flow

 schema any reason that an expression is found not to be available

 will propagate_forward in the flow_graph along all possible paths

 until is recomputed and becomes available again

 Second there are only two_reasons could be unavailable



 enumerate



 is killed in block because or is defined without a

 subsequent computation of

 In this case the first time we apply the transfer_function

 will be removed_from



 is never computed along some path

 Since is never in and it is never

 generated along the path in question we can show by induction on the

 length of the path that is eventually removed_from 's and

 's along that path



 enumerate

 Thus after changes subside the solution provided by the interative

 algorithm of Fig figavail-exp-alg will include only truly

 available_expressions

 Introduction to Data-Flow_Analysis



 All the optimizations introduced in Section_secopt-sources can

 benefit from data-flow_analysis Data-flow_analysis is

 flow-sensitive it extracts program properties relating to the flow of

 data along control paths For_example global_common subexpression

 elimination_requires knowing if two expressions ab appearing in

 two different parts of the program refer to the same value If for

 example a is redefined in between the two expressions then the

 two expressions may not have the same value As_another example in

 dead_code elimination an assignment can be_eliminated if the result

 is never referred to again subsequently All these questions

 can be answered by data-flow_analyses



 The Data-Flow Abstraction



 A program execution can be_viewed as a series of transitions on the

 state of computation which comprises all the values of the variables

 as_well as the program counter Each execution of a statement

 transforms an input state to a new output state The input state is

 associated_with the program point before the statement and the

 output state is associated_with the program point after the statement



 In straight-line code the program point after a statement is the same

 as the program point before the next statement A control edge

 connects the program point after the last statement of the source

 basic_block to the program point before the first statement of the

 destination basic_block

 A path from to is a sequence of points

 such that for each

 between 1 and either

 enumerate

 is the point_immediately preceding a statement and

 is the point_immediately following that

 statement in the same block or

 is the end of some block and

 is the beginning

 of a successor block

 enumerate



 The number of possible_execution paths through a program is unbounded

 and some of these execution_paths may themselves be unbounded A

 program analysis tries to summarize all the possible dynamic behaviors

 of a program with a finite set of facts As_discussed in

 Chapter secprog-abs different analyses may choose to abstract

 out different information Data-flow_analysis is flow-sensitive but

 not sensitive to iterations or the paths in a program It computes

 some information for each program point in the program treating all

 the instances of the same program point as a whole It does_not

 distinguish_between when and how a particular instance is reached



 ex



 Even the simple program in Figure figdf-abs describes an

 unbounded_number of execution_paths

 Executing 0 iteration of the loop in the program

 the shortest execution consists of program points 12348

 The next shortest path executes one iteration of the loop and it

 consists of program points

 1234567348 and so forth We know that for example

 the first time program point_(5) is executed

 the value of a is 0 due to definition We_say that

 reaches point_(5) in the first iteration In subsequent

 iterations reaches point_(5) and the value of a is 243



 figurehtbp

 figureuullmanalsuch9figsdf-abseps

 Example program illustrating the data-flow_abstraction

 figdf-abs

 figure



 Not keeping_track of the different iterations we may summarize all

 the computation states reached for example at program point_(5) by saying

 that the value of a is one of and that it may be

 defined by one of

 The set of definitions that may reach a program point along some path

 is known_as the reaching_definitions

 This kind of information is useful if we are writing a tool that helps

 programmers in understanding the program



 Suppose instead we are_interested in implementing constant_folding

 If it happens that only one definition for a variable reaches a use of

 the variable and that definition has a constant value we can simply

 replace the variable with a constant If on the other_hand multiple

 definitions may reach a single program point then there are no

 opportunies for constant_folding In this case keeping all the

 possible definitions will only increase the implementation cost for no

 additional benefit We may thus choose to abstract the computational

 state of all the instances of a program point by just keeping

 definitions that would reach a program point regardless of which

 execution_path is taken We_say that these definitions must

 reach a program point So for this particular example there is no

 single definition that must reach every instance of program point_(5)

 Furthermore we can simply summarize the value of variable a as

 not-a-constant because constant_folding cannot take_advantage of

 any more details anyway Thus the way we summarize the computational

 states depends_on the purpose of the analysis

 ex



 Data-Flow_Analysis Schema

 secdf-schema



 In data-flow_analysis we associate with every program point a data-flow value that represents an abstraction of the set of all

 possible computational states that can be observed for that program

 point As_discussed above the choice of abstraction is dependent on

 the goal of the analysis to be efficient we only need to keep_track

 of information that is relevant For_example the data-flow_values

 for the reaching_definitions problem are subsets of definitions in the

 program and they represent all the possible definitions that may

 reach a program point



 We denote the data-flow_values before and after each statement as

 and respectively The data-flow_problem is

 generally defined as finding a solution to a set of constraints on

 and for all statements There_are two sets of

 constraints those based_on the semantics of the statements and those

 based_on the flow of control



 The data-flow_values before and after a statement are constrained by

 the semantics of the statement For_example if variable a has

 value before executing statement b a then variable b will have the value after the statement This relation is

 known_as a transfer_function in data-flow_analysis Transfer

 functions come in two flavors depending_on whether we are_interested

 in information propagated along the execution_paths or

 backwards up the execution_paths In a forward-flow problem the

 transfer_function of a statement transfers the data-flow

 value before the statement to a new data-flow value after the

 statement That is



 outs fs(ins)



 Conversely in a backward-flow problem

 a transfer_function of statement

 transfers a data-flow value after the statement to a new

 data-flow value before the statement That is



 ins f(outs)





 The second set of constraints are derived_from the flow of control

 Inside a basic_block all statements are executed sequentially thus

 for all pairs of adjacent statements and

 The data-flow_values at the entry and exit of basic_blocks are

 related by the control_flow edges For_example if we are_interested

 in collecting all the definitions that may reach a program point then

 the set of definitions_reaching the end of a basic_block must_be a

 subset of those reaching the start of its successor blocks



 One common optimization used in solving_data-flow problems is to

 reduce the size of the problem by operating at the basic-block rather

 than the statement level We denote the data-flow_values before and

 after each basic_block as and respectively We

 reduce the constaints involving and to those

 involving and



 If is the first statement of basic_block then

 Similarly if is the last statement of basic_block



 First the transfer_function of a basic_block

 can be derived by_composing the transfer_functions of the

 statements in the block Let be the transfer

 functions of statements

 in basic_block then

 Second the constraints_due to

 control_flow across basic_blocks can easily be_rewritten in terms of

 and



 In summary in data-flow_analysis we typically create for each basic

 block two data-flow variables representing the start and end points

 of the block First variables of the same basic_block are

 constrained by a transfer_function representing the semantics of the

 statements in the block The second set of contraints reflect the

 control_flow across basic_blocks Our_goal is to find a solution in

 fact a maximum fixed point that satisfies these two sets of

 constraints



 Many problems can be formulated as data-flow_problems We will use

 four examples below to illustrate this point



 Reaching_Definitions



 A definition of a variable x is a statement that assigns or may

 assign a value to x As_discussed in

 Section secdata-acc parameters array_accesses and indirect

 references may have aliases and it is not easy to tell if a statement

 is referring to a particular variable of x Program analysis

 must_be conservative if we do_not know if a statement is assigning a

 value of x we must assume that it may write to it and

 that variable x may have either the original value before the

 definition or the new value For the sake of simplicity our

 discussion in the rest of the chapter assumes that we are dealing only

 with variables with no_aliases This includes all local_scalar

 variables in the case of C and C local_variables

 whose addresses have_been taken are also excluded as discussed in

 Section secdata-acc



 We_say a definition_reaches a point if there is a path

 from the point_immediately following to such that is not

 killed along that path Intuitively if a definition of some

 variable a reaches point then might be the place at

 which the value of a used at might last have_been defined

 We kill a definition of a variable a if between two points

 along the path there is a definition of a



 For_example both the definitions

 i_m - 1

 and

 j_n

 in block in Fig 1019 reach the beginning

 of block

 as does the definition

 j j - 1

 provided there are no assignments to

 j

 in

 or the portion of following that definition

 However the assignment to

 j

 in kills the definition

 j_n

 so the latter does_not reach or



 By defining reaching_definitions as we have we sometimes allow

 inaccuracies

 However they are all in the safe or conservative direction

 For_example

 notice our_assumption that all edges

 of a flow_graph can be traversed

 This may not be true in practice

 For_example for no values of

 a

 and

 b

 can control actually reach the assignment

 a 4

 in the following program_fragment

 verbatim

 if (a_b)

 a 2

 else

 if (a_b)

 a 4



 verbatim

 To decide in general whether each path in a flow_graph can

 be taken is an undecidable problem and we_shall not attempt to solve it



 A recurring theme in the design of code improving transformations is

 that we must make only conservative

 decisions in the face of any doubt

 although conservative strategies may cause_us to miss

 some transformations that we actually could make safely

 A decision is

 conservative

 if it never leads to a change in what the program computes

 In applications of reaching_definitions it

 is normally conservative to assume that a

 definition can reach a point even if it might not

 Thus we allow paths that may never be traversed in any execution

 of the program and we allow definitions to pass

 through ambiguous definitions of the same variable





 Data-Flow Equations for Reaching_Definitions



 We_now examine the details of a single statement Consider a

 definition



 tabularr_l

 u_v w

 tabular



 This statement generates a definition

 kills all the other definitions in the program that

 writes to variable u

 while leaving the remaining incoming definitions untouched

 The transfer_function of the definition



 fd(x) gend (x_- killd)



 where the set of definitions generated_by the statement

 and is the set of all other definitions of u



 As_discussed in Section secdf-schema the transfer_function of a

 basic_block can be found by_composing the transfer_functions of the

 statements contained therein For_example suppose basic_block has

 three statements then





 fb(x) gend3 ((gend2 ((gend1 (x_- killd1)) -

 killd2)) - killd3)



 Applying Distributive Law we can rewrite the equation as



 fb(x) (gend3 ((gend2 (gend1 - killd2)) -

 killd3)) (x_- (killd1 killd2 - killd3))





 Thus we can rewrite the function also in terms of a and

 set



 fb(x) genb (x_- killb)



 where



 genb (gend3 ((gend2 (gend1 - killd2)) -

 killd3))



 and



 killb killd1 killd2 - killd3





 In a sense like a statement a basic_block also generates a set of

 definitions and kills a set of definitions The set contains

 all the definitions that are visible outside the block-we refer to

 them as downwards-exposed A definition in a basic_block is

 visible outside a basic_block only if it is not_killed by a

 subsequent definition to the same variable inside the same basic

 block A basic block's set is simply the union of all the

 definitions killed by the individual_statements Notice_that a

 definition may appear in both the and set of a basic

 block

 For_example the set for the following basic_block



 tabularr_l

 a 3



 a 4



 tabular

 is The set contains both and since

 kills and vice_versa It is easy to see however the

 transfer_function is defined such that will be included in the

 result of the transfer_function



 The boundary_condition for this data-flow_problem is given by the

 data-flow value associated_with the entry of the procedure

 entry

 No definitions reach the beginning of the program thus

 entry



 Next we consider the set of constraints derived_from the control_flow

 across basic_blocks Since a definition_reaches a program point as

 long_as there_exists one path that allows it to if control_flows from

 and then

 In_fact the set of needs only to be no larger_than the

 union of the reaching_definitions of all the predecessor blocks



 inB P a predecessor of B outP





 We refer to the operator as the meet_operator The

 meet_operator is applied to create a summary of the contributions from

 all the different paths at the confluence of these paths



 Iterative_Algorithm for Reaching_Definitions



 The reaching_definitions problem is defined by the following

 equations





 outentry



 For all basic_blocks other_than entry



 outB genb (inB - killb)





 inB P a predecessor of B outP





 These recurrences can be_solved using

 Algorithm reaching-definitions



 alg

 algreaching-definitions

 Reaching_definitions



 A flow_graph for which and have_been computed for

 each block



 and for each block



 We use an iterative_approach starting_with the estimate

 for all and converging to the

 desired values of and

 As we must iterate_until the 's (and hence the 's) converge

 we use a boolean variable to record on each pass_through the

 blocks whether any has changed

 The algorithm is sketched in Fig 1026

 alg



 figure

 tabularr_l

 1)



 2) for ( each basic_block other_than entry)



 3)



 4) true



 5) while ()



 6) false



 7) for ( each basic_block other_than entry)



 8)



 9)



 10)



 11) if ()



 12) true



 13)



 14)



 15)



 tabular

 Iterative_algorithm to compute reaching_definitions

 figreach-def-alg

 figure



 Intuitively Algorithm_algreaching-definitions

 propagates definitions as far as they

 will go without being killed in a sense simulating all possible_executions

 of the program

 The bibliographic notes contain references where formal proofs of

 the correctness of this and other data-flow_analysis problems can be found



 We can see that the algorithm will_eventually halt because

 never decreases in size for any once a definition is

 added it stays there forever

 (The proof of this fact is left as an inductive exercise)

 Since the set of all definitions is finite there must eventually

 be a pass of the while-loop

 in which for each at line (11)

 Then will remain and the algorithm

 terminates

 We are safe terminating then because if the 's have not changed

 the 's will not change on the next pass

 And if the 's do_not change the 's cannot so on

 all subsequent_passes there can be no changes



 It may be shown that an upper_bound on the number of times

 around the while-loop is the number of nodes in the flow_graph

 Intuitively the reason is that if a definition_reaches a point

 it can do so along a cycle-free_path and the number

 of nodes in a flow_graph is an upper_bound on the number

 of nodes in a cycle-free_path

 Each time around the while-loop the definition progresses

 by at_least one node along the path in question



 In_fact if we properly order the blocks in the for-loop of line

 (7) there is empirical evidence that the average_number

 of iterations on real programs is under 5 (see_Section 1010)

 Since the sets can be represented_by bit_vectors

 and the operations on these sets can be_implemented by

 logical operations on the bit_vectors

 Algorithm 102 is surprisingly efficient in practice



 ex

 exreaching-definitions

 The flow_graph of Fig_figreach-def has_been derived_from the program

 in Fig 1022 of the last section

 figure

 figureuullmanalsuch9figsreach-defeps

 Flow_graph for illustrating reaching_definitions

 figreach-def

 figure



 Only the definitions defining

 i

 j

 and

 a

 in Fig_figreach-def are of interest

 We_shall represent sets of definitions by

 bit_vectors where bit from the left represents

 definition



 The loop of line_(1) in Fig 1026 initializes

 for each and these initial values of are shown in

 the table in Fig 1028

 The initial values () of each

 are not computed or used but are shown for completeness

 Suppose the for-loop of line_(5) is executed with

 in that order

 With there are no predecessors for the initial_node

 so remains the empty_set represented_by 000_0000

 as a result remains equal to

 This value does_not differ from computed at line_(7)

 so we do_not yet set to true



 Then we consider and compute



 inB2 outB1 outB3 outB4



 111_0000 000 0010 000 0001 111 0011







 outB2 genB2 (inB2 - killB2)



 000 1100 (111 0011 - 110 0001) 001_1110





 This computation is summarized in Fig 1028

 At the end of the first pass

 reflecting the fact that is generated

 and that and reach

 and are not_killed in



 figure

 tabularc_c c_c c_c c

 BLOCK



 000_0000 111_0000 000_0000 111_0000 000_0000 111_0000



 000_0000 000 1100 111 0011 001_1110 111 1111 001_1110



 000_0000 000 0010 001_1110 000_1110 001_1110 000_1110



 000_0000 000 0001 001_1110 001_0111 001_1110 001_0111



 tabular

 Computation of and

 figreach-def-comp

 figure



 From the second pass on there are no changes in any of the

 sets so the algorithm_terminates

 ex



 Live-Variable Analysis

 A number of code improving transformations

 depend_on information computed in the direction

 opposite to the flow of control in a program

 we_shall consider some of these now

 In live-variable

 we_wish to know for variable

 x

 and

 point whether the value of

 x

 at

 could be used along some path in the flow

 graph starting_at

 If so we say

 x

 is

 live

 at otherwise

 x

 is

 dead

 at



 As we saw in Section 97

 an important use for live-variable information

 comes when we generate object code

 After a value is computed in a register and

 presumably used within a block it is not

 necessary to store that value if it is dead at

 the end of the block

 Also if all registers are full and we need another

 register we should favor using a register with a dead

 value since that value does_not have to be stored



 Let_us define to be the set

 of variables live at the point_immediately

 before block and define to be the

 same at the point_immediately after the block

 Let be the set of variables

 definitely assigned values in prior to any use of that

 variable in and let be the

 set of variables whose values may be

 used in prior to any definition of the variable

 Then the equations relating and

 to the

 unknowns and are





 in exit

 For all basic_blocks other_than exit



 in B use B ( out B - defB)





 outB S a successor of B inS





 The first equation specifies the boundary_condition which is that no

 variables are live_on exit of the program The second

 equation_says that

 a variable is live_coming into a block if

 either it is used before redefinition in the block or it is live

 coming_out of the block and is not redefined in the block

 The third equation_says that a

 variable is live_coming out of a block

 if and only if it is live_coming into one of its successors



 The relation between (1011) and

 the reaching-definitions equations (109)

 should be noticed

 Here instead of initializing we initialize

 because this is a backward-flow problem

 and have their roles interchanged and and

 substitute for and respectively

 As for (109) the solution to (1011) is not_necessarily

 unique and we_want the smallest solution

 The algorithm used for the minimum solution is

 essentially a backwards version of Algorithm 102

 Since the mechanism for detecting changes to any of the 's

 is so similar to the way we detected changes to 's in

 Algorithms 102 and 103

 we elide the details of checking for termination



 alg

 algliveness

 Live variable analysis



 A flow_graph with and computed for each block



 the set of variables

 live_on exit from each block of the flow_graph



 Execute the program in Fig 1033

 alg



 figure

 tabularr_l

 1)



 2) for ( each basic_block other_than exit)



 3)



 4) true



 5) while ()



 6) false



 7) for ( each basic_block other_than exit)



 8)



 9)



 10)



 11) if ()



 12) true



 13)



 14)



 15)



 tabular

 Iterative_algorithm to compute live_variables

 figliveness

 figure



 Available_Expressions

 An expression

 xy

 is

 available

 at a point if every_path (not_necessarily cycle-free) from

 the initial_node to evaluates

 xy

 and after the last

 such evaluation prior to reaching there are no_subsequent

 assignments to

 x

 or

 y

 For available_expressions we say that a block

 kills

 expression

 xy

 if it assigns (or may assign)

 x

 or

 y

 and does_not

 subsequently recompute

 xy

 A block

 generates



 expression

 xy

 if it definitely evaluates

 xy

 and

 does_not subsequently redefine

 x

 or

 y



 Note_that the notion of killing or generating an available

 expression is not exactly the same as that for reaching

 definitions

 Nevertheless these notions of kill and generate obey the same

 laws as they do for reaching_definitions

 We could compute them exactly as we did in Section 105 provided

 we modified the rules in 1021(a)

 for a simple assignment_statement



 The primary use of available_expressions information is for

 detecting common_subexpressions

 For_example in Fig figcse the expression

 4i

 in block will be a common_subexpression if

 4i

 is available at the entry point of block

 It will be available if

 i

 is not assigned a new value in block or if as

 in Fig figcse(b)

 4i

 is recomputed after

 i

 is assigned in

 figure

 figureuullmanalsuch9figscseeps

 Potential common_subexpressions across blocks

 figcse

 figure



 We can easily compute the set of generated expressions for each point in a block

 working from beginning to end of the block

 At the point prior to the block assume no expressions are available

 If at point set of expressions is available

 and is the point after with statement xyz between

 them then we form the set of expressions

 available at by the following two steps

 enumerate

 Add to the expression yz

 Delete from any expression involving x

 enumerate



 Note the steps must_be done in the correct order as x could be

 the same as y or z

 After we reach the end of the block is the set of generated

 expressions for the block

 The set of killed expressions is all expressions say

 yz

 such that either y or z is defined in the block and

 yz

 is not generated_by the block



 Example 1016

 Consider the four statements of Fig 1030

 After the first bc is available

 After the second a-d becomes available but bc is no_longer

 available because b has_been redefined

 The third does_not make bc available again because the value

 of c is immediately changed

 After the last statement a-d is no_longer available because

 d has changed

 Thus no expressions are generated and all expressions_involving

 a b_c or d are killed

 figure





 tabularl_l

 Statement Available_Expressions



 none



 a bc



 b_c



 b a-d



 a - d



 c b_c



 a - d



 d a - d



 none



 tabular

 Computation of available_expressions

 figavail-exp-comp

 figure



 We can find available_expressions in a manner reminiscent of the

 way reaching_definitions are computed

 Suppose is the universal_set of all expressions

 appearing on the right of one or_more statements of the program

 For each block let be the set of expressions in

 that are available at the point just_before the beginning of

 Let be the same for the point following the end

 of

 Define to be the expressions generated_by

 and to be the set of expressions in killed

 in

 Note_that and can all be

 represented_by bit_vectors

 The following equations relate the unknowns and to

 each other and the known quantities and



 out entry

 For all basic_blocks other_than entry



 out B egenB (in B - ekillB)





 in B_P a predecessor of B outP





 Equations (1010) look almost identical to equations (109) for

 reaching_definitions

 Like reaching_definitions the boundary_condition is the same because

 at the exit of the entry_node there are no available

 expressions

 The most_important difference is that the meet_operator

 is intersection rather_than union

 This operator is the proper one because an expression

 is available at the beginning of a block only if it is available

 at the end of all its_predecessors

 In_contrast a definition_reaches the beginning of a block

 whenever it reaches the end of one or_more of its_predecessors



 The use of rather_than makes equations (1010) behave

 differently from (109)

 While neither set has a unique solution for (109) it is the

 smallest solution that corresponds to the definition of reaching and we

 obtained that solution by starting_with the assumption

 that nothing reached anywhere and building

 up to the solution

 In that way we never assumed that a definition

 could reach a point unless

 an actual path propagating to could be found

 In_contrast for equations (1010) we_want the largest possible

 solution so we start with an approximation that is too large and work down



 It may not be obvious that by starting_with the assumption

 everything ie the set is available everywhere

 and eliminating

 only those expressions for which we can discover a path along which it

 is not available we do reach a set of truly_available expressions

 In the case of available_expressions it is conservative to produce a subset

 of the exact set of available_expressions and this is what we do

 The argument for subsets being conservative is that our intended use of the

 information is to replace the computation of an available expression

 by a previously_computed value

 and not knowing an expression is available only inhibits

 us from changing the code



 Example 1017

 We_shall concentrate_on a single block in Fig figavail-exp-ex

 to illustrate the effect of the initial approximation of

 on

 Let and abbreviate and

 respectively

 The data-flow_equations for block are



 inB2 outB1 outB2





 outB2 G (inB2 -_K)



 These_equations have_been rewritten as recurrences in Fig figavail-exp-ex

 with and being the th approximations of

 and respectively

 The figure also shows that starting_with

 we get

 While starting_with



 figure

 figureuullmanalsuch9figsavail-expeps





















 hskip4in











 hskip4in





 Initializing the sets to is too

 restrictive

 figavail-exp-ex

 figure



 Intuitively the solution obtained starting_with



 is more desirable because it correctly reflects the fact

 that expressions in

 that are not_killed by are available at

 the end of just as the expressions generated

 by are



 alg

 algavailable-expr

 Available expressions



 A flow_graph with and

 computed for each block

 The initial block is



 The set

 for each block



 Execute the algorithm of Fig 1032

 The explanation of the steps is similar to that for Fig 1026

 alg



 figure

 tabularr_l

 1)



 2) for ( each basic_block other_than entry)



 3)



 4) true



 5) while ()



 6) false



 7) for ( each basic_block other_than entry)



 8)



 9)



 10)



 11) if ()



 12) true



 13)



 14)



 15)



 tabular

 Iterative_algorithm to compute available_expressions

 figavail-exp-alg

 figure



 Definition-Use Chains

 A calculation done in virtually the same manner as

 live-variable_analysis is

 definition-use chaining

 (du-chaining)

 We_say a variable is

 used

 at statement if its -value may be required

 For_example

 b

 and

 c

 (but_not

 a

 are used in

 each of the statements

 a bc

 and

 ab c

 The du-chaining

 problem is to compute for a point the set of uses

 of a variable say

 x

 such that there is a path from

 to that does_not redefine

 x



 As with live_variables

 if we can compute the set of uses reachable from

 the end of block then we can compute

 the definitions reached from any point within block by

 scanning the portion of block that follows

 In_particular if there is a definition of variable

 x

 in the block we can determine the

 du-chain

 for that definition the list of all possible uses of that definition

 The method is analogous to that discussed in Section 105

 for computing ud-chains and we leave it to the reader



 The equations for computing du-chaining information look

 exactly like (1011) with substitution for and

 In place of take the set of

 upwards-exposed

 uses in that is the set of pairs_such

 that is a statement in which uses variable

 x

 and such

 that no prior definition of

 x

 occurs in

 Instead of take the set of pairs_such

 that is a statement which uses

 x

 is not in and

 has a definition of x

 These_equations are solved by the obvious analog of Algorithm 104

 and we_shall not discuss the matter further



 Introduction to Data-Flow_Analysis

 secdf-intro



 All the optimizations introduced in Section_secopt-sources

 depend_on data-flow_analysis Data-flow_analysis

 refers to a body of

 techniques that derive information_about the flow

 of data along program execution_paths For_example one way to implement global

 common_subexpression elimination_requires us to determine_whether two textually

 identical expressions evaluate to the same value along any possible

 execution_path of the program

 As_another example if the result

 of an assignment is not used along any subsequent execution_path

 then we can eliminate the

 assignment as dead_code

 These and many other important questions can be answered

 by data-flow_analysis



 The Data-Flow Abstraction



 Following Section two-stage-subsect the execution of a

 program can be_viewed as a series of transformations of the

 program state which consists of the values of

 all the variables in the program including those associated

 with stack frames below the top of the run-time_stack Each execution of

 an intermediate-code

 statement transforms an input state to a new output state The input

 state is associated_with the program point before the statement

 and the output state is associated_with the program point after the

 statement



 When we analyze the behavior of a program we must consider all the

 possible sequences of program points (paths) through a flow_graph

 that the program execution can take

 We then extract from the possible program states at each point the

 information we need for the particular data-flow_analysis problem we_want

 to solve

 In more_complex analyses we must consider paths that jump among the

 flow_graphs for various procedures as calls and returns are

 executed

 However to begin our study we_shall concentrate_on the paths through a

 single flow_graph for a single procedure



 Let_us see what the flow_graph tells_us about the possible_execution

 paths



 itemize



 Within one basic_block the program point after a statement is the same

 as the program point before the next statement



 If there is an edge from block to block then the

 program point after the last statement of

 may be followed immediately by

 the program point before the first statement of





 itemize

 Thus we may define

 an execution_path (or just path)

 from point to point to be a sequence of points

 such that for each either



 enumerate



 is the point_immediately preceding a statement and

 is the point_immediately following that

 same statement or



 is the end of some block and

 is the beginning

 of a successor block



 enumerate



 In_general there is an_infinite

 number of possible_execution paths through a program

 and there is no finite upper_bound on the length of an execution_path

 Program analyses summarize all the possible program states that can

 occur at a point in

 the program with a finite set of facts

 Different analyses may choose to abstract

 out different information and in general no analysis is necessarily

 a perfect representation of the state



 ex

 Even the simple program in Fig figdf-abs describes an

 unbounded_number of execution_paths

 Not entering the loop at all

 the shortest complete

 execution_path consists of the program points

 The next shortest path executes one iteration of the loop

 and consists of the points



 We know that for example

 the first time program point_(5) is executed

 the value of is 1 due to definition We_say that

 reaches point_(5) in the first iteration In subsequent

 iterations reaches point_(5) and the value of is 243



 figurehtb

 figureuullmanalsuch9figsdf-abseps

 Example program illustrating the data-flow_abstraction

 figdf-abs

 figure



 In_general it is not possible to

 keep_track of all the program states for all

 possible paths In data-flow_analysis we do_not distinguish

 among the paths taken to reach a program point

 Moreover we do_not keep_track of entire states rather we abstract_out

 certain details keeping only the data we need for the purpose of the

 analysis

 Two examples will illustrate_how the same program states may lead to

 different information abstracted at a point



 enumerate



 To help users debug their programs we may wish

 to find out what are all the values a variable may have at a program

 point

 and where these values may be defined For_instance we may

 summarize all the program states at point_(5) by

 saying that the value of is one of and that it

 may be defined by one of The definitions that

 may reach a program point along some path are known_as reaching_definitions



 Suppose instead we are_interested in implementing constant_folding

 If a use of the variable is reached by only one definition and that

 definition assigns a constant to then we can simply replace by

 the constant

 If on the other_hand several definitions of may

 reach a single program point then we cannot perform constant_folding on



 Thus for constant_folding we_wish to find those definitions that

 are the unique definition of their variable

 to reach a given program point no_matter which execution_path is

 taken For point_(5) of Fig figdf-abs there is no definition

 that must_be the definition of at that point so this set is

 empty for at point_(5)

 Even_if a variable has a unique definition at a point that definition

 must assign a constant to the variable

 Thus we may simply describe certain

 variables as not a constant instead of collecting all their

 possible values or all their possible definitions



 enumerate

 Thus we see that the same information may be

 summarized differently depending_on the purpose of the analysis

 ex



 The Data-Flow_Analysis Schema

 secdf-schema



 In each application of

 data-flow_analysis we associate with every program point a data-flow value that represents an abstraction of the set of all

 possible program states that can be observed for that

 point The set of possible data-flow_values is the domain for

 this application

 For_example the domain of data-flow_values

 for reaching_definitions

 is the set of all subsets of definitions in the program

 A particular data-flow value is a set of definitions and we_want to

 associate with each point in the program the exact set of definitions

 that can reach that point

 As_discussed above the choice of abstraction depends_on the goal of the

 analysis to be efficient we only keep_track of information that is

 relevant



 We denote the data-flow_values before and after each statement by

 and respectively The data-flow_problem is

 to find a solution to a set of constraints on the

 's and 's for all statements There_are two sets of

 constraints those based_on the semantics of the statements (transfer

 functions) and those

 based_on the flow of control



 Transfer_Functions



 The data-flow_values before and after a statement are constrained by

 the semantics of the statement For_example suppose our data-flow

 analysis involves determining the constant value of variables at points

 If variable has

 value before executing statement b a then both and

 will have the value after the statement This relationship

 between the data-flow_values before and after the assignment_statement is

 known_as a transfer_function



 Transfer

 functions come in two flavors information may propagate

 forward along execution_paths or it may flow

 backwards up the execution_paths In a forward-flow problem the

 transfer_function of a statement which we_shall usually

 denote takes the data-flow

 value before the statement and produces a new data-flow value after the

 statement That is



 s_fs(s)



 Conversely in a backward-flow problem

 the transfer_function for statement

 converts a data-flow value after the statement to a new

 data-flow value before the statement That is



 s_fs(s)





 Control-Flow Constraints



 The second set of constraints on data-flow_values

 is derived_from the flow of control

 Within a basic_block control_flow is simple

 If a block consists of statements in that

 order then the control-flow value out of is the same as the

 control-flow value into

 That is



 center

 for all

 center



 However control-flow edges between basic_blocks

 create more_complex constraints between the last statement of

 one basic_block and the first statement of the following block

 For_example if we are_interested in collecting

 all the definitions that may reach a program point then the set of

 definitions_reaching the leader statement of a basic_block is the union of

 the definitions after the last statements of each of the predecessor

 blocks The next section gives the details of how data flows among the

 blocks



 Data-Flow Schemas on Basic_Blocks



 While a data-flow_schema technically involves data-flow_values at each

 point in the program we can save time and space by recognizing that

 what goes on inside a block is usually quite_simple

 Control flows from

 the beginning to the end of the block without interruption or

 branching

 Thus we can restate the schema in terms of data-flow_values entering and

 leaving the blocks

 We denote the data-flow_values immediately_before

 and immediately

 after each basic_block by and respectively

 The constraints involving and can be derived_from

 those involving

 and for the various statements in as_follows



 Suppose block consists of statements in that

 order

 If is the first statement of basic_block then

 Similarly if is the last statement of basic_block

 then The transfer_function of a basic_block

 which we denote can be derived by_composing the transfer_functions of the

 statements in the block That is let be the

 transfer_function of statement

 Then

 The relationship_between the beginning and end of the block is



 B_fB(B)





 The constraints

 due to control_flow between basic_blocks can easily be_rewritten by

 substituting and for and

 respectively

 For_instance if data-flow_values are information_about the sets of

 constants that may be assigned to a variable then

 we have a forward-flow problem in which



 B_P a predecessor of B_P





 When the data-flow is backwards as we_shall soon see in

 live-variable_analysis the equations are similar but with the

 roles of the 's and 's reversed

 That is



 align

 B_fB(B)



 B_S a successor of B

 S



 align



 Unlike linear arithmetic equations the data-flow_equations usually do_not

 have a unique solution

 Our_goal is to find the most_precise solution that satisfies

 the two sets of constraints control-flow and transfer constraints

 That is we need a solution that encourages valid code improvements but

 does_not justify unsafe transformations - those that change_what the

 program computes

 This issue is discussed briefly in the box on Conservatism and

 more extensively in Section df-semantics-subsect

 In the following subsections we discuss some of the most_important

 examples of problems that can be_solved by data-flow_analysis



 Reaching_Definitions

 secrd-df



 Reaching_definitions is one of the most

 common and useful data-flow_schemas

 By knowing where in a program each variable may have_been defined

 when control_reaches each point we can determine many things about

 For just two examples

 a compiler then knows whether is a constant at point and a

 debugger can tell_whether it is possible for to be an undefined

 variable should be used at



 Detecting Possible Uses Before Definition

 Here is how we use a solution to the reaching-definitions_problem to

 detect uses before definition

 The trick is to introduce

 a dummy definition for each variable in the entry to the flow_graph

 If the dummy definition of reaches a point where might be used

 then there might be an opportunity to use before definition Note

 that we can never be absolutely certain that the program has a bug

 since there may be some reason possibly involving a complex logical

 argument why the path along which is reached

 without a real definition of can never be taken



 We_say a definition_reaches a point if there is a path

 from the point_immediately following to such that is not

 killed along that path

 We kill a definition of a variable if

 there is any other definition of anywhere

 along the pathfootnoteNote that the path may have loops so we

 could come to another occurrence of along the path which does_not

 kill footnote

 Intuitively if a definition of some

 variable reaches point then might be the place at

 which the value of used at was last defined



 A definition of a variable is a statement that assigns or may

 assign a value to

 Procedure parameters array_accesses and indirect

 references all may have aliases and it is not easy to tell if a statement

 is referring to a particular variable Program analysis

 must_be conservative if we do_not know whether a statement is assigning a

 value to we must assume that it may assign to it that

 is variable after statement may have either its original

 value before or the new value created by For the sake of

 simplicity the rest of the chapter assumes that we

 are dealing only with variables that have no_aliases This class of

 variables includes

 all local_scalar variables in most languages in the case of C and

 C local_variables whose addresses have_been computed at some point

 are

 excluded



 Conservatism in Data-Flow_Analysis

 Since all data-flow_schemas compute approximations to the ground truth

 (as_defined by all possible_execution paths of the program) we are

 obliged to assure that any errors are in the safe_direction

 A policy decision is safe (or conservative)

 if it never allows_us to

 change_what the program computes

 Safe policies may unfortunately cause_us to miss some code

 improvements that would retain the meaning of the program but in

 essentially all code optimizations there is no safe policy that misses

 nothing

 It would generally be unacceptable to use an unsafe policy - one that

 speeds up the code at the expense of changing what the program computes



 Thus when designing a data-flow_schema we must_be conscious of how the

 information will be used and make_sure that any approximations we make

 are in the conservative or safe_direction

 Each schema and application must_be considered independently

 For_instance if we use reaching_definitions for constant_folding it is

 safe to think a definition_reaches when it doesn't (we_might think

 is not a constant when in fact it is and could have_been folded) but

 not safe to think a definition doesn't reach when it does (we_might

 replace by a constant when the program would at times have a value

 for other_than that constant)



 ex

 Shown in Fig_figreach-def is a flow_graph with seven

 definitions Let_us focus_on the definitions_reaching block

 All the definitions in block

 reach the beginning of block The

 definition j_j-1 in block also reaches the

 beginning of block because no other definitions of can

 be found in the loop leading back to This definition however

 kills the definition j_n preventing it from reaching

 or The statement i_i1 in does_not reach

 the beginning of though because the variable is

 always redefined by i u3 Finally the definition

 a u2 also reaches the beginning of block

 ex



 figurehtb

 figureuullmanalsuch9figsreach-defeps

 Flow_graph for illustrating reaching_definitions

 figreach-def

 figure



 By defining reaching_definitions as we have we sometimes allow

 inaccuracies

 However they are all in the safe or conservative direction

 For_example

 notice our_assumption that all edges

 of a flow_graph can be traversed

 This assumption may not be true in practice

 For_example for no values of and

 can the flow of control actually reach statement 2

 in the following program_fragment



 center

 tabularl

 if (a_b) statement 1

 else if (a_b) statement 2

 tabular

 center



 To decide in general whether each path in a flow_graph can

 be taken is an undecidable problem

 Thus we simply assume that every_path in the flow_graph can be followed

 in some execution of the program

 In most applications of reaching_definitions it

 is conservative to assume that a

 definition can reach a point even if it might not

 Thus we may allow paths that are never be traversed in any execution

 of the program and we may allow definitions to pass

 through ambiguous definitions of the same variable safely



 Transfer Equations for Reaching_Definitions



 We_shall now set up the constraints for the reaching

 definitions problem We start by_examining the details of a single

 statement Consider a definition



 center

 u vw

 center

 Here and frequently in what_follows

 is used as a generic binary_operator



 This statement generates a definition

 of variable

 and kills all the other definitions in the program that

 define variable

 while leaving the remaining incoming definitions unaffected

 The transfer_function of definition thus can be_expressed as



 equation

 gen-kill-eq

 fd(x) gend (x_- killd)

 equation

 where the set of definitions generated_by the statement

 and is the set of all other definitions of in the program



 As_discussed in Section secdf-schema the transfer_function of a

 basic_block can be found by_composing the transfer_functions of the

 statements contained therein

 The composition of functions of the form (gen-kill-eq) which we

 shall_refer to as - form is also of

 that form as we can see as_follows

 Suppose there are two functions and



 Then



 align

 f2(f1(x))

 gen2(gen1(x-kill1)-kill2)



 (gen2(gen1-kill2))(x-(kill1kill2))



 align



 This rule extends to a block consisting of any number of statements

 Suppose block has statements with transfer_functions

 for

 Then the transfer_function for block may be written as



 fB(x) genB (x_- killB)



 where



 killB kill1kill2killn



 and

 align

 genB

 genn(genn-1-killn)(genn-2-killn-1-killn)



 (gen1-kill2-kill3-align



 Thus like a statement a basic_block also generates a set of

 definitions and kills a set of definitions The set contains

 all the definitions inside the block

 that are visible immediately_after the block - we refer to

 them as downwards exposed A definition is downwards exposed in

 a basic_block only if it is not_killed by a

 subsequent definition to the same variable inside the same basic

 block A basic block's set is simply the union of all the

 definitions killed by the individual_statements Notice_that a

 definition may appear in both the and set of a basic

 block

 If so the fact that it is in takes_precedence

 because in - form the set is applied before the

 set



 ex

 gen-kill-ex

 The set for the basic_block



 center

 tabularr_l

 a 3



 a 4



 tabular

 center



 is since is not downwards exposed The set

 contains both and since kills and vice_versa

 Nonetheless since the subtraction of the set precedes

 the union operation with the set the result of the transfer

 function for this block always includes definition

 ex



 Control-Flow Equations



 Next we consider the set of constraints derived_from the control_flow

 between basic_blocks Since a definition_reaches a program point as

 long_as there_exists at_least one path along which the definition

 reaches

 whenever there is a control-flow edge

 from to

 However since a definition cannot reach a point unless there is a path

 along which it reaches

 needs to be no larger_than the

 union of the reaching_definitions of all the predecessor blocks

 That is it is safe to assume



 B_P a predecessor of B_P





 We refer to union as the meet_operator for

 reaching_definitions

 In any data-flow_schema the meet_operator is the one

 we use to create a summary of the

 contributions from different paths at the confluence of those

 paths



 Iterative_Algorithm for Reaching_Definitions



 We assume that every control-flow_graph has two empty basic_blocks an

 entry_node which represents the starting point of the graph

 and an exit node to which all exits_out of the graph go

 Since no definitions reach the beginning of the graph

 the transfer_function for the entry block is a simple

 constant function that returns as an answer That is





 The reaching_definitions problem is defined by the following

 equations





 entry

 and for all basic_blocks other_than entry



 B genB (B_- killB)





 B_P a predecessor of B_P



 These_equations can be_solved using

 the following algorithm

 The result of the algorithm is the least fixedpoint of the

 equations ie the solution whose

 assigned values to the 's and 's is contained in the

 corresponding values for any other solution to the equations

 The result of the algorithm below is acceptable since any definition in

 one of the sets or surely must reach the point described

 It is a desirable solution since it does_not include any definitions

 that we can be_sure do_not reach



 alg

 algreaching-definitions

 Reaching_definitions



 A flow_graph for which and have_been computed for

 each block



 and the set of definitions

 reaching the entry and exit of each block of the flow_graph



 We use an iterative_approach in which we start with the estimate

 for all and converge to the

 desired values of and

 As we must iterate_until the 's (and hence the 's) converge

 we could use a boolean variable to record on each pass_through the

 blocks whether any has changed

 However in this and in similar algorithms described later we assume

 that the exact mechanism for keeping_track of changes is understood and

 we elide those details



 The algorithm is sketched in Fig_figreach-def-alg

 The first two lines initialize certain data-flow

 valuesfootnoteThe observant reader will notice that we could

 easily combine lines_(1) and (2) However in similar data-flow

 algorithms it may be necessary to initialize the entry or

 exit node differently from the way we initialize the other nodes

 Thus we follow a pattern in all iterative_algorithms of applying a

 boundary_condition like line_(1) separately from the initialization

 of line (2)footnote

 Line (3) starts the loop in which we iterate_until convergence and the

 inner_loop of lines_(4) through_(6) applies the data-flow_equations to

 every block other_than the entry

 alg



 figurehtb

 center

 tabularr_l

 1)



 2) for_(each basic_block other_than entry)





 3)_while (changes to any_occur)



 4) for_(each basic_block other_than entry)





 5)





 6)







 tabular

 center



 Iterative_algorithm to compute reaching_definitions

 figreach-def-alg



 figure



 Intuitively Algorithm_algreaching-definitions

 propagates definitions as far as they

 will go without being killed thus

 simulating all possible_executions

 of the program

 Algorithm_algreaching-definitions will_eventually halt

 because for every

 never shrinks once a definition is

 added it stays there forever

 (See Exercise rd-ind-exer)

 Since the set of all definitions is finite eventually there must

 be a pass of the while-loop during which nothing is added to any

 and the algorithm then terminates

 We are safe terminating then because if the 's have not changed

 the 's will not change on the next pass

 And if the 's do_not change the 's cannot so on

 all subsequent_passes there can be no changes



 The number of nodes in the flow_graph is

 an upper_bound on the number of times

 around the while-loop

 The_reason is that if a definition_reaches a point

 it can do so along a cycle-free_path and the number

 of nodes in a flow_graph is an upper_bound on the number

 of nodes in a cycle-free_path

 Each time around the while-loop each definition progresses

 by at_least one node along the path in question and it often progresses

 by more_than one node depending_on the order in which the nodes are

 visited



 In_fact if we properly order the blocks in the for-loop of line

 (4) there is empirical evidence that the average_number

 of iterations of the while-loop is under 5

 (see_Section secconvergence-speed)

 Since sets of definitions can be represented_by bit_vectors

 and the operations on these sets can be_implemented by

 logical operations on the bit_vectors

 Algorithm_algreaching-definitions is surprisingly efficient in practice



 ex

 exreaching-definitions

 We_shall represent the seven_definitions

 in the flow_graph of Fig_figreach-def

 by

 bit_vectors where bit from the left represents

 definition

 The union of sets is computed by_taking the logical_OR of the

 corresponding bit_vectors

 The difference of two sets is computed by

 complementing the bit_vector of and then taking the logical

 AND of that complement with the bit_vector for



 Shown in

 the table of Fig_figreach-def-comp are the values taken on by

 the and sets in Algorithm_algreaching-definitions

 The initial

 values indicated by a superscript 0 as in are assigned by

 the loop of line (2)

 of Fig_figreach-def-alg

 They are each the empty_set represented_by bit

 vector 000_0000

 The values of subsequent_passes of the algorithm are also indicated by

 superscripts and

 labeled and for the first pass and

 and for the second



 figurehtb

 center

 tabularc_c c_c c_c

 Block



 000_0000 000_0000 111_0000 000_0000 111_0000



 000_0000 111_0000 001_1100 111 0111_001 1110



 000_0000 001_1100 000_1110 001_1110 000_1110



 000_0000 001_1110 001_0111 001_1110 001_0111



 000_0000 001_0111 001_0111 001_0111 001_0111



 tabular

 center

 Computation of and

 figreach-def-comp

 figure



 Suppose the for-loop of lines_(4) through_(6) is executed with

 taking on the values



 B1_B2 B3 B4 exit



 in that order

 With since

 is the empty_set and is

 This value differs_from the previous value so we

 now know there is a change on the first round (and will proceed to a

 second round)



 Then we consider and compute



 align

 B21 B11 B40



 111_0000 000_0000 111_0000



 B21 genB2 (B21 - killB2)



 000 1100 (111 0000 - 110 0001) 001_1100



 align



 This computation is summarized in Fig_figreach-def-comp

 For_instance at the end of the first pass

 reflecting the fact that and are generated in while

 reaches the beginning of

 and is not_killed in



 Notice_that after the second round has changed to reflect

 the fact that also reaches the beginning of and is not

 killed by We did_not learn that fact on the first pass because

 the path from to the end of which is

 is not traversed in that order by a single pass That is by the

 time we learn that reaches the end of we have_already

 computed and on the first pass



 There_are no changes in any of the

 sets after the second pass Thus after a third_pass the

 algorithm_terminates with the 's and 's as in the final

 two columns of Fig_figreach-def-comp

 ex



 Live-Variable Analysis

 live-var-subsect



 Some code-improving_transformations

 depend_on information computed in the direction

 opposite to the flow of control in a program

 we_shall examine one such example now

 In live-variable_analysis

 we_wish to know for variable

 and

 point whether the value of

 at

 could be used along some path in the flow

 graph starting_at

 If so we say

 is

 live

 at otherwise

 is

 dead

 at



 An_important use for live-variable information

 is register_allocation for basic_blocks

 Aspects of this issue were_introduced in Sections simple-cg-sect

 and ra-sect

 After a value is computed in a register and

 presumably used within a block it is not

 necessary to store that value if it is dead at

 the end of the block

 Also if all registers are full and we need another

 register we should favor using a register with a dead

 value since that value does_not have to be stored



 Here we define the data-flow_equations directly in terms of

 and which represent the set of variables live at

 the points immediately_before and after block respectively

 These_equations can also be derived by first defining the transfer

 functions of individual_statements and composing them to create the

 transfer_function of a basic_block

 Define



 enumerate



 as the set of variables defined (ie

 definitely assigned values) in prior to any use of that

 variable in and



 as the

 set of variables whose values may be

 used in prior to any definition of the variable



 enumerate



 ex

 For_instance block in Fig_figreach-def definitely uses

 It also uses before any redefinition of

 unless it is possible that and are

 aliases of one another Assuming there are no_aliases among the

 variables in Fig_figreach-def then

 clearly defines and If there are no_aliases

 since and are used before redefinition

 ex



 As a consequence of the

 definitions any variable in must_be considered live_on entrance to

 block while definitions of variables

 in definitely are dead

 at the beginning of

 In effect membership in kills any opportunity

 for a variable to be live because of paths that begin at



 Thus the equations relating and

 to the

 unknowns and are defined as_follows





 exit

 and for all basic_blocks other_than exit



 align

 B useB (B_- defB)



 B_S a successor of B_S



 align

 The first equation specifies the boundary_condition which is that no

 variables are live_on exit from the program The second

 equation_says that

 a variable is live_coming into a block if

 either it is used before redefinition in the block or it is live

 coming_out of the block and is not redefined in the block

 The third equation_says that a

 variable is live_coming out of a block

 if and only if it is live_coming into one of its successors



 The relationship_between the equations for liveness and

 the reaching-definitions equations

 should be noticed



 itemize



 Both sets of equations have union as the meet_operator

 The_reason is that in each data-flow_schema we propagate information

 along paths and we care only about_whether any path with desired

 properties exist rather_than whether_something is true along all

 paths



 However information flow for liveness travels backward opposite to

 the direction of control_flow because in this problem we_want to make

 sure that the use of a variable at a point

 is transmitted to all points prior to in an execution_path so that

 we may know at the prior point that will have its value used



 itemize



 To solve a backward

 problem instead of initializing we initialize



 Sets and have their roles interchanged and and

 substitute for and respectively

 As for reaching_definitions the solution to the liveness equations

 is not_necessarily

 unique and we_want the solution with the smallest sets of

 live_variables

 The algorithm used is

 essentially a backwards version of Algorithm_algreaching-definitions



 alg

 algliveness

 Live-variable analysis



 A flow_graph with def and computed for each block



 and the set of variables

 live_on entry and exit of each block of the flow_graph



 Execute the program in Fig figliveness

 alg



 figurehtb

 center

 tabularl





 for_(each basic_block other_than exit)





 while_(changes to any_occur)



 for_(each basic_block other_than exit)















 tabular

 center



 Iterative_algorithm to compute live_variables

 figliveness



 figure



 Available_Expressions

 ae-subsect



 An expression is

 available

 at a point if every_path from

 the entry_node to evaluates

 and after the last

 such evaluation prior to reaching there are no_subsequent

 assignments to or footnoteNote

 that as usual in this_chapter we use the operator as a

 generic_operator not_necessarily standing for additionfootnote

 For the available-expressions data-flow_schema we say that a block

 kills

 expression

 if it assigns (or may assign)

 or and does_not

 subsequently recompute

 A block

 generates

 expression

 if it definitely evaluates

 and

 does_not subsequently define or



 Note_that the notion of killing or generating an available

 expression is not exactly the same as that for reaching

 definitions

 Nevertheless these notions of kill and generate behave

 essentially

 as they do for reaching_definitions



 The primary use of available-expression information is for

 detecting global_common subexpressions

 For_example in Fig figcse(a) the expression

 in block will be a common_subexpression if

 is available at the entry point of block

 It will be available if

 is not assigned a new value in block or if as

 in Fig figcse(b)

 is recomputed after

 is assigned in



 figurehtb

 figureuullmanalsuch9figscseeps

 Potential common_subexpressions across blocks

 figcse

 figure



 We can compute the set of generated expressions for each point in a block

 working from beginning to end of the block

 At the point prior to the block no expressions are generated

 If at point set of expressions is available

 and is the point after with statement x_yz between

 them then we form the set of expressions

 available at by the following two steps



 enumerate



 Add to the expression



 Delete from any expression involving variable



 enumerate



 Note the steps must_be done in the correct order as could be

 the same as or

 After we reach the end of the block is the set of generated

 expressions for the block

 The set of killed expressions is all expressions say

 such that either or is defined in the block and

 is not generated_by the block



 ex

 exavail-exp

 Consider the four statements of Fig figavail-exp-comp

 After the first is available

 After the second statement

 becomes available but is no_longer

 available because has_been redefined

 The third statement

 does_not make available again because the value

 of is immediately changed

 After the last statement is no_longer available because

 has changed

 Thus no expressions are generated and all expressions_involving

 or are killed

 ex



 figurehtb

 center

 tabularc_c

 Statement Available_Expressions







 a b_c







 b a - d







 c b_c







 d a - d







 tabular

 center



 Computation of available_expressions

 figavail-exp-comp



 figure



 We can find available_expressions in a manner reminiscent of the

 way reaching_definitions are computed

 Suppose is the universal_set of all expressions

 appearing on the right of one or_more statements of the program

 For each block let be the set of expressions in

 that are available at the point just_before the beginning of

 Let be the same for the point following the end

 of

 Define to be the expressions generated_by

 and to be the set of expressions in killed

 in

 Note_that and can all be

 represented_by bit_vectors

 The following equations relate the unknowns and to

 each other and the known quantities and



 entry

 and for all basic_blocks other_than entry



 align

 B egenB (B_- ekillB)



 B_P a predecessor of B_P



 align



 The above equations look almost identical to the equations for

 reaching_definitions

 Like reaching_definitions the boundary_condition is

 because

 at the exit of the entry_node there are no available

 expressions

 The most_important difference is that the meet_operator

 is intersection rather_than union

 This operator is the proper one because an expression

 is available at the beginning of a block only if it is available

 at the end of all its_predecessors

 In_contrast a definition_reaches the beginning of a block

 whenever it reaches the end of any one or_more of its_predecessors



 The use of rather_than makes the available-expression

 equations behave

 differently from those of reaching_definitions

 While neither set has a unique solution for reaching_definitions it is the

 solution with the smallest sets

 that corresponds to the definition of reaching and we

 obtained that solution by starting_with the assumption

 that nothing reached anywhere and building

 up to the solution

 In that way we never assumed that a definition

 could reach a point unless

 an actual path propagating to could be found

 In_contrast for available expression equations we_want the solution

 with the

 largest sets of available_expressions

 so we start with an approximation that is too large and work down



 It may not be obvious that by starting_with the assumption

 everything (ie the set ) is available everywhere except at the

 end of

 the entry block

 and eliminating

 only those expressions for which we can discover a path along which it

 is not available we do reach a set of truly_available expressions

 In the case of available_expressions it is conservative to produce a subset

 of the exact set of available_expressions

 The argument for subsets being conservative is that our intended use of the

 information is to replace the computation of an available expression

 by a previously_computed value

 Not knowing an expression is available only inhibits

 us from improving the code while believing an expression is available

 when it is not could cause_us to change_what the program computes



 figurehtb

 figureuullmanalsuch9figsavail-expeps

 Initializing the sets to is too

 restrictive

 figavail-exp-ex

 figure



 ex

 exavail-exp-comp

 We_shall concentrate_on a single block in Fig figavail-exp-ex

 to illustrate the effect of the initial approximation of

 on

 Let and abbreviate and

 respectively

 The data-flow_equations for block are



 B2 B1_B2





 B2 G (B2 -_K)



 These_equations may be_rewritten as recurrences

 with and being the th approximations of

 and respectively

 Ij1 B1 Oj

 Oj1 G (Ij1 -_K)



 Starting with



 we get

 However if we start with

 then we get

 as we should

 Intuitively the solution obtained starting_with



 is more desirable because it correctly reflects the fact

 that expressions in

 that are not_killed by are available at

 the end of

 ex



 alg

 algavailable-expr

 Available expressions



 A flow_graph with and

 computed for each block

 The initial block is



 and the set of

 expressions available at

 the entry and exit of each block of the flow_graph





 Execute the algorithm of Fig figavail-exp-alg

 The explanation of the steps is similar to that for Fig_figreach-def-alg

 alg



 figurehtb

 center

 tabularl





 for_(each basic_block other_than entry)





 while_(changes to any_occur)



 for_(each basic_block other_than entry)















 tabular

 center

 Iterative_algorithm to compute available_expressions

 figavail-exp-alg

 figure





 Definition-Use Chains

 A calculation done in virtually the same manner as

 live-variable_analysis is

 definition-use chaining

 (du-chaining)

 We_say a variable is

 used

 at statement if its -value may be required

 For_example

 b

 and

 c

 (but_not

 a

 are used in

 each of the statements

 a bc

 and

 ab c

 The du-chaining

 problem is to compute for a point the set of uses

 of a variable say

 x

 such that there is a path from

 to that does_not redefine

 x



 The data-flow_values in the du-chaining_problem are sets of pairs

 such that is a statement that uses variable

 As with live_variables

 if we can compute the set of uses reachable from

 the end of block then we can compute

 the definitions reached from any point within block by

 scanning the portion of block that follows

 In_particular if there is a definition of variable

 x

 in the block we can determine the

 du-chain

 for that definition the list of all possible uses of that definition



 The equations for computing du-chaining information look

 exactly like those for liveness with substitution for and

 In place of we have the set of pairs_such

 that is a statement in which uses variable

 x

 and such

 that no prior definition of

 x

 occurs in

 Instead of we have

 the set of pairs_such

 that is a statement which uses

 x

 is not in and

 has a definition of x

 These_equations are solved by the obvious analog of

 Algorithm_algliveness





 Summary



 In this_section we have discussed three instances of data-flow_problems

 reaching_definitions live_variables and available_expressions

 As summarized in Fig figframework

 the definition of each problem

 is given by the domain of the data-flow_values

 the direction of the data flow the

 family of transfer_functions the boundary_condition and the meet

 operator We denote the meet_operator generically as



 The last_row shows the initial values used in the iterative

 algorithm These values are chosen so that the iterative_algorithm

 will find the most_precise

 solution to the equations This choice is not strictly a part of

 the definition of the data-flow_problem since it is an artifact needed

 for the iterative_algorithm There_are other ways of solving the

 problem For_example we saw how the transfer_function of a basic

 block can be derived by_composing the transfer_functions of the individual

 statements in the block a similar compositional approach may be used

 to compute a transfer_function for the entire procedure or transfer

 functions from the entry of the procedure to any program point

 We_shall discuss such an approach in Section_region-sect



 figurehtb

 center

 tabularl_l l_l

 Reaching_Definitions Live Variables Available_Expressions



 Domain_Sets of definitions Sets of variables Sets of expressions





 Direction_Forwards Backwards_Forwards



 Transfer







 function



 Boundary







 Meet_()



 Equations















 Initialize



 tabular

 center



 Summary of three data-flow_problems

 figframework

 figure



 Why the Available-Expressions Algorithm Works

 We need to explain why starting all 's except that for the entry

 block with the set of all expressions leads to a conservative

 solution to the data-flow_equations that is all expressions found to

 be available really are available

 First because intersection is the meet_operation in this data-flow

 schema any reason that an expression is found not to be available

 at a point

 will propagate_forward in the flow_graph along all possible paths

 until is recomputed and becomes available again

 Second there are only two_reasons could be unavailable



 enumerate



 is killed in block because or is defined without a

 subsequent computation of

 In this case the first time we apply the transfer_function

 will be removed_from



 is never computed along some path

 Since is never in and it is never

 generated along the path in question we can show by induction on the

 length of the path that is eventually removed_from 's and

 's along that path



 enumerate

 Thus after changes subside the solution provided by the iterative

 algorithm of Fig figavail-exp-alg will include only truly

 available_expressions



 sexer

 For the flow_graph of Fig_fg1-fig (see the exercises for

 Section_secopt-sources) compute



 itemize



 a) The gen and kill_sets for each block



 b) The and sets for each block



 itemize

 sexer



 exer

 For the flow_graph of Fig_fg1-fig compute the

 and sets for available_expressions

 exer



 exer

 For the flow_graph of Fig_fg1-fig compute the def

 and sets for live variable analysis

 exer



 hexer

 Suppose is the set of complex numbers Which of the following

 operations can serve as the meet_operation for a semilattice on



 itemize

 a) Addition



 b) Multiplication



 c) Componentwise minimum



 d) Componentwise maximum



 itemize

 hexer



 hexer

 We claimed that if a block consists of statements and the th

 statement has gen and kill_sets and then the

 transfer_function for block has gen and kill_sets and

 given by



 killB kill1kill2killn



 align

 genB

 genn(genn-1-killn)(genn-2-killn-1-killn)



 (gen1-kill2-kill3-align

 Prove this claim by induction on

 hexer



 hexer

 rd-ind-exer

 Prove by induction on the number of iterations of the for-loop of lines

 (4)_through (6) of Algorithm_algreaching-definitions

 that none of the 's or 's ever shrinks

 That is once a definition is placed in one of these sets on some round

 it never disappears on a subsequent round

 hexer



 hexer

 Show the correctness of Algorithm_algreaching-definitions That

 is show that



 itemize



 a)

 If definition is put in or

 then there is a path from to the beginning or end of block

 respectively along which the variable defined by might not be

 redefined



 b)

 If definition is not put in or

 then there is no path from to the beginning or end of block

 respectively along which the variable defined by might not be

 redefined



 itemize

 hexer



 hexer

 Prove the following about Algorithm_algliveness



 itemize



 a)

 The 's and 's never shrink



 b)

 If variable is put in or

 then there is a path from the beginning or end of block

 respectively along which might be used



 c)

 If variable is not put in or

 then there is no path from the beginning or end of block

 respectively along which might be used



 itemize

 hexer



 hexer

 Prove the following about Algorithm algavailable-expr



 itemize



 a)

 The 's and 's never grow that is successive values of

 these sets are subsets (not_necessarily proper) of their previous

 values



 b)

 If expression is removed_from or

 then there is a path from the entry of the flow_graph to the

 beginning or end of block

 respectively along which is either never computed or after its

 last computation one of its arguments might be redefined



 c)

 If expression remains in or

 then along every_path from the entry of the flow_graph to the

 beginning or end of block

 respectively is computed and after the last computation no

 argument of could be redefined



 itemize

 hexer



 hexer

 The astute reader will notice that in

 Algorithm_algreaching-definitions we could have saved some time

 by initializing to for all blocks Likewise in

 Algorithm_algliveness we could have initialized to

 We did_not do so for uniformity in the treatment of the

 subject as we_shall see in Algorithm_algiterative However is

 it possible to initialize to in

 Algorithm algavailable-expr Why or why not

 hexer



 hexer

 Our data-flow_analyses so_far do_not take_advantage of the semantics of

 conditionals Suppose we find at the end of a basic_block a test such

 as



 verbatim

 if (x 10) goto

 verbatim

 How could we use our understanding of what the test means to

 improve our knowledge of reaching_definitions Remember improve

 here means that we eliminate certain reaching_definitions that really

 cannot ever reach a certain program point

 hexer

 Foundations of Data-Flow_Analysis



 We have showed that the data-flow_abstraction is useful by the use of

 a number of concrete examples Before we move on to discuss_how we

 can use these analysis is optimizations we now study the family of

 data-flow_analysis as a whole abstractly We will study formally

 several very important questions about data-flow algorithms

 itemize

 Is the iterative_algorithm used in data-flow_analysis correct

 How precise is the solution obtained_by the data-flow_analysis

 Will the iterative_algorithm converge

 How long does it take for the algorithm to converge

 What is the meaning of the solution of the equations

 itemize



 In our_discussion of reaching_definitions in

 Section_secdf-intro we address each of the questions above

 quite thoroughly albeit in an informal manner Subsequently we

 explain the other problems by drawing the analogies between the new

 problems and the ones we have studied Here we_wish to address these

 issues more formally We identify the properties desired of data-flow

 problems and prove the implications of these properties with_respect

 to the issues of the correctness precision convergence of the

 data-flow algorithm and the meaning of the solution This makes it

 easy for us to understand new data-flow_problems and also formulate

 new ones We only need to show that the data-flow_problem definition

 to have certain properties and we have the rigorous answers to all

 the above difficult questions immediately



 The concept of having a common theoretical framework for a class of

 analyses also has important practical implications The framework

 helps us identify the reusable components of the algorithm in our

 software design Not_only does this reduce the coding effort it also

 reduces the chances of programming_errors by not having to re-code the

 same details over and over again



 The Axioms of Data-Flow_Analysis Frameworks



 A data-flow_analysis framework consists of

 enumerate

 direction of the data flow which can be either forwards or backwards

 a semi-lattice which includes a domain of values

 and a meet_operator

 A family of transfer_functions

 from to This includes also boundary conditions given by

 the transfer_functions of the

 entry and exit

 enumerate



 Semi-lattices



 A semi-lattice is a set and a binary meet_operator

 such that

 itemize

 idempotent



 () (_)



























 V











 TOPV





 V











 Fgenkill































 K K1 K2

 G (G2 (G1 -K2 ))

 f1f2

 f(X) G (X-K)F









 phi X XX







 F







 E

 X

 EX X









 F











 ( x ) ( x ) ( x )

 ( x ) ( x )

 ( x ) nonconst

 ( x ) nonconst

 nonconst

 nonconst





 ( x ) undef







 ( x ) ( x )

 undef















 fFf

 V

 ff







 V













 VXYX Y X

 XY

 V







 X YXY XXY

















 XY

 X YY X



 V



 V

 XYY X

 V

 d1

 d2d3





 X

 YXY

 d1 d2 d3d1



 d1 d2











 X YZ

 ZXY

 Xd1Yd2

 Zd1 d2





 TOP





 (F V )





 rm implies f(_) f(_)

 VfF





 f(_) f(_) f(_)

 VfF





















 xy(x y) y x_y

 f(_) f(_)

 f(_) f(_)



 x yx zx y_z



 xf( )y f( )z f(_)







 f(_) f(_)



 f(_) f(_) f(_)



 f(_) f(_) f(_)



 x_y zx z



 f(_) f(_)













 VfF

 x yx y x

 x_y











 XY

 f

 f(Z) G (Z-K)GK







































 nonconst cnonconstc

 c dnonconstc d

 c undefcc

 nonconst undefnonconst

 x xxx









 ( a ) ( a ) ( a )













 nonconstcc

 cundefc

 nonconstundef







 ci

 V



 ( a )

 V





 V

 ( a ) ( a )

















 ( a )c

 ( a )undef( a )undef

 ( a )

 f



 f(_) f(_)



 f

 ( a )( a )

 ( x ) ( x )







 f(_) ( a )_f( ) ( a )





 f(_) ( a )

 ff( )

 Then we apply to

 a

 and the result is one of the values in the diagram of Fig 1060



 We must consider all possible values of

 and subject to the constraints that

 and

 For_example if



 tabularl_l l















 tabular



 then

 and

 Since we have made the check in one case

 The other cases are left as an_exercise for the reader



 Now we must check our claim that the constant computation framework

 is not_distributive

 For this part let be the function associated_with the assignment

 abc and let

 and

 Let

 Then

 Similarly

 Equivalently

 It follows that since the sum of

 two nonconstant values is presumed to be nonconstant



 On the other_hand

 since given and

 the assignment abc sets a to 5

 Similarly

 Thus

 We_now see that

 so the distributivity condition is

 violated



 Intuitively the reason we get a distributivity violation is that the

 constant computation framework is not powerful enough to remember

 all invariants in particular the fact that along paths whose

 effects on variables are described by either or the

 equation holds even_though neither

 b nor c are themselves

 constant

 We could devise more_complicated frameworks to avoid

 this particular problem although the payoff

 from doing_so is not clear

 Fortunately monotonicity is adequate for the iterative_data-flow

 algorithm to work as we_shall next see

 Meet-Over-Paths Solutions to Data-Flow Problems





 Let_us imagine that a flow_graph has associated

 with each of its nodes a transfer_function one of the functions in the

 set

 For each block let be the transfer_function for



 Consider any path

 from the initial_node to some block

 We can define the

 transfer_function for



 to be the composition of

 Note_that is not part of the composition reflecting

 the point of view that this path is taken to reach the beginning of block

 not its end



 We have assumed that the values in represent information_about

 data used by the program and that the confluence operator

 tells how that information is combined when paths converge

 It also makes_sense to see the top_element as representing no information

 since a path carrying the top_element yields to any other path as far as

 what information is carried after confluence of the paths

 Thus if is a block in the flow_graph the information entering

 should be computable by considering every possible path from

 the initial_node to and seeing what_happens along that path

 starting out with no information

 That is for each path from to we compute

 and take the meet of all the resulting elements



 In principle this meet could be over an_infinite number of different

 values since there are an_infinite number of different paths

 In_practice it is often adequate to consider only acyclic_paths

 and even when it is not as for the constant computation framework

 discussed_above there are usually other reasons we can find to make this

 infinite meet be finite for any particular flow_graph



 Formally we define the

 meet-over-paths solution

 to a flow_graph to be



 mop ( B ) from pile rm paths P above rm from B0 rm to B fP ( TOP )



 The solution to a flow_graph makes_sense when we realize that

 as far as information reaching block is concerned the flow

 graph may as_well be the one suggested in Fig 1061

 where the transfer_function associated_with each of the (possibly

 infinite_number of) paths in the original

 flow_graph has_been given an entirely separate path to

 In Fig 1061 the information reaching

 is given by the meet_over all paths

 figure

 Graph showing the set of all possible paths to

 figure

 Conservative Solutions to Flow Problems



 When we try to solve the data-flow_equations that come from an

 arbitrary framework we may or may not be_able to obtain the

 solution easily

 Fortunately as with the concrete examples of data-flow

 frameworks in Sections 105 and 106

 there is a safe_direction in which to err and the iterative_data-flow

 algorithm we discussed in those sections turns_out always to provide

 us with a safe solution

 We_say a solution is a

 safe solution

 if for all blocks



 Despite what the reader might imagine

 we did_not pull this definition out of thin air

 Recall that in any flow_graph the set of

 apparent

 paths to a node (those that are paths in the flow graph)

 can be a proper

 subset of the

 real

 paths those that are taken on some execution

 of the program corresponding to this flow_graph

 In order that the result of the data-flow_analysis be usable for whatever

 purpose we intend it

 the data must still be safe if we modify the flow_graph by deleting

 some paths since we cannot in general distinguish real paths

 from apparent paths that are not real



 Suppose among the infinite set of paths suggested in Fig 1061

 is the meet of taken

 over all real paths that are followed

 on some execution

 Also let be the meet of over all other paths

 Thus is

 Then the true answer to our data-flow_problem at node is

 but the solution is

 Recall that since

 Thus the solution is the true solution



 While we may prefer the true solution to the data-flow_problem

 we_shall almost surely have no efficient way to tell exactly which paths are

 real and which are not so we are forced to accept the solution

 as the closest feasible solution

 Thus whatever use we make of the data-flow information must_be

 consistent_with the possibility that the solution we obtain is

 the true solution

 Once we accept that we should also be_able to accept a solution that

 is the (and_therefore the true solution)

 Such solutions are easier to obtain than the for those frameworks

 that are monotone but not_distributive

 For distributive frameworks like those of Section 106

 the simple iterative_algorithm computes the solution

 The Iterative_Algorithm for General Frameworks



 There is an obvious generalization of Algorithm 102 that works for

 a large variety of frameworks

 The iterative_algorithm requires that the framework be monotone and

 it requires finiteness in the sense that the meet_over the

 infinite set of paths suggested in Fig 1061 is equivalent to

 a meet_over a finite subset

 We_shall give the algorithm and then discuss the ways in which we

 could assure finiteness

 However one common guarantee of finiteness is the one we have had

 all along propagation along acyclic

 paths is sufficient



 Algorithm_1018



 Iterative solution to general data-flow_frameworks

 A data-flow graph a set of values a set of functions

 a meet_operation and an assignment of a member of to

 each node of the flow_graph

 A value in for each node of the flow_graph

 The algorithm is given in Fig_1062

 As with the familiar iterative_data-flow algorithms we compute

 and for each node by successive approximations

 We assume that is the function in associated_with

 block

 that function plays the role of and in Section 106

 figure



 1) each node initialize assuming





 2)





 3) changes to any occur





 4) each block in depth-first_order





 5)





 in above the meet of an empty_set is





 6) )









 Iterative_algorithm for general frameworks

 figure



 A Data-Flow_Analysis Tool



 We can now see the way the ideas of this_section can be applied to

 a tool for data-flow_analysis

 Algorithm_1018 depends for its working on the following subroutines

 enumerate

 A routine to apply a given in to a given value in

 This routine is used in lines_(2) and (6) of Fig_1062

 A routine to apply the meet_operator to two values in

 this routine is needed

 zero_or more times in line_(5)

 A routine to tell_whether two values are equal

 This test is not made explicitly in Fig_1062 but it is implicit

 in the test for change in any of the values of

 enumerate



 We also need to have specific data type declarations for and

 so we can pass arguments to the routines mentioned above

 The values of and in Fig_1062 are also of the type

 declared for

 Finally we need a routine that will take the ordinary representation

 of the contents of a basic_block that is a list of statements and

 produce an element of the transfer_function for that block



 Example 1048



 For the reaching_definitions framework we might first build a table

 that identified each statement of the given flow_graph

 with a unique integer from 1 up to some maximum

 Then the type of could be bit_vectors of length

 could be represented_by pairs of bit_vectors of that size ie

 by the and sets

 The routine to construct the and bit_vectors given the

 statements of a block and the table associating definition statements

 with bit-vector positions is straightforward as are the routines

 to compute meets (logical or of bit vectors) compare bit_vectors

 for equality and apply functions defined by a

 - pair to bit_vectors



 The data-flow_analysis tool is thus little more_than an

 implementation of Fig_1062 with calls to the given subroutines

 whenever a meet function application or comparison is needed

 The tool would support a fixed representation of flow_graphs and

 thus be_able to perform tasks like finding all the predecessors

 of a node finding the depth-first_ordering of the flow_graph

 or applying to each block the routine that computes the function in

 associated_with that block

 The advantage of using such a tool is that the graph-manipulation

 and convergence-checking aspects of Algorithm_1018 do_not have to be

 rewritten for each data-flow_analysis that we do



 Properties of Algorithm_1018



 We should make clear the assumptions under which Algorithm_1018

 actually works and exactly what the algorithm_converges to when it

 does converge

 First if the framework is monotone and converges then we claim the result of

 the algorithm is that for all blocks

 The intuitive reason is that along any path

 from the initial_node to

 we can show by induction on that the effect of the path from

 to is felt after

 at most iterations of the while-loop in Fig_1062

 That is if is the path

 then after rounds

 Thus when and if the algorithm_converges will be

 for every_path from to

 Using the rule that if and then

 (

 There is the technicality that we must in principle show this rule not

 just for two values and (from which follows the rule that if

 for any finite set of 's

 then )

 but that the same rule holds for an_infinite number of 's

 However in practice whenever we get convergence of Algorithm_1018

 we_shall find a finite number of paths

 such that the meet_over all paths is equal to the meet_over this

 finite set

 )

 we can show that )



 When the framework is distributive we can show Algorithm_1018

 does in fact converge to the solution

 The essential idea is to prove that at all times during the

 running of the algorithm and are each equal to

 the meet of for some set of paths to the

 beginning and end of respectively

 However we show in the next example that need

 not be the case when the framework

 is monotone but not_distributive



 Example 1049

 Let_us exploit the example of nondistributivity of the constant

 computation framework discussed in Example 1047

 the relevant flow_graph is shown in Fig 1063

 The mappings and coming_out of and

 are the ones from Example 1047

 Mapping entering is and

 is the mapping coming_out of setting a to

 even_though every real path (and every apparent path) computes

 after

 figure

 Example of solution less_than solution

 figure



 The problem intuitively is that Algorithm_1018 dealing with a

 nondistributive framework behaves as if some sequences of nodes

 that are not even apparent paths (paths in the flow graph) were

 real paths

 Thus in Fig 1063 the algorithm behaves as if paths like

 or



 were real paths setting b and c to a combination of values that

 do_not sum to five



 Convergence of Algorithm_1018



 There_are various ways that we could prove Algorithm_1018 converges for

 a particular framework

 Probably the most_common case is where only acyclic_paths are needed

 ie we can show that the meet_over acyclic_paths is the same as the

 solution over all paths

 If that is the case not only does the algorithm converge it will

 normally do so very rapidly in two more passes than the depth

 of the flow_graph as we discussed in Section 1010



 On the other_hand frameworks like our constant computation example require

 more_than acyclic_paths be considered

 For_example Fig 1064 shows a simple flow_graph where we have to

 consider the path

 to realize that x does_not have a constant value on entering

 figure

 Flow_graph requiring cyclic path be included in

 figure



 However for constant computations we can reason that Algorithm_1018

 converges as_follows

 First of all it is easy to show for an arbitrary monotone framework

 that and for any block form a nonincreasing

 sequence in the sense that the new value for one of these variables

 is always the old value

 If we recall Fig 1060 the lattice_diagram for the values of

 mappings applied to variables we realize that for any variable

 the value of or can only drop twice once from

 to a constant and once from that constant to



 Suppose there are nodes and variables

 Then on every iteration of the while-loop in Fig_1062 at_least one

 variable must have its value drop in some or the algorithm

 converges and even infinite iteration of the while-loop will not

 change the values of the 's or 's

 Thus the number of iterations is limited to

 if that number of changes occurs

 then every variable must have reached at

 every block of the flow_graph

 Fixing the Initialization



 In some data-flow_problems there is a discrepancy

 between what Algorithm_1018 gives_us as a

 solution and what we intuitively want

 Recall that for available_expressions is intersection

 so must_be the set of all expressions

 Since Algorithm_1018

 initially assumes is for each block including

 the initial_node the solution produced_by Algorithm_1018

 is actually the set of expressions that assuming they are available

 at the initial_node (which they are not) would be available on

 entrance to block



 The difference of course is that there might be paths from the

 initial_node to along which an expression xy is neither generated

 nor killed

 Algorithm_1018 would say xy is available when in fact it is not

 because no variable along that path can be found to hold its value

 The fixup is simple

 We can either modify Algorithm_1018 so that for the available

 expression framework is set and held equal to

 the empty_set or we can modify the flow_graph by introducing a

 dummy initial_node a predecessor of the real initial_node that kills every expression



 Foundations of Data-Flow_Analysis

 secdf-foundation



 Having shown several useful examples

 of the data-flow_abstraction we now study the family of

 data-flow_schemas as a whole abstractly We_shall answer

 several basic questions about data-flow algorithms formally



 enumerate



 Under what circumstances is

 the iterative_algorithm used in data-flow_analysis correct



 How precise is the solution obtained_by the iterative_algorithm



 Will the iterative_algorithm converge



 What is the meaning of the solution to the equations



 enumerate



 In Section_secdf-intro we addressed each of the questions

 above informally when describing the reaching-definitions_problem

 Instead of answering the same questions for each subsequent problem

 from scratch we relied on analogies with the problems we had already

 discussed to explain the new problems

 Here we present a general approach that answers all

 these questions once and for all rigorously and for a large family of

 data-flow_problems We first identify the properties

 desired of data-flow_schemas and prove the implications of these

 properties on the correctness precision and convergence of the

 data-flow algorithm as_well as the meaning of the solution Thus to

 understand old algorithms or formulate new ones we simply show that

 the proposed data-flow_problem definitions have certain properties

 and the answers to all the above difficult questions are available

 immediately



 The concept of having a common theoretical framework for a class of

 schemas also has practical implications The framework

 helps us identify the reusable components of the algorithm in our

 software design Not_only is coding effort reduced but

 programming_errors are reduced by not having to recode

 similar details several_times



 A data-flow_analysis framework consists of



 enumerate



 A direction of the data flow which is either forwards or

 backwards



 A semilattice (see_Section secsemilattice for the

 definition) which includes a domain of values

 and a meet_operator



 A family of transfer_functions

 from to This family must include functions suitable for the

 boundary conditions which are

 constant transfer_functions for the special nodes

 entry and exit in

 any flow_graph



 enumerate



 Semilattices

 secsemilattice



 A semilattice is a set and a binary meet_operator

 such that for all and in



 enumerate



 (meet is idempotent)





 (meet is commutative)





 (meet is associative)



 enumerate



 A semilattice has a top_element denoted such that



 center

 for all in

 center

 Optionally a semilattice may have a bottom_element denoted such that

 center

 for all in

 center



 Partial Orders



 As we_shall see

 the meet_operator of a semilattice defines a partial_order on

 the values of the domain A relation is a partial_order on

 a set if for all and in



 enumerate





 (the_partial order is reflexive)



 If and then

 (the_partial order is antisymmetric)



 If and then

 (the_partial order is transitive)

 enumerate

 The pair

 is called a poset or partially ordered set

 It is also convenient to have a relation for a poset defined as



 center

 if and only if and

 center



 The Partial Order for a Semilattice



 It is useful to define a partial_order for a

 semilattice

 For all and in we define



 center

 if and only if

 center

 Because the meet_operator is

 idempotent commutative and associative the order as defined is

 reflexive antisymmetric and transitive

 To_see why observe that



 itemize



 Reflexivity for all



 The proof is that since

 meet is idempotent



 Antisymmetry

 if and then

 In proof means and means

 By commutativity of



 Transitivity

 if and then

 In proof

 and means that and

 Then



 using associativity of meet

 Since has_been shown we have proving

 transitivity



 itemize



 ex

 The meet operators used in the examples in Section_secdf-intro

 are set union and set_intersection They are

 both idempotent commutative and associative

 For set union the

 top_element is and

 the bottom_element is the universal_set since

 for any subset of

 and

 For set_intersection

 is and

 is

 the domain of values of the semilattice is the

 set of all subsets of which is sometimes_called

 the power set of and denoted



 For all and in

 implies therefore

 the partial_order imposed_by set union is set inclusion

 Correspondingly the partial_order imposed_by set_intersection is

 set containment

 That is for set_intersection

 sets with fewer elements are considered to be smaller in the

 partial_order

 However for set union

 sets with more elements are considered to be smaller in the

 partial_order To say that sets larger in size are

 smaller in the partial_order is counterintuitive however this situation

 is an unavoidable consequence of the definitions(And if we

 defined the partial_order to be instead of then the problem

 would surface when the meet was intersection although not for union)



 As_discussed in Section_secdf-intro there are

 usually many solutions

 to a set of data-flow_equations with the greatest solution (in the

 sense of the partial_order ) being the most

 precise For_example in reaching_definitions the most_precise

 among all the solutions to the data-flow_equations is the one with the

 smallest number of definitions

 which corresponds to the greatest element in the

 partial_order defined by the meet_operation union In available

 expressions the most_precise solution is the one with the largest

 number of expressions Again it is the greatest solution in the

 partial_order defined by intersection as the meet_operation

 ex



 Greatest Lower Bounds



 There is another useful relationship_between the meet

 operation and the partial ordering it imposes

 Suppose is a semilattice

 A greatest_lower bound

 (or glb) of domain elements and is an element such

 that



 enumerate







 and



 If is any element such that and then





 enumerate

 It_turns out that the meet of and is their only greatest_lower

 bound

 To_see why let Observe that



 itemize



 because

 The proof involves

 simple uses of associativity commutativity and idempotence

 That is



 align

 gx ((xy)x)

 (x(yx))



 (x(xy))

 ((xx)y)



 (xy) g



 align



 by a similar argument



 Suppose is any element such that and

 We claim and therefore cannot be a glb of and

 unless it is also

 In proof



 Since we know so

 Since we know and therefore

 We have proven and conclude is the only glb of and





 itemize



 Joins Lub's and Lattices

 In symmetry to the glb operation on elements of a poset we may define

 the least upper_bound (or lub) of elements and to be

 that element such that and if is any

 element such that and then

 One can show that there is at most one such element if it exists



 In a true lattice there are two operations on domain elements

 the meet which we have_seen and the operator join

 denoted which gives the lub of two elements (which therefore

 must always exist in the lattice)

 We have_been discussing only semi lattices where only one of the

 meet and join operators exist

 That is our semilattices are meet semilattices

 One could also speak of join semilattices where only the join

 operator exists and in fact some literature on program analysis does

 use the notation of join semilattices

 Since the traditional data-flow literature speaks of meet semilattices we

 shall also do so in this_book



 Lattice Diagrams



 It often helps to draw the domain as a

 lattice_diagram

 which is a graph whose nodes are the elements of and whose

 edges are directed downward_from to if

 For_example Fig_figlattice shows the set for a reaching-definitions

 data-flow_schema where there are three definitions

 and

 Since is an edge is directed downward_from any

 subset of these three definitions to each of its supersets

 Since is transitive we conventionally omit the edge from

 to as_long as there is another path from to left in the diagram

 Thus although

 we do_not draw this edge since it is represented_by the path through

 for example



 figurehtfb

 figureuullmanalsuch9figslatticeeps

 Lattice of subsets of definitions

 figlattice

 figure



 It is also useful to note_that we can read the meet off such diagrams

 Since is the glb it is

 always the highest for which there are paths

 downward to from both and

 For_example if is and is then

 in Fig_figlattice is which makes_sense because

 the meet_operator is union

 The top_element will appear at the top of the

 lattice_diagram that is there is a path downward_from to

 each element

 Likewise the bottom_element will appear at the bottom with a path

 downward_from every element to



 Product Lattices



 While

 Fig_figlattice involves only three definitions

 the lattice_diagram of a typical program can

 be quite large

 The set of data-flow_values is the power set of

 the definitions which therefore

 contains elements if there are

 definitions in the program However whether a

 definition_reaches a program is independent of the reachability of the

 other definitions

 We may thus express the latticefootnoteIn this discussion and

 subsequently we_shall often drop the semi since lattices like the

 one under discussion do have a join or lub operator even if we do_not

 make use of itfootnote

 of definitions in terms of a product_lattice built from one simple

 lattice for each definition

 That is if there were only one definition in the program then the

 lattice would have two elements the empty_set which is the top

 element and which is the bottom_element



 Formally we may build product lattices as_follows

 Suppose and are (semi)lattices

 The product_lattice for these two lattices is defined as_follows



 enumerate



 The domain of the product_lattice is



 The meet for the product_lattice is defined as_follows

 If and are domain elements of the product_lattice

 then



 enumerate

 displayeqprod-lat-meet

 317ptstabularl





 tabular

 (eqprod-lat-meet)

 display



 It is simple to express the partial_order for the product_lattice

 in terms of the partial orders and for and

 displayeqprod-lat-poset

 317ptstabularl

 if and only if and



 tabular

 (eqprod-lat-poset)

 display

 To_see why (eqprod-lat-poset) follows from

 (eqprod-lat-meet) observe that



 (ab)(a'b')(aAa'bBb')



 So we might ask under

 what circumstances does

 That happens exactly when and

 But these two conditions are the same as and



 The product of lattices is an associative operation so one can show

 that the rules (eqprod-lat-meet) and (eqprod-lat-poset)

 extend to any number of

 lattices

 That is if we are given lattices for

 then the product of all lattices in this order has domain

 a meet_operator defined by



 center





 center

 and a partial_order defined by



 center

 if and only if

 for all

 center



 Height of a Semilattice



 We may learn something about the rate of convergence of a data-flow

 analysis algorithm by studying the height

 of the associated semilattice An ascending chain in a poset

 is a sequence where



 The height of a

 semilattice is the largest_number of relations in

 any ascending chain that is the height is one less_than the

 number of elements in the chain For_example the height of the reaching

 definitions semilattice for a program with definitions is



 Showing convergence of an iterative_data-flow algorithm is much

 easier if the semilattice has finite_height Clearly a lattice

 consisting of a finite set of values will have a finite_height it is

 also possible for a lattice with an_infinite number of values to have

 a finite_height The lattice used in the constant_propagation

 algorithm is one such example that we_shall examine closely in

 Section_const-prop-sect



 Transfer_Functions



 The family of transfer_functions in a data-flow_framework

 has the following properties



 enumerate



 has an identity_function such that for

 all in



 is closed_under composition that is for any two functions

 and in the function defined by

 is in



 enumerate



 ex

 In reaching_definitions

 has the identity the function where and are

 both the empty_set

 Closure under_composition was actually shown in

 Section secrd-df we repeat the argument succinctly here

 Suppose we have two functions



 center

 and



 center

 Then



 f2 (f1 (x)) G2 ((G1 (x-K1 ))-K2 )





 The right_side of the above is algebraically equivalent to



 (G2 (G1 -K2 )) (x-(K1 K2 ))



 If we let and



 then we have shown that the composition of and

 which is is of the form that makes it a member of

 If we consider available_expressions the same arguments

 used for reaching_definitions also show that

 has an identity and is closed_under composition

 ex



 Monotone Frameworks



 To make an iterative_algorithm for data-flow_analysis work we need

 for the data-flow_framework to satisfy one more condition

 We_say that a framework is monotone if when we apply

 any transfer_function

 in to two members of the first being no_greater than the

 second then the first result is no_greater than the second result



 Formally a data-flow_framework is monotone

 if



 displayeqmono1

 317ptstabularl

 tabular

 For all and in and in implies





 (eqmono1)

 display

 Equivalently monotonicity can be defined as



 displayeqmono2

 317ptstabularl

 For all and in and in





 tabular

 (eqmono2)

 display

 Equation (eqmono2)

 says_that if we take the meet of two values and then apply the result

 is never_greater than what is obtained_by applying to the values

 individually first and then meeting the results

 Because the two definitions of monotonicity seem so

 different

 they are both

 useful We_shall find one or the other more useful under different

 circumstances Later we sketch a proof to show that they are indeed

 equivalent



 We_shall first assume (eqmono1) and show that

 (eqmono2) holds

 Since is the greatest_lower bound of and we know

 that



 x_y x and x_y y



 Thus by (eqmono1)



 f(_x y_) f(_x ) and f(_x y_) f(_y )



 Since is the greatest_lower bound of and

 we have (eqmono2)



 Conversely let_us assume (eqmono2) and prove

 (eqmono1)

 We suppose and use (eqmono2) to conclude



 thus proving (eqmono1)

 Equation (eqmono2) tells_us



 f(_x y_) f(_x )_f( y_)



 But since is assumed by definition

 Thus (eqmono2) says



 f(_x )_f( x )_f( y_)



 Since is the glb of

 and we know

 Thus



 f(_x )_f( x )_f( y_) f(y)



 and (eqmono2) implies (eqmono1)



 Distributive Frameworks



 Often a framework obeys a condition stronger than (eqmono2)

 which we call the distributivity condition



 f(_x y_) f(_x )_f( y_)



 for all and in and in

 Certainly if then by idempotence

 so

 Thus distributivity implies monotonicity although the converse is not

 true



 ex

 Let and be sets

 of definitions

 in the reaching-definitions_framework

 Let be a function defined by

 for some sets of definitions and

 We can verify that the reaching-definitions_framework satisfies

 the distributivity condition by checking that



 G((y z)-K) (G(y-K))(G(z-K))



 While the equation above may appear formidable consider first those

 definitions in

 These definitions are surely in the sets defined by both the left and

 right_sides

 Thus we have only to consider definitions that are not in

 In that case we can eliminate everywhere and verify the equality



 (yz)-K(y-K)(z-K)



 The latter equality is easily checked using

 a Venn diagram

 ex



 The Iterative_Algorithm for General Frameworks

 df-iterative



 We can generalize

 Algorithm_algreaching-definitions to make it work for

 a large variety of data-flow_problems



 alg

 algiterative

 Iterative solution to general data-flow_frameworks



 A data-flow_framework with the following components

 enumerate



 A data-flow graph with specially labeled and

 nodes



 A direction of the data-flow



 A set of values



 A meet_operator



 A set of functions

 where in is the transfer_function for block

 and



 A constant value or in

 representing the boundary_condition for forward and backward

 frameworks respectively



 enumerate



 Values in for and for each block

 in the data-flow graph



 The algorithms for solving forward and backward_data-flow problems are

 shown in Fig_figdf-alg(a) and figdf-alg(b) respectively

 As with the familiar iterative_data-flow algorithms from

 Section_secdf-intro we compute

 and for each block by successive approximation

 alg



 figurehtfb



 center

 tabularr_l

 1)



 2) for_(each basic_block other_than entry)





 3)_while (changes to any_occur)



 4) for_(each basic_block other_than entry)



 5)



 6)







 tabular

 center



 center

 (a) Iterative_algorithm for a forward_data-flow problem

 center



 center

 tabularr_l

 1)



 2) for_(each basic_block other_than exit)





 3)_while (changes to any_occur)



 4) for_(each basic_block other_than exit)



 5)



 6)







 tabular

 center



 center

 (b) Iterative_algorithm for a backward_data-flow problem

 center



 Forward and backward versions of the iterative_algorithm

 figdf-alg

 figure



 It is possible to write the forward and backward versions of

 Algorithm_algiterative so that a function implementing

 the meet_operation is a parameter as is a function that implements the

 transfer_function for each block

 The flow_graph itself and the boundary value are also

 parameters

 In this way the compiler implementor can avoid recoding the basic

 iterative_algorithm for each data-flow_framework used by the

 optimization_phase of the compiler



 We can use the abstract framework discussed so_far to prove a number of

 useful properties of the iterative_algorithm



 enumerate



 If Algorithm_algiterative converges the result is a

 solution to the data-flow_equations



 If the framework is monotone then the solution found is the maximum

 fixedpoint (MFP) of the data-flow_equations

 A maximum_fixedpoint is a solution with the property that in any

 other solution the values of and are the

 corresponding values of the MFP



 If the semilattice of the framework is monotone and of finite_height then

 the algorithm is guaranteed to converge



 enumerate



 We_shall argue these points assuming that the framework is forward

 The case of backwards frameworks is essentially the same

 The first property is easy to show

 If the equations are not

 satisfied by the time the while-loop ends

 then there will be at_least one change to an (in the forward

 case) or (in the backward case) and we must go_around the loop

 again



 To prove the second property we first show that the values taken on

 by and for any can only decrease (in the

 sense of the relationship for lattices) as the algorithm

 iterates This claim can be proven by induction



 The base case is to show

 that the value of and after the first iteration is

 not greater_than the initialized value This statement is trivial because

 and for all blocks

 are initialized_with



 Assume that after the th

 iteration the values are all no_greater than those after the st

 iteration

 and show the same for iteration compared with iteration

 Line (5) of Fig_figdf-alg(a) has



 B_P a predecessor of B_P



 Let_us use the notation

 and to

 denote the values of and after iteration

 Assuming

 we know that

 because of the properties of the meet_operator

 Next line_(6) says



 B_fB(B)



 Since

 we have



 by monotonicity



 Note_that every change observed for values of and is

 necessary to satisfy the equation The meet operators return the

 greatest_lower bound of their inputs and the transfer_functions return

 the only solution that is consistent_with the block itself and

 its given input Thus if the iterative

 algorithm_terminates the result must have values that are at_least as

 great as the corresponding values in any other solution that is the

 result of Algorithm_algiterative is the MFP of the equations



 Finally consider the third point

 where the data-flow_framework has finite_height

 Since the values of every and decrease

 with each change and the algorithm stops if at some round nothing changes

 the algorithm is guaranteed to converge after a number of rounds no

 greater_than the product of the height of the framework and the number

 of nodes of the flow_graph



 Meaning of a Data-Flow Solution

 df-semantics-subsect



 We_now know that the solution found using the iterative_algorithm is

 the maximum_fixedpoint but what does the result represent from a

 program-semantics point of view

 To understand the solution of a data-flow_framework

 let_us first describe what an ideal_solution to the

 framework would be We show that the ideal cannot be obtained in

 general but that Algorithm_algiterative

 approximates the

 ideal conservatively



 The Ideal Solution



 Without loss of generality we_shall assume for now that the data-flow

 framework of interest is a forward-flowing problem Consider the

 entry point of a basic_block The ideal_solution begins by finding

 all

 the possible_execution paths_leading from the program entry to

 the beginning of A path is possible only if there is some

 computation of the program that follows exactly that path

 The ideal_solution would then compute the data-flow value at

 the end of each possible

 path and apply the meet_operator to these values

 to find their greatest_lower bound Then

 no execution of the program can produce a

 smaller value for that program point In_addition the bound is

 tight there is no_greater data-flow value that is a glb for the value

 computed along every possible path to in the flow_graph



 We_now try to define the ideal_solution more formally

 For each block in a flow_graph

 let be the transfer_function for

 Consider any path



 P entry B1B2Bk-1 Bk



 from the initial_node to some block The

 program path may have cycles so one basic_block may appear several

 times on the path Define the

 transfer_function for



 to be the composition of

 Note_that is not part of the composition reflecting

 the fact that this path is taken to reach the beginning of block

 not its end The data-flow value created by executing this

 path is thus where is the

 result of the constant transfer_function representing the initial_node

 The ideal result for block is thus



 B_P a possible path from

 entry to B fP (ventry)



 We claim that in terms of the lattice-theoretic partial_order

 for the framework in question



 itemize



 Any answer that is greater_than is incorrect



 Any value smaller_than or equal to the ideal is conservative ie

 safe



 itemize

 Intuitively the closer the

 value to the ideal the more_precise it isfootnoteNote that in

 forward problems the value is what we

 would like

 to be

 In backward problems which we do_not discuss here we would define

 to be the ideal value of

 footnote

 To_see why solutions must_be the ideal_solution note_that

 any solution greater_than for any block could be obtained_by

 ignoring some execution_path that the program could take and we cannot

 be_sure that there is not some effect along that path to invalidate any

 program improvement we might make based_on the greater solution

 Conversely any solution less_than can be_viewed as including

 certain paths that either do_not exist in the flow_graph or that exist

 but that the program can never follow

 This lesser solution will allow only transformations that are correct

 for all possible_executions of the program but may forbid some

 transformations that would permit



 The Meet-Over-Paths Solution



 However as discussed in Section_secopt-sources finding all

 possible_execution paths is undecidable We must therefore

 approximate In the data-flow_abstraction we assume that

 every_path in the flow_graph can be taken

 Thus we can define the

 meet-over-paths solution for to be



 B_P a path from

 entry to B fP (ventry)



 Note_that as for the solution gives values for

 in forward-flow frameworks

 If we were to consider backward-flow frameworks then we would think of

 as

 a value for



 The paths considered in the solution are a superset of all the

 paths that are possibly executed Thus the solution meets

 together not only the data-flow_values of all the executable

 paths but also additional values associated_with the paths that

 cannot possibly be executed Taking the meet of the ideal_solution plus

 additional terms cannot create a solution larger_than the ideal

 Thus for all we have and we

 will simply say that



 The Maximum Fixedpoint Versus the MOP Solution



 Notice_that in the solution

 the number of paths considered is still unbounded

 if the flow_graph contains cycles Thus the definition does_not lend

 itself to a direct algorithm

 The iterative_algorithm certainly

 does_not first find all the paths

 leading to a basic_block before applying the meet_operator

 Rather



 enumerate



 The iterative_algorithm visits basic_blocks not_necessarily

 in the order of

 execution



 At each confluence point

 the algorithm applies the meet_operator

 to the data-flow_values obtained so_far Some of these values

 used were_introduced artificially in the initialization process not

 representing the result of any execution from the beginning of the

 program



 enumerate

 So what is the relationship_between the solution and the

 solution produced_by Algorithm_algiterative



 We first discuss the order in which the nodes are visited In

 an iteration we

 may visit a basic_block before having visited its

 predecessors If the predecessor is the node

 would have_already been initialized_with the

 proper constant value Otherwise it has_been initialized to

 a value no smaller_than the final answer By monotonicity the result

 obtained_by using as input is no smaller_than the desired

 solution In a sense we can think of as representing no

 information



 figurehtb



 figureuullmanalsuch9figsmop-fgeps



 Flow_graph illustrating the effect of early meet_over paths

 mop-fg-fig



 figure



 What is the effect of applying the meet_operator early

 Consider the simple example of Fig mop-fg-fig and

 suppose we are_interested in the value of

 By the definition of



 B4 ((fB3 fB1) (fB3 fB2))(ventry)



 In the iterative_algorithm if we visit the nodes in the order

 then



 B4 fB3 ((fB1(ventry)fB2(ventry)))



 While the meet_operator is applied at the end in the definition

 of the iterative

 algorithm applies it early The answer is the same only if the data-flow

 framework is distributive If the data-flow_framework is

 monotone but not_distributive we still have

 Recall that in general a solution is safe (conservative)

 if for all blocks Surely





 We_now provide a quick sketch of why in general the solution provided

 by the iterative_algorithm is

 always safe An easy induction on shows that

 the values obtained after iterations

 are smaller_than or equal to the meet_over all paths of length or less

 But the iterative_algorithm terminates only if it arrives at the

 same answer as would be obtained_by iterating an_unbounded number of times

 Thus the

 result is no_greater than the solution

 Since and we know that

 and therefore the solution provided by the iterative_algorithm is

 safe



 exer

 Construct a lattice_diagram for the product of three lattices

 each based_on a single definition for

 How is your lattice_diagram related to that in Fig_figlattice

 exer



 hsexer

 In Section df-iterative we argued that if the framework has finite

 height then the iterative_algorithm converges Here is an example

 where the framework does_not have finite

 height and the iterative_algorithm does_not converge

 Let the set of values be the nonnegative real numbers and let the

 meet_operator be the minimum There_are three transfer_functions



 itemize



 The identity



 half that is the function



 one that is the function

 itemize

 The set of transfer_functions is these three plus the functions

 formed_by composing them in all possible ways



 itemize



 a)

 Describe the set



 b)

 What is the relationship for this framework



 c)

 Give an example of a flow_graph with assigned transfer_functions such

 that Algorithm_algiterative does_not converge



 d)

 Is this framework monotone Is it distributive



 itemize

 hsexer



 hexer

 We argued that Algorithm_algiterative converges if the framework

 is monotone and of finite_height Here is an example of a framework

 that shows monotonicity is essential finite_height is not enough

 The domain is

 the meet_operator is min and

 the set of functions is only the identity () and the switch

 function () that swaps 1 and 2



 itemize



 a)

 Show that this framework is of finite_height but not monotone



 b)

 Give an example of a flow_graph and assignment of transfer_functions so

 that Algorithm_algiterative does_not converge



 itemize

 hexer



 hexer

 Let be the meet_over all paths of length or less from

 the entry to block Prove that after iterations of

 Algorithm_algiterative Also show that

 as a consequence if Algorithm_algiterative converges then it

 converges to something that is the solution

 hexer



 hexer

 Suppose the set of functions for a framework are all of gen-kill

 form

 That is the domain is the power set of some set and

 for some sets and

 Prove that if the meet_operator is either (a) union or (b) intersection

 then the framework is distributive

 hexer

 Dynamic Programming Code-Generation

 dyn-prog-sect



 Algorithm labeled-tree-lim-alg in Section ershov-sect

 produces optimal_code from an expression tree using an amount of

 time that is a linear function of the size of the tree This

 procedure works for machines in which all computation is done in

 registers and in which instructions consist of an operator applied

 to two registers or to a register and a memory_location



 An algorithm based_on the principle of dynamic_programming can be

 used to extend the class of machines for which optimal_code can be

 generated from expression trees in linear time The dynamic

 programming algorithm applies to a broad class of register

 machines with complex instruction sets



 The dynamic_programming algorithm can be used to generate code for

 any machine with interchangeable registers

 and load store and operation

 instructions For_simplicity we assume every instruction costs

 one_unit although the dynamic_programming algorithm can easily be

 modified to work even if each instruction has its_own cost



 Contiguous Evaluation



 The dynamic_programming algorithm partitions the problem of

 generating optimal_code for an expression into the subproblems of

 generating optimal_code for the subexpressions of the given

 expression As a simple example consider an expression of the

 form An optimal program for is formed_by combining

 optimal programs for and in one or the other order

 followed_by code to evaluate the operator The subproblems of

 generating optimal_code for and are solved similarly



 An optimal program produced_by the dynamic_programming algorithm

 has an important_property It evaluates an expression

 contiguously We can appreciate what

 this means by_looking at the syntax_tree for





 Here and are trees for and respectively



 We_say a program evaluates a tree contiguously if it

 first evaluates those subtrees of that need to be computed

 into memory Then it evaluates the remainder of either in the

 order and then the root or in the order

 and then the root in either case using the previously

 computed values from memory whenever necessary As an example of

 noncontiguous evaluation might first evaluate part of

 leaving the value in a register (instead of memory) next evaluate

 and then return to evaluate the rest of



 For the register machine in this_section we can prove that given

 any machine-language program to evaluate an expression tree

 we can find an equivalent program such that



 enumerate



 is of no higher cost than



 uses no more registers than and



 evaluates the tree contiguously

 enumerate



 This result implies that every expression tree can be evaluated

 optimally by a contiguous program



 By way of contrast machines with even-odd register pairs do_not

 always have optimal contiguous evaluations the x86 architecture

 uses register pairs for multiplication and division For such

 machines we can give examples of expression trees in which an

 optimal machine language program must first evaluate into a

 register a portion of the left subtree of the root then a portion

 of the right subtree then another part of the left subtree then

 another part of the right and so on This type of oscillation is

 unnecessary for an optimal evaluation of any expression tree using

 the machine in this_section



 The contiguous evaluation property defined above ensures that for

 any expression tree there always exists an optimal program

 that consists of optimal programs for subtrees of the root

 followed_by an instruction to evaluate the root This property

 allows_us to use a dynamic_programming algorithm to generate an

 optimal program for



 The Dynamic Programming Algorithm



 The dynamic_programming algorithm proceeds in three phases

 (suppose the target_machine has registers)



 enumerate



 Compute bottom-up for each node of the expression tree

 an array of costs in which the th component is

 the optimal cost of computing the subtree rooted_at into a

 register assuming registers are available for the

 computation for



 Traverse using the cost_vectors to determine which

 subtrees of must_be computed into memory



 Traverse each tree using the cost_vectors and associated

 instructions to generate the final target code The code for the

 subtrees computed into memory_locations is generated first

 enumerate

 Each of these phases can be_implemented to run in time linearly

 proportional to the size of the expression tree



 The cost of computing a node includes whatever loads and

 stores are necessary to evaluate in the given number of

 registers It also includes the cost of computing the operator at

 the root of The zeroth component of the cost_vector is the

 optimal cost of computing the subtree into memory The

 contiguous evaluation property ensures that an optimal program for

 can be generated_by considering combinations of optimal

 programs only for the subtrees of the root of This

 restriction reduces the number of cases that need to be

 considered



 In order to compute the costs at node we view the

 instructions as tree-rewriting_rules as in

 Section tile-sect Consider each template that matches

 the input tree at node By examining the cost_vectors at the

 corresponding descendants of determine the costs of

 evaluating the operands at the leaves of For those operands

 of that are registers consider all possible orders in which

 the corresponding subtrees of can be evaluated into registers

 In each ordering the first subtree corresponding to a register

 operand can be evaluated using available registers the second

 using registers and so on To account for node add in

 the cost of the instruction associated_with the template The

 value is then the minimum_cost over all possible orders



 The cost_vectors for the entire tree can be computed bottom up

 in time linearly proportional to the number of nodes in It is

 convenient to store at each node the instruction used to achieve

 the best cost for for each value of The smallest cost

 in the vector for the root of gives the minimum_cost of

 evaluating



 exdyn-prog-ex

 Consider a machine having two registers R0 and R1 and the following instructions each of unit

 cost

 center

 tabularl_l l

 LD R_M R_M



 op R_R R_R R op R



 op R_R M R_R op M



 LD R_R R_R



 ST M R_M R



 tabular

 center



 In these instructions R is either R0 or

 R1 and M is a memory_location The operator op represents any arithmetic operator



 Let_us apply the dynamic_programming algorithm to generate_optimal

 code for the syntax_tree in Fig dyn-costs-fig In the first

 phase we compute the cost_vectors shown at each node To

 illustrate this cost computation consider the cost_vector at the

 leaf a the cost of computing a into memory

 is 0 since it is already there the cost of computing a into a register is 1 since we can load it into a register with

 the instruction_LD R0a the cost of loading a into a register with two registers available is the same as

 that with one register available The cost_vector at leaf a

 is therefore



 figurehtfb



 Syntax tree for (a-b)c(de) with cost_vector at

 each node dyn-costs-fig

 figure



 Consider the cost_vector at the root We first determine the

 minimum_cost of computing the root with one and two registers

 available The machine_instruction ADD R0R0M matches

 the root because the root is labeled with the operator Using

 this instruction the minimum_cost of evaluating the root with one

 register available is the minimum_cost of computing its right

 subtree into memory plus the minimum_cost of computing its left

 subtree into the register plus 1 for the instruction No other

 way exists The cost_vectors at the right and left children of the

 root show that the minimum_cost of computing the root with one

 register available is



 Now_consider the minimum_cost of evaluating the root with two

 registers available Three cases arise depending_on which

 instruction is used to compute the root and in what order the left

 and right subtrees of the root are evaluated



 enumerate



 Compute the left subtree with two registers available into

 register R0 compute the right subtree with one register

 available into register_R1 and use the instruction ADD R0R0R1 to compute the root This sequence has cost





 Compute the right subtree with two registers available into

 R1 compute the left subtree with one register available

 into R0 and use the instruction ADD R0R0R1

 This sequence has cost



 Compute the right subtree into memory_location M

 compute the left subtree with two registers available into

 register R0 and use the instruction ADD R0R0M

 This sequence has cost

 enumerate



 The second choice gives the minimum_cost 7



 The minimum_cost of computing the root into memory is determined

 by_adding one to the minimum_cost of computing the root with all

 registers available that is we compute the root into a register

 and then store the result The cost_vector at the root is

 therefore



 From the cost_vectors we can easily construct the code sequence by

 making a traversal of the tree From the tree in

 Fig dyn-costs-fig assuming two registers are available an

 optimal_code sequence is



 center

 tabularl_l

 LD_R0 c R0 c



 LD_R1 d R1 d



 DIV R1_R1 e R1_R1 e



 MUL R0_R0 R1 R0_R0 R1



 LD_R1 a R1 a



 SUB R1_R1 b R1_R1 - b



 ADD R1_R1 R0_R1 R1 R0



 tabular

 center

 ex



 Dynamic programming techniques have_been used in a number of

 compilers including the second version of the portable C

 compiler PCC2 The technique facilitates retargeting because of

 the applicability of the dynamic_programming technique to a broad

 class of machines



 exer

 Augment the tree-rewriting scheme in Fig_tile-rules-fig

 with costs and use dynamic_programming and tree matching

 to generate code for the statements in

 Exercise tile-exer

 exer



 vhexer

 How_would you extend dynamic_programming to do optimal

 code_generation on 's

 vhexer



 Emphasis of the Text



 The main emphasis of this text is to teach the methodology and

 fundamental ideas used in compiler design There_are many beautiful

 frameworks and abstractions on which many compiler algorithms are

 based They are excellent illustrations of how abstractions can be

 used to solve problems how to take a problem formulate a

 mathematical abstraction that captures the key characteristics and

 solve them using mathematic techniques



 It is not the intention of this text to teach all the algorithms and

 techniques necessary for building a state-of-the-art compiler

 However readers of this text should have the basic knowledge and

 understanding to learn how to build a compiler relatively easily



 The Evolution of Programming_Languages



 The first electronic computers appeared in the 1940's and

 were programmed in machine language by sequences of 0's and 1's

 that explicitly told the computer what operations to execute

 and in what order

 The operations themselves were very low level

 move data from one location to another

 add the contents of two registers

 compare two values and so on

 Needless to say this kind of programming was slow tedious

 and error prone And once written the programs were hard

 to understand and modify



 The Move to Higher-level Languages



 The first step towards more people-friendly programming_languages

 was the development of mnemonic assembly languages in the early

 1950's Initially the instructions in an assembly language were

 just mnemonic representations of machine_instructions Later

 macro instructions were added to assembly languages so that a

 programmer could define parameterized shorthands for frequently

 used sequences of machine_instructions



 A major step towards higher-level languages was made in the latter

 half of the 1950's with the development of Fortran for scientific

 computation Cobol for business data processing and Lisp for

 symbolic computation The philosophy behind these languages was to

 create higher-level notations with which programmers could more

 easily write numerical computations business applications and

 symbolic programs These languages were so successful that they

 are still in use today



 In the following decades many more languages were created with

 innovative features to help make programming easier more natural

 and more robust Later in this_chapter we_shall discuss some key

 features that are common to many modern programming_languages



 Today there are thousands of programming_languages

 They can be classified in a variety of ways

 One classification is by generation

 First-generation languages are the machine languages

 second-generation the assembly languages

 and third-generation the higher-level languages_like Fortran

 Cobol Lisp C C C and Java

 Fourth-generation languages are languages designed for

 specific applications like NOMAD for report generation

 SQL for database queries and Postscript for text formatting

 The term fifth-generation language has_been applied

 to logic- and constraint-based languages_like Prolog

 and OPS5



 Another classification of languages uses the term

 imperative for languages in which a program specifies

 how a computation is to be done and

 declarative for languages in which a program specifies

 what computation is to be done

 Languages such_as C C C and Java

 are imperative languages

 In imperative languages there is a notion of program state

 and statements that change the state

 Functional languages such_as ML and Haskell

 and constraint logic languages such_as Prolog are often considered

 to be declarative languages



 The term von Neumann language is applied to

 programming_languages whose computational model is

 based_on the von Neumann computer

 architecture Many of today's languages such_as

 Fortran and C are von Neumann languages



 An object-oriented language is one that supports

 object-oriented_programming a programming style in which a

 program consists of a collection of objects that interact with one

 another Simula 67 and Smalltalk are the earliest major

 object-oriented languages Languages such_as C C Java and

 Ruby are more_recent object-oriented languages



 Scripting languages are interpreted

 languages with high-level operators designed

 for gluing together computations

 These computations were_originally called scripts

 Awk JavaScript Perl PHP Python Ruby and Tcl

 are popular examples of scripting languages

 Programs written in scripting languages are often

 much shorter than equivalent programs_written in

 languages_like C



 Impacts on Compilers



 Since the design of programming_languages and compilers

 are intimately related

 the advances in programming_languages placed new demands

 on compiler writers They had to devise algorithms and

 representations to translate and support the new language features

 Since the 1940's computer architecture has evolved as_well

 Not_only did the compiler writers have to track new language

 features they also had to devise translation algorithms that

 would take maximal advantage of the new hardware capabilities



 Compilers can help promote the use of high-level languages

 by minimizing the execution overhead

 of the programs_written in these languages Compilers

 are also critical in making

 high-performance computer architectures effective on users'

 applications In_fact the performance of a computer system is so

 dependent on compiler technology that compilers are used as a tool in

 evaluating architectural concepts before a computer is built



 Compiler writing is challenging A compiler by itself is a large

 program Moreover many modern language-processing systems handle

 several source languages and target machines within the same

 framework that is they serve as collections of compilers

 possibly consisting of millions of lines of code Consequently

 good software-engineering techniques are essential for creating

 and evolving modern language processors



 A compiler must translate correctly

 the potentially infinite set of programs that could be written in the

 source_language

 The problem of generating the optimal target code from a source

 program is undecidable in general thus compiler writers must

 evaluate tradeoffs about what problems to tackle and

 what heuristics to use to approach the problem of generating

 efficient code



 A study of compilers is also a study of how theory meets practice

 as we_shall see in Section science-sect



 The purpose of this text is to teach the methodology and

 fundamental ideas used in compiler design It is not the intention

 of this text to teach all the algorithms and techniques that could

 be used for building a state-of-the-art language-processing

 system However readers of this text will acquire the basic

 knowledge and understanding to learn how to build a compiler

 relatively easily



 sexer

 Indicate which of the following terms

 center

 tabularr_l r_l r_l

 a) imperative b) declarative c) von Neumann



 d) object-oriented e) functional f) third-generation



 g) fourth-generation h) scripting



 tabular

 center

 apply to which of the

 following languages

 center

 tabularr_l r_l r_l r_l r_l

 1) C 2) C 3) Cobol 4) Fortran 5) Java



 6) Lisp 7) ML 8) Perl 9) Python 10) VB

 tabular

 center

 sexer

 Optimal Code_Generation for Expressions

 ershov-sect



 We can choose registers optimally when a basic_block consists of a

 single expression evaluation or if we accept that it is

 sufficient to generate code for a block one expression at a time

 In the following algorithm we introduce a numbering_scheme for

 the nodes of an expression tree

 (a syntax_tree for an expression) that allows_us to generate

 optimal_code for an expression tree

 when there is a fixed_number of registers with which

 to evaluate the expression



 Ershov Numbers



 We begin by assigning to each node of an expression tree a number that

 tells how many registers are needed to evaluate that node without

 storing any temporaries These numbers are sometimes_called Ershov

 numbers after A Ershov who used a similar scheme for machines with a

 single arithmetic register For our machine_model the rules are



 enumerate



 leaf-lab-item

 Label all leaves 1



 The label of an interior_node with one child is the label of its child



 The label of an interior_node with two children is

 enumerate

 uneq-lab-item

 The larger of the labels of its_children if those labels are different

 eq-lab-item

 One plus the label of its_children if the labels are the same

 enumerate



 enumerate



 ex

 In Fig labeled-tree-fig we see an expression tree (with operators

 omitted) that might be the tree for expression

 or the three-address_code



 verbatim

 t1 a - b

 t2 c_d

 t3 e t2

 t4 t1 t3

 verbatim

 Each of the five leaves is labeled 1 by

 rule (leaf-lab-item) Then we can label the interior_node

 for t1a-b since both of its_children are labeled

 Rule (eq-lab-item) applies so it gets label one more_than

 the labels of its_children that is 2 The same holds for the

 interior_node for t2cd



 figurehtfb



 A tree

 labeled with Ershov numbers labeled-tree-fig

 figure



 Now we can work on the node for t3et2 Its

 children have labels 1 and 2 so the label of the node for t3 is the maximum 2 by rule (uneq-lab-item) Finally the

 root the node for t4t1t3 has two children with

 label 2 and therefore it gets label 3

 ex



 Generating Code From Labeled Expression Trees



 It can be proved that in our machine_model where all operands must_be

 in registers and registers can be used by both an operand and the

 result of an operation the label of a node is the fewest registers with

 which the expression can be evaluated using no stores of temporary

 results Since in this model we are forced to load each operand and

 we are forced to compute the result corresponding to each interior_node

 the only thing that can make the generated code inferior to the optimal

 code is if there are unnecessary stores of temporaries

 The argument for this claim is embedded in the following algorithm for

 generating code with no stores of temporaries using a number of

 registers equal to the label of the root



 alg

 labeled-tree-alg

 Generating code from a labeled expression tree



 A labeled tree with each operand appearing once (that is no

 common subexpressions)



 An optimal sequence of machine_instructions to evaluate the root into a

 register



 The following is a recursive algorithm to generate the machine code

 The steps below are applied starting_at the root of the tree

 If the algorithm is applied to a node with label then only

 registers will be used However there is a base for the

 registers used so that the actual registers used are

 The result always appears in



 enumerate



 To generate machine code for an interior_node with label and two

 children with equal labels (which must_be ) do the following

 enumerate

 Recursively generate code for the right_child using base

 The result of the right_child appears in register

 Recursively generate code for the left_child using base the result

 appears in

 Generate the instruction where

 OP is the appropriate operation for the interior_node in question

 enumerate



 Suppose we have an interior_node with label and

 children with unequal labels Then one of the children which we'll

 call the big_child has label and the other child the

 little_child has some label Do the following to generate

 code for this interior_node using base

 enumerate

 Recursively generate code for the big_child using base the result

 appears in register

 Recursively generate code for the little_child using base the

 result appears in register Note_that since neither

 nor any higher-numbered register is used

 Generate the instruction

 or the instruction depending

 on whether the big_child is the right or left_child respectively

 enumerate



 For a leaf representing operand

 if the base is generate the instruction



 enumerate

 alg



 ex

 Let_us apply_Algorithm labeled-tree-alg to the tree of

 Fig labeled-tree-fig Since the label of the root is 3 the

 result will appear in and only and will be

 used The base for the root is

 Since the root has children of equal labels we generate code for the

 right_child first with base 2



 When we generate code for the right_child of the root labeled

 t3 we find the big_child is the right_child and the little

 child is the left_child We thus generate code for the right

 child first with Applying the rules for equal-labeled

 children and leaves we generate the following code for the node

 labeled t2



 verbatim

 LD R3 d

 LD_R2 c

 ADD_R3 R2 R3

 verbatim

 Next we generate code for the left_child of the right_child of the

 root this node is the leaf_labeled Since the proper

 instruction is



 verbatim

 LD_R2 e

 verbatim

 Now we can complete the code for the right_child of the root by_adding

 the instruction



 verbatim

 MUL R3 R2 R3

 verbatim

 The algorithm proceeds to generate code for the left_child of the root

 leaving the result in and with base 1 The complete sequence of

 instructions is shown in Fig labeled-tree-code-fig

 ex



 figurehtfb



 center

 tabularl

 'LD R3 d'



 'LD R2 c'



 'ADD R3 R2 R3'



 'LD R2 e'



 'MUL R3 R2 R3'



 'LD R2 b'



 'LD_R1 a'



 'SUB R2_R1 R2'



 'ADD R3 R2 R3'

 tabular

 center

 Optimal three-register code for the tree of

 Fig labeled-tree-fig labeled-tree-code-fig



 figure



 Evaluating Expressions with an Insufficient Supply of Registers



 When there are fewer registers available than the label of the root of

 the tree we cannot apply_Algorithm labeled-tree-alg directly We

 need to introduce some store_instructions that spill values of subtrees

 into memory and we then need to load those values back into registers

 as needed Here is the modified algorithm that takes into_account a

 limitation on the number of registers



 alg

 labeled-tree-lim-alg

 Generating code from a labeled expression tree



 A labeled tree with each operand appearing once (ie no common

 subexpressions) and a number of registers



 An optimal sequence of machine_instructions to evaluate the root into a

 register using no more_than registers which we assume are





 Apply the following recursive algorithm starting_at the root of the tree

 with base

 For a node with label or less the algorithm is exactly the same as

 Algorithm labeled-tree-alg and we_shall not repeat those steps

 here However for interior_nodes with a label we need to work

 on each side of the tree separately and store the result of the larger

 subtree That result is brought back from memory just_before node

 is evaluated and the final_step will take place in registers

 and The modifications to the basic algorithm are as_follows



 enumerate



 Node has at_least one child with label or greater Pick the

 larger child (or either if their labels are the same) to be the big

 child and let the other child be the little_child



 Recursively generate code for the big_child using base The

 result of this evaluation will appear in register



 Generate the machine_instruction where is a

 temporary_variable used for temporary results used to help evaluate nodes

 with label



 Generate_code for the little_child as_follows

 If the little_child has label or greater pick base If the

 label of the little_child is then pick

 Then recursively apply this algorithm to

 the little_child the result appears in





 Generate the instruction



 If the big_child is the right_child of then generate the

 instruction



 If the big_child is the left_child generate





 enumerate

 alg



 ex

 Let_us revisit the expression represented_by

 Fig labeled-tree-fig but now assume that that is

 only registers R1 and R2 are available to hold

 temporaries used in the evaluation of expressions When we apply

 Algorithm labeled-tree-lim-alg to

 Fig labeled-tree-fig we see that the root with label 3

 has a label that is larger_than Thus we need to identify

 one of the children as the big_child Since they have equal

 labels either would do Suppose we pick the right_child as the

 big_child



 Since the label of the big_child of the root is 2 there are

 enough registers We thus apply_Algorithm labeled-tree-alg

 to this subtree with and two registers The result looks

 very much like the code we generated in

 Fig labeled-tree-code-fig but with registers R1 and

 R2 in place of R2 and R3 This code is



 verbatim

 LD_R2 d

 LD_R1 c

 ADD R2_R1 R2

 LD_R1 e

 MUL R2_R1 R2

 verbatim

 Now since we need both registers for the left_child of the root we

 need to generate the instruction



 verbatim

 ST t3 R2

 verbatim

 Next the left_child of the root is handled Again the number of

 registers is sufficient for this child and the code is



 verbatim

 LD_R2 b

 LD_R1 a

 SUB R2_R1 R2

 verbatim

 Finally we reload the temporary that holds the right_child of the root

 with the instruction



 verbatim

 LD_R1 t3

 verbatim

 and execute the operation at the root of the tree with the instruction



 verbatim

 ADD R2 R2_R1

 verbatim

 The complete sequence of instructions is shown in

 Fig labeled-tree-lim-code-fig

 ex



 figurehtfb



 center

 tabularl

 'LD R2 d'



 'LD_R1 c'



 'ADD R2_R1 R2'



 'LD_R1 e'



 'MUL R2_R1 R2'



 'ST t3 R2'



 'LD R2 b'



 'LD_R1 a'



 'SUB R2_R1 R2'



 'LD_R1 t3'



 'ADD R2 R2 R1'

 tabular

 center



 Optimal three-register code for the tree of

 Fig labeled-tree-fig using only two registers

 labeled-tree-lim-code-fig



 figure



 exer

 ershov-exer

 Compute Ershov numbers for the following expressions



 itemize



 a)

 b)

 c)



 itemize

 exer



 exer

 Generate optimal_code using two registers for each of the expressions of

 Exercise ershov-exer

 exer



 exer

 Generate optimal_code using three registers for each of the expressions of

 Exercise ershov-exer

 exer



 hsexer

 Generalize the computation of Ershov numbers to expression trees with

 interior_nodes with three or_more children

 hsexer



 hexer

 An assignment to an array_element such_as ai x appears to be

 an operator with three operands and

 How_would you modify the tree-labeling scheme to generate_optimal code

 for this machine_model

 hexer



 hexer

 The original Ershov numbers were used for a machine that allowed the

 right operand of an expression to be in memory rather_than a register

 How_would you modify the tree-labeling scheme to generate_optimal code

 for this machine_model

 hexer



 hexer

 Some machines require two registers for certain single-precision values

 Suppose that the result of a multiplication of single-register

 quantities requires two consecutive registers and when we divide

 the value of must_be held in two consecutive registers

 How_would you modify the tree-labeling scheme to generate_optimal code

 for this machine_model

 hexer

 Evaluation Orders for SDD's

 eval-sdd-sect



 Dependency graphs are a useful tool for determining an

 evaluation order for the attribute instances in a given parse

 tree While an annotated_parse tree shows the values of

 attributes a dependency_graph helps us determine how those values

 can be computed



 In this_section in addition to dependency_graphs we define two

 important classes of SDD's the S-attributed and the more

 general L-attributed SDD's The translations specified_by

 these two classes fit well with the parsing methods we have

 studied and most translations encountered in practice can be

 written to conform to the requirements of at_least one of these

 classes



 Dependency Graphs

 dep-gr-subsect



 A dependency_graph depicts the flow of information among the

 attribute instances in a particular parse_tree an edge from one

 attribute instance to another means that the value of the first is

 needed to compute the second Edges express constraints implied by

 the semantic_rules In more_detail



 itemize



 For each parse-tree_node say a node_labeled by grammar

 symbol the dependency_graph has a node for each attribute

 associated_with



 Suppose that a semantic_rule associated_with a production

 defines the value of synthesized_attribute in terms of

 the value of (the rule may define in terms of other

 attributes in addition to ) Then the dependency_graph has

 an edge from to More_precisely at every node

 labeled where production is applied create an edge to

 attribute at from the attribute at the child of

 corresponding to this instance of the symbol in the body of

 the productionfootnoteSince a node can have several

 children_labeled we again assume that subscripts distinguish

 among uses of the same symbol at different places in the

 productionfootnote



 Suppose that a semantic_rule associated_with a production

 defines the value of inherited_attribute in terms of the

 value of Then the dependency_graph has an edge from

 to For each node_labeled that corresponds to an

 occurrence of this in the body of production create an

 edge to attribute at from the attribute at the node

 that corresponds to this occurrence of Note_that

 could be either the parent or a sibling of



 itemize



 ex

 dg1-ex Consider the following production and rule



 center

 tabularl_l

 Production_Semantic Rule







 tabular

 center

 At every node_labeled with children corresponding to the

 body of this production the synthesized_attribute at

 is computed using the values of at the two children

 labeled and Thus a portion of the dependency_graph for

 every parse_tree in which this production is used looks like

 Fig dg1-fig As a convention we_shall show the parse_tree

 edges as dotted lines while the edges of the dependency_graph are

 solid

 ex



 figurehtfb



 is synthesized from and dg1-fig

 figure



 ex

 dg-term-ex An_example of a complete dependency_graph

 appears in Fig dg-term-fig The nodes of the dependency

 graph represented_by the numbers 1 through 9 correspond to the

 attributes in the annotated_parse tree in

 Fig ann-ll-term-fig



 figurehtfb



 Dependency graph for the annotated_parse tree of

 Fig ann-ll-term-fig dg-term-fig

 figure



 Nodes 1 and 2 represent the attribute lexval associated_with

 the two leaves labeled digit Nodes 3 and 4 represent the

 attribute val associated_with the two nodes labeled The

 edges to node 3 from 1 and to node 4 from 2 result from the

 semantic_rule that defines in terms of

 In_fact equals

 but the edge represents dependence

 not equality



 Nodes 5 and 6 represent the inherited_attribute

 associated_with each of the occurrences of nonterminal The

 edge to 5 from 3 is due to the rule which

 defines at the right_child of the root from

 at the left_child We see edges to 6 from node 5 for

 and from node 4 for because these values are

 multiplied to evaluate the attribute at node 6



 Nodes 7 and 8 represent the synthesized_attribute

 associated_with the occurrences of The edge to node 7 from 6

 is due to the semantic_rule

 associated_with production 3 in Fig sdd-ll-term-fig The

 edge to node 8 from 7 is due to a semantic_rule associated_with

 production 2



 Finally node 9 represents the attribute The edge to 9

 from 8 is due to the semantic_rule

 associated_with production 1

 ex



 Ordering the Evaluation of Attributes

 attr-order-subsect



 The dependency_graph characterizes the possible orders in which we

 can evaluate the attributes at the various nodes of a parse_tree

 If the dependency_graph has an edge from node to node

 then the attribute corresponding to must_be evaluated before

 the attribute of Thus the only allowable orders of

 evaluation are those sequences of nodes such

 that if there is an edge of the dependency_graph from to

 then Such an ordering embeds a directed graph into a

 linear order and is called a topological_sort of the graph







 If there is any cycle in the graph then there are no topological

 sorts that is there is no way to evaluate the SDD on this

 parse_tree If there are no cycles however then there is always

 at_least one topological_sort To_see why since there are no

 cycles we can surely find a node with no edge entering For if

 there were no such node we could proceed from predecessor to

 predecessor until we came back to some node we had already seen

 yielding a cycle Make this node the first in the topological

 order remove it from the dependency_graph and repeat the process

 on the remaining nodes



 ex

 topsort-ex The dependency_graph of Fig dg-term-fig

 has no cycles One topological_sort is the order in which the

 nodes have_already been numbered Notice_that

 every edge of the graph goes from a node to a higher-numbered

 node so this order is surely a topological_sort There_are other

 topological sorts as_well such_as

 ex



 S-Attributed Definitions

 s-attrib-subsect



 As_mentioned earlier given an SDD it is very_hard to tell

 whether there exist any parse_trees whose dependency_graphs have

 cycles In_practice translations can be_implemented using classes

 of SDD's that guarantee an evaluation order since they do_not

 permit dependency_graphs with cycles Moreover the two classes

 introduced in this_section can be_implemented efficiently in

 connection_with top-down or bottom-up_parsing



 The first class is defined as_follows



 itemize



 An SDD is S-attributed if every attribute is

 synthesized



 itemize



 ex

 s-attr-ex The SDD of Fig_sdd-desk-calc-fig is an

 example of an S-attributed definition Each attribute

 and is synthesized

 ex



 When an SDD is S-attributed we can evaluate its attributes in

 any bottom-up order of the nodes of the parse_tree It is often

 especially simple to evaluate the attributes by performing a

 postorder_traversal of the parse_tree and evaluating the

 attributes at a node when the traversal leaves for the

 last time That is we apply the function postorder defined

 below to the root of the parse_tree (see also the box Preorder

 and Postorder Traversals in Section dfs-subsect)



 center

 tabularl

 postorder()



 for ( each child of from the left )

 postorder()



 evaluate the attributes associated_with node







 tabular

 center



 S-attributed definitions can be_implemented during bottom-up

 parsing since a bottom-up_parse corresponds to a postorder

 traversal Specifically postorder corresponds exactly to the

 order in which an LR_parser reduces a production_body to its head

 This fact will be used in Section stack-postfix-subsect to

 evaluate synthesized_attributes and store them on the stack during

 LR_parsing without creating the tree nodes explicitly



 L-Attributed Definitions

 l-attrib-subsect



 The second class of SDD's is called L-attributed

 definitions The idea_behind this class is that between the

 attributes associated_with a production_body dependency-graph

 edges can go from left to right but not from right to left (hence

 L-attributed) More_precisely each attribute must_be either



 enumerate



 Synthesized or



 Inherited but with the rules limited as_follows Suppose

 that there is a production and that

 there is an inherited_attribute computed by a rule

 associated_with this production Then the rule may use only

 enumerate

 Inherited_attributes associated_with the head



 Either inherited or synthesized_attributes associated_with

 the occurrences of symbols located to the

 left of



 Inherited or synthesized_attributes associated_with this

 occurrence of itself but only in such a way that there are

 no cycles in a dependency_graph formed_by the attributes of this



 enumerate



 enumerate



 ex

 l-attrib-ex The SDD in Fig sdd-ll-term-fig is

 L-attributed To_see why consider the semantic_rules for

 inherited_attributes which are repeated_here for convenience



 center

 tabularl_l

 Production_Semantic Rule











 tabular

 center



 The first of these rules defines the inherited_attribute

 using only and appears to the left

 of in the production_body as required The second rule

 defines using the inherited_attribute

 associated_with the head and where

 appears to the left of in the production_body



 In each of these cases the rules use information from above or from the

 left as required by the class The remaining attributes are

 synthesized Hence the SDD is L-attributed

 ex



 ex

 l-attr2-ex Any SDD containing the following production

 and rules cannot be -attributed



 center

 tabularl_l

 Production_Semantic Rules











 tabular

 center

 The first rule is a legitimate rule in either an

 S-attributed or L-attributed_SDD It defines a synthesized

 attribute in terms of an attribute at a child (that is a

 symbol within the production body)



 The second rule defines an inherited_attribute so the

 entire SDD cannot be S-attributed Further although the rule

 is legal the SDD cannot be L-attributed because the attribute

 is used to help define and is to the right of

 in the production_body While attributes at siblings in a parse

 tree may be used in L-attributed SDD's they must_be to the left

 of the symbol whose attribute is being defined

 ex



 Semantic_Rules with Controlled Side Effects

 sdd-side-subsect



 In_practice translations involve side_effects a desk_calculator

 might print a result a code_generator might enter the type of an

 identifier into a symbol_table With SDD's we strike a balance

 between attribute grammars and translation_schemes Attribute

 grammars have no side_effects and allow any evaluation order

 consistent_with the dependency_graph Translation schemes impose

 left-to-right evaluation and allow semantic_actions to contain any

 program_fragment translation_schemes are discussed in

 Section sdt-sect



 We_shall control side_effects in SDD's in one of the following

 ways



 itemize



 Permit incidental side_effects that do_not constrain

 attribute evaluation In other_words permit side_effects when

 attribute evaluation based_on any topological_sort of the

 dependency_graph produces a correct translation where

 correct depends_on the application



 Constrain the allowable evaluation orders so that the same

 translation is produced for any allowable order The constraints

 can be thought of as implicit edges added to the dependency_graph



 itemize



 As an example of an incidental side_effect let_us modify the desk

 calculator of Example sdd-desk-calc-ex to print a result

 Instead of the rule which saves the result in the

 synthesized_attribute consider



 center

 tabularl_l l

 Production_Semantic Rule



 1) n



 tabular

 center

 Semantic rules that are executed for their side_effects such_as

 will be treated_as the definitions of

 dummy synthesized_attributes associated_with the head of the

 production The modified SDD produces the same translation

 under any topological_sort since the print statement is executed

 at the end after the result is computed into



 ex

 sdd-type-ex The SDD in Fig sdd-type-fig takes a

 simple declaration consisting of a basic type followed_by

 a list of identifiers can be int or float

 For each identifier on the list the type is entered into the

 symbol-table_entry for the identifier We assume that entering the

 type for one identifier does_not affect the symbol-table_entry for

 any other identifier Thus entries can be updated in any order

 This SDD does_not check_whether an_identifier is declared

 more_than once it can be modified to do so



 figurehtfb



 center

 tabularl_ll

 Production_Semantic Rules



 1)



 2)



 3)



 4)







 5)



 tabular

 center



 Syntax-directed definition for simple type declarations

 sdd-type-fig



 figure



 Nonterminal represents a declaration which from

 production 1 consists of a type followed_by a list of

 identifiers has one attribute which is the

 type in the declaration Nonterminal also has one

 attribute which we call to emphasize that it is an

 inherited_attribute The purpose of is to pass the

 declared type down the list of identifiers so that it can be

 added to the appropriate symbol-table entries



 Productions 2 and 3 each evaluate the synthesized_attribute

 giving it the appropriate value integer or float

 This type is passed to the attribute in the rule

 for production 1 Production 4 passes down the

 parse_tree That is the value is computed at a

 parse-tree_node by copying the value of from the

 parent of that node the parent corresponds to the head of the

 production



 Productions 4 and 5 also have a rule in which a function addType is called with two arguments



 enumerate



 identry a lexical value that points to a

 symbol-table object and



 the type being assigned to every identifier

 on the list



 enumerate

 We suppose that function addType properly installs the type

 as the type of the represented identifier



 A dependency_graph for the input_string

 appears

 in Fig dg2-fig Numbers 1 through 10 represent the nodes of

 the dependency_graph Nodes 1_2 and 3 represent the attribute

 entry associated_with each of the leaves labeled id

 Nodes 6 8 and 10 are the dummy attributes that represent the

 application of the function addType to a type and one of

 these entry values



 figurehtfb

 Dependency

 graph for a declaration



 dg2-fig

 figure



 Node_4 represents the attribute and is actually

 where attribute evaluation begins This type is then passed to

 nodes 5 7 and 9 representing associated_with each

 of the occurrences of the nonterminal

 ex



 exer

 What are all the topological sorts for the dependency_graph of

 Fig dg-term-fig

 exer



 exer

 sdd-type-exer For the SDD of Fig sdd-type-fig

 give annotated_parse trees for the following expressions



 itemize



 a) int a b_c



 b) float w x_y z



 itemize

 exer



 exer

 Suppose that we have a production Each of the four

 nonterminals and have two attributes is a

 synthesized_attribute and is an inherited_attribute For each

 of the sets of rules below tell_whether the rules are

 consistent_with an S-attributed definition the rules are

 consistent_with an L-attributed_definition and whether

 the rules are consistent_with any evaluation order at all



 itemize



 a)



 b) and



 c)



 d) and



 itemize

 exer



 hexer

 bit-position-exer This grammar generates binary numbers

 with a decimal point



 center

 tabularl













 tabular

 center

 Design an L-attributed_SDD to compute the decimal-number

 value of an input_string For_example the translation of string

 101101 should be the decimal number 5625 Hint use an

 inherited_attribute that tells which side of the decimal

 point a bit is on

 hexer



 vhsexer

 Design an S-attributed SDD for the grammar and translation

 described in Exercise bit-position-exer

 vhsexer



 vhexer

 Implement Algorithm_my-alg which converts a regular

 expression into a nondeterministic_finite automaton by an

 L-attributed_SDD on a top-down parsable grammar Assume that

 there is a token char representing any character and that

 charlexval is the character it represents You_may

 also assume the existence of a function new() that returns

 a new state that is a state never before returned by this

 function Use any convenient notation to specify the transitions

 of the NFA

 vhexer

 Translation of Expressions

 expr-3code-sect



 The rest of this_chapter explores issues that arise during the

 translation of expressions and statements We begin in this

 section with the translation of expressions into three-address

 code An expression with more_than one operator like

 will translate into instructions with at most one operator per

 instruction An array reference will expand into a

 sequence of three-address_instructions that calculate an address

 for the reference We_shall consider type_checking of expressions

 in Section type-check-sect and the use of boolean

 expressions to direct the flow of control through a program in

 Section control-3code-sect



 Operations Within Expressions



 The syntax-directed_definition in Fig_sdd-3code-fig builds

 up the three-address_code for an assignment_statement using

 attribute code for and attributes addr and code for an expression Attributes and

 denote the three-address_code for and

 respectively Attribute denotes the address that

 will hold the value of Recall from

 Section 3code-kinds-subsection that an address can be a name a

 constant or a compiler-generated_temporary



 figurehtbf

 small

 center

 tabularr_c l_l r_c l

 3cProduction 3cSemantic Rules



 id











 new















 new















 (_)











 id







 tabular

 center

 small

 Three-address_code for expressionssdd-3code-fig

 figure



 Consider the last production in the

 syntax-directed_definition in Fig_sdd-3code-fig When an

 expression is a single identifier say then itself holds

 the value of the expression The semantic_rules for this

 production define to point to the symbol-table

 entry for this instance of Let top denote the

 current symbol_table Function topget retrieves the

 entry when it is applied to the string representation idlexeme of this instance of

 is set to the empty_string



 When the translation of

 is the same as that of the subexpression Hence

 equals and

 equals



 The operators and unary - in

 Fig_sdd-3code-fig are representative of the operators in a

 typical language The semantic_rules for

 generate code to compute the value of from the

 values of and Values are computed into newly

 generated temporary names If is computed into

 and into then

 translates_into

 where is a new_temporary

 name is set to A sequence of distinct

 temporary names is created by successively

 executing



 For_convenience we use the notation

 to represent the

 three-address_instruction Expressions appearing in place

 of variables like and are evaluated when passed to

 gen and quoted strings like are taken

 literally(In syntax-directed_definitions gen

 builds an instruction and returns it In translation_schemes

 gen builds an instruction and incrementally emits it by

 putting it into the stream of generated instructions) Other

 three-address_instructions will be built up similarly by_applying

 gen to a combination of expressions and strings



 When we translate the production

 the semantic_rules in Fig_sdd-3code-fig build_up

 by concatenating

 and an instruction that adds the values of

 and The instruction puts the result of the addition

 into a new_temporary name for denoted_by



 The translation of is similar The rules

 create a new_temporary for and generate an instruction to

 perform the unary_minus operation



 Finally the production

 generates instructions that assign the value of

 expression to the identifier The semantic_rule

 for this production uses function topget to determine

 the address of the identifier represented_by as in

 the rules for consists of

 the instructions to compute the value of into an address given

 by followed_by an assignment to the address

 for this instance

 of



 ex

 The syntax-directed_definition in Fig_sdd-3code-fig

 translates the assignment_statement

 into

 the three-address_code sequence

 center

 tabularl

 minus c



 b



 a

 tabular

 center

 ex



 Incremental Translation

 expr-incr-subsect



 Code attributes can be long strings so they are usually generated

 incrementally as discussed in Section sdt-fly-subsect

 Thus instead of building_up as in

 Fig_sdd-3code-fig we can arrange to generate only the new

 three-address_instructions as in the translation_scheme of

 Fig_expr-1pass-fig In the incremental approach gen

 not only constructs a three-address_instruction it appends the

 instruction to the sequence of instructions generated so_far The

 sequence may either be retained in memory for further processing

 or it may be output incrementally



 figurehtbf



 center

 tabularr_c l_l l

 id







 new











 new











 (_)







 id





 tabular

 center

 Generating three-address_code for

 expressions incrementally

 expr-1pass-fig

 figure



 The translation_scheme in Fig_expr-1pass-fig generates the

 same code as the syntax-directed_definition in

 Fig_sdd-3code-fig With the incremental approach the code attribute is not used since there is a single sequence of

 instructions that is created by successive calls to gen For

 example the semantic_rule for in

 Fig_expr-1pass-fig simply calls gen to generate an

 add instruction the instructions to compute into

 and into have_already

 been generated



 The approach of Fig_expr-1pass-fig can also be used to

 build a syntax_tree The new semantic_action for

 creates a node by using a constructor as in



 center





 center

 Here attribute addr represents the address of a node rather_than

 a variable or constant



 Addressing Array Elements array-calc-subsect



 Array elements can be accessed quickly if they are stored in a

 block of consecutive locations In C and Java array_elements are

 numbered for an array with elements If

 the width of each array_element is then the th_element of

 array begins in location

 equationarray-w1-eq

 base i w

 equation

 where base is the relative_address of the storage allocated

 for the array That is base is the relative_address of





 The formula (array-w1-eq) generalizes to two or_more

 dimensions In two dimensions let_us write as in C

 for element in row Let be the width of a

 row and let be the width of an element in a row The

 relative_address of can then be calculated by the

 formula

 equationarray-w2-eq

 base i1 w1 i2 w2

 equation

 In dimensions the formula is

 equationarray-wk-eq

 base i1 w1 i2 w2 ik

 wk

 equation

 where for is the generalization of

 and in (array-w2-eq)









 Alternatively the relative_address of an array reference can be

 calculated in terms of the numbers of elements along

 dimension of the array and the width of a single

 element of the array In two dimensions (ie and )

 the location for is given by

 equationarray-n2-eq

 base (i1 n2 i2) w

 equation

 In dimensions the following formula calculates the same

 address as (array-wk-eq)

 equationarray-nk-eq

 base (( ((i1 n2 i2) n3 i3)

 ) nk ik) w

 equation



 More_generally array_elements need not be numbered starting_at 0

 In a one-dimensional array the array_elements are numbered

 and base is

 the relative_address of Formula

 (array-w1-eq) for the address of is replaced_by

 equationarray-low1-eq

 base (i - low) w

 equation



 The expressions (array-w1-eq) and (array-low1-eq) can

 be both be_rewritten as where the subexpression

 can be precalculated at

 compile_time Note_that when low is 0 We assume

 that is saved in the symbol_table entry for so the

 relative_address of is obtained_by simply adding

 to



 Compile-time precalculation can also be applied to address

 calculations for elements of multidimensional arrays see

 Exercise array-access-exer However there is one situation

 where we cannot use compile-time precalculation when the array's

 size is dynamic If we do_not know the values of low and

 high (or their generalizations in many dimensions) at

 compile_time then we cannot compute constants such_as Then

 formulas like (array-low1-eq) must_be evaluated as they are

 written when the program executes



 The above address_calculations are based_on row-major layout for

 arrays which is used in C for example A two-dimensional_array is

 normally stored in one of two forms either row-major

 (row-by-row) or column-major (column-by-column)

 Figure array-major-fig shows the layout of a

 array in (a) row-major form and (b) column-major form

 Column-major form is used in the Fortran family of languages



 figurehtfb





 Layouts for a two-dimensional_array

 array-major-fig

 figure



 We can generalize row- or column-major form to many dimensions

 The generalization of row-major form is to store the elements in

 such a way that as we scan down a block of storage the rightmost

 subscripts appear to vary fastest like the numbers on an

 odometer Column-major form generalizes to the opposite

 arrangement with the leftmost subscripts varying fastest



 Translation of Array References

 array-3code-subsect



 The chief problem in generating code for array references is to

 relate the address-calculation formulas in

 Section array-calc-subsect to a grammar for array

 references Let nonterminal generate an array name followed_by

 a sequence of index expressions

 center



 center



 As in C and Java assume that the lowest-numbered array_element is

 0 Let_us calculate addresses based_on widths using the formula

 (array-wk-eq) rather_than on numbers of elements as in

 (array-nk-eq)

 The translation_scheme in Fig array-3code-fig generates

 three-address_code for expressions with array references It

 consists of the productions and semantic_actions from

 Fig_expr-1pass-fig together_with productions involving

 nonterminal



 figurehtbf

 center

 tabularr_c l_l l

 id















 new











 id







 new











 id







 new



















 new



 new













 tabular

 center

 Semantic actions for array

 referencesarray-3code-fig

 figure



 Nonterminal has three synthesized_attributes



 enumerate



 denotes a temporary that is used while

 computing the offset for the array reference by summing the terms

 in (array-wk-eq)



 is a pointer to the symbol-table_entry for

 the array name The base_address of the array say

 is used to determine the actual

 -value of an array reference after all the index expressions

 are analyzed



 is the type of the subarray generated_by

 For any type we assume that its width is given by

 We use types as attributes rather_than widths

 since types are needed anyway for type_checking For any array

 type suppose that gives the element type



 enumerate



 The production represents

 an assignment to a nonarray variable which is handled as usual

 The semantic_action for

 generates an indexed copy instruction to assign the value denoted

 by expression to the location denoted_by the array reference

 Recall that attribute gives the symbol-table

 entry for the array The array's base_address - the address of

 its th_element - is given by

 Attribute denotes the temporary that holds the

 offset for the array reference generated_by The location for

 the array reference is therefore

 The generated instruction copies the r-value from address into the location for



 Productions and

 are the same as before The semantic_action for the new

 production generates code to copy the value from the

 location denoted_by into a new_temporary This location is

 as discussed_above

 for the production Again

 attribute gives the array name and

 gives its base_address Attribute

 denotes the temporary that holds the offset The

 code for the array reference places the r-value at the

 location designated by the base and offset into a new_temporary

 denoted_by



 exarray-layout-ex

 Let a denote a array of integers and let c i and j all denote integers Then the type of

 a is Its

 width is 24 assuming that the width of an_integer is 4 The

 type of ai is of width

 The type of aij is



 An annotated_parse tree for the expression caij

 is shown in Fig array-tree-fig The expression is

 translated_into the sequence of three-address_instructions in

 Fig array-ref-3code-fig As usual we have used the name of

 each identifier to refer to its symbol-table_entry

 ex



 figurehtbf



 Annotated parse_tree for caijarray-tree-fig

 figure



 figurehtfb



 center

 tabularl

 i 12



 j 4







 a



 c

 tabular

 center



 Three-address_code for expression caij array-ref-3code-fig



 figure



 Symbolic Type Widths The intermediate_code should

 be relatively independent of the target_machine so the optimizer

 does_not have to change much if the code_generator is replaced_by

 one for a different machine However as we have described the

 calculation of type widths an assumption regarding basic

 types is built into the translation_scheme For_instance

 Example array-layout-ex assumes that each element of an

 integer array takes four_bytes Some intermediate codes eg

 P-code for Pascal leave it to the code_generator to fill in the

 size of array_elements so the intermediate_code is independent of

 the size of a machine word We could have done the same in our

 translation_scheme if we replaced 4 (as the width of an integer)

 by a symbolic_constant



 exer

 exp-sdd-exer Add to the translation of

 Fig_sdd-3code-fig rules for the following productions



 itemize

 a)_b)

 (unary plus)



 itemize

 exer



 exer

 Repeat_Exercise exp-sdd-exer for the incremental translation

 of Fig_expr-1pass-fig

 exer



 exer

 Use the translation of Fig array-3code-fig to translate the

 following assignments



 itemize

 a) x_ai bj



 b) x aij bij



 c) x abijck

 itemize

 exer



 hexer

 Revise the translation of Fig array-3code-fig for array

 references of the Fortran style that is id

 for an -dimensional array

 hexer



 exer

 array-access-exer Generalize formula (array-low1-eq)

 to multidimensional arrays and indicate what values can be stored

 in the symbol_table and used to compute offsets Consider the

 following cases



 itemize



 a) An array of two dimensions in row-major form

 The first dimension has indexes running from to and

 the second dimension has indexes from to The width

 of a single array_element is



 b) The same as (a) but with the array stored in

 column-major form



 c) An array of dimensions stored in

 row-major form with elements of size The th dimension

 has indexes running from to



 d) The same as (c) but with the array stored in

 column-major form



 itemize

 exer



 exer

 row-major-exer An integer array stored row-major

 has index

 ranging_from 1 to 10 and index ranging_from 1 to 20 Integers

 take 4 bytes each Suppose array is stored starting_at byte

 0 Find the location of



 itemize

 a)_b) c)

 itemize

 exer



 exer

 Repeat_Exercise row-major-exer if is stored in

 column-major_order

 exer



 exer

 row-major2-exer A real array has index

 ranging_from 1 to 4 ranging_from 0 to 4 and

 ranging_from 5 to 10 Reals take 8 bytes each If

 is stored row-major starting_at byte 0 find the location of



 itemize

 a)_b) c)

 itemize

 exer



 exer

 Repeat_Exercise row-major2-exer if is stored in

 column-major_order

 exer

 Intermediate_Code for Expressions

 expr-java-sect



 Package inter contains the Node class_hierarchy

 Node has two subclasses Expr for expression nodes and

 Stmt for statement nodes This_section introduces Expr and

 its subclasses Some of the methods in Expr deal_with

 booleans and jumping_code they will be discussed in

 Section jumping-java-sect along with the remaining

 subclasses of Expr



 Nodes in the syntax_tree are implemented as objects of class

 Node For error reporting field lexline (line 4 file

 Nodejava) saves the source-line number of the construct at this

 node Lines 7-10 are used to emit three-address_code



 footnotesize

 flushleft

 1)_package inter_File Nodejava



 2)_import lexer



 3)_public class Node



 4) int lexline 0



 5) Node() lexline Lexerline



 6) void error(String s) throw_new Error(near line lexline s)



 7) static_int labels 0



 8)_public int newlabel() return labels



 9)_public void emitlabel(int i) Systemoutprint(L i )



 10)_public void emit(String s) Systemoutprintln( s)



 11)



 flushleft

 footnotesize



 Expression constructs are implemented_by subclasses of Expr

 Class Expr has fields op and type (lines 4-5

 file Exprjava) representing the operator and type

 respectively at a node



 footnotesize

 flushleft

 1)_package inter_File Exprjava



 2)_import lexer_import symbols



 3)_public class Expr extends Node



 4)_public Token op



 5)_public Type type



 6) Expr(Token tok Type p) op tok type p



 flushleft

 footnotesize



 Method gen (line 7) returns a term that can fit the

 right_side of a three-address_instruction Given expression

 method gen returns a term

 where and are addresses for the values of

 and respectively The return value this is

 appropriate if this object is an address subclasses of Expr

 typically reimplement gen



 Method reduce (line 8) computes or reduces an expression

 down to a single address that is it returns a constant an

 identifier or a temporary_name Given expression method

 reduce returns a temporary holding the value of

 Again this is an appropriate return value if this object is

 an address



 We defer discussion of methods jumping and emitjumps

 (lines 9-18) until Section jumping-java-sect they generate

 jumping_code for boolean_expressions



 footnotesize

 flushleft

 7) public_Expr gen() return this



 8)_public Expr reduce() return this



 9)_public void_jumping(int t int_f) emitjumps(toString() t f)



 10)_public void emitjumps(String test int t int_f)



 11) if( t 0 f 0 )



 12) emit(if test goto_L t)



 13) emit(goto_L f)



 14)



 15) else_if( t 0 ) emit(if test goto_L t)



 16) else_if( f 0 ) emit(iffalse test goto_L f)



 17) else nothing since both t and f fall_through



 18)



 19) public_String toString()_return optoString()



 20)



 flushleft

 footnotesize



 Class Id inherits the default implementations of gen

 and reduce in class Expr since an_identifier is an

 address



 footnotesize

 flushleft

 1)_package inter_File Idjava



 2)_import lexer_import symbols



 3)_public class Id extends_Expr



 4)_public int offset relative_address



 5)_public Id(Word id Type p int b) super(id p) offset b



 6)



 flushleft

 footnotesize



 The node for an_identifier of class Id is a leaf

 The call super(idp) (line 5 file Idjava) saves

 id and p in inherited fields op and type

 respectively Field offset (line_4) holds the relative

 address of this identifier



 Class Op provides an implementation of reduce (lines

 5-10 file Opjava) that is inherited by subclasses

 Arith for arithmetic operators Unary for unary operators

 and Access for array_accesses In each case reduce

 calls gen to generate a term emits an instruction to assign

 the term to a new_temporary name and returns the temporary



 footnotesize

 flushleft

 1)_package inter_File Opjava



 2)_import lexer_import symbols



 3)_public class Op extends_Expr



 4)_public Op(Token tok Type p) super(tok p)



 5)_public Expr reduce()



 6) Expr_x gen()



 7) Temp t new Temp(type)



 8) emit( ttoString() xtoString() )



 9) return t



 10)



 11)



 flushleft

 footnotesize



 Class Arith implements binary operators like and

 Constructor Arith begins by calling

 super(toknull) (line 6) where tok is a token representing

 the operator and null is a placeholder for the type The

 type is determined on line 7 by using Typemax which checks

 whether the two operands can be coerced to a common numeric type

 the code for Typemax is in Section symbols-java-sect

 If they can be coerced type is set to the result type

 otherwise a type error is reported (line 8) This simple compiler

 checks types but it does_not insert type_conversions



 footnotesize

 flushleft

 1)_package inter_File Arithjava



 2)_import lexer_import symbols



 3)_public class Arith extends_Op



 4)_public Expr expr1 expr2



 5)_public Arith(Token tok_Expr x1_Expr x2)



 6) super(tok null) expr1 x1 expr2 x2



 7) type Typemax(expr1type expr2type)



 8) if (type null_) error(type_error)



 9)



 10)_public Expr gen()



 11) return_new Arith(op expr1reduce() expr2reduce())



 12)



 13) public_String toString()



 14) return expr1toString() optoString() expr2toString()



 15)



 16)



 flushleft

 footnotesize



 Method gen constructs the right_side of a three-address

 instruction by reducing the subexpressions to addresses and

 applying the operator to the addresses (line 11 file

 Arithjava) For_example suppose gen is called at the root

 for abc The calls to reduce return a as the

 address for subexpression a and a temporary t as the

 address for bc Meanwhile reduce emits the

 instruction tbc Method gen returns a new

 Arith node with operator and addresses a and

 t as operands(For error reporting field lexline

 in class Node records the current lexical line number when a

 node is constructed We leave it to the reader to track line

 numbers when new nodes are constructed during intermediate_code

 generation)



 It is worth noting that temporary names are typed along with all

 other expressions The constructor Temp is therefore called

 with a type as a parameter (line 6 file

 Tempjava)(An alternative approach might be for the

 constructor to take an expression node as a parameter so it can

 copy the type and lexical position of the expression node)



 footnotesize

 flushleft

 1)_package inter_File Tempjava



 2)_import lexer_import symbols



 3)_public class Temp extends_Expr



 4) static_int count 0



 5) int number 0



 6)_public Temp(Type p) super(Wordtemp p) number count



 7) public_String toString()_return t number



 8)



 flushleft

 footnotesize



 Class Unary is the one-operand counterpart of class

 Arith



 footnotesize

 flushleft

 1)_package inter_File Unaryjava



 2)_import lexer_import symbols



 3)_public class Unary extends_Op



 4)_public Expr_expr



 5)_public Unary(Token tok_Expr x) handles minus for see Not



 6) super(tok null) expr x



 7) type Typemax(TypeInt exprtype)



 8) if (type null_) error(type_error)



 9)



 10)_public Expr gen() return_new Unary(op exprreduce())



 11) public_String toString()_return optoString() exprtoString()



 12)



 flushleft

 footnotesize

 Optimization of DFA-Based PatternMatchers

 Optimization of DFA-Based Pattern Matchers

 fa-opt-sect



 In this_section we present three algorithms that have_been used to

 implement and optimize pattern matchers constructed from regular

 expressions



 enumerate



 The first algorithm is useful in a Lex compiler because it

 constructs a DFA directly from a regular_expression without

 constructing an intermediate NFA

 The resulting DFA also may have fewer states than the DFA constructed

 via an NFA



 The second algorithm minimizes the number of states of any DFA by

 combining states that have the same future behavior

 The algorithm itself is quite efficient running in time

 where is the number of states of the DFA



 The third algorithm produces more compact representations of transition

 tables than the standard two-dimensional table



 enumerate



 Important States of an NFA

 nfa-important-subsect



 To_begin our_discussion of how to go directly from a regular_expression

 to a DFA we must first dissect the NFA construction of

 Algorithm_my-alg and consider the roles played by various states

 We call a state of an NFA important if it has a non-

 out-transition

 Notice_that the subset_construction (Algorithm subset-cons-alg)

 uses only the important states in a set when it computes

 the set of states reachable from on input



 That is the set of states is nonempty only if state is

 important

 During the subset_construction two sets of NFA_states can be identified

 (treated as if they_were the same set) if they



 enumerate



 Have the same important states and



 Either both have accepting_states or neither does



 enumerate



 When the NFA is constructed from a regular_expression by

 Algorithm_my-alg we can say more about the important states

 The only important states are those introduced as initial states in the

 basis part for a particular symbol position in the regular_expression

 That is each important state corresponds to a particular operand in the

 regular_expression



 The constructed NFA has only one accepting_state but this state having

 no out-transitions is not an important state

 By concatenating a unique right_endmarker to a regular

 expression we give the accepting_state for a transition on

 making it an important state of the NFA for

 In other_words by using the augmented regular_expression

 we can forget about accepting_states as the subset

 construction proceeds when the construction is complete any state with

 a transition on must_be an accepting_state



 The important states of the NFA correspond directly to the positions in

 the regular_expression that hold symbols of the alphabet

 It is useful as we_shall see to present the regular_expression by its

 syntax_tree where the leaves correspond to operands and the

 interior_nodes correspond to operators

 An interior_node is called a cat-node or-node or star-node if it is labeled by the concatenation operator (dot) union

 operator or star operator respectively

 We can construct a syntax_tree for a regular_expression just as we did

 for arithmetic_expressions in Section absyn-subsect



 ex

 syntax-imp-ex

 Figure syntax-imp-fig shows the syntax_tree for the regular

 expression of our_running example

 Cat-nodes are represented_by circles

 ex



 figurehtfb

 fileuullmanalsuch3figssyntax-impeps

 Syntax tree for

 syntax-imp-fig

 figure



 Leaves in a syntax_tree are labeled by or by an alphabet symbol

 To each leaf not labeled we attach a unique integer

 We refer to this integer as the position of the leaf and also as a

 position of its symbol

 Note_that a symbol can have several positions for instance has

 positions 1 and 3 in Fig syntax-imp-fig

 The positions in the syntax_tree correspond to the important states of

 the constructed NFA



 ex

 nfa-imp-ex

 Figure nfa-imp-fig shows the NFA for the same regular

 expression as Fig syntax-imp-fig with the important states

 numbered and other states represented_by letters

 The numbered states in the NFA and the positions in the syntax_tree

 correspond in a way we_shall soon see

 ex



 figurehtfb

 fileuullmanalsuch3figsnfa-impeps

 NFA constructed by Algorithm_my-alg for



 nfa-imp-fig

 figure



 nullable

 firstpos

 lastpos

 followpos



 Functions Computed From the Syntax Tree

 xpos-subsect



 To construct a DFA directly from a regular_expression we construct its

 syntax_tree and then compute four functions

 and defined as_follows

 Each definition refers to the syntax_tree for a particular augmented regular

 expression



 enumerate



 is true for a syntax-tree_node if and only if the

 subexpression represented_by has in its language

 That is the subexpression can be made null or the empty_string

 even_though there may be other strings it can represent as_well



 is the set of positions in the subtree rooted_at that

 correspond to the first symbol of at_least one string in the language of

 the subexpression rooted_at



 is the set of positions in the subtree rooted_at that

 correspond to the last symbol of at_least one string in the language of

 the subexpression rooted_at



 for a position is the set of positions in the

 entire syntax_tree such that there is some string in

 such that for some there is a way to explain

 the membership of in by matching to

 position of the syntax_tree and to position



 enumerate



 ex

 xpos-ex

 Consider the cat-node in Fig syntax-imp-fig that corresponds

 to the expression

 We claim is false since this node generates all strings

 of 's and 's ending in an it does_not generate

 On the other_hand the star-node below it is nullable it generates

 along with all other strings of 's and 's





 In a typical generated string like the first position of the

 string corresponds to position 1 of the tree and in a string like

 the first position of the string comes_from position 2 of the tree

 However when the string generated_by the expression of node is just

 then this comes_from position 3





 That is no_matter what string is generated from the expression of node

 the last position is the from position 3 of the tree



 is trickier to compute but we_shall see the rules for

 doing_so shortly

 Here is an example of the reasoning

 Consider a string where the is either or

 and the comes_from position 1

 That is this is one of those generated_by the in expression



 This could be followed_by another or coming from the same

 subexpression in which case comes_from position 1 or 2

 It is also possible that this is the last in the string generated_by

 in which case the symbol must_be the that comes

 from position 3

 Thus 1_2 and 3 are exactly the positions that can follow position 1

 ex



 Computing and

 comp-xpos-subsect



 We can compute and by a straightforward

 recursion on the height of the tree

 The basis and inductive rules for and are summarized in

 Fig comp-xpos-fig

 The rules for are essentially the same as for but

 the roles of children and must_be swapped in the rule for

 a cat-node



 figurehtfb



 center

 tabularccc

 Node



 A leaf_labeled true



 A leaf with position false



 An or-node

 or







 A cat-node and

 if_( )







 else



 A star-node true



 tabular

 center



 Rules for computing and

 comp-xpos-fig



 figure



 ex

 xpos2-ex

 Of all the nodes in Fig syntax-imp-fig only the star-node is

 nullable

 We note from the table of Fig comp-xpos-fig that none of the

 leaves are nullable because they each correspond to non-

 operands

 The or-node is not nullable because neither of its_children is

 The star-node is nullable because every star-node is nullable

 Finally each of the cat-nodes having at_least one nonnullable child

 is not nullable



 figurehtfb

 fileuullmanalsuch3figssyntax-xposeps

 and for nodes in the syntax_tree

 for

 syntax-xpos-fig

 figure



 The computation of and for each of the nodes is

 shown in Fig syntax-xpos-fig with to the left of

 node and to its right

 Each of the leaves has only itself for and as

 required by the rule for non- leaves in

 Fig comp-xpos-fig

 For the or-node we take the union of at the children and do

 the same for

 The rule for the star-node says_that we take the value of or

 at the one child of that node



 Now_consider the lowest cat-node which we_shall call

 To_compute we first consider whether the left_operand is

 nullable which it is in this case

 Therefore for is the union of for each of its

 children that is

 The rule for does_not appear explicitly in

 Fig comp-xpos-fig but as we mentioned the rules are the same as

 for with the children interchanged

 That is to compute we must ask whether its right_child (the

 leaf with position 3) is

 nullable which it is not

 Therefore is the same as of the right_child or



 ex



 Computing

 comp-followpos-subsect



 Finally we need to see_how to compute

 There_are only two ways that a position of a regular_expression can be

 made to follow another



 enumerate



 If is a cat-node with left_child and right_child then

 for every position in all positions in

 are in



 If is a star-node and is a position in then

 all positions in are in



 enumerate



 ex

 xpos3-ex

 Let_us continue with our_running example recall that and

 were computed in Fig syntax-xpos-fig

 Rule 1 for requires that we look_at each cat-node and put

 each position in of its right_child in for each

 position in of its left_child

 For the lowest cat-node in Fig syntax-xpos-fig that rule_says

 position 3 is in and

 The next cat-node above says_that 4 is in and the

 remaining two cat-nodes give us 5 in and 6 in





 We must also apply rule 2 to the star-node

 That rule tells_us positions 1 and 2 are in both

 and since both and for this node

 are

 The complete sets are summarized in

 Fig followpos-sum-fig

 ex



 figurehtfb



 center

 tabularcc

 Position



 1



 2



 3



 4



 5



 6



 tabular

 center



 The function

 followpos-sum-fig



 figure



 We can represent the function by creating a directed graph

 with a node for each position and an arc from position to

 position if and only if

 is in

 Figure followpos-fig shows this graph for the function of

 Fig followpos-sum-fig



 figurehtfb

 fileuullmanalsuch3figsfollowposeps

 Directed graph for the function

 followpos-fig

 figure



 It should come as no surprise that the graph for is almost

 an NFA without s for the underlying regular_expression and

 would become one if we



 enumerate



 Make all positions in of the root be initial states



 Label each arc from to by the symbol at position and



 Make the position associated_with endmarker be the only accepting

 state



 enumerate



 Converting a Regular Expression Directly to a DFA

 re-dfa-subsect



 alg

 re-dfa-alg

 Construction of a DFA from a regular_expression



 A regular_expression



 A DFA that recognizes



 enumerate



 Construct a syntax_tree from the augmented regular_expression



 Compute and for

 using the methods of Sections comp-xpos-subsect and

 comp-followpos-subsect



 Construct Dstates the set of states of DFA and Dtran

 the transition function for by the procedure of Fig comp-dfa-fig

 The states of are sets of positions in

 Initially each state is unmarked and a state becomes marked

 just_before we consider its out-transitions

 The start_state of is where node is the root

 of

 The accepting_states are those containing the position for the endmarker

 symbol



 enumerate

 alg



 figurehtfb



 center

 tabularl

 initialize Dstates to contain only the unmarked state





 where is the root of syntax_tree for



 while_( there is an unmarked state in Dstates )



 mark



 for ( each input_symbol )



 let be the union of for all



 in that correspond to



 if_( is not in Dstates )



 add as an unmarked state to Dstates















 tabular

 center



 Construction of a DFA directly from a regular_expression

 comp-dfa-fig



 figure



 ex

 re-dfa-ex

 We can now put together the steps of our_running example to construct a

 DFA for the regular_expression

 The syntax_tree for appeared in Fig syntax-imp-fig

 We observed that for this tree is true only for the

 star-node and we exhibited and in

 Fig syntax-xpos-fig

 The values of appear in Fig followpos-sum-fig



 The value of for the root of the tree is so this

 set is the start_state of

 Call this set of states

 We must compute and

 Among the positions of 1 and 3 correspond to while 2

 corresponds to

 Thus

 and



 The latter is state and so does_not have to be added to Dstates but the former is new so we add it to Dstates and proceed to compute its transitions

 The complete DFA is shown in Fig dfa-imp-fig

 ex



 figurehtfb

 fileuullmanalsuch3figsdfa-impeps

 DFA constructed from Fig nfa-imp-fig

 dfa-imp-fig

 figure



 Minimizing the Number of States of a DFA

 dfa-min-subsect



 There can be many DFA's that recognize the same language

 For_instance note_that the DFA's of Figs dfa2-fig and

 dfa-imp-fig both recognize language



 Not_only do these automata have states with different names but they

 don't even have the same number of states

 If we implement a lexical_analyzer as a DFA we would generally prefer

 a DFA with as few states as possible since each state requires entries

 in the table that describes the lexical_analyzer



 The matter of the names of states is minor

 We_shall say that two automata are the same up to state names if

 one can be transformed into the other by doing nothing more_than

 changing the names of states

 Figures dfa2-fig and dfa-imp-fig are not the same up to

 state names

 However there is a close relationship_between the states of each

 States and of Fig_dfa2-fig are actually equivalent in

 the sense that neither is an accepting_state and on any input they

 transfer to the same state - to on input and to on input

 Moreover both states and behave like state 123 of

 Fig dfa-imp-fig

 Likewise state of Fig_dfa2-fig behaves like state 1234 of

 Fig dfa-imp-fig state behaves like state 1235 and state

 behaves like state 1236



 It_turns out that there is always a unique (up to state names)

 minimum state DFA for any regular language

 Moreover this minimum-state_DFA can be constructed from any DFA for the

 same language by grouping sets of equivalent states

 In the case of

 Fig dfa-imp-fig is the minimum-state_DFA and it can be

 constructed by partitioning the states of Fig_dfa2-fig as





 In order to understand the algorithm for creating the partition of

 states that converts any DFA into its minimum-state equivalent DFA we

 need to see_how input strings distinguish states from one another

 We_say that string distinguishes state from state if

 exactly one of the states reached from and by following the path

 with label is an accepting_state

 State is distinguishable from state if there is some

 string that distinguishes them



 ex

 distinguish-ex

 The empty_string distinguishes any accepting_state from any nonaccepting

 state

 In Fig_dfa2-fig the string distinguishes state from

 state since takes to a nonaccepting state but takes

 to accepting_state

 ex



 The state-minimization algorithm works by partitioning the states of a

 DFA into groups of states that cannot be distinguished

 Each group of states is then merged into a single state of the

 minimum-state_DFA

 The algorithm works by maintaining a partition whose groups are sets of

 states that have not_yet been distinguished while any two states

 from different_groups are known to be distinguishable

 When the partition cannot be refined further by breaking any group into

 smaller groups we have the minimum-state_DFA



 Initially the partition consists of two groups the accepting_states

 and the nonaccepting states

 The fundamental step is to take some group of the current partition say

 and some input_symbol and see whether

 can be used to distinguish_between any states in group

 We examine the transitions from each of on input

 and if the states reached fall into two or_more groups of the

 current partition we split into a collection of groups so that

 and are in the same group if and only if they go to the same

 group on input

 We repeat this process of splitting groups until for no group and for

 no input_symbol can the group be split further

 The idea is formalized in the next algorithm



 new

 final



 alg

 dfa-min-alg

 Minimizing the number of states of a DFA



 A DFA with set of states input alphabet start_state

 and set of accepting_states



 A DFA_accepting the same language as and having as few states as

 possible



 enumerate



 Start with an initial partition with two groups and

 the accepting and nonaccepting states of



 Apply the procedure of Fig new-part-fig to construct a new

 partition



 Why the State-Minimization Algorithm Works

 We need to prove two things that states remaining in the same group in

 are indistinguishable by any string

 and that states winding up in

 different_groups are distinguishable

 The first is an induction on that if after the th_iteration of step_(2)

 of Algorithm dfa-min-alg and are in the same group then

 there is no string of length or less that distinguishes them

 We_shall leave the details of the induction to you



 The second is an induction on that if states and are placed

 in different_groups at the th_iteration of step_(2) then there is a

 string that distinguishes them

 The basis when and are placed in different_groups of the

 initial partition is easy one must_be accepting and the other not so

 distinguishes them

 For the induction there must_be an input and states and

 such that and go to states and respectively on input



 Moreover and must already have_been placed in different_groups

 Then by the inductive_hypothesis there is some string that

 distinguishes from

 Therefore distinguishes from



 figurehtfb



 center

 tabularl

 initially let



 for ( each group of )



 partition into subgroups such that two states and





 are in the same subgroup if and only if for all



 input symbols states and have transitions on





 to states in the same group of



 at worst a state will be in a subgroup by itself



 replace in by the set of all subgroups

 formed







 tabular

 center



 Construction of

 new-part-fig



 figure



 If let and continue with step (4)

 Otherwise repeat step_(2) with in place of



 Choose one state in each group of as the representative

 for that group

 The representatives will be the states of the minimum-state_DFA

 The other components of are constructed as_follows

 enumerate

 The start_state of is the representative of the group containing

 the start_state of

 The accepting_states of are the representatives of

 those groups that contain an accepting_state of

 Note_that each group contains either only accepting_states or only

 nonaccepting states because we started by separating those two classes

 of states and the procedure of Fig new-part-fig always forms new

 groups that are subgroups of previously constructed groups

 Let be the representative of some group of and let

 the transition of from on input be to state

 Let be the representative of 's group

 Then in there is a transition from to on input

 Note_that in every state in group must go to some state of group

 on input or else group would have_been split according to

 Fig new-part-fig

 enumerate



 enumerate

 alg



 Eliminating the Dead State

 The minimization algorithm sometimes produces a DFA with one dead

 state - one that is not accepting and transfers to itself on each

 input_symbol

 This state is technically needed because a DFA must have a transition

 from every state on every symbol

 However as discussed in Section lex-dfa-subsect we often want to

 know when there is no_longer any possibility of acceptance so we can

 establish that the proper lexeme has already_been seen

 Thus we may wish to eliminate the dead_state and use an automaton that

 is missing some transitions

 This automaton has one fewer state than the minimum-state_DFA but is

 strictly speaking not a DFA because of the missing transitions to the

 dead_state



 ex

 dfa-min-ex

 Let_us reconsider the DFA of Fig_dfa2-fig

 The initial partition consists of the two groups

 which are respectively the nonaccepting

 states and the accepting_states

 To construct the procedure of Fig new-part-fig

 considers both groups and inputs and

 The group cannot be split because it has only one state so

 will remain intact in



 The other group can be split so we must consider the

 effect of each input_symbol

 On input each of these states goes to state so there is no way

 to distinguish these states using strings that begin_with

 On input states and go to members of group

 while state goes to a member of another group

 Thus in group is split into

 and for this round is



 In the next round we can split into since

 and each go to a member of on input while

 goes to a member of another group

 Thus after the second round

 For the third round we cannot split the one remaining group with more

 than one state since and each go to the same state (and

 therefore to the same group) on each input

 We_conclude that



 Now we_shall construct the minimum-state_DFA

 It has four states corresponding to the four groups of and

 let_us pick and as the representatives of these

 groups

 The initial_state is and the only accepting_state is

 Figure min-dfa-trans-fig shows the transition function for the

 DFA

 For_instance the transition from state on input is to

 since in the original DFA goes to on input and is the

 representative of 's group

 For the same reason the transition on from state is to

 itself while all other transitions are as in Fig_dfa2-fig

 ex



 figurehtfb



 center

 tabularccc

 State



















 tabular

 center



 Transition table of minimum-state_DFA

 min-dfa-trans-fig



 figure



 State Minimization in Lexical Analyzers

 lex-min-subsect



 To apply the state minimization procedure to the DFA's generated in

 Section lex-dfa-subsect we must begin Algorithm dfa-min-alg

 with the partition

 that groups together all states that recognize a particular token and also

 places in one group all those states that do_not indicate any token

 An_example should make the extension clear



 ex

 lex-min-ex

 For the DFA of Fig lex-dfa-fig the initial partition is





 That is states 0137 and 7 belong together because neither announces any

 token

 States 8 and 58 belong together because they both announce token



 Note_that we have added a dead_state which we suppose has

 transitions to itself on inputs and

 The dead_state is also the target of missing transitions on from

 states 8 58 and 68



 We must split 0137 from 7 because they go to different_groups on

 input

 We also split 8 from 58 because they go to different_groups on

 Thus all states are in groups by themselves and Fig lex-dfa-fig

 is the minimum-state_DFA recognizing its three tokens

 Recall that a DFA serving as a lexical_analyzer will normally drop the

 dead_state while we treat missing transitions as a signal to end

 token recognition

 ex



 Trading Time for Space in DFA Simulation

 ts-tradeoff-subsect



 The simplest and fastest way to represent the transition function of a

 DFA is a two-dimensional table indexed by states and characters

 Given a state and next_input character we access the array to find the

 next state and any special action we must take eg returning a token

 to the parser

 Since a typical lexical_analyzer has several hundred states in its DFA

 and involves the ASCII alphabet of 128 input_characters the array

 consumes less_than a megabyte



 However compilers are also appearing in very small devices where even

 a megabyte of storage may be too_much

 For such situations there are many methods that can be used to compact

 the transition table

 For_instance we can represent each state by a list of transitions - that

 is character-state pairs - ended by a default state that is to be

 chosen for any input_character not on the list

 If we choose as the default the most frequently occurring next state we

 can often reduce the amount of storage needed by a large factor



 There is a more subtle data_structure that allows_us to combine the

 speed of array_access with the compression of lists with defaults

 We may think of this structure as four arrays as suggested in

 Fig tt-ds-figfootnoteIn practice there would be

 another array indexed by states to give the action_associated with that

 state if anyfootnote

 The base array is used to determine the base location of the

 entries for state which are located in the next and check arrays

 The default array is used to determine an alternative base

 location if the check array tells_us the one given by base is invalid



 figurehtfb

 fileuullmanalsuch3figstt-dseps

 Data structure for representing transition tables

 tt-ds-fig

 figure



 To_compute the transition for state on

 input we examine the next and check entries in location

 where character is treated_as an_integer

 presumably in the range 0 to 127

 If then this entry is valid and the next state

 for state on input is

 If then we determine another state

 and repeat the process as if were the current

 state

 More formally the function nextState is defined as_follows



 center

 tabularl

 int



 if_( )_return





 else_return







 tabular

 center



 The intended use of the structure of Fig tt-ds-fig is to make the

 next-check arrays short by_taking advantage of the similarities

 among states

 For_instance state the default for state might be the state

 that says we are working on an_identifier like state 10 in

 Fig id-td-fig

 Perhaps state is entered after seeing the letters th which

 are a prefix of keyword then as_well as potentially being the prefix

 of some lexeme for an_identifier

 On input_character e we must go from state to a special state

 that remembers we have_seen the but otherwise state behaves

 as does

 Thus we set to (to confirm

 that this entry is valid for ) and we set

 to the state that remembers the

 Also is set to



 While we may not be_able to choose base values so that no next-check entries remain unused experience has shown that the simple

 strategy of assigning base values to states in turn and assigning

 each value the lowest integer so that the special entries

 for state are not previously occupied utilizes little more space

 than the minimum possible



 exer

 Extend the table of Fig comp-xpos-fig to include the operators

 (a) and (b)

 exer



 exer

 Use_Algorithm re-dfa-alg to convert the regular_expressions of

 Exercise re-dfa-exer directly to deterministic_finite automata

 exer



 hexer

 We can prove that two regular_expressions are equivalent by showing that

 their minimum-state DFA's are the same up to renaming of states Show

 in this way that the following regular_expressions



 and

 are all equivalent

 Note

 You_may have constructed the DFA's for these expressions in response to

 Exercise re-dfa-exer

 hexer



 hexer

 min-lb-exer

 Construct the minimum-state DFA's for the following regular_expressions



 itemize



 a)

 b)

 c)



 itemize

 Do you see a pattern

 hexer



 vhexer

 To make formal the informal claim of Example exponential-ex

 show that any deterministic_finite automaton for the regular_expression



 where

 appears times at the end must have at_least

 states

 Hint Observe the pattern in Exercise min-lb-exer

 What condition regarding the history of inputs does each state

 represent

 vhexer

 -closure

 move

 -transition

 Dtran

 Finite_Automata

 fa-sect



 We_shall now discover how Lex turns its input program into a

 lexical_analyzer

 At the heart of the transition is the formalism known_as finite

 automata

 These are essentially graphs like transition_diagrams with a few

 differences



 enumerate



 Finite automata are recognizers they simply say yes or

 no about each possible input_string



 Finite automata come in two flavors

 enumerate

 Nondeterministic finite_automata (NFA) have no restrictions on the

 labels of their edges

 A symbol can label several edges out of the same state and

 the empty_string is a possible label

 Deterministic finite_automata (DFA) have for each state and for

 each symbol of its input alphabet exactly

 one edge with that symbol leaving that state

 enumerate



 enumerate



 Both deterministic and nondeterministic_finite automata are capable of

 recognizing the same languages

 In_fact these languages are exactly the same languages called the regular languages that regular_expressions can

 describefootnoteThere is a small lacuna as we defined them

 regular_expressions cannot describe the empty language since we would

 never want to use this pattern in practice

 However finite_automata can define the empty language

 In the theory is treated_as an additional regular

 expression for the sole purpose of defining the empty

 languagefootnote



 Nondeterministic Finite_Automata

 nfa-subsect



 A nondeterministic_finite automaton (NFA) consists of



 enumerate



 A finite set of states



 A set of input symbols the input alphabet

 We assume that which stands_for the empty_string is never a

 member of



 A transition function that gives for each state and for

 each symbol in a set of next states



 A state from that is distinguished as the start_state

 (or initial state)



 A set of states a subset of that is distinguished as the accepting_states (or final states)



 enumerate



 We can represent either an NFA or DFA by a transition graph where

 the nodes are states and the labeled edges represent the transition

 function

 There is an edge labeled from state to state if and only if

 is one of the next states for state and input

 This graph is very much like a transition_diagram except



 itemize



 a)

 The same symbol can label edges from one state to several different

 states and



 b)

 An edge may be labeled by the empty_string instead of or

 in addition to

 symbols from the input alphabet



 itemize



 ex

 nfa1-ex

 The transition graph for an NFA recognizing the language of regular

 expression is shown in Fig_nfa1-fig

 This abstract example describing all strings of 's and 's ending

 in the particular string will be used_throughout this_section

 It is similar to regular_expressions that describe languages of real

 interest however

 For_instance an expression describing all files whose name ends in

 o is anyo where any stands_for any

 printable character



 figurehtfb

 fileuullmanalsuch3figsnfa1eps

 A nondeterministic_finite automaton

 nfa1-fig

 figure



 Following our convention for transition_diagrams the double circle

 around state 3 indicates that this state is accepting

 Notice_that the only ways to get from the start_state 0 to the accepting

 state is to follow some path that stays in state 0 for a while then

 goes to states 1_2 and 3 by reading from the input

 Thus the only strings getting to the accepting_state are those that end

 in

 ex



 Transition Tables

 trans-table-subsect



 We can also represent an NFA by a transition table whose rows

 correspond to states and whose columns correspond to the input symbols

 and

 The entry for a given state and input is the value of the transition

 function applied to those arguments

 If the transition function has no information_about that state-input

 pair we put in the table for the pair



 ex

 nfa1-table-ex

 The transition table for the NFA of Fig_nfa1-fig is shown in

 Fig trans-table1-fig

 ex



 figurehtfb



 center

 tabularcc_c c

 state



 0



 1



 2



 3



 tabular

 center



 Transition table for the NFA of Fig_nfa1-fig

 trans-table1-fig



 figure



 The transition table has the advantage that we can easily find the

 transitions on a given state and input

 Its disadvantage is that it takes a lot of space when the input

 alphabet is large yet most states do_not have any moves on most of the

 input symbols



 Acceptance of Input Strings by Automata

 fa-accept-subsect



 An NFA accepts input_string if and only if there is some path

 in the transition graph from the start_state to one of the accepting

 states such that the symbols along the path spell out

 Note_that labels along the path are effectively ignored

 since the empty_string does_not contribute to the string constructed

 along the path



 ex

 nfa-accept-ex

 The string is accepted_by the NFA of Fig_nfa1-fig

 The path labeled by from state 0 to state 3 demonstrating this fact is



 fileuullmanalsuch3figspath1eps



 Note_that several paths labeled by the same string may lead to different

 states

 For_instance path



 fileuullmanalsuch3figspath2eps



 is another path from state 0 labeled by the string

 This path leads to state 0 which is not accepting

 However remember that an NFA accepts a string as_long as some path

 labeled by that string leads from the start_state to an accepting_state

 The existence of other paths_leading to a nonaccepting state is

 irrelevant

 ex



 The language defined (or accepted)

 by an NFA is the set of strings labeling some

 path from the start to an accepting_state

 As was mentioned the NFA of Fig_nfa1-fig defines the same language

 as does the regular_expression that is all

 strings from the alphabet that end in

 We may use to stand_for the language accepted_by automaton



 ex

 nfa2-ex

 Figure nfa2-fig is an NFA accepting



 String is accepted because of the path



 fileuullmanalsuch3figspath3eps



 Note_that 's disappear in a concatenation so the label of

 the path is

 ex



 figurehtfb

 fileuullmanalsuch3figsnfa2eps

 NFA accepting

 nfa2-fig

 figure



 Deterministic Finite_Automata

 dfa-subsect



 A deterministic_finite automaton (DFA) is a special_case of an NFA

 where



 enumerate



 There_are no moves on input and



 For each state and input_symbol there is exactly one edge out

 of labeled



 enumerate

 If we are using a transition table to represent a DFA then each entry

 is a single state

 We may therefore represent this state without the curly_braces that we

 use to form sets



 While the NFA is an abstract representation of an algorithm to recognize

 the strings of a certain language the DFA is a simple concrete

 algorithm for recognizing strings

 It is fortunate indeed that every regular_expression and every NFA can

 be converted to a DFA_accepting the same language because it is the DFA

 that we really implement or simulate when building lexical_analyzers

 The following algorithm shows_how to apply a DFA to a string



 alg

 dfa-alg

 Simulating a DFA



 An input_string terminated by an end-of-file character eof

 A DFA with start_state accepting_states and transition

 function



 Answer yes if accepts no otherwise



 Apply the algorithm in Fig dfa-sim-fig to the input_string

 The function gives the state to which there is an edge from

 state on input

 The function nextChar returns the next_character of the input

 string

 alg



 figurehtfb



 center

 tabularl





 nextChar()



 while_( eof )







 nextChar()







 if_( is in )_return yes



 else_return no



 tabular

 center



 Simulating a DFA

 dfa-sim-fig



 figure



 ex

 dfa1-ex

 In Fig dfa1-fig we see the transition graph of a DFA_accepting

 the language the same as that accepted_by the

 NFA of Fig_nfa1-fig

 Given the input_string this DFA enters the sequence of states

 and returns yes

 ex



 figurehtfb

 fileuullmanalsuch3figsdfa1eps

 DFA_accepting

 dfa1-fig

 figure



 hexer

 Figure failure-fn-fig in the exercises of Section token-rec-sect

 computes the failure_function for the KMP algorithm Show_how given

 that failure_function we can construct from a keyword



 an -state DFA that recognizes where the dot

 stands_for any character

 Moreover this DFA can be constructed in time

 hexer



 exer

 Design finite_automata (deterministic or nondeterministic) for each of

 the languages of Exercise reg-def-exer

 exer



 sexer

 nfa3-exer

 For the NFA of Fig nfa3-fig

 indicate all the paths

 labeled Does the NFA accept

 sexer



 figurehtfb



 fileuullmanalsuch3figsnfa3eps



 NFA for Exercise nfa3-exer

 nfa3-fig



 figure



 exer

 nfa4-exer

 Repeat_Exercise nfa3-exer for the NFA of

 Fig nfa4-fig

 exer



 figurehtfb



 fileuullmanalsuch3figsnfa4eps



 NFA for Exercise nfa4-exer

 nfa4-fig



 figure



 exer

 Give the transition tables for the NFA of



 itemize



 a) Exercise nfa3-exer

 b) Exercise nfa4-exer

 c) Figure nfa2-fig



 itemize

 exer



 tocchapter10Instruction-Level Parallelism623

 lof10

 lot10

 inst-para-ch10623

 tocsection101Processor Architectures624

 secarch101624

 tocsubsection1011Instruction-Pipeline and Branch Delays624

 loffigure101Five consecutive instructions in a 5-stage instruction pipeline625

 figinstruction-pipeline101625

 tocsubsection1012Pipelined Execution625

 pipe-exec-subsect1012625

 tocsubsection1013Multiple Instruction Issue626

 tocsection102Code-Scheduling Constraints626

 secilpconstraints102626

 tocsubsection1021Data Dependence627

 dd-3kinds-subsect1021627

 tocsubsection1022Finding Dependences Among Memory Accesses628

 tocsubsubsectionArray Data-Dependence Analysis628

 tocsubsubsectionInterprocedural Analysis629

 tocsubsubsectionPointer-Alias Analysis629

 tocsubsection1023Tradeoff Between Register Usage and Parallelism629

 secmem-trade-off1023629

 exsimple-reg102629

 reg-instr-ex103630

 exreg-instr103630

 loffigure102Expression tree in Example103631

 figexpr11102631

 loffigure103Machine code for expression of Fig102631

 mem-trade2-fig103631

 loffigure104Parallel evaluation of the expression of Fig102631

 mem-trade3-fig104631

 tocsubsection1024Phase Ordering Between Register_Allocation and Code Scheduling631

 tocsubsection1025Control Dependence632

 seccontdep1025632

 excontdep104633

 tocsubsection1026Speculative Execution Support633

 spec-exec-subsect1026633

 tocsubsubsectionPrefetching634

 tocsubsubsectionPoison Bits634

 tocsubsubsectionPredicated Execution634

 tocsubsection1027A Basic Machine Model635

 loffigure105A sequence of assignments exhibiting data dependences636

 data-dep-exer-fig105636

 tocsubsection1028Exercises for Section 102636

 par-reg-exer1022637

 loffigure106Minimal-register implementation of an arithmetic expression637

 data-dep2-exer-fig106637

 tocsection103Basic Block Scheduling637

 secsched-bb103637

 tocsubsection1031Data-Dependence Graphs638

 loffigure107Data-dependence graph in Example106638

 figbb-dep107638

 exbb106639

 tocsubsection1032List-Scheduling of Basic Blocks639

 alglist107640

 tocsubsection1033Prioritized Topological Orders640

 prioritized-subsect1033640

 loffigure108A list scheduling algorithm641

 figlist108641

 loffigure109Result of applying list scheduling to the example in Fig107641

 figbb-sched109641

 loffigure1010Machine code for Exercise1031642

 dep-graph-exer-fig1010642

 tocsubsection1034Exercises for Section 103642

 dep-graph-exer1031642

 dep-graph2-exer1032642

 dep-graph3-exer1034642

 loffigure1011Machine code for Exercise1034643

 dep-graph3-exer-fig1011643

 tocsection104Global Code Scheduling643

 secglsched104643

 tocsubsection1041Primitive Code Motions643

 secglforms1041643

 exglsch109643

 loffigure1012Flow graphs before and after global_scheduling in Example109644

 figglsch1012644

 tocsubsection1042Upward Code Motion645

 tocsubsubsectionIf src does_not post-dominate dst645

 tocsubsubsectionIf dst does_not dominate src646

 tocsubsection1043Downward Code Motion646

 tocsubsubsectionIf src does_not dominate dst646

 tocsubsubsectionIf dst does_not post-dominate src647

 tocsubsubsectionSummary of Upward and Downward Code Motion647

 loffigure1013Summary of code motions647

 figglcm1013647

 tocsubsection1044Updating Data Dependences647

 exsch-live1010648

 loffigure1014Example illustrating the change in data_dependences due to code motion648

 figgl-live1014648

 tocsubsection1045Global Scheduling Algorithms648

 tocsubsubsectionRegion-Based Scheduling648

 algglsch1011649

 loffigure1015A region-based global_scheduling algorithm649

 figglschalg1015649

 tocsubsubsectionLoop Unrolling650

 tocsubsubsectionNeighborhood Compaction650

 loffigure1016Unrolled loops651

 unrolled-fig1016651

 tocsubsection1046Advanced Code_Motion Techniques651

 tocsubsection1047Interaction with Dynamic Schedulers652

 secdyn-inter1047652

 tocsubsection1048Exercises for Section 104653

 tocsection105Software Pipelining653

 secsp105653

 tocsubsection1051Introduction653

 exdoall1012654

 loffigure1017Locally scheduled code for Example1012655

 figsp11017655

 exunroll1013655

 tocsubsection1052Software Pipelining of Loops655

 loffigure1018Unrolled code for Example1012656

 figunroll1018656

 loffigure1019Five unrolled iterations of the code in Example1012656

 figsp-unroll1019656

 exsp1014657

 loffigure1020Software-pipelined code for Example1012657

 figspcode1020657

 tocsubsection1053Register Allocation and Code Generation658

 exspreg1015658

 loffigure1021Source-level unrolling of the loop of Example1012659

 do-all2-fig1021659

 loffigure1022Code after software_pipelining and register_allocation in Example1015659

 figspmodular1022659

 tocsubsection1054Do-Across Loops659

 exdoacross1016659

 loffigure1023Software-pipelining of a do-across loop660

 add-mult-fig1023660

 tocsubsection1055Goals and Constraints of Software Pipelining661

 secspgoals1055661

 tocsubsubsectionModular Resource Reservation661

 loffigure1024Resource requirements of four consecutive_iterations from the code in Example1013662

 figsp-resource1024662

 tocsubsubsectionData-Dependence Constraints663

 exdoacross-dep1018663

 loffigure1025Data-dependence graph for Example1018663

 figdoacross-dep1025663

 tocsubsection1056A Software-Pipelining Algorithm664

 tocsubsection1057Scheduling Acyclic Data-Dependence Graphs665

 algsp1019665

 loffigure1026Software-pipelining algorithm for acyclic graphs666

 figspalg1026666

 tocsubsection1058Scheduling Cyclic Dependence Graphs666

 cyc-dep-subsect1058666

 cycle-eq101667

 exspscc1020667

 loffigure1027Dependence graph and resource requirement in Example1020668

 figsp-cycle1027668

 loffigure1028Transitive dependences in Example1020669

 figsp-trans1028669

 algsp-cyclic1021669

 loffigure1029A software-pipelining algorithm670

 figspalg21029670

 exspscc21022672

 tocsubsection1059Improvements to the Pipelining Algorithms672

 loffigure1030Behavior of Algorithm1021 on Example1020673

 figbacktrack1030673

 tocsubsection10510Modular Variable Expansion674

 algsp-mod1023675

 exmod1024676

 tocsubsection10511Conditional Statements677

 secspcond10511677

 tocsubsection10512Hardware Support for Software Pipelining677

 loffigure1031Machine code for Exercise1051678

 sched-loop-exer-fig1031678

 tocsubsection10513Exercises for Section 105678

 sched-loop-exer1051678

 simple-path-exer1056680

 uullmanalsuch10ch10

 page681

 equation1

 enumi3

 enumii2

 enumiii0

 enumiv0

 footnote1

 mpfootnote0

 part0

 chapter10

 section5

 subsection13

 subsubsection0

 paragraph0

 subparagraph0

 figure31

 table0

 parentequation0

 theorem24

 exerz7



 Algorithm Formulation



 Derived Relations



 enumerate

 vP - Variable points-to (V H) vP

 contains the set of variable points-to_relations derived by the

 algorithm A pair vP if variable can point to

 heap_object Relations are added to vP by transitive

 closure on inclusion edges (Rule rule1) and by the resolution of

 loads (Rule rule3)

 hP - Heap points-to (H F H)

 hP contains the set of heap points-to_relations derived by the

 algorithm A pair hP if field of heap

 object can point to heap_object Relations are added to

 hP by store_instructions (Rule rule2)

 IE -_Invocation edges (I M) IE

 records the edges in the call_graph Relations are added to IE

 by the resolution of virtual call_targets in the

 algorithm (Rule IErule)

 A - Arguments and return values (V V)

 A contains the inclusion relations by the passing of arguments

 and return values if variable includes the

 points-to set of variable Relations are added by the matching

 of formal and actual_parameters (Rule Arule) Although we do

 not cover return values and exceptions here they work in an analogous

 manner

 vPfilter - Variable points-to filter (V

 H) vPfilter is a filter on the vP relation based_on

 types A pair vPfilter iff the type of heap_object

 is assignable to the declared type of variable It is derived

 from the vT hT and aT

 relations (Rule vPfilterrule)

 enumerate



 Inference_Rules



 Points-to Rules



 equation

 (v1 v2) A (v2 h) vP (v1 h) vPfilter

 (v1 h) vP

 rule1

 equation



 equation

 (v1_f v2) S (v1 h1) vP (v2 h2) vP

 (h1 f h2) hP

 rule2

 equation



 equation

 (v1_f v2) L (v1 h1) vP (h1 f h2) hP (v2 h2) vPfilter

 (v2 h2) vP

 rule3

 equation



 The inference_rules for points-to_relations (vP and hP)

 work as_follows Rule rule1 is simply transitive_closure over

 inclusion edges If variable can point to object and

 includes then can also point to Rule rule2

 models the effect of store_instructions on the heap Given a

 statement if can point to and can

 point to then can point to Rule rule3

 resolves load instructions Given a statement if

 can point to and can point to then can

 point to Rules rule1 and rule3 include a

 constraint so that only points-to_relations that match the type_filter

 are inserted(We could also make an hPfilter relation

 to filter the hP relation based_on types in Rule rule2

 but because all operations must go_through variables we do_not

 significantly improve the result beyond simply using vPfilter

 The single case that is not caught by the vPfilter is when

 the base object is null)



 Type Filter Rule



 equation

 (v tv) vT (h th) hT (tv th) aT

 (v h) vPfilter

 vPfilterrule

 equation



 The type_filter rule combines vT hT and aT to

 encode the legal variable points-to_relations If variable has

 declared type heap_object has type and is

 assignable from then pointing to is type-safe



 Invocation Binding Rules



 equation

 (m1 i n) mI (i_0 v) actual (v h) vP (h t) hT (t n m2) cha

 (i m2) IE

 IErule

 equation



 equation

 (i m) IE (m z v1) formal (i z v2) actual

 (v1 v2) A

 Arule

 equation



 The inference rule for binding virtual calls (Rule IErule)

 matches invocation_sites with the type of the this pointer and the

 class_hierarchy information to find the possible target methods If

 an invocation_site with method name is invoked on variable

 and can point to and has type and invoking

 on type leads to method then is a possible target of

 invocation



 Matching parameters is achieved with Rule Arule If invocation

 site has a target method variable is passed as argument

 number and the formal_parameter of method is then

 the points-to set of includes the points-to set of Return

 values and thrown exceptions are handled in a likewise manner only

 the inclusion relation is in the opposite direction



 Creating the Front End

 front-java-sect



 The code for the packages appears in five directories main

 lexer symbol parser and inter The

 commands for creating the compiler vary from system to system The

 following are from a UNIX implementation



 footnotesize

 flushleft

 javac lexerjava



 javac symbolsjava



 javac interjava



 javac parserjava



 javac mainjava

 flushleft

 footnotesize



 The javac command creates class files for each class

 The translator can then be exercised by typing java

 mainMain followed_by the source_program to be translated eg

 the contents of file test



 footnotesize

 flushleft

 1) File test



 2) int i int j float v float_x float100 a



 3) while( true )



 4) do i_i1 while( ai v)



 5) do j_j-1 while( aj v)



 6) if( i_j ) break



 7) x_ai ai_aj aj_x



 8)



 9)



 flushleft

 footnotesize



 On this input the front_end produces



 footnotesize

 flushleft

 1) L1L3 i i 1



 2) L5 t1 i 8



 3) t2 a t1



 4) if t2 v_goto L3



 5) L4 j j - 1



 6) L7 t3 j 8



 7) t4 a t3



 8) if t4 v_goto L4



 9) L6 iffalse i_j goto L8



 10) L9 goto_L2



 11) L8 t5 i 8



 12) x a t5



 13) L10 t6 i 8



 14) t7 j 8



 15) t8 a t7



 16) a t6 t8



 17) L11 t9 j 8



 18) a t9 x



 19) goto L1



 20) L2



 flushleft

 footnotesize



 Try it

 Introduction to Garbage_Collection

 secgc



 Data that cannot be referenced is generally known_as garbage Many high-level programming_languages remove the burden

 of manual memory_management from the programmer by offering

 automatic_garbage collection which deallocates unreachable data

 Garbage_collection dates_back to the initial implementation of

 Lisp in 1958 Other significant languages that offer garbage

 collection include Java Perl ML Modula-3 Prolog and

 Smalltalk



 In this_section we introduce many of the concepts of garbage

 collection The notion of an object being reachable is perhaps

 intuitive but we need to be precise the exact rules are

 discussed in Section gc-reach-subsect We also discuss in

 Section secref-counting a simple but imperfect method of

 automatic_garbage collection reference_counting which is based

 on the idea that once a program has lost all references to an

 object it simply cannot and so will not reference the storage



 Section trace-sect covers trace-based collectors which are

 algorithms that discover all the objects that are still useful

 and then turn all the other chunks of the heap into free_space



 Design Goals for Garbage Collectors

 secgc-overview



 Garbage_collection is the reclamation of chunks of storage holding

 objects that can no_longer be accessed by a program We need to

 assume that objects have a type that can be determined by the

 garbage_collector at_run time From the type information we can

 tell how large the object is and which components of the object

 contain references (pointers) to other objects We also assume

 that references to objects are always to the address of the

 beginning of the object never pointers to places within the

 object Thus all references to an object have the same value and

 can be identified easily



 A user program which we_shall refer to as the mutator

 modifies the collection of objects in the heap The mutator

 creates objects by acquiring space from the memory_manager and

 the mutator may introduce and drop references to existing objects

 Objects become garbage when the mutator program cannot reach

 them in the sense made precise in Section gc-reach-subsect

 The garbage_collector finds these unreachable_objects and reclaims

 their space by handing them to the memory_manager which keeps

 track of the free_space



 A Basic Requirement Type Safety



 Not all languages are good candidates for automatic_garbage

 collection For a garbage_collector to work it must_be able to

 tell_whether any given data element or component of a data element

 is or could be used as a pointer to a chunk of allocated memory

 space A language in which the type of any data component can be

 determined is said to be type safe There_are type-safe languages

 like ML for which we can determine types at_compile time There

 are other type-safe languages_like Java whose types cannot be

 determined at_compile time but can be determined at_run time The

 latter are called dynamically typed languages If a language

 is neither statically nor dynamically type safe then it is said

 to be unsafe



 Unsafe languages which unfortunately include some of the most

 important languages such_as C and C are bad candidates for

 automatic_garbage collection In unsafe languages memory

 addresses can be manipulated arbitrarily arbitrary arithmetic

 operations can be applied to pointers to create new pointers and

 arbitrary integers can be cast as pointers Thus a program

 theoretically could refer to any location in memory at any time

 Consequently no memory_location can be considered to be

 inaccessible and no storage can ever be reclaimed safely



 In_practice most C and C programs do_not generate pointers

 arbitrarily and a theoretically unsound garbage_collector that

 works well empirically has_been developed and used We_shall

 discuss conservative_garbage collection for C and C in

 Section secconservativegc





 Performance Metrics



 Garbage_collection is often so expensive that although it was

 invented decades ago and absolutely prevents memory_leaks it has

 yet to be adopted by many mainstream programming_languages Many

 different approaches have_been proposed over the years and there

 is not one clearly best garbage-collection algorithm Before

 exploring the options let_us first enumerate the performance

 metrics that must_be considered when designing a garbage

 collector



 itemize



 Overall Execution Time Garbage_collection can be

 very slow It is important that it not significantly increase the

 total run_time of an application Since the garbage_collector

 necessarily must touch a lot of data its performance is

 determined greatly by how it leverages the memory subsystem



 Space Usage It is important that garbage_collection

 avoid fragmentation and make the best use of the available

 memory



 Pause Time Simple garbage_collectors are notorious

 for causing programs - the mutators - to pause suddenly for an

 extremely long time as garbage_collection kicks in without

 warning Thus besides minimizing the overall execution time it

 is desirable that the maximum pause time be minimized As an

 important special_case real-time applications require certain

 computations to be completed within a time limit We must either

 suppress garbage_collection while performing real-time tasks or

 restrict maximum pause time Thus garbage_collection is seldom

 used in real-time applications



 Program Locality We cannot evaluate the speed of a

 garbage_collector solely by its running_time The garbage

 collector controls the placement of data and thus influences the

 data_locality of the mutator program It can improve a mutator's

 temporal_locality by freeing up space and reusing it it can

 improve the mutator's spatial_locality by relocating data used

 together in the same cache or pages



 itemize



 Some of these design goals conflict with one another and

 tradeoffs must_be made carefully by considering how programs

 typically behave Also objects of different characteristics may

 favor different treatments requiring a collector to use different

 techniques for different_kinds of objects



 For_example the number of objects allocated is dominated by small

 objects so allocation of small objects must not incur a large

 overhead On the other_hand consider garbage_collectors that

 relocate reachable_objects Relocation is expensive when dealing

 with large objects but less so with small objects



 As_another example in general the longer we wait to collect

 garbage in a trace-based collector the larger the fraction of

 objects that can be collected The_reason is that objects often

 die young so if we wait a while many of the newly_allocated

 objects will become_unreachable Such a collector thus costs less

 on the average per unreachable object collected On the other

 hand infrequent collection increases a program's memory_usage

 decreases its data_locality and increases the length of the

 pauses



 In_contrast a reference-counting collector by introducing a

 constant overhead to many of the mutator's operations can slow

 down the overall execution of a program significantly On the

 other_hand reference_counting does_not create long pauses and it

 is memory efficient because it finds garbage as_soon as it is

 produced (with the exception of certain cyclic structures

 discussed in Section secref-counting)



 Language design can also affect the characteristics of memory

 usage Some languages encourage a programming style that generates

 a lot of garbage For_example programs in functional or almost

 functional programming_languages create more objects to avoid

 mutating existing objects In Java all objects other_than base

 types like integers and references are allocated on the heap and

 not the stack even if their lifetimes are confined to that of one

 function invocation This design frees the programmer from

 worrying about the lifetimes of variables at the expense of

 generating more garbage Compiler optimizations have_been

 developed to analyze the lifetimes of variables and allocate them

 on the stack whenever possible



 Reachability

 gc-reach-subsect



 We refer to all the data that can be accessed directly by a

 program without_having to dereference any pointer as the root_set For_example in Java the root_set of a program

 consists of all the static field members and all the variables on

 its stack A program obviously can reach any member of its root

 set at any time Recursively any object with a reference that is

 stored in the field members or array_elements of any reachable

 object is itself reachable



 Reachability becomes a bit more_complex when the program has_been

 optimized by the compiler First a compiler may keep reference

 variables in registers These references must also be considered

 part of the root_set Second even_though in a type-safe language

 programmers do_not get to manipulate memory addresses directly a

 compiler often does so for the sake of speeding up the code Thus

 registers in compiled code may point to the middle of an object or

 an array or they may contain a value to which an offset will be

 applied to compute a legal address Here are some things an

 optimizing_compiler can do to enable the garbage_collector to find

 the correct root_set



 itemize



 The compiler can restrict the invocation of garbage

 collection to only certain code points in the program when no

 hidden references exist



 The compiler can write out information that the garbage

 collector can use to recover all the references such_as

 specifying which registers contain references or how to compute

 the base_address of an object that is given an internal address



 The compiler can assure that there is a reference to the

 base_address of all reachable_objects whenever the garbage

 collector may be invoked



 itemize



 The set of reachable_objects changes as a program executes It

 grows as new objects get created and shrinks as objects become

 unreachable It is important to remember that once an object

 becomes_unreachable it cannot become reachable again There_are

 four basic operations that a mutator performs to change the set of

 reachable_objects



 itemize



 Object Allocations These are performed by the memory

 manager which returns a reference to each newly_allocated chunk

 of memory This operation adds members to the set of reachable

 objects



 Parameter Passing and Return Values References to

 objects are passed from the actual input parameter to the

 corresponding formal_parameter and from the returned result back

 to the caller Objects pointed to by these references remain

 reachable



 Reference Assignments Assignments of the form u

 v where and are references have two effects First

 is now a reference to the object referred to by As_long

 as is reachable the object it refers to is surely reachable

 Second the original reference in is lost If this reference

 is the last to some reachable object then that object becomes

 unreachable Any time an object becomes_unreachable all objects

 that are reachable only through references contained in that

 object also become_unreachable



 Procedure Returns As a procedure exits the frame

 holding its local_variables is popped_off the stack If the frame

 holds the only reachable reference to any object that object

 becomes_unreachable Again if the now unreachable_objects hold

 the only references to other objects they too become_unreachable

 and so on



 itemize



 Survival of Stack Objects When a procedure is

 called a local variable whose object is allocated on the

 stack may have pointers to placed in nonlocal variables

 These pointers will continue to exist after the procedure returns

 yet the space for disappears resulting in a

 dangling-reference situation Should we ever allocate a local like

 on the stack as C does for example The answer is that the

 semantics of many languages requires that local_variables

 cease to exist when their procedure returns Retaining a reference

 to such a variable is a programming error and the compiler is not

 required to fix the bug in the program



 In summary new objects are introduced through object allocations

 Parameter passing and assignments can propagate reachability

 assignments and ends of procedures can terminate reachability As

 an object becomes_unreachable it can cause more objects to become

 unreachable



 There_are two basic ways to find unreachable_objects Either we

 catch the transitions as reachable_objects turn unreachable or we

 periodically locate all the reachable_objects and then infer that

 all the other objects are unreachable Reference_counting

 introduced in Section manual-mem-subsect is a well-known

 approximation to the first approach We maintain a count of the

 references to an object as the mutator performs actions that may

 change the set of reachable_objects When the count goes to zero the

 object becomes_unreachable We discuss this approach in more

 detail in Section secref-counting



 The second approach computes reachability by tracing all the

 references transitively A trace-based garbage_collector

 starts by labeling (marking) all objects in the root_set as

 reachable examines iteratively all the references in

 reachable_objects to find more reachable_objects and labels them

 as such This_approach must trace all the references before it

 can determine any object to be unreachable But once the reachable

 set is computed it can find many unreachable_objects all at once

 and locate a good deal of free storage at the same time Because

 all the references must_be analyzed at the same time we have an

 option to relocate the reachable_objects and thereby reduce

 fragmentation There_are many different trace-based algorithms

 and we discuss the options in Sections trace-sect and

 incremental-sect



 Reference Counting Garbage Collectors

 secref-counting



 We_now consider a simple although imperfect garbage_collector

 based_on reference_counting which identifies garbage as an object

 changes from being reachable to unreachable the object can be

 deleted when its count drops to zero With a reference-counting

 garbage_collector every object must have a field for the

 reference_count Reference counts can be maintained as_follows



 enumerate



 Object Allocation The reference_count of the new

 object is set to 1



 Parameter Passing The reference_count of each object

 passed into a procedure is incremented



 Reference Assignments For statement u_v

 where and are references the reference_count of the

 object referred to by goes up by one and the count for the

 old object referred to by goes down by one



 Procedure Returns As a procedure exits objects

 referred to by the local_variables in its activation_record

 have their counts decremented If several local

 variables hold references to the same object that object's count

 must_be decremented once for each such reference



 Transitive Loss of Reachability Whenever the

 reference_count of an object becomes zero we must also decrement

 the count of each object pointed to by a reference within the

 object



 enumerate



 Reference_counting has two main disadvantages it cannot collect

 unreachable cyclic data_structures and it is expensive Cyclic

 data_structures are quite plausible data_structures often point

 back to their parent nodes or point to each other as cross

 references



 ex

 ref-count-cycle-ex Figure cyclic-garbage-fig shows

 three objects with references among them but no references from

 anywhere else If none of these objects is part of the root_set

 then they are all garbage but their reference_counts are each

 greater_than 0 Such a situation is tantamount to a memory leak if

 we use reference_counting for garbage_collection since then this

 garbage and any structures like it are never deallocated

 ex



 figurehtfb





 An unreachable cyclic data_structure

 cyclic-garbage-fig

 figure



 The overhead of reference_counting is high because additional

 operations are introduced with each reference assignment and at

 procedure entries and exits This overhead is proportional to the

 amount of computation in the program and not just to the number

 of objects in the system Of particular concern are the updates

 made to references in the root_set of a program The concept of

 deferred reference_counting has_been proposed as a means to

 eliminate the overhead associated_with updating the reference

 counts due to local stack accesses That is reference_counts do

 not include references from the root_set of the program An

 object is not considered to be garbage until the entire root_set

 is scanned and no references to the object are found



 The advantage of reference_counting on the other_hand is that

 garbage_collection is performed in an incremental fashion

 Even_though the total overhead can be large the operations are

 spread throughout the mutator's computation Although removing

 one reference may render a large_number of objects unreachable

 the operation of recursively modifying reference_counts can easily

 be deferred and performed piecemeal across time Thus reference

 counting is particularly attractive algorithm when timing

 deadlines must_be met as_well as for interactive applications

 where long sudden pauses are unacceptable Another advantage is

 that garbage is collected immediately keeping space usage low



 figurehtfb









 A network of objects gc-exer-fig



 figure



 exer

 What happens to the reference_counts of the objects in Fig

 gc-exer-fig if



 itemize

 a) The pointer from to is deleted b)

 The pointer from to is deleted c) The node is

 deleted

 itemize

 exer



 figurehtfb



 Another

 network of objects gc2-exer-fig

 figure



 exer

 What happens to reference_counts when the pointer from to

 in Fig gc2-exer-fig is deleted

 exer

 Global Code Scheduling

 secglsched



 For a machine with a moderate amount of instruction-level_parallelism

 schedules created by compacting individual basic_blocks tend to leave

 many resources idle In order to make better use of machine resources

 it is necessary to consider code-generation strategies that move

 instructions from one basic_block to another

 Strategies that consider more_than one basic_block at a time are

 referred to as global_scheduling algorithms

 To do global_scheduling correctly

 we must consider not only data_dependences but also

 control dependences We must ensure_that



 enumerate



 All instructions in the original_program are executed in the

 optimized program and



 While the optimized program may execute extra instructions

 speculatively these instructions must not have any unwanted side_effects



 enumerate



 Primitive Code_Motion

 secglforms



 Let_us first study the issues involved in moving operations around by way

 of a simple example



 ex

 exglsch

 Suppose we have a machine that can execute any two

 operations in a single clock Every operation executes

 with a delay of one clock except for the load operation which has a

 latency of two_clocks For_simplicity we assume that all memory

 accesses in the example are valid and will hit in the cache

 Figure figglsch(a) shows a simple flow_graph with three basic

 blocks The code is expanded into machine operations in

 Figure figglsch(b) All the instructions in each basic_block

 must execute serially because of data_dependences in fact a

 no-op instruction has to be inserted in every basic_block



 figurehtbp

 fileuullmanalsuch10figsglscheps

 Flow graphs before and after global_scheduling in Example exglsch

 figglsch

 figure



 Assume that the addresses of variables and

 are distinct and that those addresses are stored in registers R1

 through R5 respectively The computations from

 different basic_blocks therefore share no data_dependences We observe that all

 the operations in block are executed regardless of whether the

 branch is taken and can therefore be executed in parallel with

 operations from block We cannot move operations from

 down to because they are needed to determine the outcome of the

 branch



 Operations in block are control-dependent on the test in block

 We can perform the load from speculatively in block

 for free and shave two_clocks from the execution time whenever

 the branch is taken



 Stores should not be performed speculatively because

 they overwrite the old value in a memory_location

 It is possible however to delay a store operation We cannot

 simply place the store operation from block in block

 because it should only be executed if the flow of control passes

 through block However we can place the store operation in a duplicated

 copy of Figure figglsch(c) shows such an optimized

 schedule The optimized code executes in 4 clocks which

 is the same as the time it takes to execute alone

 ex



 Example exglsch shows that it is possible to move operations up

 and down an execution_path Every pair of basic_blocks in this

 example has a different dominance relation and thus the

 considerations of when and how instructions can be moved between each pair

 are different As_discussed in Section secdominance a block

 is said to dominate block if every_path from the entry of the

 control-flow_graph to goes_through Similarly a block

 postdominates block if every_path from to the

 exit of the graph goes_through When dominates and

 postdominates we say that and are control equivalent

 meaning that one is executed when and only when the other is

 For the example in Fig figglsch assuming is the entry

 and the exit



 enumerate



 and are control equivalent dominates and

 postdominates



 dominates but does_not postdominate and



 does_not dominate but postdominates



 enumerate

 It is also possible for a pair of blocks along a path to share neither

 a dominance nor postdominance relation



 Upward Code_Motion



 We_now examine carefully what it means to move an operation up a path

 Suppose we_wish to move an operation from block src up a

 control-flow path to block dst We assume that such a move does

 not violate any data_dependences and that it makes paths through

 dst and src run_faster If dst dominates src

 and src postdominates dst then the operation moved is

 executed once and only once when it should



 If src does_not postdominate dst



 Then there

 exists a path that passes_through dst that does_not reach src An

 extra operation would have_been executed in this case This code

 motion is illegal unless the operation moved has no unwanted side_effects

 If the moved operation executes for free (ie it uses only

 resources that otherwise would be idle) then this move has no cost

 It is beneficial only if the control_flow reaches src



 If dst does_not dominate src



 Then there

 exists a path that reaches src without first going_through dst We need to insert copies of the moved operation along such

 paths We know how to achieve exactly that from our_discussion of

 partial_redundancy elimination in Section_secpre We place

 copies of the operation along basic_blocks that form a cut set

 separating the entry block from src At each place where the

 operation is inserted the following constraints must_be satisfied



 enumerate



 The operands of the

 operation must hold the same values as in the original



 The result does

 not overwrite a value that is still needed and



 It itself is not

 subsequently overwritten_before reaching src



 enumerate

 These copies render the

 original instruction in src fully_redundant and it

 thus can be_eliminated



 We refer to the extra copies of the operation as compensation_code As_discussed in Section_secpre basic

 blocks can be inserted along critical_edges to create places for

 holding such copies The compensation_code can potentially make some

 paths run slower Thus this code_motion improves program execution

 only if the optimized paths are executed more frequently than the

 nonoptimized ones



 Downward Code_Motion



 Suppose we are_interested in moving an operation from block src

 down a control-flow path to block dst

 We can reason_about

 such code_motion in the same way as above



 If src does_not dominate dst



 Then there

 exists a path that reaches dst without first visiting src

 Again an

 extra operation will be executed in this case Unfortunately

 downward code_motion is often applied to writes which have the side

 effects of overwriting old values We can get around this problem by

 replicating the basic_blocks along the paths from src to dst and

 placing the operation only in the new copy of dst Another_approach

 if available is to use predicated instructions We guard the

 operation moved with the predicate that guards the src block Note_that

 the predicated instruction must_be scheduled only in a block dominated by

 the computation of the

 predicate because the predicate would not be available otherwise



 If dst does_not postdominate src



 As in the discussion_above compensation_code needs to be inserted so that

 the operation moved is executed on all paths not visiting dst This

 transformation

 is again analogous to partial_redundancy elimination except that the

 copies are placed below the src block in a cut set that separates src

 from the exit



 Summary of Upward and Downward Code_Motion



 From this discussion we see that there is a range of possible global

 code motions which vary in terms of benefit cost and implementation

 complexity Figure figglcm shows a summary of these various

 code motions the lines correspond to the following four cases



 figurehtb

 tabularccccc

 up src postdom dst dst dom src speculation compensation



 2-4

 down src dom dst dst postdom src code dup code



 1 yes yes no no



 2 no yes yes no



 3 yes no no yes



 4 no no yes yes



 tabular

 Summary of code motions

 figglcm

 figure



 enumerate



 Moving instructions between control-equivalent blocks is simplest and

 most cost effective No extra operations are ever executed and no

 compensation_code is needed



 Extra operations may be executed if the source does_not postdominate

 (dominate) the destination in upward (downward) code_motion This

 code_motion is beneficial if the extra operations can be executed for

 free and the path passing through the source block is executed



 Compensation code is needed if the destination does_not dominate

 (postdominate) the source in upward (downward) code_motion The

 paths with the compensation_code may be slowed down so it is

 important that the optimized paths are more frequently executed



 The last case combines the disadvantages of the second and third case

 extra operations may be executed and compensation_code is needed

 enumerate



 Updating Data Dependences



 As illustrated by Example exsch-live below code_motion can change

 the data-dependence relations between operations

 Thus data

 dependences must_be updated after each code movement



 ex

 exsch-live

 For the flow_graph shown in Fig figgl-live either

 assignment to can be moved up to the top block since all the

 dependences in the original_program are preserved with this

 transformation However once we have moved one assignment up we

 cannot move the other More_specifically we see that variable

 is not live_on exit in the top block before the code_motion but it

 is live after the motion If a variable is live at a program point

 then we cannot move speculative definitions to the variable above that

 program point

 ex

 figurehtb

 fileuullmanalsuch10figsgl-liveeps

 Example illustrating the change in data_dependences due to

 code_motion

 figgl-live

 figure



 Global Scheduling Algorithms



 We saw in the last section that code_motion can benefit some paths

 while hurting the performance of others The good news is that instructions

 are not all created equal In_fact it is well established that over

 90 of a program's execution time is spent on less_than 10 of the

 code Thus we should aim to make the frequently executed paths run

 faster while possibly making the less frequent paths run slower



 There_are a number of techniques a compiler can use to estimate

 execution frequencies It is reasonable to assume that instructions

 in the innermost_loops are executed more often than code in outer loops and

 that branches that go backward are more likely to be taken than

 not taken Also

 branch statements found to guard program exits or exception-handling

 routines are unlikely to be taken The best frequency

 estimates however come from dynamic profiling In this technique

 programs are instrumented to record the outcomes of conditional

 branches as they run The programs are then run on representative inputs to

 determine how they are likely to behave in general The results

 obtained from this technique have_been found to be quite accurate

 Such information can be fed back to the compiler to use in its

 optimizations



 Region-Based Scheduling



 We_now describe a straightforward global scheduler that supports the

 two easiest forms of code_motion



 enumerate



 Moving operations up to control-equivalent basic_blocks and



 Moving operations speculatively up one branch to a dominating

 predecessor



 enumerate



 Recall from Section region-def-subsect

 that a region is a subset of a control-flow_graph that can

 be reached only through one entry block We may represent any

 procedure

 as a hierarchy of regions The entire procedure constitutes the

 top-level region nested in it are subregions representing the natural

 loops in the function We assume that the control-flow_graph is

 reducible



 alg Region-based scheduling

 algglsch



 A control-flow_graph and a machine-resource description



 A schedule mapping each

 instruction to a basic_block and a time slot



 Execute the program in Fig figglschalg

 Some shorthand terminology should be apparent

 is the set of blocks that are

 control-equivalent to block and applied to

 a set of blocks is the set of blocks that are successors of at_least one

 block in the set and are dominated by all



 figurehtb



 center

 tabularl

 for_(each region in topological_order so that inner regions



 are processed before outer regions)



 compute data_dependences



 for_(each basic_block of in prioritized topological order)











 ready instructions in



 for ( until all instructions from

 are scheduled)



 for_(each instruction in in priority order)



 if_( has no resource_conflicts at time )







 update resource commitments



 update data_dependences







 update















 tabular

 center



 A region-based global_scheduling algorithm

 figglschalg



 figure



 Code scheduling in Algorithm algglsch proceeds from the

 innermost regions to the outermost When scheduling a region each

 nested subregion is treated_as a black box instructions are not

 allowed to move in or out of a subregion

 They can however move around a

 subregion provided their data and control dependences are

 satisfied



 All control and dependence edges flowing back to the header of the

 region are ignored so the resulting control-flow and data-dependence

 graphs are acyclic The basic_blocks in each region are visited in

 topological_order This ordering guarantees that a basic_block is not

 scheduled until all the instructions it depends_on have_been

 scheduled Instructions to be scheduled in a basic_block are

 drawn from all the blocks that are control-equivalent to

 (including ) as_well as their immediate successors that are

 dominated by



 A list-scheduling algorithm is used to create the schedule for each

 basic_block The algorithm keeps a list of candidate instructions

 CandInsts which contains all the instructions in the

 candidate blocks whose predecessors all have_been scheduled It

 creates the schedule clock-by-clock For each clock it checks each

 instruction from the CandInsts in priority order and schedules it

 in that clock if resources permit Algorithm algglsch

 then updates CandInsts

 and repeats the process until all instructions

 from are scheduled



 The priority order of instructions in CandInsts

 uses a priority function similar to that discussed in

 Section secsched-bb We make one important modification

 however

 We give instructions from blocks that are control equivalent to

 higher priority than those from the successor blocks

 The_reason is that

 instructions in the latter category are only speculatively executed in

 block

 alg



 Loop Unrolling



 In region-based scheduling the boundary of a loop iteration is a

 barrier to code_motion Operations from one iteration cannot overlap

 with those from another One simple but highly effective technique to

 mitigate this problem is to unroll the loop a small number of times

 before code scheduling A for-loop such_as



 verbatim

 for_(i 0 i_N i)

 S(i)



 verbatim

 can be written as in Fig unrolled-fig(a)

 Similarly a repeat-loop such_as



 verbatim

 repeat

 S

 until C

 verbatim

 can be written as in Fig unrolled-fig(b)

 Unrolling creates more instructions in the loop body permitting

 global_scheduling algorithms to find more parallelism



 figurehtfb



 verbatim

 for_(i 0 i4 N i4)

 S(i)

 S(i1)

 S(i2)

 S(i3)



 for ( i_N i)

 S(i)



 verbatim



 center

 (a) Unrolling a for-loop

 center



 verbatim

 repeat

 S

 if (C) break

 S

 if (C) break

 S

 if (C) break

 S

 until C

 verbatim



 center

 (b) Unrolling a repeat-loop

 center



 Unrolled loops

 unrolled-fig



 figure



 Neighborhood Compaction



 Algorithm algglsch only supports the first two forms of code

 motion described in Section secglforms Code motions that

 require the introduction of compensation_code can sometimes be useful

 One_way to support such code motions is to follow the region-based

 scheduling with a simple pass In this pass we can examine each pair of

 basic_blocks that are executed one after the other

 and check if any

 operation can be moved up or down between them to improve the execution time of

 those blocks If such a pair is found we check if the instruction to

 be moved needs to be duplicated along other paths The code_motion is

 made if it results in an expected net gain



 This simple extension can be quite effective in improving the

 performance of loops For_instance it

 can move an operation at the beginning of

 one iteration to the end of the preceding iteration while also moving the

 operation from the first iteration out of the loop This optimization

 is particularly attractive for tight loops which are loops that

 execute only a few instructions per iteration

 However the impact of this technique is limited by the fact that

 each code-motion decision is made locally and independently



 Advanced Code_Motion Techniques



 If our target_machine is statically scheduled and has plenty of

 instruction-level_parallelism we may need a more aggressive

 algorithm Here is a high-level description of further extensions

 enumerate

 To facilitate the extensions below we can add new basic_blocks along

 control-flow edges originating from blocks with more_than one_predecessor

 These basic_blocks will be_eliminated at the end of code scheduling if

 they are empty A useful heuristic is to move instructions out of

 a basic_block that is nearly empty so that the block can be_eliminated

 completely



 In Algorithm algglsch the code to be executed in each basic

 block is scheduled once and for all as each block is visited This

 simple approach suffices because the algorithm can only move

 operations up to dominating blocks To allow motions that require the

 addition of compensation_code we take a slightly_different approach

 When we visit block we only schedule instructions from and

 all its control-equivalent blocks We first try to place these

 instructions in predecessor blocks which have_already been_visited

 and for which a partial schedule already exists We try to find a

 destination block that would lead to an improvement on a frequently

 executed path and then place copies of the instruction on other paths

 to guarantee correctness If the instructions cannot be moved up

 they are scheduled in the current basic_block as before



 Implementing downward code_motion is harder in an algorithm that

 visits basic_blocks in topological_order since the target blocks have

 yet to be scheduled However there are relatively fewer

 opportunities for such code_motion anyway We move all operations

 that

 enumerate

 can be moved and

 cannot be executed for free in their

 native block

 enumerate

 This simple strategy works well if the target_machine

 is rich with many unused hardware resources

 enumerate



 Interaction with Dynamic Schedulers

 secdyn-inter



 A dynamic_scheduler has the advantage that it can create new schedules

 according to the run-time conditions without_having to encode all these

 possible schedules ahead of time If a target_machine has a dynamic

 scheduler the static scheduler's primary function is to ensure_that

 instructions with high latency are fetched early so that the dynamic

 scheduler can issue them as early as possible



 Cache misses are a class of unpredictable events that can make a big

 difference to the performance of a program If data-prefetch

 instructions are available the static scheduler can help the dynamic

 scheduler significantly by placing these prefetch_instructions early

 enough that the data will be in the cache by the time they are

 needed

 If prefetch_instructions are not available it

 is useful for a compiler to estimate which operations are likely to

 miss and try to issue them early



 If dynamic scheduling is not available on the target_machine

 the static scheduler must_be

 conservative and separate every data-dependent pair of operations by

 the minimum delay If dynamic scheduling is available however the

 compiler only needs to place the data-dependent operations in the

 correct order to ensure program correctness For best performance

 the compiler should assign long delays to dependences that are likely

 to occur and short ones to those that are not likely



 Branch misprediction is an important cause of loss in

 performance Because of the long misprediction penalty instructions

 on rarely executed paths can still have a significant effect on the

 total execution time Higher priority should be given to such

 instructions to reduce the cost of misprediction



 exer

 Show_how to unroll the generic while-loop



 verbatim

 while (C)

 S

 verbatim

 exer



 hexer

 Consider the code_fragment



 verbatim

 if (x 0) a b

 else a c

 d a

 verbatim

 Assume a machine that uses the delay model of Example exbb (loads

 take two_clocks all other instructions take one clock) Also assume

 that the machine can execute any two instructions at once Find a

 shortest possible_execution of this fragment Do not forget to consider

 which register is best used for each of the copy steps

 Also remember to exploit the information given by register descriptors

 as was described in Section_simple-cg-sect to avoid unnecessary

 loads and stores

 hexer

 Context-Free Grammars

 grammars-sect



 Grammars are used to systematically describe the inherently

 recursive structure of programming_language constructs like

 expressions and statements Informal rules such_as



 displayinformal-rule-display

 317ptsminipage4in if and are

 statements and is an expression

 then



 if_( ) else

 is a statement

 minipage

 (informal-rule-display)

 display

 allow room for ambiguity so grammars were_introduced as a precise

 notation The structure of this form of conditional expression can

 be readily specified_by a grammar production using a syntactic

 variable stmt to denote statements and expr to denote

 expressions



 displayconditional-display

 stmt if_( expr )

 stmt_else stmt(conditional-display)

 display



 This_section reviews the definition of a context-free_grammar and

 introduces terminology for talking_about parsing Many of the

 examples in this_chapter deal_with grammars for expressions



 From Chapter 2 a context-free_grammar (grammar for short)

 consists of terminals nonterminals a start_symbol and

 productions



 enumerate





 Terminals are the basic symbols from which strings are formed The

 word token is a synonym for terminal when talking_about

 grammars for programming_languages In

 (conditional-display) the terminals are the keywords

 if and else and the symbols ( and )





 Nonterminals are syntactic variables that denote sets of strings

 In (conditional-display) stmt and expr are

 nonterminals The sets of strings denoted_by nonterminals help

 define the language generated_by the grammar Nonterminals impose

 a hierarchical_structure on the language that is key to syntax

 analysis and translation





 In a grammar one nonterminal is distinguished as the start

 symbol and the set of strings it denotes is the language

 generated_by the grammar





 The productions of a grammar specify the manner in which the

 terminals and nonterminals can be_combined to form strings Each

 production consists of two things (i) a nonterminal and (ii) a

 string consisting of nonterminals and terminals Productions will

 be written with the nonterminal followed_by an arrow (sometimes

 the symbol is used in place of the arrow) followed_by the

 string of nonterminals and terminals The nonterminal to the left

 of the arrow is called the left_side of the production and

 the string of nonterminals and terminals to the right is called

 the right_side of the production



 enumerate



 ex

 verbose-gram-ex The grammar with the following productions

 defines simple arithmetic_expressions



 disp

 tabularr_c l

 expression expression term



 expression expression - term



 term_term factor



 term_term factor



 factor ( expression )



 factor id



 tabular

 disp



 In this grammar the terminal_symbols are



 disp

 id - (_)

 disp

 The nonterminal symbols are expression term and

 factor and expression is the start_symbol

 ex



 Notational Conventions



 To_avoid always having to state that these are the terminals

 these are the nonterminals and so on the following notational

 conventions for grammars will be used_throughout the remainder of

 this_book



 enumerate





 These symbols are terminals

 enumerate



 Lower-case letters early in the alphabet such_as



 Operator symbols such_as - etc



 Punctuation symbols such_as parentheses comma etc



 The digits 0_1 9



 Boldface strings such_as id or if

 enumerate





 These symbols are nonterminals

 enumerate



 Upper-case letters early in the alphabet such_as



 The letter which when it appears is usually the start

 symbol



 Lower-case italic names such_as expr or stmt



 When discussing programming_constructs upper-case letters may be

 used to represent nonterminals for the constructs For_example

 nonterminals for expressions terms and factors may be

 represented_by E_T and F respectively

 enumerate





 Upper-case letters late in the alphabet such_as X Y

 Z represent grammar_symbols that is either

 nonterminals or terminals





 Lower-case letters late in the alphabet chiefly

 represent strings of terminals





 Lower-case Greek letters for

 example represent strings of grammar_symbols Thus a generic

 production can be written as indicating

 that there is a single nonterminal on the left_side and the

 string of grammar_symbols on the right_side of the

 production





 A set of productions

 each with on the

 left_side (call them -productions) may be written

 Call

 the alternatives

 for





 Unless stated otherwise the left_side of the first production is

 the start_symbol



 enumerate



 exgram-conventions-ex

 Using these conventions the grammar of Example

 verbose-gram-ex can be_rewritten concisely as



 disp

 tabularl_c l

 E_E T E - T



 T_T F_T F



 F ( E

 ) id

 tabular

 disp

 The notational_conventions tell_us that E_T and

 F are nonterminals with E the start_symbol The remaining

 symbols are terminals

 ex



 Derivations

 The construction of a parse_tree can be made precise by_taking a

 derivational view in which productions are treated_as rewriting

 rules Starting with the start_symbol each rewriting step

 replaces a nonterminal by the string on the right_side of one of

 its productions In_fact this derivational view corresponds to

 the top-down construction of a parse_tree The precision afforded

 by derivations will be especially helpful when bottom-up_parsing

 is discussed



 For_example consider the following grammar with a single

 nonterminal E which adds a production E

 -E to the grammar_(expr-ambig-display)



 displayderiv-gram-display

 E_E

 E_E E -

 E ( E )

 id(deriv-gram-display)

 display

 The production E -E signifies

 that if E denotes an expression then -E

 must also denote an expression The replacement of a single

 E by -E will be described by writing

 disp

 E - E

 disp

 which is read E derives -E The

 production E(E) can

 be applied to replace any instance of an E in any string of

 grammar_symbols by (E) eg

 EE (E

 )E or E

 E E(E)

 We can take a single E and repeatedly apply productions in

 any order to get a sequence of replacements For_example

 disp

 E - E

 - ( E ) - (

 id )

 disp

 We call such a sequence of replacements a derivation of

 -(id) from E This derivation provides a proof

 that the string -(id) is one particular instance

 of an expression



 For a general definition of derivation consider a nonterminal

 in the middle of a sequence of grammar_symbols as in

 where and are arbitrary strings of

 symbols Suppose is a production Then we

 write The

 symbol means derives in one step When a

 sequence of derivation steps

 rewrites to

 we say derives Often we

 wish to say derives in zero_or more steps For this purpose

 we can use the symbol Thus

 enumerate



 for any string and



 If and

 then

 enumerate

 Likewise means derives in one or_more steps



 If where is the start_symbol of a

 grammar and may contain nonterminals we say that

 is a sentential_form of A sentence of

 is a sentential_form with no nonterminals The language

 generated_by a grammar is its set of sentences Thus a string

 of terminals is in the language generated_by if and

 only if is a sentence of (or ) A

 language that can be generated_by a grammar is said to be a

 context-free language If two grammars generate the same

 language the grammars are said to be equivalent



 The string -(idid) is a

 sentence of grammar (deriv-gram-display) because there is a

 derivation

 displayleftmost-display

 E -E

 -(E)

 -(EE

 ) -(id

 E) -(

 idid

 )(leftmost-display)

 display

 The strings E -E -(E

 ) -(idid) are all

 sentential_forms of this grammar We write

 E -(idid) to

 indicate that -(idid) can be

 derived_from E



 At each step in a derivation there are two choices to be made We

 need to choose which nonterminal to replace and having made this

 choice which production to use for that nonterminal For

 example the following alternative derivation of -(

 idid) differs_from the above derivation in the

 last two steps

 displayrightmost-display

 E -

 E -(E

 ) -(E

 E) -

 (Eid

 ) -(id

 id)(rightmost-display)

 display

 Each nonterminal is replaced_by the same right_side in the two

 derivations but the order of replacements is different



 To understand how parsers work we_shall consider derivations in

 which the nonterminal to be replaced at each step is chosen as

 follows



 enumerate





 In leftmost_derivations the leftmost nonterminal is chosen

 If is a step in which the leftmost

 nonterminal in is replaced we write







 In rightmost_derivations the rightmost nonterminal is

 chosen we write



 enumerate



 Derivation (leftmost-display) is leftmost so it can be

 rewritten as

 disp

 E -

 E -(E

 ) -(EE

 ) -(idE

 ) -(idid)

 disp

 Note_that (rightmost-display) is a rightmost_derivation



 Using our notational_conventions every leftmost stem can be

 written is where

 consists of terminals only is the

 production applied and is a string of grammar_symbols

 To emphasize that derives by a leftmost

 derivation we write If

 then we say that is a

 left-sentential form of the grammar at hand



 Analogous definitions hold for rightmost_derivations Rightmost

 derivations are sometimes_called canonical derivations



 Parse Trees and Derivations



 A parse_tree is a graphical representation of a derivation that

 filters out the order in which productions are applied to replace

 nonterminals Each interior_node of a parse_tree represents the

 application of a production The interior_node is labeled with

 nonterminal on the left_side of the production the children

 of the node are labeled from left to right by the symbols in the

 right_side of the production by which this was replaced during

 the derivation



 For_example the parse_tree for -(id

 id) in Fig parse-tree-fig results from the

 derivation_(leftmost-display) as_well as derivation

 (rightmost-display)

 figurehtfb

 center



 Parse_tree for -(idid)

 parse-tree-fig

 center

 figure



 The leaves of a parse_tree are labeled by nonterminals or

 terminals and read from left to right constitute a sentential

 form called the yield or frontier of the tree



 To_see the relationship_between derivations and parse_trees

 consider any derivation

 where is a

 single nonterminal For each sentential_form in

 the derivation we can construct a parse_tree whose yield is

 The process is an induction on For the basis

 the tree for is a single_node labeled To do

 the induction suppose we have_already constructed a parse_tree

 with yield (From the

 notational_conventions each grammar symbol is either a

 nonterminal or a terminal) Suppose is derived_from

 by_replacing a nonterminal by

 That is at the th step of the derivation

 production is applied to to

 derive



 To model this step of the derivation find the th leaf from the

 left in the current parse_tree This leaf is labeled Give

 this leaf children_labeled from the

 left As a special_case if then and

 we give the th leaf one child_labeled



 ex

 parse-tree-seq-ex The sequence of parse_trees constructed

 from the derivation_(leftmost-display) is shown in

 Fig parse-tree-seq-fig In the first step of the

 derivation E -E To model

 this step add two children_labeled - and E to the

 root E of the initial tree to create the second tree

 figurehtfb

 center



 Building the parse_tree from derivation

 parse-tree-seq-fig

 center

 figure



 In the second step of the derivation -

 E -(E) Consequently

 add three children_labeled ( E and ) to the

 leaf_labeled E of the second tree to obtain the third tree

 with yield -(E) Continuing in this

 fashion we obtain the complete parse_tree as the sixth tree

 ex



 Since a parse_tree ignores variations in the order in which

 symbols in sentential_forms are replaced there is a many-to-one

 relationship_between derivations and parse_trees For_example

 both derivations (leftmost-display) and

 (rightmost-display) are associated_with the same final

 parse_tree of Fig parse-tree-seq-fig



 In what_follows we_shall frequently parse by producing a leftmost

 or a rightmost_derivation since there is a one-to-one

 relationship_between parse_trees and either leftmost or rightmost

 derivations Both leftmost and rightmost_derivations pick a

 particular order for replacing symbols in sentential_forms so

 they too filter out variations in the order It is not hard to

 show that every parse_tree has associated_with it a unique

 leftmost and a unique rightmost_derivation



 Ambiguity

 Do not assume that every sentence necessarily has only one parse

 tree or only one leftmost or rightmost_derivation



 ex

 expr-ambig-trees-ex The arithmetic expression grammar

 (expr-ambig-display) permits two distinct leftmost

 derivations for the sentence idid

 id

 disp

 tabularl_c l_l c_l

 E_E E_E E_E



 id E

 E_E E



 id E_E



 id E_E



 id_id E

 id_id E



 id_id id

 id_id id



 tabular

 disp

 The corresponding parse_trees appear in

 Fig expr-ambig-trees-fig

 figurehtfb

 center



 Two parse_trees for idid

 id

 expr-ambig-trees-fig

 center

 figure



 Note_that the parse_tree of Fig expr-ambig-trees-fig(a)

 reflects the commonly assumed precedence of and

 while the tree of Fig expr-ambig-trees-fig(b) does_not

 That is it is customary to treat operator as having

 higher_precedence than corresponding to the fact that we

 would normally evaluate an expression like as

 rather_than as

 ex



 A grammar that produces more_than one parse_tree for some sentence

 is said to be ambiguous Put_another way an ambiguous

 grammar is one that produces more_than one leftmost or_more than

 one rightmost_derivation for the same sentence For certain types

 of parsers it is desirable that the grammar be made unambiguous

 for if it is not we cannot uniquely determine which parse_tree to

 select for a sentence In some_cases it is convenient to use

 carefully chosen ambiguous_grammars together_with

 disambiguating rules that throw away undesirable parse

 trees leaving only one tree for each sentence



 Verifying the Language Generated by a Grammar

 Although compiler designers rarely do it for a complete

 programming_language grammar it is useful to be_able to reason

 that a given set of productions generates a particular language

 Troublesome constructs can be studied by writing a concise

 abstract grammar and studying the language that it generates We

 shall construct such a grammar for conditionals below



 A proof that a grammar generates a language has two_parts

 show that every string generated_by is in and conversely

 that every string in can indeed be generated_by



 ex

 parens-exConsider the following grammar

 (parens-display)

 displayparens-display

 (_)

 (parens-display)

 display



 It may not be initially apparent but this simple grammar

 generates all strings of balanced parentheses and only such

 strings To_see this we_shall show first that every sentence

 derivable_from is balanced and then that every_balanced

 string is derivable_from To show that every sentence

 derivable_from is balanced we use an inductive proof on the

 number of steps in a derivation For the basis step note_that

 the only string of terminals derivable_from in one step is the

 empty_string which surely is balanced



 Now assume that all derivations of fewer_than steps produce

 balanced sentences and consider a leftmost_derivation of exactly

 steps Such a derivation must_be of the form

 disp

 ()

 ()

 ()

 disp



 The derivations of and from take fewer_than steps

 so by the inductive_hypothesis and are balanced

 Therefore the string () must_be balanced



 Having thus shown that any string_derivable from is balanced

 we must next show that every_balanced string is derivable_from

 To do this use induction on the length of a string For the

 basis step the empty_string is derivable_from



 Now assume that every_balanced string of length less_than is

 derivable_from and consider a balanced_string of length

 Surely begins_with a left_parenthesis Let

 () be the shortest prefix of having an equal

 number of left and right parentheses Then can be written as

 () where both and are balanced Since

 and are of length less_than they are derivable_from

 by the inductive_hypothesis Thus we can find a derivation

 of the form

 disp

 ()

 ()

 ()

 disp

 proving that () is also derivable_from



 ex



 Context-Free Grammars vs Regular Expressions

 Before leaving this_section on grammars and their properties we

 establish that grammars are a more_powerful notation than regular

 expressions Every construct that can be described by a regular

 expression can be described by a grammar but not vice_versa

 Alternatively every regular set is a context-free language but

 not vice_versa



 For_example the regular_expression and the grammar



 disptabularl c_l















 tabular

 disp

 describe the same language the set of strings of 's and 's

 ending in



 We can mechanically construct a grammar to recognize the same

 language as a nondeterministic_finite automaton (NFA) The

 grammar above was constructed from the NFA in Fig OLD323 using

 the following construction For each state of the NFA create

 a nonterminal If state has a transition to state

 on input add the production If

 state goes to state on input add the production

 If is an accepting_state add

 If is the start_state make be

 the start_symbol of the grammar



 The language with an equal

 number of 's and 's is a prototypical example of a language

 that can be described by a grammar but not a regular_expression



 To_see this suppose were the language defined by some regular

 expression We could construct a DFA with a finite number of

 states say to accept Since has only states

 for an input sequence of symbols longer than must

 enter some state twice say as in Fig pumping-fig

 Suppose that the path from back to itself is labeled with a

 sequence Since is in the language there

 must_be a path from to an accepting_state But then

 there is also a path from the initial_state through to

 labeled as shown in Fig pumping-fig Thus

 also accepts which is not in the language

 contradicting the assumption that is the language accepted_by





 figurehtfb

 center



 DFA_accepting both and

 pumping-fig

 center

 figure



 Colloquially we say that finite_automata cannot count meaning

 that a finite_automaton cannot accept a language like

 that would_require it to keep count of the

 number of 's before it_sees the 's A grammar can keep

 count of two items but not three as we_shall see when we

 consider non-context-free language_constructs in

 Section writing-grammars-sect

 id

 Context-Free Grammars

 grammars-sect



 Grammars were_introduced in Section syntax-sect to

 systematically describe the syntax of programming_language

 constructs like expressions and statements Using a syntactic

 variable stmt to denote statements and variable expr

 to denote expressions the production



 displayconditional-display

 stmt if_( expr )

 stmt_else stmt(conditional-display)

 display

 specifies the structure of this form of conditional_statement

 Other productions then define precisely what an expr is and

 what else a stmt can be



 This_section reviews the definition of a context-free_grammar and

 introduces terminology for talking_about parsing In_particular

 the notion of derivations is very helpful for discussing the order

 in which productions are applied during_parsing

























 The Formal Definition of a Context-Free Grammar

 cfg-def-subsect



 From Section syntax-sect a context-free_grammar (grammar for short)

 consists of terminals nonterminals a start_symbol and

 productions



 enumerate



 Terminals are the basic symbols from which strings are formed The

 term token_name is a synonym for terminal

 and frequently we will use the word token for terminal

 when it is clear that we are talking_about just the token_name

 We assume that the terminals are the first components of the tokens

 output by the lexical_analyzer In

 (conditional-display) the terminals are the keywords if and else and the symbols ( and )



 Nonterminals are syntactic variables that denote sets of strings

 In (conditional-display) stmt and expr are

 nonterminals The sets of strings denoted_by nonterminals help

 define the language generated_by the grammar Nonterminals impose

 a hierarchical_structure on the language that is key to syntax

 analysis and translation



 In a grammar one nonterminal is distinguished as the start_symbol and the set of strings it denotes is the language

 generated_by the grammar Conventionally the productions for the

 start_symbol are listed_first



 The productions of a grammar specify the manner in which the

 terminals and nonterminals can be_combined to form strings Each

 production consists of

 enumerate



 A nonterminal called the head or left_side of

 the production this production defines some of the strings

 denoted_by the head



 The symbol Sometimes has_been used in

 place of the arrow A body or right_side

 consisting of zero_or more terminals and nonterminals The

 components of the body describe one way in which strings of the

 nonterminal at the head can be constructed

 enumerate



 enumerate



 ex

 verbose-gram-ex

 The grammar in Fig simp-ae-fig

 defines simple arithmetic_expressions

 In this grammar the terminal_symbols are



 disp

 id - (_)

 disp

 The nonterminal symbols are expression term and factor and expression is the start_symbol

 ex



 figurehtfb



 center

 tabularr_c l

 expression expression term



 expression expression - term



 expression term



 term_term factor



 term_term factor



 term_factor



 factor ( expression )



 factor id



 tabular

 center



 Grammar for simple arithmetic_expressions

 simp-ae-fig



 figure



 Notational Conventions



 To_avoid always having to state that these are the terminals

 these are the nonterminals and so on the following notational

 conventions for grammars will be used_throughout the remainder of

 this_book



 enumerate



 These symbols are terminals

 enumerate

 Lowercase letters early in the alphabet such_as

 Operator symbols such_as and so on Punctuation symbols such_as parentheses comma and so on The digits Boldface strings such_as id

 or if each of which represents a single terminal symbol

 enumerate



 These symbols are nonterminals

 enumerate

 Uppercase letters early in the alphabet such_as

 The letter which when it appears is usually the start

 symbol

 Lowercase italic names such_as expr or stmt

 When discussing programming_constructs uppercase letters may be

 used to represent nonterminals for the constructs For_example

 nonterminals for expressions terms and factors are often

 represented_by E_T and F respectively

 enumerate



 Uppercase letters late in the alphabet such_as X Y

 Z represent grammar_symbols that is either

 nonterminals or terminals



 Lowercase letters late in the alphabet chiefly

 represent (possibly empty) strings of terminals



 Lowercase Greek letters for

 example represent (possibly empty) strings of grammar_symbols Thus a generic

 production can be written as where is the

 head and the body



 A set of productions

 with a common head

 (call them -productions) may be written

 Call

 the alternatives

 for



 Unless stated otherwise the head of the first production is

 the start_symbol



 enumerate



 exgram-conventions-ex

 Using these conventions the grammar of Example

 verbose-gram-ex can be_rewritten concisely as



 center

 tabularl_l l













 tabular

 center

 The notational_conventions tell_us that E_T and F are nonterminals with E the start_symbol The remaining

 symbols are terminals

 ex



 Derivations

 The construction of a parse_tree can be made precise by_taking a

 derivational view in which productions are treated_as rewriting

 rules Beginning with the start_symbol each rewriting step

 replaces a nonterminal by the body of one of its productions This

 derivational view corresponds to the top-down construction of a

 parse_tree but the precision afforded by derivations will be

 especially helpful when bottom-up_parsing is discussed As we

 shall_see bottom-up_parsing is related to a class of derivations

 known_as rightmost_derivations in which the rightmost

 nonterminal is rewritten at each step



 For_example consider the following grammar with a single

 nonterminal E which adds a production



 to the grammar_(expr-ambig-display)



 displayderiv-gram-display



 (deriv-gram-display)

 display

 The production signifies

 that if denotes an expression then

 must also denote an expression The replacement of a single

 by will be described by writing

 center



 center

 which is read derives The

 production can

 be applied to replace any instance of in any string of

 grammar_symbols by eg

 or

 We can take a single and repeatedly apply productions in

 any order to get a sequence of replacements For_example

 center



 center

 We call such a sequence of replacements a derivation of

 from This derivation provides a proof

 that the string is one particular instance

 of an expression



 For a general definition of derivation consider a nonterminal

 in the middle of a sequence of grammar_symbols as in

 where and are arbitrary strings of

 grammar_symbols Suppose is a production

 Then we write The

 symbol means derives in one step When a sequence of

 derivation steps

 rewrites to we say

 derives Often we_wish to say derives in

 zero_or more steps For this purpose we can use the symbol

 Thus

 enumerate

 for any string and

 If and

 then

 enumerate

 Likewise means derives in one or_more steps



 If where is the start_symbol of a grammar

 we say that is a sentential_form of Note

 that a sentential_form may contain both terminals and

 nonterminals and may be empty A sentence of is a

 sentential_form with no nonterminals The language generated

 by a grammar is its set of sentences Thus a string of

 terminals is in the language generated_by if and

 only if is a sentence of (or ) A language

 that can be generated_by a grammar is said to be a context-free language If two grammars generate the same

 language the grammars are said to be equivalent



 The string is a

 sentence of grammar (deriv-gram-display) because there is a

 derivation



 displayleftmost-display



 (leftmost-display)

 display

 The strings are all

 sentential_forms of this grammar We write

 to

 indicate that can be

 derived_from E



 At each step in a derivation there are two choices to be made We

 need to choose which nonterminal to replace and having made this

 choice we must pick a production with that nonterminal as head

 For_example the following alternative derivation of

 differs_from derivation_(leftmost-display) in the

 last two steps



 displayrightmost-display



 (rightmost-display)

 display

 Each nonterminal is replaced_by the same body in the two

 derivations but the order of replacements is different



 To understand how parsers work we_shall consider derivations in

 which the nonterminal to be replaced at each step is chosen as

 follows



 enumerate



 In leftmost_derivations the leftmost nonterminal

 in each sentential is always

 chosen If is a step in which the leftmost

 nonterminal in is replaced we write





 In rightmost_derivations the rightmost nonterminal is

 always chosen we write in this case



 enumerate



 Derivation (leftmost-display) is leftmost so it can be

 rewritten as



 center





 center

 Note_that (rightmost-display) is a rightmost_derivation



 Using our notational_conventions every leftmost step can be

 written as where

 consists of terminals only is the

 production applied and is a string of grammar_symbols

 To emphasize that derives by a leftmost

 derivation we write If

 then we say that is a left-sentential form of the grammar at hand



 Analogous definitions hold for rightmost_derivations Rightmost

 derivations are sometimes_called canonical derivations



 Parse Trees and Derivations

 pt-der-subsect



 A parse_tree is a graphical representation of a derivation that

 filters out the order in which productions are applied to replace

 nonterminals Each interior_node of a parse_tree represents the

 application of a production The interior_node is labeled with

 the nonterminal in the head of the production the children

 of the node are labeled from left to right by the symbols in the

 body of the production by which this was replaced during

 the derivation



 For_example the parse_tree for

 in Fig parse-tree-fig results from the

 derivation_(leftmost-display) as_well as derivation

 (rightmost-display)



 figurehtfb

 center



 Parse_tree for

 parse-tree-fig

 center

 figure



 The leaves of a parse_tree are labeled by nonterminals or

 terminals and read from left to right constitute a sentential

 form called the yield or frontier of the tree



 To_see the relationship_between derivations and parse_trees

 consider any derivation

 where is a

 single nonterminal For each sentential_form in

 the derivation we can construct a parse_tree whose yield is

 The process is an induction on



 The tree for is a single_node labeled



 Suppose we already have constructed a parse_tree

 with yield (note that according to

 our

 notational_conventions each grammar symbol is either a

 nonterminal or a terminal)

 Suppose is derived_from

 by_replacing a nonterminal by

 That is at the th step of the derivation

 production is applied to to

 derive



 To model this step of the derivation find the th non-

 leaf from the

 left in the current parse_tree This leaf is labeled Give

 this leaf children_labeled from the

 left As a special_case if then and

 we give the th leaf one child_labeled



 ex

 parse-tree-seq-ex The sequence of parse_trees constructed

 from the derivation_(leftmost-display) is shown in

 Fig parse-tree-seq-fig In the first step of the

 derivation To model this step add two children

 labeled and to the root of the initial tree The

 result is the second tree



 figurehtfb

 center



 Sequence of parse_trees for derivation_(leftmost-display)

 parse-tree-seq-fig

 center

 figure



 In the second step of the derivation Consequently

 add three children_labeled ( and ) to the

 leaf_labeled of the second tree to obtain the third tree

 with yield Continuing in this

 fashion we obtain the complete parse_tree as the sixth tree

 ex



 Since a parse_tree ignores variations in the order in which

 symbols in sentential_forms are replaced there is a many-to-one

 relationship_between derivations and parse_trees For_example

 both derivations (leftmost-display) and

 (rightmost-display) are associated_with the same final

 parse_tree of Fig parse-tree-seq-fig



 In what_follows we_shall frequently parse by producing a leftmost

 or a rightmost_derivation since there is a one-to-one

 relationship_between parse_trees and either leftmost or rightmost

 derivations Both leftmost and rightmost_derivations pick a

 particular order for replacing symbols in sentential_forms so

 they too filter out variations in the order It is not hard to

 show that every parse_tree has associated_with it a unique

 leftmost and a unique rightmost_derivation



 Ambiguity

 amb-cfg-subsect



 From Section ambig-subsect a grammar that produces more

 than one parse_tree for some sentence is said to be ambiguous Put_another way an ambiguous_grammar is one that

 produces more_than one leftmost_derivation or_more than one

 rightmost_derivation for the same sentence



 ex

 expr-ambig-trees-ex The arithmetic expression grammar

 (expr-ambig-display) permits two distinct leftmost

 derivations for the sentence





 center

 tabularl_l l_l l_l





















 tabular



 center

 The corresponding parse_trees appear in

 Fig expr-ambig-trees-fig

 figurehtfb

 center



 Two parse_trees for ididid

 expr-ambig-trees-fig

 center

 figure



 Note_that the parse_tree of Fig expr-ambig-trees-fig(a)

 reflects the commonly assumed precedence of and

 while the tree of Fig expr-ambig-trees-fig(b) does_not

 That is it is customary to treat operator as having

 higher_precedence than corresponding to the fact that we

 would normally evaluate an expression like as

 rather_than as

 ex



 For most parsers it is desirable that the grammar be made

 unambiguous for if it is not we cannot uniquely determine which

 parse_tree to select for a sentence In other cases it is

 convenient to use carefully chosen ambiguous_grammars together

 with disambiguating rules that throw away undesirable

 parse_trees leaving only one tree for each sentence



 Verifying the Language Generated by a Grammar

 ver-grammar-subsect



 Although compiler designers rarely do so for a complete

 programming-language grammar it is useful to be_able to reason

 that a given set of productions generates a particular language

 Troublesome constructs can be studied by writing a concise

 abstract grammar and studying the language that it generates We

 shall construct such a grammar for conditional_statements below



 A proof that a grammar generates a language has two_parts

 show that every string generated_by is in and conversely

 that every string in can indeed be generated_by



 ex

 parens-exConsider the following grammar



 displayparens-display

 (parens-display)

 display



 It may not be initially apparent but this simple grammar

 generates all strings of balanced parentheses and only such

 strings To_see why we_shall show first that every sentence

 derivable_from is balanced and then that every_balanced

 string is derivable_from To show that every sentence

 derivable_from is balanced we use an inductive proof on the

 number of steps in a derivation



 The basis is

 The only string of terminals derivable_from in one step is the

 empty_string which surely is balanced



 Now assume that all derivations of fewer_than steps produce

 balanced sentences and consider a leftmost_derivation of exactly

 steps Such a derivation must_be of the form



 center







 center



 The derivations of and from take fewer_than steps

 so by the inductive_hypothesis and are balanced

 Therefore the string must_be balanced

 That is it has an equal number of left and right parentheses and every

 prefix has at_least as many left parentheses as right



 Having thus shown that any string_derivable from is balanced

 we must next show that every_balanced string is derivable_from

 To do so use induction on the length of a string



 If the string is of length 0 it must_be which is balanced



 First observe that every_balanced string has even length

 Assume that every_balanced string of length less_than is

 derivable_from and consider a balanced_string of length

 Surely begins_with a left_parenthesis Let

 be the shortest nonempty prefix of having an equal

 number of left and right parentheses Then can be written as

 where both and are balanced Since

 and are of length less_than they are derivable_from

 by the inductive_hypothesis Thus we can find a derivation

 of the form



 center







 center

 proving that is also derivable_from



 ex



 Context-Free Grammars Versus RegularExpressions

 cfg-re-subsect



 Before leaving this_section on grammars and their properties we

 establish that grammars are a more_powerful notation than regular

 expressions Every construct that can be described by a regular

 expression can be described by a grammar but not vice-versa

 Alternatively every regular language is a context-free language but

 not vice-versa



 For_example the regular_expression

 and the grammar



 center



 tabularl_l l

















 tabular



 center

 describe the same language the set of strings of 's and 's

 ending in



 We can construct mechanically a grammar to recognize the same

 language as a nondeterministic_finite automaton (NFA) The

 grammar above was constructed from the NFA in Fig_nfa1-fig using

 the following construction



 enumerate



 For each state of the NFA create

 a nonterminal



 If state has a transition to state

 on input add the production If

 state goes to state on input add the production





 If is an accepting_state add





 If is the start_state make be

 the start_symbol of the grammar



 enumerate



 On the other_hand

 the language with an equal

 number of 's and 's is a prototypical example of a language

 that can be described by a grammar but not by a regular_expression

 To_see why suppose were the language defined by some regular

 expression We could construct a DFA with a finite number of

 states say to accept Since has only states

 for an input beginning with more_than 's must

 enter some state twice say as in Fig pumping-fig

 Suppose that the path from back to itself is labeled with a

 sequence Since is in the language there

 must_be a path labeled

 from to an accepting_state But then

 there is also a path from the initial_state through to

 labeled as shown in Fig pumping-fig Thus

 also accepts which is not in the language

 contradicting the assumption that is the language accepted_by





 figurehtfb

 center



 DFA_accepting both and

 pumping-fig

 center

 figure



 Colloquially we say that finite_automata cannot count

 meaning that a finite_automaton cannot accept a language like

 that would_require it to keep count

 of the number of 's before it_sees the 's Likewise a

 grammar can count two items but not three as we_shall see when

 we consider non-context-free language_constructs in

 Section non-cfl-subsect



 sexer

 cfg-exer

 Consider the context-free_grammar



 center



 center

 and the string



 itemize



 a)_Give a leftmost_derivation for the string

 b)_Give a rightmost_derivation for the string

 c) Give a parse_tree for the string

 d)_Is the grammar ambiguous or unambiguous Justify your

 answer

 e) Describe the language generated_by this grammar



 itemize

 sexer



 exer

 more-cfgs-exer

 Repeat_Exercise cfg-exer for each of the following grammars and

 strings



 itemize



 a) with string 000111

 b) with string

 c) with string

 d) with string



 e) and with

 string

 f) with string



 g) The following grammar for boolean_expressions



 center

 tabularl_l l

 bexpr bexpr or bterm bterm



 bterm bterm and bfactor bfactor



 bfactor not bfactor ( bexpr )

 true false



 tabular

 center



 itemize

 exer



 exer

 Design grammars for the following languages



 itemize



 a) The set of all strings of 0s and 1s such that every 0 is

 immediately followed_by at_least one 1



 b) The set of all strings of 0s and 1s that are palindromes that is the string reads the same backward as

 forward



 c) The set of all strings of 0s and 1s with an

 equal number of 0s and 1s



 d) The set of all strings of 0s and 1s with an

 unequal number of 0s and 1s



 e) The set of all strings of 0s and 1s in which 011

 does_not appear as a substring



 f) The set of all strings of 0s and 1s of the

 form where and and are of the same length



 itemize

 exer



 hsexer

 grammar-braces-exer

 There is an extended grammar notation in common use In this notation

 square and curly_braces in production_bodies

 are metasymbols (like or ) with the

 following meanings



 itemize



 ) Square braces around a grammar symbol or symbols denotes

 that these constructs are optional Thus production

 has the same effect as the two productions and





 ) Curly braces around a grammar symbol or symbols says_that

 these symbols may be repeated any number of times including zero times

 Thus has the same effect as the infinite sequence

 of productions and

 so on



 itemize

 Show that these two extensions do_not add power to grammars that is

 any language that can be generated_by a grammar with these extensions

 can be generated_by a grammar without the extensions

 hsexer



 exer

 Use the braces described in Exercise grammar-braces-exer to

 simplify the following grammar for statement blocks and

 conditional_statements



 center

 tabularl_c l

 stmt if expr_then stmt_else stmt



 if expr_then stmt



 begin stmtList end



 stmtList stmt stmtList stmt



 tabular

 center

 exer



 hexer

 Extend the idea of Exercise grammar-braces-exer to allow any

 regular_expression of grammar_symbols in the body of a production Show

 that this extension does_not allow grammars to define any new languages

 hexer



 hexer

 A grammar symbol (terminal or nonterminal) is useless if there

 is no derivation of the form That is can

 never appear in the derivation of any sentence



 itemize



 a)_Give an algorithm to eliminate from a grammar

 all productions containing

 useless symbols



 b) Apply your algorithm to the grammar

 center

 tabularl_l l









 1



 tabular

 center

 itemize

 hexer



 exer

 The grammar in Fig decl-gram-fig generates declarations for a

 single numerical identifier these declarations involve four different

 independent properties of numbers



 figurehtfb



 centertabularl_l l

 stmt declare id optionList



 optionList optionList option



 option mode scale precision

 base



 mode real complex



 scale fixed floating



 precision single double



 base binary decimal



 tabularcenter



 A grammar for multi-attribute declarations

 decl-gram-fig



 figure



 itemize



 a)

 Generalize the grammar of Fig decl-gram-fig by allowing options

 for some fixed and for

 where can be either or

 Your grammar should use only grammar_symbols and have a total

 length of productions that is



 b)

 The grammar of Fig decl-gram-fig and its generalization in

 part (a) allow declarations that are contradictory andor redundant

 such_as



 center

 declare foo real fixed real floating

 center

 We could insist that the syntax of the language forbid such

 declarations that is every declaration generated_by the grammar has

 exactly one value for each of the options

 If we do then for any fixed there is only a finite number of legal

 declarations

 The language of legal declarations thus has a grammar (and also a

 regular expression) as any

 finite language does

 The obvious grammar in which the start_symbol has a production for

 every legal declaration has productions and a total production

 length of

 You must do better a total production length that is



 c)

 Show that any grammar for part (b) must have a total production length of at

 least



 d)

 What does part (c) say about the feasibility of enforcing nonredundancy

 and noncontradiction among options in declarations via the syntax of the

 programming_language



 itemize

 exer

 Heap Management

 heap-sect



 The heap is the portion of the store that is used for data that

 lives indefinitely or until the program explicitly deletes it

 While local_variables typically become inaccessible when their

 procedures end many languages enable us to create objects or

 other data whose existence is not tied to the procedure activation

 that creates them For_example both C and Java give the

 programmer new to create objects that may be passed - or

 pointers to them may be passed - from procedure to procedure so

 they continue to exist long after the procedure that created them

 is gone Such objects are stored on a heap



 In this_section we discuss the memory_manager the

 subsystem that allocates and deallocates space within the heap it

 serves as an interface between application programs and the

 operating_system For languages_like C or C that deallocate

 chunks of storage manually (ie by explicit statements of

 the program such_as free or delete) the memory

 manager is also responsible_for implementing deallocation



 In Section secgc we discuss garbage_collection

 which is the process of finding spaces within the heap that are no

 longer used by the program and can therefore be reallocated to

 house other data items For languages_like Java it is the

 garbage_collector that deallocates memory When it is required

 the garbage_collector is an important subsystem of the memory

 manager



 The Memory Manager



 The memory_manager keeps_track of all the free_space in heap

 storage at all times It performs two basic functions



 itemize



 Allocation When a program requests memory for a

 variable or objectfootnoteIn what_follows we_shall

 refer to things requiring memory space as objects even if

 they are not true objects in the object-oriented_programming

 sensefootnote the memory_manager produces a chunk of

 contiguous heap memory of the requested size If possible it

 satisfies an allocation request using free_space in the heap if

 no chunk of the needed size is available it seeks to increase the

 heap_storage space by getting consecutive bytes of virtual_memory

 from the operating_system If space is exhausted the memory

 manager passes that information back to the application program



 Deallocation The memory_manager returns deallocated

 space to the pool of free_space so it can reuse the space to

 satisfy other allocation requests Memory managers typically do

 not return memory to the operating_system even if the program's

 heap usage drops



 itemize



 Memory management would be simpler if (a) all allocation requests

 were for chunks of the same size and (b) storage were released

 predictably say first-allocated first-deallocated There_are

 some languages such_as Lisp for which condition (a) holds pure

 Lisp uses only one data element - a two-pointer cell - from

 which all data_structures are built Condition (b) also holds in

 some situations the most_common being data that can be allocated

 on the run-time_stack However in most languages neither (a) nor

 (b) holds in general Rather data elements of different sizes are

 allocated and there is no good way to predict the lifetimes of

 all allocated_objects



 Thus the memory_manager must_be prepared to service in any

 order allocation and deallocation requests of any size ranging

 from one byte to as large as the program's entire address space



 Here are the properties we desire of memory managers



 itemize



 Space Efficiency A memory_manager should minimize

 the total heap space needed by a program Doing_so allows larger

 programs to run in a fixed virtual address space Space efficiency

 is achieved by minimizing fragmentation discussed in

 Section alloc-frag-subsect



 Program Efficiency A memory_manager should make good

 use of the memory subsystem to allow programs to run_faster As

 we_shall see in Section secmemoryhierarchy the time taken

 to execute an instruction can vary widely depending_on where

 objects are placed in memory Fortunately programs tend to

 exhibit locality a phenomenon discussed in

 Section temp-spa-loc-subsect which refers to the nonrandom

 clustered way in which typical_programs access memory By

 attention to the placement of objects in memory the memory

 manager can make better use of space and hopefully make the

 program run_faster



 Low Overhead Because memory allocations and

 deallocations are frequent operations in many programs it is

 important that these operations be as efficient as possible That

 is we_wish to minimize the overhead - the fraction of

 execution time_spent performing allocation and deallocation

 Notice_that the cost of allocations is dominated by small

 requests the overhead of managing large objects is less

 important because it usually can be amortized over a larger

 amount of computation



 itemize





 The Memory Hierarchy of a Computer

 secmemoryhierarchy



 Memory management and compiler optimization must_be done with an

 awareness of how memory behaves Modern machines are designed so

 that programmers can write correct programs without concerning

 themselves with the details of the memory subsystem However the

 efficiency of a program is determined not just by the number of

 instructions executed but also by how long it takes to execute

 each of these instructions The time taken to execute an

 instruction can vary significantly since the time taken to access

 different parts of memory can vary from nanoseconds to

 milliseconds Data-intensive programs can therefore benefit

 significantly from optimizations that make good use of the memory

 subsystem As we_shall see in Section temp-spa-loc-subsect

 they can take_advantage of the phenomenon of locality - the

 nonrandom behavior of typical_programs



 The large variance in memory access times is due to the

 fundamental limitation in hardware technology we can build small

 and fast storage or large and slow storage but not storage that

 is both large and fast It is simply impossible today to build

 gigabytes of storage with nanosecond access times which is how

 fast high-performance processors run Therefore practically all

 modern computers arrange their storage as a memory

 hierarchy A memory hierarchy as shown in

 Fig fighier-fig consists of a series of storage elements

 with the smaller faster ones closer to the processor and the

 larger slower ones further away



 figurehtb



 Typical

 Memory Hierarchy Configurations fighier-fig

 figure



 Static and Dynamic RAM Most random-access memory

 is dynamic which means that it is built of very_simple

 electronic circuits that lose their charge (and thus forget

 the bit they_were storing) in a short time These circuits need

 to be refreshed - that is their bits read and rewritten -

 periodically On the other_hand static RAM is designed with

 a more_complex circuit for each bit and consequently the bit

 stored can stay indefinitely until it is changed Evidently a

 chip can store more bits if it uses dynamic-RAM circuits than if

 it uses static-RAM circuits so we tend to see large main memories

 of the dynamic variety while smaller memories like caches are

 made from static circuits



 Typically a processor has a small number of registers whose

 contents are under software control Next it has one or_more

 levels of cache usually made out of static RAM that are

 kilobytes to several megabytes in size The next level of the

 hierarchy is the physical (main) memory made out of hundreds of

 megabytes or gigabytes of dynamic RAM The physical_memory is then

 backed up by virtual_memory which is implemented_by gigabytes of

 disks Upon a memory access the machine first looks for the data

 in the closest (lowest-level) storage and if the data is not

 there looks in the next higher level and so on



 Registers are scarce so register usage is tailored for the

 specific applications and managed by the code that a compiler

 generates All the other levels of the hierarchy are managed

 automatically in this way not only is the programming task

 simplified but the same program can work effectively across

 machines with different memory configurations With each memory

 access the machine searches each level of the memory in

 succession starting_with the lowest level until it locates the

 data Caches are managed exclusively in hardware in order to

 keep up with the relatively fast RAM access times Because disks

 are relatively slow the virtual_memory is managed by the

 operating_system with the assistance of a hardware structure

 known_as the translation lookaside buffer



 Data is transferred as blocks of contiguous storage To amortize

 the cost of access larger blocks are used with the slower levels

 of the hierarchy Between main memory and cache data is

 transferred in blocks known_as cache_lines which are

 typically from 32 to 256 bytes long Between virtual_memory

 (disk) and main memory data is transferred in blocks known_as

 pages typically between 4K and 64K bytes in size



 Cache Architectures How do we know if a cache

 line is in a cache It would be too_expensive to check every

 single line in the cache so it is common practice to restrict the

 placement of a cache_line within the cache This restriction is

 known_as set associativity A cache is -way set

 associative if a cache_line can reside only in locations The

 simplest cache is a 1-way associative cache also known_as a direct-mapped cache In a direct-mapped cache data with memory

 address can be placed only in cache address mod where

 is the size of the cache Similarly a -way set associative

 cache is divided_into sets where a datum with address can

 be mapped only to the location mod in each set Most

 instruction and data caches have associativity between 1 and 8

 When a cache_line is brought into the cache and all the possible

 locations that can hold the line are occupied it is typical to

 evict the line that has_been the least recently used



 Locality in Programs

 temp-spa-loc-subsect



 Most programs exhibit a high degree of locality that is

 they spend_most of their time executing a relatively_small

 fraction of the code and touching only a small_fraction of the

 data We_say that a program has temporal_locality if the

 memory_locations it accesses are likely to be accessed again

 within a short period of time We_say that a program has spatial_locality if memory_locations close to the location

 accessed are likely also to be accessed within a short period of

 time



 The conventional wisdom is that programs_spend 90 of their time

 executing 10 of the code Here is why



 itemize



 Programs often contain many instructions that are never

 executed Programs built with components and libraries use only a

 small_fraction of the provided functionality Also as requirements

 change and programs evolve legacy systems often contain many

 instructions that are no_longer used



 Only a small_fraction of the code that could be invoked is

 actually executed in a typical run of the program For_example

 instructions to handle illegal inputs and exceptional cases

 though critical to the correctness of the program are seldom

 invoked on any particular run



 The typical program spends most of its time executing

 innermost_loops and tight recursive_cycles in a program



 itemize



 Locality allows_us to take_advantage of the memory hierarchy of a

 modern computer as shown in Fig fighier-fig By placing

 the most_common instructions and data in the fast-but-small

 storage while leaving the rest in the slow-but-large storage we

 can lower the average memory-access time of a program

 significantly



 It has_been found that many programs exhibit both temporal and

 spatial_locality in how they access both instructions and data

 Data-access patterns however generally show a greater variance

 than instruction-access patterns Policies such_as keeping the

 most_recently used data in the fastest hierarchy work well for

 common programs but may not work well for some data-intensive

 programs - ones that cycle through very_large arrays for

 example



 We often cannot_tell just from looking_at the code which

 sections of the code will be heavily used especially for a

 particular input Even_if we know which instructions are executed

 heavily the fastest cache often is not large_enough to hold all

 of them at the same time We must therefore adjust the contents of

 the fastest storage dynamically and use it to hold instructions

 that are likely to be used heavily in the near future



 Optimization Using the Memory Hierarchy



 The policy of keeping the most_recently used instructions in the

 cache tends to work well in other_words the past is generally a

 good predictor of future memory_usage When a new instruction is

 executed there is a high probability that the next instruction

 also will be executed This phenomenon is an example of spatial

 locality One effective technique to improve the spatial_locality

 of instructions is to have the compiler place basic_blocks

 (sequences of instructions that are always executed sequentially)

 that are likely to follow each other contiguously - on the same

 page or even the same cache_line if possible Instructions

 belonging to the same loop or same function also have a high

 probability of being executed together(As a machine

 fetches a word in memory it is relatively inexpensive to prefetch the next several contiguous words of memory as_well

 Thus a common memory-hierarchy feature is that a multiword block

 is fetched from a level of memory each time that level is

 accessed)



 We can also improve the temporal and spatial_locality of data

 accesses in a program by changing the data layout or the order of

 the computation For_example programs that visit large amounts

 of data repeatedly each time performing a small amount of

 computation do_not perform well It is better if we can bring

 some data from a slow level of the memory hierarchy to a faster

 level (eg disk to main memory) once and perform all the

 necessary computations on this data while it resides at the faster

 level This concept can be applied recursively to reuse data in

 physical_memory in the caches and in the registers



 Reducing Fragmentation

 alloc-frag-subsect



 At the beginning of program execution the heap is one contiguous

 unit of free_space As the program allocates and deallocates

 memory this space is broken up into free and used chunks of

 memory and the free_chunks need not reside in a contiguous area

 of the heap We refer to the free_chunks of memory as holes With each allocation request the memory_manager must

 place the requested chunk of memory into a large-enough

 hole Unless a hole of exactly the right size is found we need to

 split some hole creating a yet smaller hole



 With each deallocation request the freed chunks of memory are

 added back to the pool of free_space We coalesce

 contiguous holes into larger holes as the holes can only get

 smaller otherwise If we are not careful the free memory may end up

 getting fragmented consisting of large_numbers of small

 noncontiguous holes It is then possible that no hole is large

 enough to satisfy a future request even_though there may be

 sufficient aggregate free_space



 Best-Fit and Next-Fit Object Placement



 We reduce fragmentation by controlling how the memory_manager

 places new objects in the heap It has_been found empirically that

 a good strategy for minimizing fragmentation for real-life

 programs is to allocate the requested memory in the smallest

 available hole that is large_enough This best-fit

 algorithm tends to spare the large holes to satisfy subsequent

 larger requests An alternative called first-fit where an

 object is placed in the first (lowest-address) hole in which it

 fits takes less time to place objects but has_been found

 inferior to best-fit in overall performance



 To implement best-fit_placement more efficiently we can separate

 free_space chunks into bins according to their sizes One

 practical idea is to have many more bins for the smaller sizes

 because there are usually many more small objects For_example

 the Lea memory_manager used in the GNU C compiler gcc

 aligns all chunks to 8-byte boundaries There is a bin for every

 multiple of 8-byte chunks from 16 bytes to 512 bytes

 Larger-sized bins are logarithmically spaced (ie the minimum

 size for each bin is twice that of the previous bin) and within

 each of these bins the chunks are ordered by their size There is

 always a chunk of free_space that can be extended by requesting

 more pages from the operating_system Called the wilderness

 chunk this chunk is treated by Lea as the largest-sized bin

 because of its extensibility



 Binning makes it easy to find the best-fit chunk



 itemize



 If as for small sizes requested from the Lea memory

 manager there is a bin for chunks of that size only we may take

 any chunk from that bin



 For sizes that do_not have a private bin we find the one

 bin that is allowed to include chunks of the desired size Within

 that bin we can use either a first-fit or a best-fit strategy

 ie we either look for and select the first chunk that is

 sufficiently large or we spend more time and find the smallest

 chunk that is sufficiently large Note_that when the fit is not

 exact the remainder of the chunk will generally need to be placed

 in a bin with smaller sizes



 However it may be that the target bin is empty or all

 chunks in that bin are too small to satisfy the request for space

 In that case we simply repeat the search using the bin for the

 next larger size(s) Eventually we either find a chunk we can

 use or we reach the wilderness chunk from which we can

 surely obtain the needed space possibly by going to the operating

 system and getting additional pages for the heap



 itemize



 While best-fit_placement tends to improve space utilization it

 may not be the best in terms of spatial_locality Chunks

 allocated at about the same time by a program tend to have similar

 reference patterns and to have similar lifetimes Placing them

 close_together thus improves the program's spatial_locality One

 useful adaptation of the best-fit algorithm is to modify the

 placement in the case when a chunk of the exact requested size

 cannot be found In this case we use a next-fit strategy

 trying to allocate the object in the chunk that has last been

 split whenever enough space for the new object remains in that

 chunk Next-fit also tends to improve the speed of the allocation

 operation



 Managing and Coalescing Free Space



 When an object is deallocated manually the memory_manager must

 make its chunk free so it can be allocated again In some

 circumstances it may also be possible to combine (coalesce)

 that chunk with adjacent_chunks of the heap to form a larger

 chunk There is an advantage to doing_so since we can always use

 a large chunk to do the work of small chunks of equal total size

 but many small chunks cannot hold one large object as the

 combined chunk could



 If we keep a bin for chunks of one fixed size as Lea does for

 small sizes then we may prefer not to coalesce adjacent blocks of

 that size into a chunk of double the size It is simpler to keep

 all the chunks of one size in as many pages as we need and never

 coalesce them Then a simple allocationdeallocation scheme is to

 keep a bitmap with one bit for each chunk in the bin A 1

 indicates the chunk is occupied 0 indicates it is free When a

 chunk is deallocated we change its 1 to a 0 When we need to

 allocate a chunk we find any chunk with a 0 bit change that bit

 to a 1 and use the corresponding chunk If there are no free

 chunks we get a new page divide it into chunks of the

 appropriate size and extend the bit_vector



 Matters are more_complex when the heap is managed as a whole

 without binning or if we are willing to coalesce adjacent_chunks

 and move the resulting chunk to a different bin if necessary

 There_are two data_structures that are useful to support

 coalescing of adjacent free blocks



 itemize



 Boundary Tags At both the low and high ends of each

 chunk whether free or allocated we keep vital information At

 both ends we keep a freeused_bit that tells whether or not the

 block is currently allocated (used) or available (free) Adjacent

 to each freeused_bit is a count of the total_number of bytes in

 the chunk



 A Doubly Linked Embedded Free List The free_chunks

 (but_not the allocated chunks) are also linked in a doubly_linked

 list The pointers for this list are within the blocks

 themselves say adjacent to the boundary_tags at either end Thus

 no additional space is needed for the free_list although its

 existence does place a lower_bound on how small chunks can get

 they must accommodate two boundary_tags and two pointers even if

 the object is a single byte The order of chunks on the free_list

 is left unspecified For_example the list could be sorted by

 size thus facilitating best-fit_placement



 itemize



 ex

 Figure boundary-tag-fig shows part of a heap with three

 adjacent_chunks and Chunk of size 100 has

 just been deallocated and returned to the free_list Since we

 know the beginning (left end) of we also know the end of the

 chunk that happens to be immediately to 's left namely in

 this example The freeused_bit at the right_end of is

 currently so too is free We may therefore coalesce

 and into one chunk of 300 bytes



 figurehtfb





 Part of a heap and a doubly_linked free_list

 boundary-tag-fig

 figure



 It might be the case that chunk the chunk immediately to

 's right is also free in which case we can combine all of

 and Note_that if we always coalesce chunks when we

 can then there can never be two adjacent free_chunks so we never

 have to look further than the two chunks adjacent to the one being

 deallocated In the current case we find the beginning of by

 starting_at the left end of which we know and finding the

 total_number of bytes in which is found in the left boundary

 tag of and is 100 bytes With this information we find the

 right_end of and the beginning of the chunk to its right At

 that point we examine the freeused_bit of and find that it

 is for used hence is not available for coalescing



 Since we must coalesce and we need to remove one of them

 from the free_list The doubly_linked free-list structure lets_us

 find the chunks before and after each of and Notice_that

 it should not be assumed that physical neighbors and are

 also adjacent on the free_list Knowing the chunks preceding and

 following and on the free_list it is straightforward to

 manipulate pointers on the list to replace and by one

 coalesced chunk

 ex



 Automatic garbage_collection can eliminate fragmentation

 altogether if it moves all the allocated_objects to contiguous

 storage The interaction between garbage_collection and memory

 management is discussed in more_detail in

 Section secgcrelocate



 Manual Deallocation Requests

 manual-mem-subsect



 We close this_section with manual memory_management where the

 programmer must explicitly arrange for the deallocation of data

 as in C and C Ideally any storage that will no_longer be

 accessed should be deleted Conversely any storage that may be

 referenced must not be deleted Unfortunately it is hard to

 enforce either of these properties In_addition to considering the

 difficulties with manual deallocation we_shall describe some of

 the techniques programmers use to help with the difficulties



 Problems with Manual Deallocation



 Manual memory_management is error-prone The common mistakes take

 two forms failing ever to delete data that cannot be referenced

 is called a memory-leak error and referencing deleted data

 is a dangling-pointer-dereference error



 It is hard for programmers to tell if a program will never

 refer to some storage in the future so the first common mistake

 is not deleting storage that will never be referenced Note_that

 although memory_leaks may slow down the execution of a program due

 to increased memory_usage they do_not affect program correctness

 as_long as the machine does_not run out of memory Many programs

 can tolerate memory_leaks especially if the leakage is slow

 However for long-running programs and especially nonstop

 programs like operating systems or server code it is critical

 that they not have leaks



 Automatic garbage_collection gets rid of memory_leaks by

 deallocating all the garbage Even with automatic_garbage

 collection a program may still use more memory than necessary A

 programmer may know that an object will never be referenced even

 though references to that object exist somewhere In that case

 the programmer must deliberately remove references to objects that

 will never be referenced so the objects can be deallocated

 automatically



 Being overly zealous about deleting objects can lead to even worse

 problems than memory_leaks The second common mistake is to

 delete some storage and then try to refer to the data in the

 deallocated storage Pointers to storage that has_been

 deallocated are known_as dangling pointers Once the freed

 storage has_been reallocated to a new variable any read write

 or deallocation via the dangling pointer can produce seemingly

 random effects We refer to any operation such_as read write or

 deallocate that follows a pointer and tries to use the object it

 points to as dereferencing the pointer



 Notice_that reading through a dangling pointer may return an

 arbitrary value Writing through a dangling pointer arbitrarily

 changes the value of the new variable Deallocating a dangling

 pointer's storage means that the storage of the new variable may

 be allocated to yet another variable and actions on the old and

 new variables may conflict with each other



 Unlike memory_leaks dereferencing a dangling pointer after the

 freed storage is reallocated almost_always creates a program error

 that is hard to debug As a result programmers are more inclined

 not to deallocate a variable if they are not certain it is

 unreferencable



 An Example Purify Rational's Purify is one of

 the most_popular commercial tools that helps programmers find

 memory access errors and memory_leaks in programs Purify

 instruments binary code by_adding additional instructions to check

 for errors as the program executes It keeps a map of memory to

 indicate where all the freed and used spaces are Each allocated

 object is bracketed with extra space accesses to unallocated

 locations or to spaces between objects are flagged as errors This

 approach finds some dangling pointer references but not when the

 memory has_been reallocated and a valid object is sitting in its

 place This_approach also finds some out-of-bound array_accesses

 if they happen to land in the space inserted at the end of the

 objects



 Purify also finds memory_leaks at the end of a program execution

 It searches the contents of all the allocated_objects for possible

 pointer values Any object without a pointer to it is a leaked

 chunk of memory Purify reports the amount of memory leaked and

 the locations of the leaked objects We may compare Purify to a

 conservative_garbage collector which will be discussed in

 Section secconservativegc



 A related form of programming error is to access an illegal

 address Common examples of such errors_include dereferencing

 null pointers and accessing an out-of-bounds array_element It is

 better for such errors to be detected than to have the program

 silently corrupt the results In_fact many security violations

 exploit programming_errors of this type where certain program

 inputs allow unintended access to data leading to a hacker

 taking control of the program and machine One antidote is to

 have the compiler insert checks with every access to make_sure it

 is within bounds The compiler's optimizer can discover and

 remove those checks that are not really necessary because the

 optimizer can deduce that the access must_be within bounds







 Programming Conventions and Tools



 We_now present a few of the most_popular conventions and tools

 that have_been developed to help programmers cope with the

 complexity in managing memory



 itemize



 Object ownership is useful when an object's lifetime

 can be statically reasoned about The idea is to associate an owner with each object at all times The owner is a pointer to

 that object presumably belonging to some function invocation The

 owner (ie its function) is responsible_for either deleting the

 object or for passing the object to another owner It is possible

 to have other nonowning pointers to the same object these

 pointers can be overwritten any time and no deletes should ever

 be applied through them This convention eliminates memory_leaks

 as_well as attempts to delete the same object twice However it

 does_not help solve the dangling-pointer-reference problem

 because it is possible to follow a nonowning pointer to an object

 that has_been deleted



 Reference_counting is useful when an object's lifetime

 needs to be determined dynamically The idea is to associate a

 count with each dynamically allocated object Whenever a reference

 to the object is created we increment the reference_count

 whenever a reference is removed we decrement the reference_count

 When the count goes to zero the object can no_longer be

 referenced and can therefore be deleted This technique however

 does_not catch useless circular data_structures where a

 collection of objects cannot be accessed but their reference

 counts are not zero since they refer to each other For an

 illustration of this problem see

 Example ref-count-cycle-ex Reference_counting does

 eradicate all dangling-pointer references since there are no

 outstanding references to any deleted objects Reference_counting

 is expensive because it imposes an overhead on every operation

 that stores a pointer



 Region-based allocation is useful for collections of

 objects whose lifetimes are tied to specific phases in a

 computation When objects are created to be used only within some

 step of a computation we can allocate all such objects in the

 same region We then delete the entire region once that

 computation step completes This region-based allocation

 technique has limited applicability However it is very efficient

 whenever it can be used instead of deallocating objects one at a

 time it deletes all objects in the region in a wholesale fashion



 itemize





 exer

 Suppose the heap consists of seven chunks starting_at address 0

 The sizes of the chunks in order are 80 30 60 50 70 20 40

 bytes When we place an object in a chunk we put it at the high

 end if there is enough space remaining to form a smaller chunk (so

 that the smaller chunk can easily remain on the linked_list of

 free space) However we cannot tolerate chunks of fewer that 8

 bytes so if an object is almost as large as the selected chunk

 we give it the entire chunk and place the object at the low end of

 the chunk If we request space for objects of the following sizes

 32 64 48 16 in that order what does the free_space list look

 like after satisfying the requests if the method of selecting

 chunks is



 itemize



 a) First fit b) Best fit



 itemize

 exer

 Affine Array Indexes

 ch11index



 We have_seen how to characterize the iteration_space of a loop_nest as a

 matrix-vector calculation A companion theory also based_on

 matrix-vector computation describes the array_accesses that are

 performed within the loop_nest The theory depends_on array_accesses

 being affine - linear functions of the loop_indexes

 We_shall see that the theory can be used to determine important

 properties of the array_accesses such_as whether or not two accesses

 can ever modify the same array_element That information in turn is

 essential if we are to decide_whether or not it is safe to parallelize

 or reorder the code within the loop_nest



 Affine Functions



 A function of one or_more variables

 is affine if it can be_expressed as a sum of a constant plus

 constant multiples of the variables ie

 where and are

 constants

 An array_access in a loop_nest is affine if its indexes are all

 affine functions of the loop_indexes

 When discussing affine array_accesses we are allowed to treat symbolic

 constants (ie loop-invariant variables) as constants



 ex

 Some examples of affine array_accesses are Ai Aij1

 A2i13j-10 A0 and Aii

 If is a symbolic_constant for a loop_nest then Anin-j is

 another example of an affine array_access

 However if and are loop_indexes then Aij is not an

 affine access

 ex



 It should be apparent that most array_accesses found in real code are

 affine For_example all the accesses in Section ch11mm on

 matrix_multiplication are affine

 Since the theory of affine array_accesses gives_us substantial

 code-improving power that we do_not have for non-affine accesses we

 shall_concentrate exclusively on the affine case



 Affine Access Functions

 access-function-subsect



 Each affine array_access can be described by two matrices and two

 vectors The first matrix-vector pair are the B and b that

 describe the iteration_space for the access as in the inequality of

 Equation (bib-eq) The second pair which we usually refer to as

 F and f represent the function(s) of the loop-index variables

 that produce the array index(es) used in the various dimensions of the

 array_access



 Formally we represent an array_access in a loop_nest that uses a vector

 of index variables i by the four-tuple



 it maps a vector i within

 the bounds



 Bi b 0



 to the array_element location



 Fi f





 ex

 exaccesses

 In Fig array-accesses-fig

 are some common array_accesses expressed in matrix

 notation The two loop_indexes are

 and and these form the vector i

 Also and

 are arrays with 1_2 and 3 dimensions respectively



 figurehtfb



 center

 tabularlc

 Access Affine Expression







 Xi-1













































 Yij

















































 Yjj1

















































 Y12

















































 Z1i2ij

















































 tabular

 center



 Some array_accesses and their matrix-vector representations

 array-accesses-fig



 figure



 The first access Ai-1 is represented_by a matrix

 F and a vector f of length 1 Notice_that when we perform

 the matrix-vector multiplication and add in the vector f we are

 left with a single function which is exactly the formula for the

 access to the one-dimensional array Also notice the third

 access Yjj1 which after matrix-vector multiplication and

 addition yields a pair of functions These are the indexes

 of the two dimensions of the array_access



 Finally let_us observe the fourth access Y12 This access is a

 constant and unsurprisingly the matrix_F is all 0's

 Thus the vector of loop_indexes i does_not appear in the access

 function

 ex



 Summary of Applications of the Theory of Affine Array

 Access



 To lay the groundwork for the developments to follow let_us consider

 two different examples of array_accesses



 ex

 good-access-ex

 Some accesses are easy to understand For_instance if and are

 the indexes in a 2-deep_loop nest then the access Aij

 touches each element of the array that lies within the two-dimensional

 polyhedron defined by the loops' limits Moreover it is evident that

 different values of the pair access different elements of the

 array and so the order in which these accesses occur is completely

 flexible Thus we are free to reorder these accesses if we like

 ex



 ex

 bad-access-ex

 On the other_hand in the same loop_nest as

 Example good-access-ex the access B5ij is more

 problematic Different values of the pair can cause the same

 array_element to be accessed For_instance choosing and accesses

 just as choosing does



 Further it is not obvious which elements of are accessed

 For_example suppose the loop_bounds on and define the polyhedron

 of Fig figitspace We need to find which values of can

 be constructed from integers and inside that

 polyhedron(Recall this polyhedron is defined by the inequalities

 and )

 In this simple case you may discover with a moment's calculation that B0 through B22 may all be accessed as can B24

 through B27 and B30 through B32 Other 's including B23 B28 and B29 cannot be accessed

 ex



 While the programmer can eyeball certain accesses and figure out

 answers such_as those in Examples good-access-ex and

 bad-access-ex if we_want the compiler to generate optimized parallel

 code we need general solutions to certain important

 questions

 Here is a brief synopsis of the topics to follow the

 reader may wish to try to evaluate the quantities discussed below for

 the accesses in Example exaccesses



 enumerate



 Dimension of the data_accessed We find the dimension

 by considering only the access function and ignoring

 the loop_bounds which are assumed to be large The dimension

 corresponds to the concept of the rank of a matrix in linear

 algebra For_instance Example good-access-ex is a 2-dimensional

 access function while Example bad-access-ex is a 1-dimensional

 access More subtlely Aii1 is a 1-dimensional access



 Repeated accesses to the same array_element

 As in Example bad-access-ex a loop_nest can read or write the

 same array_element repeatedly Knowing about such dynamic reuse of

 data allows_us to optimize locality The key mathematical idea is the

 null_space of a matrix



 Bounds on the region of data_accessed We find the smallest convex

 polyhedron that encloses all the array_elements accessed This problem

 is solved by considering both the access functions and the loop_bounds

 Important tools are the idea of projection of inequalities and the

 Fourier-Motzkin_elimination algorithm



 Data_dependence We wish to know precisely if two accesses can

 refer to the same_location

 This information determines the execution-order

 constraints that must_be satisfied by any schedule of array_accesses

 To solve this

 problem we consider the access functions and loop_bounds together

 taking into consideration also that indexes can be only integers

 This problem is equivalent to linear integer programming



 enumerate



 Dimensionality of Data Accessed

 ch11dims



 The dimensionality of an access is essentially the order of magnitude of

 the

 data it touches and therefore an estimate of the locality of this

 access To describe the concept of dimensionality formally

 we start by assuming that all loops have iterations regardless

 of the actual bounds on the loop

 Thus a -deep_loop

 nest has iterations A reference to an -dimensional array

 in a -deep_loop nest is specified_by an matrix_F

 and an -element vector f Clearly such a reference can

 access at most array_elements but in fact the

 number is sometimes smaller



 The volume of data_accessed is where is the rank of

 F that is

 the largest_number of columns (or equivalently rows) of

 F that are linearly_independent A set of vectors is linearly_independent if none of the vectors can be written as a

 linear_combination of finitely many other vectors in the set



 ex

 rank-ex

 Consider the matrix





 arrayl_l l

 1_2 3



 5 7 9



 4_5 6



 2 1_0



 array





 Notice_that the second row is the sum of the first and third rows while the

 fourth row is the third row minus twice the second row However the

 first and third rows are linearly_independent neither is a multiple of

 the other

 Thus the rank of the matrix is 2



 We could also draw this conclusion by_examining the columns The third

 column is twice the second column minus the first column On the other

 hand any two columns are linearly_independent Again we conclude that

 the rank is 2

 ex



 ex

 Let_us look_at the array_accesses in Fig array-accesses-fig The

 first access Xi-1 has dimension 1 because the rank of the

 matrix is 1 That is the one row is linearly_independent as

 is the first column



 The second access Yij has dimension 2 The_reason is that

 the matrix





 arrayl_l

 1_0



 0_1



 array





 has two independent_rows (and_therefore two independent columns of

 course)

 The third access Yjj1 is of dimension 1 because the matrix





 arrayl_l

 0_1



 0_1



 array





 has_rank 1 Note_that the two rows are identical so only one is

 linearly_independent Equivalently the first column is 0 times the second

 column so the columns are not independent

 Intuitively in a large square array

 the only elements_accessed lie along a one-dimensional line just above

 the main diagonal



 The fourth access Y12 has dimension 0 because a matrix of all

 0's has_rank 0 Note_that for such a matrix we cannot find a linear

 sum of even one row that is nonzero

 Finally the last access Zii2ij has dimension 2 Note

 that in the matrix for this access





 arrayl_l

 0_0



 1_0



 2 1



 array





 the last two rows are linearly_independent neither is a multiple of the

 other

 However the first row is a linear sum of the other two rows with both

 coefficients 0

 ex



 Data Reuse

 ch11reuse



 A central application of the theory of array_accesses is to discover

 when accesses can refer to the same array_element This information

 constrains when we can parallelize accesses or reorder instructions in

 various ways There_are several distinctions that we must make in our

 discussion and we begin_with an outline of these issues



 itemize



 First we need to distinguish_between the access as an instruction in a

 program eg x Aij from the execution of this instruction

 many_times as we execute the loop_nest For emphasis we may refer to

 the statement itself as a static_access while the various

 iterations of the statement as we execute its loop_nest are called dynamic_accesses



 Second we may ask regarding a single (static) access whether different

 iterations of that access can touch the same array_element Such a

 situation is called self reuse We may also find that two

 different (static) accesses may refer to the same array_elements or to

 array_elements on the same cache_line Such a phenomenon is called group_reuse



 Finally there is a distinction to be made between accessing the same

 element of the array and accessing different elements on the same cache

 line Access of the same element is called temporal reuse

 access of different elements on the same cache_line is spatial

 reuse



 itemize



 Self Reuse



 A reference in a -deep_loop nest with rank

 accesses data elements in iterations so on average

 iterations must refer to the same array_element Which

 iterations access the same data Suppose an access in this loop_nest is

 represented_by matrix-vector combination F and f

 Let i and be two

 iterations that refer to the same array_element Then



 Rearranging terms



 F(i-i') 0





 There is a well-known concept from linear_algebra that characterizes

 when i and satisfy the above equation

 The set of all solutions to the equation

 is called the null_space of F Thus two iterations

 refer to the same array_element if the difference of their loop-index

 vectors belongs to the null_space of matrix_F



 It is easy to see

 that the null vector

 always satisfies

 That is two iterations surely refer to the same array_element if

 their difference is 0 in other_words if they are really the same

 iteration Also the null_space is truly a vector space That is if

 and then

 and



 If the matrix_F is fully ranked that is its

 rank is then the null_space of F

 consists of only the null vector In

 that case iterations in a loop_nest all refer to different data In

 general the dimension of the null_space also known_as the nullity is If then for each element

 there is a -dimensional space

 of iterations that access that element



 The null_space can be represented_by its basis_vectors A

 -dimensional null_space is represented_by independent vectors

 any vector that can be_expressed as a linear_combination of the basis

 vectors belongs to the null_space



 ex

 null-space-ex

 Let_us reconsider the matrix of Example rank-ex





 arrayl_l l

 1_2 3



 5 7 9



 4_5 6



 2 1_0



 array





 We determined in that example that the rank of the matrix is 2 thus the

 nullity is To_find

 a basis for the null_space which in this case must_be a single nonzero

 vector of length 3 we may suppose a vector in the null_space to be

 and try to solve the equation





 arrayl_l l

 1_2 3



 5 7 9



 4_5 6



 2 1_0



 array





 arrayl

 x



 y



 z



 array







 arrayl

 0



 0



 0



 array





 If we multiply the first two rows by the vector of unknowns we get the

 two equations



 center

 tabularr_l l

 0



 0



 tabular

 center

 We could write the equations that come from the third and fourth rows as

 well but because there are no three linearly_independent rows we know

 that the additional equations add no new constraints on and

 For_instance the equation we get from the third row

 can be obtained_by subtracting the first equation from the second



 We must eliminate as many variables as we can from the above equations

 Start by using the first equation to solve for that is

 Then substitute for in the second equation to get or

 Since and it follows that

 Thus the vector is really We may pick any

 nonzero value of to form the one and only basis_vector for

 the null_space For_example we may choose and use as

 the basis of the null_space

 ex



 figurehtb



 center

 tabularlcccc

 Access Affine Expression

 Rank Null- Basis Vectors



 ity of Null Space







 Xi-1



































 1 1























 Yij







































 2 0











 Yjj1







































 1 1























 Y12







































 0 2





































 Z1i2ij











































 2 0







 tabular

 center



 Rank and nullity of affine accesses

 fignullity

 figure



 ex

 exnullity

 The rank nullity and null_space for each of the references in

 Example exaccesses are shown in Fig fignullity

 Observe that the sum of the rank and nullity is all the cases

 the depth of the loop_nest 2 Since the accesses Yij and

 Z1i2ij have a rank of 2 all iterations refer to different

 locations



 Accesses Xi-1 and Yjj1 both have rank-1 matrices

 so iterations refer to the same_location In the former

 case entire rows in the iteration_space refer to the same_location

 In other_words iterations that differ only in the dimension share

 the same_location which is succinctly represented_by the basis of the

 null_space 01 For Yjj1 entire columns in the iteration

 space refer to the same_location and this fact is succinctly

 represented_by the basis of the null_space 10



 Finally the

 reference Y12 refers to the same_location in all the

 iterations The null_space corresponding has 2 basis_vectors 01

 10 meaning that all pairs of iterations in the loop_nest refer to

 exactly the same_location

 ex



 Self-Spatial Reuse

 self-spatial-subsect



 The analysis of spatial_reuse depends_on the data layout of the

 matrix C matrices are laid_out in row major order and Fortran

 matrices are laid_out in column major order(Recall the

 discussion of array layout in Section serial-mm-subsect)

 In other_words array_elements

 Xij and Xij1 are consecutive in C and

 Xij and Xi1j are consecutive in Fortran Without

 loss of generality in the rest of the chapter we_shall adopt the C

 (row-major) array layout



 As a first approximation we consider two array_elements to share the

 same cache_line if and only if they share the same row in a

 two-dimensional_array More_generally in an array of dimensions we

 take array_elements to share a cache_line if they differ only in the

 last dimension Since for a typical array and cache many array

 elements can fit in one cache_line there is significant speedup to be

 had by accessing an entire row in order even_though strictly speaking

 we occasionally have to wait to load a new cache_line



 The trick to discovering and taking_advantage of self-spatial_reuse is

 to drop the last_row from the access matrix_F If the resulting

 truncated

 matrix has_rank that is less_than the depth of the loop_nest then we can

 assure_spatial locality by making sure that the innermost_loop varies

 only the last coordinate of the array



 ex

 spatial-reuse-ex

 Consider the last access Z1i2ij in

 Fig fignullity

 If we delete the last_row we are left with the truncated_matrix





 arrayrr

 0_0



 1_0



 array





 The rank of this matrix is evidently 1 and since the loop_nest has

 depth 2 there is the opportunity for spatial_reuse In this case since

 is the inner-loop_index the inner_loop visits consecutive

 elements of the array stored in row-major_order Making the

 inner-loop_index will not yield spatial

 locality since as changes both the second

 and third dimensions change

 ex



 The general rule for determining whether there is self-spatial_reuse is

 as_follows As always we assume that the loop_indexes correspond to

 columns of the access matrix in order with the outermost_loop first

 and the innermost_loop last Then in order for there to be spatial

 reuse the vector must_be in the null_space of the

 truncated_matrix

 The_reason is that if this vector is in the null_space then when we fix

 all loop_indexes but the innermost one we know that all dynamic

 accesses during one run through the inner_loop vary in only the last

 array index If the array is stored in row-major_order then these

 elements are all near one another perhaps in the same cache_line



 ex

 Note_that (transposed as a column vector) is in the null_space

 of the truncated_matrix of Example spatial-reuse-ex Thus as

 mentioned there we expect that with as the inner-loop_index there

 will be spatial_locality On the other_hand if we reverse the order of

 the loops so is the inner_loop then the access matrix becomes





 arrayrr

 0_0



 0_1



 array





 Now is not in the null_space of this matrix Rather the null

 space is generated_by the basis_vector Thus as we suggested

 in Example spatial-reuse-ex we do_not expect spatial_locality if

 is the inner_loop



 We should observe however that the test for being

 in the null_space is not quite sufficient to assure_spatial locality

 For_instance suppose the access were not Z1i2ij but Z1i2i50j Then only every fiftieth element of would be

 accessed during one run of the inner_loop and we would not reuse a

 cache_line unless it were long enough to hold more_than 50 elements

 ex



 Group Reuse



 Let_us begin_with a simple example that reviews the concepts of

 self-temporal and self-spatial_reuse and motivates the concept of group

 reuse

 The 2-deep_loop nest of Fig group-reuse-fig has two array

 accesses

 Aij and Ai-1j

 Observe that these two accesses are both characterized by the matrix





 arrayl_l

 1_0



 0_1



 array





 like the second access Yij in Fig fignullity

 This matrix has_rank 2 so there is no self-temporal_reuse



 figurehtfb



 verbatim

 for_(i 1_i n_i)

 for_(j 1_j n_j)

 Aij Ai-1j





 verbatim



 A loop_nest exhibiting group_reuse

 group-reuse-fig



 figure



 However each access exhibits self-spatial_reuse As described in

 Section self-spatial-subsect when we delete the bottom row of the

 matrix we are left with only the top row which has_rank 1

 Since is in the null_space of this truncated_matrix we expect

 spatial_reuse As each incrementation of inner-loop_index

 increases the second array index by one we in fact do access adjacent

 array_elements and will make maximum use of each cache_line



 Although there is no self-temporal_reuse for either access observe

 that the two references Aij and Ai-ij access

 almost the same set of array_elements That is

 there is group-temporal reuse because the data read by access

 Ai-1j is

 the same as the data written by access Aij

 except for the case

 This simple pattern applies to the entire iteration_space and can

 be_exploited to improve data_locality in the code Formally

 discounting the loop_bounds the two accesses Aij and Ai-1j refer to the same_location

 in iterations and respectively provided





 arrayrr

 1_0



 0_1



 array





 arrayr

 i1



 j1



 array







 arrayr

 0



 0



 array







 arrayrr

 1_0



 0_1



 array





 arrayr

 i2



 j2



 array







 arrayr

 -1



 0



 array





 Rewriting the terms





 arrayrr

 1_0



 0_1



 array





 arrayr

 i1 - i2



 j1 - j2



 array







 arrayr

 -1



 0



 array





 That is and



 Notice_that the reuse occurs along the -axis of the iteration_space

 That is the iteration occurs iterations after the

 iteration

 Thus many iterations are executed before the data written is

 reused This data may or may not still be in the cache If the cache

 manages to hold two consecutive rows of matrix then access Ai-1j does_not miss in the cache and the total_number of cache

 misses for the entire loop_nest is where is the number of

 elements per cache_line Otherwise there will be

 twice as many misses since both static accesses require a new cache

 line for each dynamic_accesses

 This analysis suggests that this code may not

 take full advantage of the reuse in the program and can be optimized

 to improve its locality



 In_general given two

 dynamic_accesses and

 reuse of the same data requires that



 F1i1 f1 F2i2 f2



 In_particular there is a lot of easily exploitable data_locality if

 In that case we need to solve



 F1(i1-i2) (f1-f2)



 for the difference

 Suppose v is one solution to this equation Then if w is

 any vector in the null_space of is also a

 solution and in fact those are all the solutions to the equation



 ex

 group-reuse-ex

 Suppose there are two accesses Aijij and



 center

 Ai1j-1ij

 center

 in a 3-deep loop_nest with indexes and from the outer to

 The inner_loop Then two accesses and

 reuse the same element whenever





 arrayrrr

 1_0 0



 0_1 0



 1 1_0



 array





 arrayr

 i1



 j1



 k1



 array







 arrayr

 0



 0



 0



 array







 arrayrrr

 1_0 0



 0_1 0



 1 1_0



 array





 arrayr

 i2



 j2



 k2



 array







 arrayr

 1



 -1



 0



 array







 One_solution to this equation for a vector

 is that is

 and (It is interesting to

 observe that although there is a solution in this case there would be

 no solution if we changed one of the third components from to

 That is in the example as given both accesses touch those

 array_elements that lie in the 2-dimensional subspace defined by the

 third component is the sum of the first two components If we changed

 to none of the elements_touched by the second access

 would lie in and there would be no reuse at all)

 However the null_space of the matrix



 F



 arrayrrr

 1_0 0



 0_1 0



 1 1_0



 array





 is generated_by the basis_vector that is the third loop

 index can be arbitrary Thus all solutions to the equation are

 of the form for some Put_another way a dynamic access

 to Aijij characterized by loop_indexes and is

 reused not only by other dynamic_accesses Aijij with the

 same values of and but different values of but also by

 dynamic_accesses Ai1j-1ij with loop_index values

 and any value of

 ex



 Although we_shall not do so here we

 can reason_about group-spatial reuse analogously

 As per the discussion of self-spatial_reuse we simply drop

 the last dimension from consideration



 The extent of reuse is different for the different categories of

 reuse Self-temporal reuse gives the most benefit a reference with

 a -dimensional null_space reuses the same data times The

 extent of self-spatial_reuse is limited by the length of the cache

 line Finally the extent of group_reuse is limited by the number

 references in a group sharing the reuse



 exer

 In Section self-spatial-subsect we commented that we get spatial

 locality if the innermost_loop varies only the the last coordinate of an

 array_access However that assertion depended on our_assumption that

 the array was stored in row-major_order What condition would assure

 spatial_locality if the array were stored in column-major_order

 exer



 hexer

 In Example group-reuse-ex we observed that the the existence of

 reuse between two similar accesses depended heavily on the particular

 expressions for the coordinates of the array Generalize our

 observation there to determine for which functions there is

 reuse between the accesses Aijij and Ai1j-1

 hexer

 Induction Variable Analysis

 secinduction



 As_discussed in Section_secopt-sources it is very important that

 we optimize loops in programs One particularly useful analysis is

 induction_variable analysis A variable is said to be an induction_variable of a loop if it is incremented by a constant

 (positive or negative) amount in each iteration of a loop

 Induction variables enable loop optimizations

 like strength_reduction which replaces more_expensive operations

 (multiplications) with

 cheaper operations (additions) They also may help to make explicit the

 relationships among

 different variables which can improve our ability to do other

 kinds of optimizations

 In the following we first introduce the concept of induction

 variable analysis and discuss its uses Then we present a

 region-based algorithm for finding induction_variables



 Introduction of the Problem



 In many programming_languages like Fortran or Pascal there is a

 loop_index in a do- or for-loop that is incremented or

 decremented exactly once per iteration

 and by the same amount at each iteration

 These loop indices are obviously induction_variables There is more

 flexibility in how we write a for construct in

 C but for-loops are often written in the form



 verbatim

 for_(i lb i ub ic)

 verbatim

 where

 is changed only once inside a loop and is updated by a constant

 amount each iteration So C for-loops also often have loop indices

 that are induction_variables

 but they may require a little bit more work to detect



 figurehtfb



 verbatim

 for (_)

 b 4 a

 d b_c

 e d 5

 a a 1



 verbatim



 A simple loop exhibiting different_kinds of induction

 variables

 ind-var-loop-fig



 figure



 Loop indices are not the only induction_variables in a loop though

 Consider the loop in Fig ind-var-loop-fig

 Although all the assignment statements in the loop appear to be very

 different variables and

 are all induction_variables

 Variable is known_as a basic_induction variable because

 it is defined by a recurrence that increments the variable by a

 constant amount (A recurrence is an assignment that uses a

 variable's old value to define a new value) The values of the

 other variables and

 are all greater_than their values in the

 previous iteration by exactly 4 Note_that has not been

 redefined inside the loop which is why is incremented by the

 same amount each time



 For the purpose of having a canonical way to specify the values of

 induction_variables we create for each loop a normalized

 loop_index (or just loop index) a variable that counts the number of

 iterations completed A normalized loop_index has value 0 at

 the beginning of the first iteration 1 at the beginning of the second

 iteration and so on

 In the future we_shall treat normalized loop_indexes as if they_were

 true variables of the program at hand knowing that we could create

 variables to hold their value if needed



 Let be the normalized index for the

 loop above and let

 be the initial value of variable at the beginning of

 the loopfootnoteIn what_follows we_shall be talking_about

 data-flow_values that are

 maps such_as which describe the value of each variable at a

 program point

 In this preliminary discussion you may think of as the map

 associated_with the entry to the loop in questionfootnote

 The value of at the beginning of iteration

 is We

 can rewrite all the assignments in the above example as



 center

 tabularl

 b



 d



 e



 a



 tabular

 center

 Notice_that although the value of changes in the

 loop the value of representing the initial value of

 remains unchanged regardless of the number of iterations that

 may have_been executed We refer to symbols like and

 whose values do_not change inside a loop as symbolic

 constants That is these variables can be treated_like constants in

 the loop even_though we do_not know the value of the constants



 All the assignments in the example above are of the form



 c0 c1 v1 c2 v2

 where

 is a constant and is either a symbolic_constant or a loop

 index An expression of the form

 is known_as an affine_expression of variables

 (We say that the expression is linear if )

 Thus we can define the problem of finding

 induction_variables as the problem of finding all variables that are

 assigned affine_expressions of symbolic_constants and loop

 indices

 Before we discuss the induction-analysis algorithm let_us first look

 at an example to find out what the algorithm must do



 figurehtb

 figureuullmanalsuch9figsind-regioneps

 An_example flow_graph for the induction_variable analysis problem

 figind-region

 figure



 ex

 exind

 Shown in Fig_figind-region is a flow_graph with a single loop

 and its region_hierarchy There_are five regions

 representing the five basic_blocks in the loop

 represents the only loop body in the

 program and

 represents the entire loop



 Let be the normalized index variable of the loop in the program

 and let be the initial value of variable

 at the beginning of the

 loop We see that there are two basic_induction variables in the

 loop and

 Variable is incremented by 1 at the beginning

 of the loop and is incremented by 1 on both the then-

 and else-branches of the conditional_statement Thus and

 hold values and respectively at

 the start of iteration for all



 Although the values of and are altered during the

 execution of an iteration their values at any program point can be

 expressed_as affine_expressions of and symbolic_constants

 For_example has the

 value before the assignment to in block

 and has value afterward Similarly has the

 value before the assignments to in blocks

 and and afterward



 Variable is a loop-invariant variable since it is not

 redefined in the loop In this example all the other variables are

 assigned values that can be_expressed as affine_expressions of the

 loop_index and symbolic_constants Variable is assigned

 thus it has the value at the end of

 block in iteration By the time we execute c 4a in

 block has the value

 Thus gets the value Similarly variables

 and are assigned affine_expressions

 and

 respectively in block



 Unlike the basic_induction variables and the values

 of and are not affine_expressions of the

 loop_index and symbolic_constants at all program points in the

 loop (although they may be so expressible at some points)

 First these variables keep their initial values whatever they

 happen to be until their first assignments in the loop For_example if

 we use variable in block it has the value of

 in the first iteration and in

 iteration for all Variable does_not have an

 affine value in block either because its assignment is in block

 which does_not dominate

 Thus might hold its

 initial value or one of the values assigned in some earlier

 iteration when control_reaches

 ex



 Uses of Induction_Variables



 Information on induction_variables has many uses First

 rewriting array indices as affine_expressions of loop indices and

 symbolic_constants sometimes makes it clearer which array_elements are

 accessed in the loop We_shall see the use of this

 information throughout Chapter affine-ch

 in Section parallel-sect



 Second we can use information on induction_variables to implement

 strength_reduction in particular the replacement of certain

 multiplications by additions

 For_instance we

 can optimize the assignment of e 4d in Fig_figind-region as

 follows We assign to a new variable

 outside the loop Replace e 4d by e t and

 insert statement t t8 at the end of the iteration The

 multiplication inside the loop is thus replaced_by



 enumerate



 An addition

 inside the loop and



 Several multiplications and additions outside

 the loop



 enumerate

 Assuming that multiplication is more_expensive than

 addition and the loop iterates several_times

 this optimization has significant benefits



 Another optimization made possible by induction

 variables is loop test replacement

 Consider the code below



 verbatim

 for_(j 0 jn_j)

 k 0

 k 4



 verbatim

 From induction-variable analysis we know that the value of is

 at the beginning of each iteration We can

 eliminate variable from the loop altogether

 by rewriting the code as in Fig test-replacement-fig

 While the loop body still appears to contain only two statements what

 we have eliminated is the assignment j that was implied by the

 for-loop

 Note_that we must assign j_n after the loop only in the case

 that is used elsewhere in the program



 figurehtfb



 verbatim

 t k 4n

 while (k t)

 k 0

 k 4



 j_n

 verbatim



 Elimination of variable from computations within a

 loop

 test-replacement-fig



 figure



 Primitive Transfer Function Operations

 prim-tf-subsect



 The problem of finding induction_variables is particularly amenable to

 region-based_analysis We use a transfer_function to capture how the

 body of a loop affects the values of the variables It keeps_track of

 all values that can be_expressed as affine_expressions of values at

 the beginning of an iteration

 We find all the basic_induction variables by checking whether is

 incremented by the same constant amount no_matter how control returns to

 the header That is we compare the transfer_functions for at each

 of the nodes that have back_edges to the header of the loop

 We then compute the

 transfer_function that represents the values of the variables

 at the beginning of any

 iteration If

 the number of iterations is a known constant

 this information can then be used in the analysis of an outer_loop as

 well_as telling about induction_variables in the present loop



 A data-flow value in this problem is a map which

 maps each variable in the program to a value denoted_by A

 transfer_function takes as input a value map and returns as a result

 also a value map The output map of a transfer_function maps a

 variable to an affine_expression of the values of variables in the

 input map whenever possible and to a special symbol when it

 is not



 Cautions Regarding Value Maps

 A subtlety in the way we describe the maps that are data-flow_values for

 induction-variable_elimination is that we have options regarding how

 values are expressed

 When is the map for the input of a region is really

 just whatever value variable happens to have on entry to the

 region

 When is the map that describes the situation on exit from a block

 within the region or that describes the situation going_around a loop

 then we try very_hard to express as an affine

 transformation of values that are described by another map - the map

 associated_with the region entry



 You_should also observe the proper interpretation of expressions like

 where is a transfer_function a map and

 a variable

 As is conventional in mathematics we apply functions from the left

 meaning that we first compute which is a map

 Since a map is a function we may then apply it to variable to

 produce a value



 Transfer Function of a Statement



 To_compute the transfer_function of an assignment_statement we

 interpret the semantics of the statement and determine if the newly

 defined variable can be_expressed as an affine_expression of the input

 values The values of all other variables remain unchanged

 The transfer_function of statement denoted is defined as

 follows



 enumerate



 If is not an assignment_statement then is the

 identity_function



 If is an assignment to variable then is

 defined by















 arrayll

 m(v) for all variables v x



 c0 c1 m(y) c2 m(z) if is assigned c0 c1y c2z



 otherwise

 array





 enumerate

 The expression is intended to

 represent all the possible forms of expressions_involving arbitrary

 variables

 and that may appear on the

 right_side of an assignment to and that give a value

 that is an affine transformation on prior values of variables

 These expressions are





 and



 Note_that in many_cases one or_more of and are 0



 ex

 If the assignment is x_yz then and

 If the assignment is x y5 then and

 ex



 Composition of Two Transfer_Functions



 Remember that each transfer_function is described by how it transforms

 an arbitrary map which we conventionally refer to as into another

 map

 To_compute we replace in the

 definition of by We replace all operations on

 values with That is



 enumerate



 If then



 If then















 arrayll

 if

 f1(m)(vi) for some i0



 c0 ci f1(m)(vi) otherwise

 array





 enumerate



 ex

 The transfer_function for basic_block in Fig_figind-region can be

 computed by_composing the transfer_function for statement

 d ab with that for statement e 4d

 The transfer_function for d ab transforms any map into a map

 such that and

 otherwise

 The transfer_function for e 4d transforms any map into the

 map defined by and

 otherwise



 When we use from the first mapping as for the second mapping

 we get the composition of the two mappings which is

 the transfer_function (as in the general

 region-based_analysis of Section region-sect)

 for the entire block





 fB5(m)(v)



 arrayll

 m(a) m(b) if



 4m(a) 4m(b) if



 m(v) otherwise

 array





 The only tricky computation is for variable where the output

 map is defined by



 This affine function agrees with the description of

 as it must since is in fact the map

 ex



 Meet of Two Transfer_Functions



 When computing the meet of

 two functions the value of a variable is unless

 the two functions map the variable to the same

 value

 Thus



 (f1 f2)(m)(v)



 arrayll

 f1(m)(v) if f1(m)(v) f2(m)(v)



 otherwise

 array







 ex

 For the flow_graph in Fig_figind-region and the body_region

 we can compute the transfer_functions for the blocks as_follows

 certainly

 The other formulas are



 center

 tabularl_l













 tabular

 center

 The transfer_functions themselves are

 shown in Fig figind-region2(b) the region itself is reprised in

 Fig figind-region2(a)



 figurehtb

 fileuullmanalsuch9figsind-region2eps



 center

 (a) The region

 center



 center

 tabularllllll

















































 tabular

 center



 center

 (b) The transfer_functions

 center



 Transfer_functions computed for region

 figind-region2

 figure



 The computation becomes tricky only when we compute

 Since has two predecessors we need first to compute



 These two transfer_functions agree on most variables

 For_example



 (fR6B2 fR6B4)(m)(b)m(b)1



 since both transfer_functions have that value for

 However



 (fR6B2 fR6B4)(m)(c)

 since

 while c



 ex



 Transfer_Functions for Describing the th Iteration of a

 Loop



 The closure of a transfer_function is defined for our

 induction-variable-analysis framework according to the definition of

 Section secprimitive-ops that is as an_infinite meet

 We need the closure operation in Algorithm algregion as always



 However in induction-variable analysis it is not enough to compute

 Function combines the information

 of all iterations together but here we also wish to know

 for each loop

 whether the value of a variable can be written as

 an affine_expression of loop_index and symbolic_constants

 If is the transfer_function summarizing the effect of one

 iteration of this loop then the values of the variables at the beginning of

 iteration are given by the map

 where and represents the input map of the

 loop



 Note_that when

 that is the initial value at the entry

 of the loop is naturally the value at the beginning of the first

 iteration We separate the variables into three categories



 enumerate



 A variable is a basic_induction variable if it is defined by a

 recurrence which increments the variable by a constant amount That

 is a basic_induction variable satisfies



 and therefore for every value of





 A variable is a symbolic_constant if its value remains unchanged at

 the end of an iteration

 That is and therefore

 for all



 in all other cases since we cannot use an

 affine_expression involving index variable

 to represent the value at the beginning of the first

 iteration and at all subsequent_iterations



 enumerate



 ex

 loop-r7-ex

 let_us consider the loop of region from

 Fig_figind-region the loop is repeated in

 Fig figind-region4

 There is only one back

 edge which leads from to Thus the function

 representing the computation from the start of the loop that is region

 to the beginning

 of iteration is given by

 As shown in Fig figind-region2(b) variables and

 have the values and at the end of

 the iteration and are thus basic_induction variables Also is a

 symbolic_constant since it is not changed in the loop

 Thus

 maps to to



 to

 and all other variables to



 figurehtb

 figureuullmanalsuch9figsind-region4eps

 The region

 figind-region4

 figure



 We can also compute the transfer_function that describes

 the values on exit from block during the th_iteration of the

 loop of region

 The formula is



 fR7Bji fR6BjfR6B5i-1



 Notice how this expression describes paths that go times_around

 the loop completely and then go to block during the th

 iteration



 For_instance we compute as_follows

 Note from Fig figind-region2(b) that



 Since

 we know that

 When we compose the functions we

 therefore get



 fR6B3i(m)(c) 4(m(a)i-1)4

 4m(a)4i



 Thus on the first iteration at the end of

 has value on the

 second the value and so on

 ex



 Besides wanting to know the value of the variables in iteration

 we are also interested in finding the values of the variables after all

 the iterations are executed If the number of iterations is known to

 be a constant then the values of variables on exit from the loop

 will be given by the transfer_functions

 where represents the transfer_function for

 the loop body These transfer_functions

 can be computed by simply substituting the constant for the variable





 ex

 Suppose that the loop from Example loop-r7-ex had a loop

 exit at that is there were an edge from to some block

 outside the loop

 Suppose also that we knew this exit would occur on the 10th iteration

 ie

 Then the transfer_function that describes how values transform going

 from the entry of to the entry of the external successor to block

 is

 For_instance

 That is the value of variable at the hypothetical exit from

 at block is four times the value has on entry to the

 loop plus 40

 ex



 In the case_where the

 number of iterations is unknown the value at the start of the last

 iteration is given by In this case the only kind of

 variables whose values can still be_expressed in the affine form are

 the loop-invariant variables



 f(m)(v)



 arrayll

 m(v) if f(m)(v) m(v)



 otherwise

 array









 Finding Induction_Variables with a Region-Based_Analysis



 We wish to discover for each loop the induction_variables and

 constants of that loop

 The algorithm for doing_so using the data-flow_framework we have

 developed in this_section is a small modification of

 Algorithm algregion that performs region-based data-flow_analysis

 in general

 The changes all concern how loop regions are handled there is no change

 to how body regions are treated

 These changes are



 enumerate



 In line_(6) of Fig region-alg-fig we compute to be the meet

 of all predecessors within the loop region of the loop header and

 then in line_(7) we use the closure of

 We should instead compute first



 Use the expressions to determine_whether any variable

 is an induction_variable or loop constant



 If we determine that a loop region

 iterates exactly times then we use

 in place of in line_(7)

 That change has an important consequence it causes which is

 computed as to describe only the situation that

 pertains the last time through block within the loop

 rather_than the situation after any iteration

 No problems are presented since the only time we need is if

 has a successor outside

 In that case since we have determined that that successor can only be

 reached on the th_iteration of we have the proper transfer

 function for this use



 Since the discovery of induction_variables occurs during the bottom-up

 part of Algorithm algregion it is not necessary to perform the

 final steps (3) and (4) of that algorithm



 enumerate



 figurehtb

 figureuullmanalsuch9figsind-ex2eps

 An_example showing two nested_loops

 figind-ex2

 figure



 ex

 Shown in Fig figind-ex2 is an example of a program with two

 nested_loops The needed transfer_functions are shown

 in Fig figind-ex2-tf

 The first three lines describe the transfer_functions of the three leaf

 regions and which correspond to the blocks

 and respectively

 For_instance leaves unchanged while it replaces

 by



 The inner_loop which we refer to as region has a body_region

 consisting only of the block so we use as the body_region

 for

 Since is the only predecessor of the header of that is

 inside the body_region we first compute which is

 in line_(4) of Fig figind-ex2-tf

 This transfer_function tells_us that is an induction_variable of

 and is a constant



 figurehtb

 center

 tabularr lll





 1) 0



 2)



 3)



 4)



 5)



 6) 0



 7) 10



 8) 10



 9) 10



 10)



 11) 0



 12) 10



 13) 10



 tabular

 center

 Region-based_analysis of induction_variables in Fig figind-ex2

 figind-ex2-tf

 figure



 Now we must infer that the loop iterates exactly 10 times

 Inferences of this type are generally tricky

 In the easiest case the language semantics tells_us directly from the

 source code that this loop iterates 10 times

 If we must work from the intermediate_code as is often the case then

 we can reason as_follows

 We see in that control leaves the loop as_soon as the iteration

 count satisfies

 But what is

 A reaching-definitions analysis would tell_us that when control_reaches

 the loop from the only external predecessor has the

 value 0

 In this case we could observe that and

 draw the same conclusion

 Either way we know that if is the map at the entry to then

 and therefore is the first value of for which





 We_conclude that it is permissible to use in place of

 the closure of this transfer_function to compute the transfer_functions

 associated_with

 There is only one which is computed as

 and shown in line_(5) of Fig figind-ex2-tf



 The next three transfer_functions in Fig figind-ex2-tf are for

 body_region

 This body_region consists of subregions and in a

 straight line so we compute the transfer_functions in that order by

 composition

 It is important to note_that since the loop of exits after

 exactly 10 iterations properly asserts that

 when control_reaches and this knowledge is used in

 and



 Our final task is to analyze the loop region

 Since the only internal predecessor of the header of is we

 compute which is shown in line_(9)

 That tells_us is a constant in this loop while is an

 induction_variable with an increment of 10



 Unfortunately we cannot deduce anything about the number of iterations

 of loop

 Therefore we are obliged to use when computing the

 transfer_functions for

 This function shown in line (10) has both variables since

 we cannot say anything about their values at the loop header

 after 0 iterations and in fact

 they may not even be defined

 However as the last three lines of Fig figind-ex2-tf show we

 can deduce something about the value of at the interior_nodes

 of the loop

 These transfer_functions would not be used unless were actually

 part of some larger flow_graph

 ex





 The induction_variable algorithm can be turned_into a more general

 symbolic analysis by following the algorithm with a top-down

 phase that propagates constant values and affine_expressions from a

 parent region to its subregions For_example the value of

 at the loop entry may itself be a constant Knowing that

 information may allow_us to simplify the computations Or we may find

 out that both and have the same value

 Knowing that will allow_us to tell say addresses and

 refer to the same_location If the value of is

 however at the entry of the loop we do_not want to

 propagate to all the expressions inside the loop

 We still want to exploit the fact that the variable is a symbolic

 constant within the loop This can be achieved by creating a new

 variable to stand_for the value at the entry of the loop How such an

 algorithm can be defined is left as an_exercise for the readers





 exer

 In Section prim-tf-subsect we suggested that many different_kinds

 of assignment statements to a variable could have their transfer

 function expressed_as



 Give the values of and for the following

 assignments



 itemize



 a) x 10

 b) x_y - z

 c) x 10 - y

 d) x 10y



 itemize

 exer

 Inherited Attributes

 inherit-sect



 In this_section we maintain semantic information_about

 identifiers using inherited and synthesized_attributes Recall

 from Section trans-sect that an attribute can be any

 quantity associated_with a construct eg a value a type or a

 reference to an object that holds information_about an_identifier

 Repeating the definition from Section trans-sect an

 attribute is said to be inherited if its value at a

 parse-tree_node is determined from attribute values at the node

 itself its parent and its siblings in the parse_tree An

 attribute is synthesized if its value at a node is determined from

 attribute values at the children of the node



 Environments

 Information about an_identifier s can be partitioned_into

 lexical information which includes the lexeme_s and

 semantic information which includes the type of s The

 lexical_analyzer in Section lexan-sect maintains lexical

 information using a data_structure called a symbol_table Lexical

 information is independent of how an_identifier is used An

 identifier s can be used to denote a string in one part of a

 program and to denote an_integer say a size in another part of

 the program The lexeme_s remains the same independent of

 whether the identifier is used as a string or as a size The

 symbol-table_entry for s also remains the same independent

 of the use of s



 Semantic information_about an_identifier will be maintained using

 a data_structure called an environment which is separate

 from the symbol_table An environment is a mapping from

 identifiers to their attributes Conceptually a declaration

 center

 int s

 center

 will be handled by using an environment to map the identifier

 s to the type Integer In_practice instead of identifiers

 we_shall work with the symbols for identifiers instead of types

 we_shall work with data_structures that hold types



 For concreteness suppose environments are implemented as linked

 lists of key-value pairs where the key is a symbol and the value

 is an attribute A linked_list can be manipulated efficiently

 using a pointer to the head of the list so environments can

 themselves be passed readily as attributes



 The next example_illustrates the distinction between symbol_tables

 and environments



 ex

 env-ex Consider the declaration

 center

 int s

 center

 When the lexical_analyzer isolates lexeme_s it looks up

 the symbol for s in the symbol_table if this is the first

 occurrence of s then it creates a new symbol Suppose

 sym is the symbol for identifier s The lexical

 analyzer passes sym to the parser



 For this declaration suppose that the parser creates an object

 n The object n will typically be a node in an

 intermediate_representation of the source_program The object

 n holds the symbol sym and the attributes from the

 declaration such_as the type integer (and perhaps the line number

 of the declaration for use in error messages)



 The parser also adds a key-value_pair with key sym and value

 n to the environment Suppose that the resulting environment

 is called env



 Now_consider a subsequent use of the identifier say in the

 expression

 center

 s s 1

 center

 The lexeme_s is now in the symbol_table so the lexical

 analyzer finds and passes to the parser the symbol sym for

 each of the two_occurrences of the identifier s



 As we_shall see using attributes the parser can pass the

 environment env so it is available when the expression

 ss1 is processed The parser can use the environment env

 to map symbol sym to the object n constructed for the

 declaration of identifier s



 A different declaration say

 center

 String_s

 center

 will result in the creation of an object different from

 and an environment that maps the same symbol

 sym to the new object Subsequent uses of s using

 environment will denote the object

 ex



 In terms of parse_trees the declaration of an_identifier may

 appear one subtree but uses of the identifier may appear in other

 subtrees Environments are a way of propagating or passing

 information discovered in one subtree to another subtree where it

 can be used



 Environments as Attributes



 Information about identifiers can be propagated from point of

 declaration to point of use by passing environments as inherited

 attributes When a declaration

 center

 int s

 center

 is reached a key-value_pair with the symbol for s as the

 key is added to the environment This environment is then passed

 or propagated using attributes When a use of s is reached

 information_about s is looked up in the environment by using

 the symbol for s as the key



 The running_example in this_section uses let expressions as a

 convenient construct for illustrating the handling of identifiers

 The following let expression declares x to have the value

 and uses x in the expression x3

 center

 let x 22 in x3

 center

 The value of this expression is Let expressions can be

 nested as in the following which also evaluates to

 center

 let x 22 in let y x3 in y

 center



 Let expressions combine the declaration and use of identifiers so

 they allow an example to be worked out in detail The handling of

 environments carries over to the discussion of intermediate_code

 generation for a subset of Java in Section syntree-sect



 The notation adds the key-value_pair

 at the head of environment thereby

 creating a modified environment (If environments are

 implemented as linked_lists then can literally be formed_by

 adding the pair at the head of the list for

 ) Thus when is applied to the key it returns



 The syntax-directed_definition in Fig let-sdd-fig treats

 environments as inherited_attributes and values as synthesized

 attributes Thus env representing environments is an

 inherited_attribute for nonterminals expr_term and

 factor and v representing values is a synthesized

 attribute for all of the nonterminals The tokens number and

 id have synthesized_attributes supplied_by the lexical

 analyzer The value associated_with token number is given by

 attribute v the symbol for the identifier represented_by

 token_id is given by attribute s



 figurehtfb

 center

 tabularr_c lr c_l

 3lProduction 3lSemantic Rules



 start null









 let id in







































 id



 number





 tabular

 center



 The use of inherited and synthesized_attributes

 let-sdd-fig



 figure



 Propagation of Environments



 An attribute depends_on if a semantic_rule

 defines in terms of Dependencies will be depicted

 creating a graph with nodes for attributes like and

 and an arrow from to if depends_on



 The solid arrows in Fig let-attr-fig illustrate

 dependencies between attributes representing environments For

 clarity dependencies involving the other attributes are not

 shown



 The dotted lines in the figure form a parse_tree for the

 expression

 center

 let x 4 in x

 center

 The first occurrence of x corresponds to a declaration of

 x since it defines x to have value 4 The second

 occurrence of x is a use of x This expression is

 simple enough to show the evaluation of attributes in detail



 At each parse-tree_node in Fig let-attr-fig the inherited

 attribute if any is shown to the left of the node and the

 synthesized_attribute if any is shown to the right The arrows

 are based_on the semantic_rules for the env attributes



 The solid arrows in Fig let-attr-fig illustrate the

 propagation of environments The dotted lines in the figure form a

 parse_tree At each parse-tree_node the inherited_attribute if

 any is shown to the left of the node and the synthesized

 attribute if any is shown to the right The arrows are based_on

 the semantic_rules for the env attributes

 figurehtfb



 Dependencies between attributes representing

 environments let-attr-fig

 figure



 The root of the parse_tree is for the production

 From the semantic_rules for this production in

 Fig let-sdd-fig inherited_attribute is

 defined to be the null environment Since null is a

 constant in Fig let-attr-fig there is no arrow entering

 at this node (that is at the child of the root of

 the parse tree)



 The key parse-tree_node is the child of root for production

 center

 expr let id

 in

 center



 Consider the subtree for (Subscripts are used only to

 distinguish_between occurrences of the same nonterminal in the

 production so the subtree is the one with the leaf_labeled

 number) From the semantic_rule for

 this production inherits the null environment from its

 parent (hence the arrow from to in

 Fig let-attr-fig) The semantic_rules associated_with the

 productions and

 propagate the null environment that is and

 are both null



 Looking ahead the annotated_parse tree in

 Fig let-parse-fig shows the attribute values at each node

 in the parse_tree



 figurehtfb



 Annotated parse_tree for let x 4 in x

 let-parse-fig

 figure



 The rules associated_with the production

 define to be

 the value associated_with token number by the lexical

 analyzer This value 4 then propagates in turn to and

 through the semantic_rules and

 associated_with the productions

 and respectively



 Now_consider the subtree for in the production

 center

 expr let id

 in

 center

 The inherited_attribute is defined by the rule

 center



 center

 In words inherits the modified environment created by

 adding the key-value_pair at

 the head of Since is null is

 and is 4 the inherited_attribute

 has just one pair This environment is now

 inherited as and



 Use of Environments



 The semantic_rule

 center



 center

 associated_with the production uses

 as a key in the inherited environment Since

 is and has the key-value_pair

 is defined to be 4



 The value 4 is then propagated up the tree until

 is defined to be 4



 Environments for Blocks

 In Section syntree-sect we consider blocks with the syntax

 center

 block '' decls_stmts

 ''

 center

 where decls generates an_optional sequence of declarations

 and stmts generates an_optional sequence of statements

 Rules associated_with this production set up the environment that

 is inherited by nonterminal stmts



 The effect of the declarations generated_by nonterminal

 decls will be captured by using two attributes corresponding to

 the environments before and after the sequence of declarations

 decls Let declsenv be the before environment and

 let declsnew be the after environment Attribute

 declsenv is inherited and attribute declsnew is

 synthesized



 For an illustration of such paired inherited and synthesized

 attributes consider a single declaration

 center

 decl type id

 center

 which associates a type with an_identifier Let attribute

 typet be a type and let attribute ids be a symbol

 We can associate a semantic_rule with this production to add a

 key-value_pair

 to the environment If declenv is the environment before

 the declaration then let declnew be the environment formed

 by_adding the key-value_pair The semantic_rule is

 center

 declnew



 center

 Here declenv is an inherited_attribute and declnew

 is a synthesized_attribute



 This_approach will be used to implement blocks in

 Section syntree-sect







 That is we can reduce the control

 flow_graph representing a region to a new region graph

 where each of its sub-regions is represented_by a node and there

 exists an edge between nodes and if and only if there is a

 control_flow edge leading_from a basic_block in sub-region to

 the header of the sub-region Thus a region graph has a

 header node and a number of exit nodes where flow of control can

 leave the region



 It is also possible to rewrite the above code as

 verbatim

 l 0

 j 2i - 4

 k 2i 5

 t 5 - p

 for ()



 j j 4

 k j 5

 m j t

 i i 2



 verbatim

 Here we notice that j k m all increment by the same amount

 We can rewrite the expressions so that only j is incremented and

 k and m are calculated from j The advantage is

 that we only need to dedicate registers to j and i in the

 loop the registers for and can be used to hold other temporary

 values once their value is produced and consumed



 If the body of the loop increments a variable by a constant amount in

 each iteration then that variable is a basic_induction

 variable For_example in the code below

 verbatim

 for ()

 if (test1)

 i i - 1

 else

 i i - 1



 i i 3

 if (test2) break



 verbatim

 At the end of each iteration this code will always regardless of the

 path taken increment i by 2 therefore we say that i is

 an induction_variable We associate with each loop a new normalized loop_index that counts the number of iterations executed

 That is this index will take on the value of 0_1 at the

 beginning of the first second iterations respectively

 Thus the value of i at the beginning of each execution is

 where is the initial value of at the beginning of the

 number of iterations executed



 Consider the following code_fragment

 verbatim

 for ()

 j 2i

 k j 5

 m k - p



 i i 2



 verbatim

 We notice that

 enumerate

 j is assigned the value



 k is assigned the value



 m is assigned the value Since the value of

 is not changed inside the loop the value of m is thus

 where is the initial value of

 enumerate

 In other_words all the variables in this loop are induction

 variables

 Thus in general we are_interested in

 identifying for each loop all those variables whose values can be

 represented as affine_expressions of induction_variables and

 symbolic_constants





 The data-flow value on entry of the procedure is an initial map

 such that where represents the initial value of





 The data-flow value is a map that maps each variable in a program

 to either an affine_expression of the initial values of the program or

 a special symbol That is the data-flow value at a program

 point is a map for every variable such that is one of





 carrayll

 c0 i ci vi

 where

 c0 c1 are constants





 vi is an index of a loop enclosing point







 array





 This is a more_complex data-flow value than the ones we have discussed

 so_far It can be represented a bit that determines whether the value

 is and an -vector where is the number

 of loops in the flow_graph



 A transfer_function of a region captures the semantics of a region It

 accepts as an input a map of variables to possibly affine loop_index

 expressions and produces as output also a map of variables to possibly

 affine loop_index expressions The result of the transfer_function is

 expressed parameterically in terms of the input map



 A transfer_function is a function that maps an input map to an output

 map That is for each variable

 maps to one of





 arrayll

 c0 c1 m(v1) c2 m(v2)







 array









 arrayl

 fn(m)(v)





 arrayll

 m(v) cm(l) if f(m)(v) m(v) c



 c0 i ci (m(vi) c'i(m(l)-1))

 j cj m(vj)

 if f(m)(v) c0i ci m(vi)j cj m(vj)



 where

 f(m)(vi) m(v) c'i

 f(m)(vj) m(vj)



 otherwise

 array



 arrayl





 For all



 arrayl

 fn(m)(v)





 arrayll

 m(v) cm(l) if f(m)(v) m(v) c



 c0 i ci (m(vi) c'i(m(l)-1))

 j cj m(vj)

 if m(l) 0



 f(m)(v) c0i ci m(vi)j cj m(vj)



 where

 f(m)(vi) m(v) c'i

 f(m)(vj) m(vj)



 m(v) if m(l) 0



 otherwise

 array



 arrayl







 figurehtbp

 center

 tabularrl

 2cTransfer Function























 tabular

 center

 Formulae for computing transfer_functions for region in Fig_figind-region

 figR7

 figure



 figurehtbp

 center

 tabularrl

 2cTransfer Function























 tabular

 center

 Formulae for computing transfer_functions for region in Fig_figind-region

 figR6

 figure







 We can compute the data-flow_values at the header of the

 subregions by_applying the transfer_functions from the header to the subheader

 simply_apply the transfer_functions









 We denote the transfer_function from region to the

 header of its subregion as All execution_paths

 leading_from a procedure entry to a basic_block nested in regions

 with being the top_region must travel from

 the header of to that of then from the header of to

 that of all the way up to Thus its transfer fucntion

 can be computed by function composition

 fRninB fRn-1Rn fR1inR2 fR0inR1

 The data-flow value at the entry of a basic_block can be found by

 applying such a function to the boundary value at the entry of a

 program



 We observe that many of the transfer_functions share_many of the same

 function composition steps especially at the top of the region

 hierarchy Also instead of waiting till all the transfer_functions

 are computed to compute the data-flow_values we can compute the value

 of a header



 we can start getting the

 data-flow results by_applying these



 once we know the data-flow value at the header of the region we can

 compute the data-flow_values of the header of its subregion by

 applying the transfer_function that leads from the header of the

 region to the header of the subregion



 This avoids the need of

 performing many function compositions Thus the second phase of the

 algorithm starts with the top_region The data-flow value associated

 with the header of the top_region is simply the boundary value of the

 data-flow_problem We apply the transfer_functions computed for each

 region to the data-flow value at the header of the region to calculate

 the data-flow_values at the header of each subregion





 As_discussed in Section secstrength





 we_wish to express the values of variables in terms of the initial

 values at the entry of the enclosing loops As shown in the example

 in Fig figind-region4 it is sufficient to know that variable

 e has the value of We can

 perform the analysis of strength_reduction by storing

 in a variable t Then we assign t to

 e in block and then increment t by 8 at the very

 end of the iteration That is we do_not need to know the value of

 or The value of may itself be

 considered an and that does_not affect the optimization

 The induction analysis requires primarily a bottom-up phase where the

 information is accumulated from the innermost to the outermost_loop

 Introduction

 sec9intro



 A compiler optimization must preserve the semantics of the

 orignal program Except in very special circumstances

 once a programmer chooses and implements a particular

 algorithm the compiler cannot understand enough about the program to

 replace it with a completely different and more_efficient

 algorithm A compiler only knows how to apply relatively low-level

 semantic transformations using general facts such_as algebraic

 identities like or program semantics such_as the fact that

 performing

 the same operation on the same values yields the same results



 Sources of Redundancy



 There_are many redundant operations in a typical program Sometimes

 the redundancy is available at the source_level

 For_instance a programmer may find

 it more direct and convenient to recalculate some result leaving it to

 the compiler to recognize that only one such calculation is necessary

 But more

 often the redundancy is a side-effect of having written the program

 in a high-level_language In most languages (other_than C or C where

 pointer arithmetic is allowed) programmers have no

 choice but to refer to elements of an array or fields in a structure

 through accesses like Aij or X-f1 Each of these

 high-level data-structure accesses expands into a number of

 low-level_arithmetic operations such_as the computation of the location

 of the th_element of a matrix Accesses to similar data_structures

 often share_many common low-level_operations Programmers are not

 aware of these low-level_operations and cannot eliminate the

 redundancies themselves It is in fact preferable from a

 software-engineering perspective that programmers only access data elements

 by their

 high-level names the programs are easier to write and more

 importantly easier to understand By having a compiler eliminate the

 redundancies we get the best of both worlds the programs are both

 efficient and easy to understand



 A Running Example Quicksort



 In the following we will use a sorting program called quicksort to

 illustrate_how programs are represented and how they can be

 optimized The C program in Fig_figqs is derived_from

 SedgewickfootnoteR Sedgewick Implementing Quicksort

 Programs Comm_ACM bf21 pp 847-857footnote

 who discussed the hand-optimization of such a program

 We_shall not discuss the algorithmic aspects of this

 program here for example the fact that a0 must contain the

 smallest of the sorted elements and amax the largest



 figurehtb



 verbatim

 void quicksort(m n) recursively sorts am through an

 int m n



 int i_j

 int_v x

 if_( n m )_return

 fragment begins here

 i_m-1 j_n v an

 while_( 1 )

 do i_i1 while_( ai v )

 do j_j-1 while_( aj v )

 if_( i_j ) break

 x_ai ai_aj aj_x



 x_ai ai an an x

 fragment ends here

 quicksort(mj) quicksort(i1n)



 verbatim



 C code for quicksort

 figqs



 figure



 Before we can optimize away the redundancy in address_calculations

 the address operations in a program must first be broken_down

 into low-level_arithmetic operations to expose the redundancy

 In the rest of the chapter we assume that the intermediate_code

 consists of three-address_statements where temporary_variables are

 used to hold all the results of intermediate expressions

 Intermediate_code

 for a portion of the program in Fig_figqs is shown

 in Fig_figqs-code



 figurehtb



 verbatim

 (1) i_m-1 (16) t7 4i

 (2) j_n (17) t8 4j

 (3) t1 4n (18) t9 at8

 (4) v at1 (19) at7 t9

 (5) i_i1 (20) t10_4j

 (6) t2 4i (21) at10_x

 (7) t3 at2 (22) goto (5)

 (8) if t3 v_goto (5) (23) t11 4i

 (9) j_j-1 (24) x at11

 (10) t4 4j (25) t12 4i

 (11) t5_at4 (26) t13 4n

 (12) if t5 v_goto (9) (27) t14 at13

 (13) if i_j goto (23) (28) at12 t14

 (14) t6_4i (29) t15 4n

 (15) x at6 (30) at15 x

 verbatim



 Three-address_code for fragment in Fig_figqs

 figqs-code



 figure



 Assuming in this example that integers occupy four_bytes an

 assignment such_as

 x_ai is translated_into statements



 verbatim

 t6_4i x at6

 verbatim

 as shown in steps (14) and (15) of

 Fig_figqs-code

 Similarly aj_x becomes



 verbatim

 t10_4j at10_x

 verbatim

 in steps (20) and (21)

 Notice_that every array

 access in the original_program translates_into a pair of steps

 consisting of a

 multiplication and an array subscripting operation As a result

 this short program_fragment translates_into a rather long sequence of

 three-address operations



 Computing Offsets in Intermediate_Code

 The intermediate_code can be (relatively) independent of

 the target_machine so the optimizer does_not have to change

 much if the code_generator is replaced_by one for a different machine

 The intermediate_code in Fig_figqs-code assumes that each

 element of the integer array

 a

 takes four_bytes

 Some intermediate codes eg P-code for Pascal leave

 it to the code_generator to fill in the size of array_elements

 so the intermediate_code is independent of the size of a machine word

 We could have done the same in our intermediate_code

 if we replaced 4 by a symbolic_constant



 In the code optimizer a procedure is typically represented_by a

 control_flow graph where nodes represent basic_blocks and edges

 indicate the flow of control as discussed in Section ch2-dfa



 figure

 figureuullmanalsuch9figsqs-fgeps

 Flow_graph

 figqs-fg

 figure



 Figure figqs-fg contains the flow_graph for the program in

 Fig_figqs-code

 is the initial_node

 All conditional and unconditional_jumps to statements in

 Fig_figqs-code have_been replaced in Fig_figqs-fg by jumps

 to the block of which the statements are leaders (first statements

 in their_respective blocks)

 Note_that for a conditional goto like statement (8) of Fig_figqs-code

 there are arcs in the flow_graph to both the block whose leader is

 mentioned in the goto - in this case - and the block whose

 leader follows the conditional goto - here



 In Fig_figqs-fg there are three loops

 and are loops by themselves

 Blocks and together

 form a loop with the only entry point

 Basic Concepts



 In this_section we introduce call graphs - graphs that tell_us which

 procedures can call which We also introduce the idea of

 context_sensitivity where data-flow_analyses are required to take

 cognizance of what the

 sequence of procedure_calls has_been That is context-sensitive

 analysis includes (a synopsis of) the current sequence of activation

 records on the stack along with the current point in the program when

 distinguishing among different places in the program



 Call Graphs



 A call_graph for a program is a set of nodes and edges

 such that



 enumerate



 There is one node for each procedure in the program



 There is one node for each call_site that is a place

 in the program where a procedure is invoked



 If call_site may call procedure then there is an edge from the

 node for to the node for



 enumerate

 Many programs_written in languages_like C and Fortran make

 procedure_calls directly so the call target of each invocation

 can be determined statically

 In that case each call_site has an edge to exactly one procedure in the

 call_graph

 However if the program includes the use of a procedure parameter or function

 pointer the target generally is not known until the program

 is run and in fact may vary from one invocation to another

 Then a call_site can link to many or all procedures in the call_graph



 Indirect calls are the norm for object-oriented_programming languages

 In_particular when there is overriding of methods in subclasses a use

 of method may refer to any of a number of different methods

 depending_on the subclass of the receiver_object to which it was applied

 The use of such virtual_method invocations means that we need to know the

 type of the receiver before we can determine which method is

 invoked



 figurehtfb



 verbatim

 int (pf)(int)



 int fun1(int x)

 if (x 10)

 c1 return (pf)(x1)

 else

 return x





 int fun2(int y)

 pf fun1

 c2 return (pf)(y)





 void_main()

 pf fun2

 c3 (pf)(5)



 verbatim



 A program with a function pointer

 fn-ptr1-fig



 figure



 ex

 fn-ptr-ex

 Figure fn-ptr1-fig shows a C program that declares pf to be a

 global pointer to a function whose type is integer to integer There_are

 two functions of this type fun1 and fun2 and a main

 function that is not of the type that pf points to

 The figure shows three call_sites denoted c1_c2 and c3 the labels are not part of the program



 The simplest analysis of what pf could point to would simply

 observe the types of functions Functions fun1 and fun2 are

 of the same type as what pf points to while main is not

 Thus a conservative call_graph is shown in

 Fig fn-ptr2-fig(a)

 A more careful analysis of the program would observe that pf is

 made to point to fun2 in main and is made to point to fun1 in fun2 But there are no other assignments to any pointer

 so in particular there is no way for pf to point to main

 This reasoning yields the same call_graph as

 Fig fn-ptr2-fig(a)



 figurehtfb



 fileuullmanalsuch12figsfn-ptr1eps



 Call graphs derived_from Fig fn-ptr1-fig

 fn-ptr2-fig



 figure



 An even more_precise analysis would say that at c3 it is only

 possible for pf to point to fun2 because that call is

 preceded immediately by that assignment to pf Similarly at c2 it is only possible for pf to point to fun1

 As a result the initial call to fun1 can come only

 from fun2 and fun1

 does_not change pf so whenever we are within fun1 pf

 points to fun1 In_particular at c1 we can be_sure pf points to fun1 Thus

 Fig fn-ptr2-fig(b) is a more_precise correct call_graph

 ex



 In_general the presence of references or pointers to functions or

 methods

 requires us to get a static approximation of the

 potential values of all procedure parameters function pointers and

 receiver_object types To make an accurate approximation

 interprocedural_analysis is necessary The analysis is iterative

 starting_with the statically observable targets

 As more targets are

 discovered the analysis incorporates the new edges into the call

 graph and repeats discovering more targets until convergence is reached



 Context Sensitivity

 secipa-intro-cs



 Interprocedural analysis is challenging because the behavior of each

 procedure is dependent upon the context in which it is called

 Example_ipa-1-ex uses the problem of interprocedural constant

 propagation on a small program to illustrate the significance of

 contexts



 ex

 ipa-1-ex

 Consider the program_fragment in Fig ipa-1-fig

 Function is invoked at three call_sites

 c1_c2 and c3 Constant 0 is

 passed in as the actual_parameter at c1 and constant 243 is passed

 in at c2 and c3 in each iteration the

 constants 1 and 244 are returned respectively Thus function

 is invoked with a constant in each of the contexts but the value of

 the constant is context-dependent



 figure



 verbatim

 for_(i 0 i_n i)

 c1_t1 f(0)

 c2_t2 f(243)

 c3_t3 f(243)

 Xi_t1t2t3





 int f (int_v)

 return_(v1)



 verbatim



 A program_fragment illustrating the need for context-sensitive

 analysis

 ipa-1-fig



 figure



 As we_shall see it is not possible to tell that t1_t2 and

 t3 each are assigned constant values (and thus so is )

 unless we recognize that when called in context c1 returns 1

 and when called in the other two contexts returns 244

 A naive analysis would conclude that can return

 either 1 or 244 from any call

 ex



 One simplistic but extremely inaccurate approach to interprocedural

 analysis known_as context-insensitive_analysis is to treat

 each call and return statement as goto operations We create a

 super control-flow_graph where besides the normal

 intraprocedural control_flow edges additional edges are created

 connecting



 enumerate



 Each call_site to the beginning of the procedure it calls and



 The return statements back to the call sites(The return is

 actually to the instruction following the call site)

 enumerate



 Assignment statements are added to assign each actual_parameter to

 its corresponding

 formal_parameter and to assign

 the returned value to the variable receiving the result

 We can then apply a standard analysis intended to be used within a

 procedure to the super control-flow_graph to find context-insensitive

 interprocedural results While simple this model abstracts out the

 important relationship_between input and output values in

 procedure invocations causing the

 analysis to be imprecise



 figurehtfb

 fileuullmanalsuch12figssupergrapheps

 The control-flow_graph for Fig ipa-1-fig treating

 function calls as control_flow

 supergraph-fig

 figure



 ex

 The super control-flow_graph for

 the program in Fig ipa-1-fig is shown in

 Figure supergraph-fig

 Block is the function Block contains the call_site

 c1 it sets the formal_parameter to 0 and then jumps to the

 beginning of at Similarly and represent the

 call_sites c2 and c3 respectively In which is

 reached from the end of (block ) we take the return value from

 and assign it to t1 We then set formal_parameter to 243

 and call again by jumping to Note_that there is no edge

 from to Control must flow through on the way from

 to



 is similar to It receives the return from assigns the

 return value to t2 and initiates the third call to

 Block represents the return from the third call and the assignment

 to



 If we treat Fig supergraph-fig as if it were the flow_graph of a

 single procedure then we would conclude that coming_into can

 have the value 0 or 243 Thus the most we can conclude about retval is that it is assigned 1 or 244 but no other value Similarly

 we can only conclude about t1_t2 and t3 that they

 can each be either 1 or 244 Thus appears to be either 3 246

 489 or 732

 In_contrast a context-sensitive_analysis would separate

 the results for each of the calling_contexts and produces the

 intuitive answer described in Example_ipa-1-ex t1 is always

 1 t2 and t3 are always 244 and is 489

 ex



 Call Strings

 ipa-call-strings

 In Example_ipa-1-ex we can distinguish_among the contexts by

 just knowing the call_site that calls the procedure In

 general a calling_context is defined by the contents of the entire

 call stack We refer to the string of call_sites on the stack as the

 call string



 ex

 ipa-2-ex

 Figure ipa-2-fig is a slight modification of Fig ipa-1-fig

 Here we have replaced the calls to by calls to which then calls

 with the same argument There is an additional call_site c4

 where calls



 figure



 verbatim

 for_(i 0 i_n i)

 c1_t1 g(0)

 c2_t2 g(243)

 c3_t3 g(243)

 Xi_t1t2t3





 int g (int_v)

 c4 return f(v)





 int f (int_v)

 return_(v1)



 verbatim



 Program fragment illustrating call strings

 ipa-2-fig



 verbatim

 for_(i 0 i_n i)

 c1_t1 g(0)

 c2_t2 g(243)

 c3_t3 g(243)

 Xi_t1t2t3





 int g (int_v)

 if (v 1)

 c4 return g(v-1)

 else

 c5 return f(v)





 int f (int_v)

 return_(v1)



 verbatim



 Recursive program requiring analysis of complete call strings

 ipa-recurse-fig



 figure



 There_are three call strings to

 and

 As we see in this example the value of

 in function depends not on the immediate or last site

 c4 on the call string Rather the constants are determined by

 the first element in each of the call strings

 ex



 Example ipa-2-ex illustrates that information relevant to the

 analysis can be introduced early in the call chain In_fact it is

 sometimes necessary to consider the entire call string to compute the

 most_precise answer as illustrated in Example ipa-recurse-ex



 ex

 ipa-recurse-ex

 This example_illustrates how the ability to reason_about unbounded

 call strings can yield more_precise results

 In Fig ipa-recurse-fig

 we see that if is called with a positive value

 then will be invoked recursively times

 Each time is called the value of its parameter decreases by 1

 Thus the value of 's parameter

 in the context whose call string is is



 The effect of is thus to increment 0 or any negative argument by 1

 and to return 2 on any argument 1 or greater



 There_are three possible call strings for If we start with the call

 at c1

 then calls immediately so is one

 such string If we start at c2 or c3 then we call a

 total of 243 times and then call These call strings are

 and

 where in each case there

 are 242 c4's in the sequence In the first of these contexts the

 value of 's parameter is 0 while in the other two contexts it is

 1

 ex



 In designing a context-sensitive_analysis we have a choice in

 precision For_example instead of qualifying the results by the full

 call string we may just choose to distinguish_between contexts by

 their most immediate call_sites This technique is known_as

 -limiting context analysis Context-insensitive analysis is simply

 a special_case of -limiting context analysis where is 0 We

 can find all the constants in Example_ipa-1-ex using a

 1-limiting analysis and all the constants in Example ipa-2-ex

 using a 2-limiting analysis However no -limiting analysis can

 find all the constants in Example ipa-recurse-ex provided the constant

 243 were replaced_by two different and arbitrarily_large constants



 Instead of choosing a fixed value another possibility is to be

 fully context sensitive for all acyclic call strings which are

 strings that contain no recursive_cycles For call strings with

 recursion we can collapse all recursive_cycles in order to bound the number of

 different_contexts analyzed In Example ipa-recurse-ex the

 calls initiated at call_site c2 may be approximated by the call

 string

 Note_that with this scheme even for

 programs without recursion the number of distinct calling_contexts can

 be exponential in the number of procedures in the program



 Cloning-Based Context-Sensitive_Analysis

 ipa-intro-cloning

 Another_approach to context-sensitive_analysis is to clone the procedure

 conceptually one for each unique context of interest We can then

 apply a context-insensitive_analysis to the cloned_call graph

 Examples ipa-2clone-ex and ipa-recurse-clone-ex show the

 equivalent of a cloned version of Examples ipa-2-ex

 and ipa-recurse-ex respectively In reality we do_not

 need to clone the code we can simply use an efficient internal

 representation to keep_track of the analysis results of each clone



 figurehtfb



 verbatim

 for_(i 0 i_n i)

 c1_t1 g1(0)

 c2_t2 g2(243)

 c3_t3 g3(243)

 Xi_t1t2t3



 int g1 (int_v)

 c41 return f1(v)



 int g2 (int_v)

 c42 return f2(v)



 int g3 (int_v)

 c43 return f3(v)





 int f1 (int_v)

 return_(v1)



 int f2 (int_v)

 return_(v1)



 int f3 (int_v)

 return_(v1)



 verbatim



 Cloned version of Fig ipa-2-fig

 ipa-2clone-fig



 figure



 ex

 ipa-2clone-ex

 The cloned version of Fig ipa-2-fig is shown in

 Fig ipa-2clone-fig

 Because every calling_context refers to a distinct clone there is no

 confusion For_example g1 receives 0 as input and produces 1

 as output and g2 and g3 both receive 243 as input and

 produce 244 as output

 ex



 ex

 ipa-recurse-clone-ex

 The cloned version of Fig ipa-recurse-fig is shown in

 Fig ipa-recurse-clone-fig

 For procedure we create a clone to represent all instances of

 that are first called from sites c1_c2 and c3 In this case the analysis would determine that the

 invocation at call_site c1 returns 1

 assuming the analysis can deduce that with the test fails

 This analysis does_not

 handle recursion well enough to produce the constants for call_sites

 c2 and c3 however

 ex



 figure



 verbatim

 for_(i 0 i_n i)

 c1_t1 g1(0)

 c2_t2 g2(243)

 c3_t3 g3(243)

 Xi_t1t2t3





 int g1 (int_v)

 if (v 1)

 c41 return g1(v-1)

 else

 c51 return f1(v)





 int g2 (int_v)

 if (v 1)

 c42 return g2(v-1)

 else

 c52 return f2(v)





 int g3 (int_v)

 if (v 1)

 c43 return g3(v-1)

 else

 c53 return f3(v)





 int f1 (int_v)

 return_(v1)



 int f2 (int_v)

 return_(v1)



 int f3 (int_v)

 return_(v1)



 verbatim



 Cloned version of Fig ipa-recurse-fig

 ipa-recurse-clone-fig



 figure



 Summary-Based Context-Sensitive_Analysis

 ipa-intro-summary



 Summary-based interprocedural_analysis is an extension of region-based

 analysis

 Basically in a summary-based analysis each procedure is

 represented_by a concise description (summary) that

 encapsulates some observable behavior of the procedure

 The primary purpose of the summary is to avoid reanalyzing a

 procedure's body at every call_site that may invoke the procedure



 Let_us first consider the case_where there is no recursion

 Each procedure is modeled as a region with a single entry point with

 each caller-callee pair sharing an outer-inner region relationship

 The only_difference from the intraprocedural version is that in the

 interprocedural case a

 procedure region can be nested inside several different outer regions



 The analysis consists of two_parts



 enumerate



 A bottom-up phase

 that computes a transfer_function to summarize the effect of a

 procedure and



 A top-down phase that propagates caller information to

 compute results of the callees



 enumerate

 To get fully context-sensitive

 results information from different calling_contexts must propagate

 down to the callees individually For a more_efficient but less

 precise calculation information from all callers can be

 combined using a meet_operator then propagated down to the callees



 ex

 ipa-2-mod-ex

 For constant_propagation each procedure is summarized by a transfer

 function specifying how it would propagate constants through its body

 In Example_ipa-1-ex we can summarize as a function

 that given a constant as an actual_parameter to

 returns the constant Based on this information the

 analysis would determine that t1_t2 and t3 have

 the constant values 1 244 and 244 respectively Note_that this

 analysis does_not suffer the inaccuracy due to unrealizable call

 strings



 Recall that

 Example ipa-2-ex extends Example_ipa-1-ex by having call

 Thus we could conclude that

 the transfer_function for is the same as the transfer_function for

 Again we

 conclude that t1_t2 and t3 have the constant

 values 1 244 and 244 respectively



 Now let_us consider

 what is the value of parameter in function for

 Example_ipa-1-ex As a first cut we can combine all the

 results for all calling_contexts Since may have values 0 or

 243 we can simply conclude that is not a constant This conclusion

 is fair because there is no constant that can replace

 in the code



 If we desire more_precise results we can compute

 specific results for contexts of interest Information must_be passed

 down from the context of interest to determine the context-sensitive

 answer This step is analogous to the top-down_pass in region-based

 analysis For_example the value of is 0 at call_site c1 and

 243 at sites c2 and c3 To get the advantage

 of constant_propagation within we need to capture this distinction by

 creating two clones with the first specialized for input value 0 and

 the latter with value 243 as shown in Fig ipa-2-mod-fig

 ex



 figurehtfb



 verbatim

 for_(i 0 i_n i)

 c1_t1 f0(0)

 c2_t2 f243(243)

 c3_t3 f243(243)

 Xi_t1t2t3





 int f0 (int_v)

 return (1)





 int f243 (int_v)

 return (244)



 verbatim



 Result of propagating all possible constant arguments to the

 function

 ipa-2-mod-fig



 figure



 With Example ipa-2-mod-ex we see

 that in the end if we_wish to compile the

 code differently in different_contexts we still need to clone the

 code The difference is that in the cloning-based approach cloning

 is performed prior to

 the analysis based_on the call strings In

 the summary-based approach the cloning is performed after the

 analysis using the analysis results as a basis Even_if cloning is

 not applied in the summary-based approach

 inferences about the effect of a called procedure are made

 accurately without the problem of unrealizable paths



 Instead of cloning a function

 we could also inline the code Inlining has the additional effect of

 eliminating the procedure-call overhead as_well



 We can handle recursion by computing the fixedpoint solution In the

 presence of recursion we first find the strongly_connected components

 in the call_graph In the bottom-up phase we do_not visit a strongly

 connected_component unless all its successors have_been visited For

 a nontrivial strongly_connected component we iteratively

 compute the transfer_functions for each procedure in the component

 until convergence is reached that is we iteratively update the

 transfer_functions until_no more changes occur



 figurehtfb



 verbatim

 int (p)(int)

 int (q)(int)



 int f(int i)

 if (i 10)

 p g return (q)(i)

 else

 p f return (p)(i)





 int g(int j)

 if (j 10)

 q f return (p)(j)

 else

 q g return (q)(j)





 void_main()

 p f

 q g

 (p)((q)(N))



 verbatim



 Program for Exercise call-graph-exer

 call-graph-exer-fig



 figure



 exer

 call-graph-exer

 In Fig call-graph-exer-fig is a C program with two function

 pointers and

 is a constant that could be less_than or greater_than 10

 Note_that the program results in an_infinite

 sequence of calls but that is of no concern for the purposes of this

 problem



 itemize



 a) Identify all the call_sites in this program

 b) For each call_site what can point to What can point

 to

 c) Draw the call_graph for this program

 d) Describe all the call strings for and



 itemize

 exer



 figurehtfb



 verbatim

 int id(int x) return x





 if (a 1) x id(2) y id(3)

 else x id(3) y id(2)

 z xy



 verbatim



 Code fragment for Exercise clone-exer

 clone-exer-fig



 figure



 exer

 clone-exer

 In Fig clone-exer-fig is a function id that is the

 identity_function it returns exactly what it is given as an

 argument We also see a code_fragment consisting of a branch and

 following assignment that sums



 itemize



 a) Examining the code what can we tell about the value of at

 the end



 b) Construct the flow_graph for the code_fragment treating the

 calls to id as control_flow



 c) If we run a constant-propagation analysis as in

 Section_const-prop-sect on your flow_graph from (b) what

 constant values are determined



 d) What are all the call_sites in Fig clone-exer-fig



 e) What are all the contexts in which id is called



 f) Rewrite the code of Fig clone-exer-fig by cloning a new

 version of id for each context in which it is called



 g) Construct the flow_graph of your code from (f) treating the

 calls as control_flow



 h) Perform a constant-propagation analysis on your flow_graph

 from (g) What constant values are determined now



 itemize

 exer

 Why Interprocedural_Analysis



 Given how hard interprocedural_analysis is let_us now address the

 important problem of why and when we_wish to use interprocedural

 analysis Although we used constant_propagation to illustrate

 interprocedural_analysis this interprocedural

 optimization is neither readily applicable nor

 particularly beneficial when it does occur

 Most of the benefits of constant_propagation

 can be obtained simply by performing intraprocedural analysis and inlining

 procedure_calls of the most frequently executed sections of code



 However there are many reasons_why interprocedural_analysis is

 essential

 Below we describe several important applications of interprocedural

 analysis



 Virtual Method Invocation



 As_mentioned above object-oriented programs have many small methods

 If we only optimize one method at a time then there are few

 opportunities for optimization Resolving method invocation

 enables optimization A language like Java dynamically loads its

 classes As a result we do_not know at compile-time to which of (perhaps)

 many methods named a use of refers in an invocation such_as





 Many Java implementations use a just-in-time compiler to

 compile its bytecodes at_run time One common optimization is to

 profile the execution and determine which are the common

 receiver types We can then inline the methods that are most frequently

 invoked The code includes a dynamic check on the type and executes

 the inlined methods if the run-time object has the expected type



 Another_approach to resolving uses of a method name is possible as

 long_as

 all the source code is available at_compile time

 Then it is possible to

 perform an interprocedural_analysis to determine the object

 types If the type for a variable turns_out to be unique then a

 use of can be_resolved We know exactly what method refers

 to in this context

 In that case we can in-line the code for this and the compiler

 does_not even have to include a test for the type of



 Pointer Alias Analysis



 Even_if we do_not wish to perform interprocedural versions of the common

 data-flow_analyses like reaching_definitions

 these analyses can in fact benefit from interprocedural pointer

 analysis All the analyses presented in Chapter_code-op-ch

 apply only to local_scalar variables that cannot have aliases

 However use of pointers is common especially in

 languages_like C By knowing whether pointers can be aliases (can

 point to the same location) we can

 improve the accuracy of the techniques from Chapter_code-op-ch



 ex

 Consider the following sequence of three statements which might form a

 basic_block



 verbatim

 p 1

 q 2

 x p

 verbatim

 Without knowing if

 and can point to the same_location - that is whether they can

 be aliases - we cannot conclude

 that is equal to 1 at the end of the block

 ex



 Parallelization



 As_discussed in Chapter affine-ch the most effective way to

 parallelize an application is to find the coarsest granularity of

 parallelism such_as that found in the outermost_loops of a program

 For this task interprocedural_analysis is of great importance There is

 a significant difference_between scalar optimizations (those based

 on values of simple variables as discussed in Chapter code-op-ch)

 and

 parallelization In parallelization just one spurious data

 dependence can render an entire loop not parallelizable and greatly

 reduce the effectiveness of the optimization Such

 amplification of inaccuracies

 is not seen in scalar optimizations In scalar optimization we only need to

 find the majority of the optimization opportunities Missing one

 opportunity or two seldom makes much of a difference



 Detection of Software Errors and Vulnerabilities



 Interprocedural analysis is not only important for optimizing code

 The same techniques can be used to analyze existing software for many

 kinds of coding errors These errors can render software unreliable

 coding errors that hackers can exploit

 to take control of or otherwise damage a computer system can pose

 significant security vulnerability risks



 Static analysis is useful in detecting occurrences of

 many common error patterns

 For_example a data item must_be guarded by a

 lock As_another example

 disabling an interrupt in the operating_system must_be

 followed_by a re-enabling of the interrupt

 Since a significant source of errors is the inconsistencies

 that span procedure boundaries interprocedural_analysis is of

 great importance PREfix and Metal are

 two practical tools that use interprocedural_analysis effectively to

 find many programming_errors in large_programs

 Such tools find errors statically and can improve

 software reliability greatly

 However these tools are both incomplete and unsound

 in the sense that they may not find all errors

 and not all reported warnings are real errors

 Unfortunately the interprocedural_analysis used is sufficiently

 imprecise that

 were the tools to report all potential errors

 the large_number of false warnings would render the tools unusable

 Nevertheless even_though these tools are not perfect

 their systematic use has_been shown to greatly improve software reliability



 When it comes to security vulnerabilities it is highly desirable that

 we find all the potential errors in a program In 2006 two of the

 most_popular forms of intrusions used by hackers to compromise a

 system were



 enumerate

 Lack of input validation on Web applications SQL injection is one of

 the most_popular forms of such vulnerability whereby hackers gain control

 of a database by manipulating inputs accepted_by web applications



 Buffer overflows in C and C programs Because C and C do_not

 check if accesses to arrays are in bounds hackers can write

 well-crafted strings into unintended areas and hence gain control of

 the program's execution

 enumerate

 In the next section

 we_shall discuss_how we can use interprocedural_analysis to protect

 programs against such vulnerabilities



 SQL Injection



 SQL injection refers to the vulnerability where hackers can manipulate

 user input to a Web application and gain unintended access to a database

 For_example

 banks want their users to be_able to make transactions online provided

 they supply their correct password A common architecture for such a

 system is to have the user enter strings into a Web form and then to

 have those strings form part of a database query written in the SQL

 language If systems developers are not careful

 the strings provided by the user can

 alter the meaning of the SQL statement in unexpected ways



 ex

 sql-inj-ex

 Suppose a bank offers its customers access to a relation



 verbatim

 AcctData(name password balance)

 verbatim

 That is this relation is a table of triples each consisting of the

 name of a customer the password and the balance of the account

 The intent is

 that customers can see their account balance only if they provide both

 their name and their correct password Having a hacker see an account

 balance is not the worst thing that could occur but this simple example

 is typical of more_complicated situations_where the hacker could execute

 payments from the account



 The system might implement a balance inquiry as_follows



 enumerate



 Users invoke a Web form where they enter their name and password



 The name is copied to a variable and the password to a variable



 Later perhaps in some other procedure the following SQL query is

 executed



 verbatim

 SELECT balance FROM AcctData

 WHERE name 'n' and password 'p'

 verbatim

 enumerate

 For readers not familiar with SQL this query says Find in the

 table AcctData a row with the first component (name) equal

 to the string currently in variable and the second_component (password)

 equal to the string currently in variable print the third

 component (balance) of that row Note_that SQL uses single quotes

 not double quotes to delimit strings and the colons in front of

 and indicate that they are variables of the surrounding language



 Suppose the hacker who wants to find Charles Dickens' account balance

 supplies the following values for the strings and



 center

 Charles Dickens' -- who cares

 center

 The effect of these strange strings is to convert the query into



 verbatim

 SELECT balance FROM AcctData

 WHERE name 'Charles Dickens' -' and password 'who cares'

 verbatim

 In many database systems -- is a comment-introducing

 token and has the effect of

 making whatever follows on that line a comment As a result the query

 now asks the database system to print the balance for every person whose

 name is regardless of the password that appears with

 that name in a name-password-balance

 triple That is with comments eliminated the query is



 verbatim

 SELECT balance FROM AcctData

 WHERE name 'Charles Dickens'

 verbatim

 ex



 In Example sql-inj-ex the bad strings were kept in two

 variables which might be passed between procedures However in more

 realistic cases these strings might be copied several_times or

 combined with others to form the full query We cannot hope to detect

 coding errors that create SQL-injection vulnerabilities without doing

 a full interprocedural_analysis of the entire program



 Buffer Overflow



 A buffer overflow attack occurs_when carefully crafted data

 supplied_by the user writes beyond the intended buffer and manipulates

 the program execution For_example a C

 program may read a string from the user and then copy it into a

 buffer using the function call



 verbatim

 strcpy(bs)

 verbatim

 If the string is

 actually longer than the buffer then locations that are not part of

 will have their values changed That in itself will probably cause

 the program to malfunction or at_least to produce the wrong answer

 since some data used by the program will have_been changed



 But worse

 the hacker who chose the string can pick a value that will do more

 than cause an error For_example if the buffer is on the run-time

 stack then it is near the return_address for its function An

 insidiously chosen value of may overwrite the return_address and

 when the function returns it goes to a place chosen by the hacker

 If hackers have detailed knowledge of the surrounding operating_system

 and hardware they may be_able to execute a command that will give them

 control of the machine itself In some situations they may even have the

 ability to have the false return_address transfer control to code that

 is part of the string thus allowing any sort of program to be

 inserted into the executing code



 To prevent buffer overflows every array-write operation must_be

 statically proven to be within bounds or a proper array-bounds check

 must_be performed dynamically Because these bounds checks need to be

 inserted by hand in C and C programs it is easy to forget to insert

 the test or to get the test wrong Heuristic tools have_been

 developed that will check if at_least some test though not

 necessarily a correct test has_been performed before a strcpy is

 called



 Dynamic bounds checking is unavoidable because it is impossible to

 determine statically the size of users' input All a static analysis

 can do is assure that the dynamic checks have_been inserted

 properly Thus a reasonable strategy is to have the compiler insert

 dynamic bounds checking on every write and use static analysis as a

 means to optimize away as many bounds check as possible It is no

 longer necessary to catch every potential violation moreover we only

 need to optimize only those code regions that execute frequently



 Inserting bounds checking into C programs is nontrivial

 even if we do_not mind

 the cost A pointer may point into the middle of some array and we do

 not know the extent of that array Techniques have_been developed to

 keep_track of the extent of the buffer pointed to by each pointer

 dynamically This information

 allows the compiler to insert array bounds checks

 for all accesses Interestingly enough it is not advisable to

 halt a program whenever a buffer overflow is detected In_fact

 buffer overflows do occur in practice and a program would likely fail if we

 disable all buffer overflows The solution is to extend the

 size of the array dynamically to accommodate for the buffer overruns



 Interprocedural analysis can be used to speed_up the cost of dynamic

 array bounds checks For_example suppose we are_interested only in

 catching buffer overflows involving user-input strings we can

 use static analysis to determine which variables may hold contents

 provided by the user Like SQL injection being able to track an

 input as it is copied across procedures is useful in eliminating

 unnecessary bounds checks







 How can interprocedural_analysis help Think of a string that is

 provided by the user as a bad definition of Badness of a

 variable can propagate as the variable is copied or combined with

 other variables (consider of how

 - not-a-constant -

 propagated in Section_const-prop-sect about a constant-propagation

 framework) An_important case of copying strings occurs_when a string is

 passed as an argument to a procedure or the return value of a procedure

 is assigned to a variable Without knowing how bad strings pass

 among procedures we cannot hope to trace the flow of user-defined data

 since it is quite typical for the user's data to be read in one

 procedure and used in other procedures





 Intermediate Representations

 ch2-dfa

 A program can be represented at many different_levels All the

 information_about a program is captured in abstract_syntax trees

 (AST) An AST contains the type information of all the variables the

 high-level control_flow in the program the data_structure accesses

 and the exact operations to be performed They typically also contain

 the line numbers and even the column positions in which the

 declarations or statements appear If we_wish to build a tool that

 communicates the results of an analysis back to the user we may want

 to operate directly on ASTs However ASTs are a rather rich

 representation thus making them hard to manage For_example a loop

 may appear in many different forms depending_on how the code was written

 It is easier to write analysis for program representations that contain a

 smaller number of primitives



 Accesses to data_structures in high-level programs can be represented at

 different_levels We can represent a data_structure access by the

 calculation of its address in terms of multiplications or additions

 This is the right representation if we_wish to simplify and optimize

 address_calculations On the other_hand if structure accesses are

 represented_by the variable and the field name type information of

 the variable is then available to the compiler Similarly representing

 multi-dimensional array_accesses by the array name and an index for

 each dimension makes it easier to distinguish_between different array

 elements



 We can also represent control constructs in high-level programming

 languages in different_ways To support loop level parallelization

 it is preferable that loop constructs be represented explicitly On

 the other_hand low-level optimizations do_not need to see the

 high-level structures and a lower-level representation can be used



 Low-level control_flow is commonly represented_by basic_blocks

 and control_flow graphs A basic_block is a sequence of

 consecutive statements where flow of control can enter at the

 beginning of the block and leave only at the end A control_flow

 graph is a directed graph whose nodes represent the basic

 blocks in a program and edges represent possible flows of control

 from the end of the source node to the beginning of the destination

 node The flow_graph of the program in Figure figch2-ex is

 shown in Figure figch2-ex-fg By representing the program's

 control_flow as a graph graph algorithms can be used to analyze the

 flow of control



 figurehtb

 verbatim

 (1) int a 243

 (2) int b i

 (3) int c 0

 (4) i 10

 (5) b i

 (6) While (i 0)

 (7) if (i -1)

 (8) c 1

 (9) else

 (10) c 2

 (11) i - 1

 (12)

 verbatim

 A Simple Program

 figch2-ex

 figure



 Operations may also be represented at different_levels Take for

 example the array range checks in the Java programming_language

 Every array_access must_be preceded by checking if the indices are out

 of bounds Although the comparison can be written out as generic

 comparison operations a compiler pass that tries to eliminate range

 checks may find it more convenient to operate_on higher level

 instructions that represent the range check operation directly

 figurehtb

 figureuullmanalsuch2oldfigsfgeps

 Flow_graph of program in Figure figch2-ex

 figch2-ex-fg

 figure



 Every IR of a program is a complete representation of the program

 they may not contain all the information available at the source but

 they do_not miss any information that would render the program not

 executable A higher-level IR obviously contains more information

 For_example it may have type information on all the variables in a

 program whereas a lower-level IR may not Once the IR is converted

 to a lower-level it is usually not possible to recover the

 higher-level information discarded Thus a compiler would apply all

 the higher-level transforms on the higher-level IR before it lowers

 the program to the next level of IR Note_that IRs

 share_many commonalities differing sometimes only in the

 representation of a few constructs Thus to translate a

 representation to another may just mean lowering one or two of

 the constructs in the representation



 Issues in the Design of a Code Generator

 issues-sect



 While the details are dependent on the specifics of the

 intermediate_representation the target language and the run-time

 system tasks such_as instruction_selection register_allocation

 and assignment and instruction ordering are encountered in the

 design of almost all code generators



 The most_important criterion for a code_generator is that it

 produce correct code Correctness takes on special significance

 because of the number of special_cases that a code_generator might

 face Given the premium on correctness designing a code_generator

 so it can be easily implemented tested and maintained is an

 important design goal



 Input to the Code Generator



 The input to the code_generator is the intermediate

 representation of the source_program produced_by the front_end

 along with information in the symbol_table that is used to

 determine the run-time addresses of the data objects denoted

 by the names in the IR



 The many choices for the IR include three-address representations

 such_as quadruples triples indirect triples virtual machine

 representations such_as bytecodes and stack-machine code linear

 representations such_as postfix_notation and graphical

 representations such_as syntax_trees and 's Many of the

 algorithms in this_chapter are couched in terms of the

 representations considered in Chapter_inter-ch

 three-address_code trees and 's The techniques we discuss

 can be applied however to the other intermediate_representations

 as_well



 In this_chapter we assume that the front_end has scanned

 parsed and translated the source_program into a relatively

 low-level IR so that the

 values of the names appearing in the IR can be represented_by

 quantities that the target_machine can directly manipulate such

 as integers and floating-point_numbers We also assume that all

 syntactic and static semantic errors have_been detected that the

 necessary type_checking has taken place and that type-conversion

 operators have_been inserted wherever necessary The code

 generator can therefore proceed on the assumption that its input

 is free of these kinds of errors



 The Target Program



 The instruction-set architecture of the target_machine has a

 significant impact on the difficulty of constructing a good code

 generator that produces high-quality machine code The most_common

 target-machine architectures are (reduced instruction set

 computer) (complex instruction set computer) and stack

 based



 A machine typically has many registers three-address

 instructions simple addressing_modes and a relatively_simple

 instruction-set architecture In_contrast a machine

 typically has few registers two-address instructions a variety

 of addressing_modes several register classes variable-length

 instructions and instructions with side_effects



 In a stack-based machine operations are done by pushing operands

 onto a stack and then performing the operations on the operands at

 the top of the stack To achieve high performance the top of the stack

 is typically kept in registers Stack-based machines almost

 disappeared because it was felt that the stack organization was

 too limiting and required too_many swap and copy operations



 However stack-based architectures were revived with the introduction of

 the Java Virtual Machine (JVM) The JVM is a software interpreter

 for Java_bytecodes an intermediate language produced_by Java

 compilers The interpreter provides software compatibility across

 multiple platforms a major factor in the success of Java



 To overcome the high performance penalty of interpretation which

 can be on the order of a factor of 10 just-in-time (JIT)

 Java compilers have_been created These JIT compilers translate

 bytecodes during run_time to the native hardware instruction set

 of the target_machine Another_approach to improving Java

 performance is to build a compiler that compiles directly into the

 machine_instructions of the target_machine bypassing the Java

 bytecodes entirely



 Producing an absolute machine-language program as output has the

 advantage that it can be placed in a fixed location in memory and

 immediately executed Programs can be compiled and executed

 quickly



 Producing a relocatable machine-language program (often called an

 object module) as output allows subprograms to be compiled

 separately A set of relocatable object modules can be linked

 together and loaded for execution by a linking loader Although we

 must pay the added expense of linking and loading if we produce

 relocatable object modules we gain a great deal of flexibility in

 being able to compile subroutines separately and to call other

 previously compiled programs from an object module If the target

 machine does_not handle relocation automatically the compiler

 must provide explicit relocation information to the loader to link

 the separately compiled program modules



 Producing an assembly-language program as output makes the process

 of code_generation somewhat easier We can generate symbolic

 instructions and use the macro facilities of the assembler to help

 generate code The price paid is the assembly step after code

 generation



 In this_chapter we_shall use a very_simple -like computer

 as our target_machine We add to it some -like addressing

 modes so that we can also discuss code-generation techniques for

 machines For readability we use assembly code as the

 target language As_long as addresses can be calculated from

 offsets and other information stored in the symbol_table the code

 generator can produce relocatable or absolute addresses for names

 just as easily as symbolic addresses



 Instruction Selection

 instr-select-subsect



 The code_generator must map the IR program into a code sequence

 that can be executed by the target_machine The complexity of

 performing this mapping is determined by factors such_as



 itemize



 the level of the IR



 the nature of the instruction-set architecture



 the desired quality of the generated code



 itemize



 If the IR is high_level the code_generator may translate each IR

 statement into a sequence of machine_instructions using code

 templates Such statement-by-statement code_generation however

 often produces poor code that needs further optimization If the

 IR reflects some of the low-level details of the underlying

 machine then the code_generator can use this information to

 generate more_efficient code sequences



 The nature of the instruction set of the target_machine has a

 strong effect on the difficulty of instruction_selection For

 example the uniformity and completeness of the instruction set

 are important factors If the target_machine does_not support each

 data type in a uniform manner then each exception to the general

 rule requires special handling On some machines for example

 floating-point operations are done using separate registers



 Instruction speeds and machine idioms are other important factors

 If we do_not care_about the efficiency of the target program

 instruction_selection is straightforward For each type of

 three-address statement we can design a code skeleton that

 defines the target code to be generated for that construct For

 example every three-address statement of the form xyz where x_y and z are statically

 allocated can be translated_into the code sequence



 flushleft

 tabularl_l

 ' LD_R0 y R0 y' (load y into register R0)



 ' ADD R0_R0 z R0_R0 z' (add z to R0)



 ' ST x R0 x R0' (store R0 into x)



 tabular

 flushleft



 This strategy often produces redundant loads and stores For

 example the sequence of three-address_statements



 center

 tabularl

 'a_b c'



 'd a e'



 tabular

 center

 would be translated_into

 verbatim

 LD_R0 b R0 b

 ADD R0_R0 c R0_R0 c

 ST a R0 a R0

 LD_R0 a R0 a

 ADD R0_R0 e R0_R0 e

 ST d R0 d R0

 verbatim

 Here the fourth statement is redundant since it loads a value

 that has just been stored and so is the third if a is not

 subsequently used



 The quality of the generated code is usually determined by its

 speed and size On most machines a given IR program can be

 implemented_by many different code sequences with significant

 cost differences between the different implementations A naive

 translation of the intermediate_code may therefore lead to correct

 but unacceptably inefficient target code



 For_example if the target_machine has an increment

 instruction (INC) then the three-address statement aa1 may be_implemented more efficiently by the single

 instruction INC a rather_than by a more obvious sequence

 that loads a into a register adds one to the register and

 then stores the result back into a

 verbatim

 LD_R0 a R0 a

 ADD R0_R0 1 R0_R0 1

 ST a R0 a R0

 verbatim



 We need to know instruction costs in order to design good code sequences

 but unfortunately accurate cost information is often difficult to obtain

 Deciding which machine-code sequence is best for a given three-address

 construct may also require knowledge about the context in which that

 construct appears



 In Section tile-sect we_shall see that instruction_selection

 can be modeled as a tree-pattern_matching process in which we

 represent the IR and the machine_instructions as trees We then

 attempt to tile an IR tree with a set of subtrees that

 correspond to machine_instructions If we associate a cost with

 each machine-instruction subtree we can use dynamic_programming

 to generate_optimal code sequences Dynamic programming is

 discussed in Section dyn-prog-sect





 Register_Allocation



 A key problem in code_generation is deciding what values to hold

 in what registers Registers are the fastest computational unit on

 the target_machine but we usually do_not have enough of them to

 hold all values Values not held in registers need to reside in

 memory Instructions involving register operands are invariably

 shorter and faster_than those involving operands in memory so

 efficient utilization of registers is particularly important



 The use of registers is often subdivided into two subproblems



 enumerate



 Register_allocation during which we select the set of

 variables that will reside in registers at each point in the

 program



 Register assignment during which we pick the specific

 register that a variable will reside in



 enumerate



 Finding an optimal assignment of registers to variables is

 difficult even with single-register machines Mathematically the

 problem is NP-complete The problem is further complicated because

 the hardware andor the operating_system of the target_machine may

 require that certain register-usage conventions be observed



 exreg-pair-ex

 Certain machines require register-pairs (an even and

 next odd-numbered register) for some operands and results For

 example on some machines integer multiplication and integer

 division involve register pairs The multiplication instruction is

 of the form



 verbatim

 M x_y

 verbatim

 where x the multiplicand is the odd register of an

 evenodd register pair and y the multiplier can be anywhere

 The product occupies the entire evenodd register pair

 The division instruction is of the form



 verbatim

 D x_y

 verbatim

 where the dividend occupies an evenodd register pair whose even

 register is x the divisor is y After division the

 even register holds the remainder and the odd register the

 quotient



 Now_consider the two three-address_code sequences in

 Fig two-code-seq-fig in which the only_difference in (a)

 and (b) is the operator in the second statement The shortest

 assembly-code sequences for (a) and (b) are given in

 Fig optimal-seq-fig



 figurehtfb



 center

 tabularc_c

 t a b t a b



 t t c t t c



 t t d t t d







 (a)_(b)



 tabular

 center



 Two three-address_code sequences

 two-code-seq-fig

 figure



 figurehtfb



 center

 tabularl_l

 L R1a L R0 a



 A R1b A R0 b



 M R0c A R0 c



 D R0d SRDA R0 32



 ST R1t D R0 d



 ST R1 t







 (a)_(b)



 tabular

 center



 Optimal machine-code sequences

 optimal-seq-fig

 figure



 R stands_for register SRDA stands_for

 Shift-Right-Double-Arithmetic and SRDA R032 shifts the

 dividend into R1 and clears R0 so all bits equal its

 sign bit L ST and A stand_for load store

 and add respectively Note_that the optimal choice for the

 register into which a is to be loaded depends_on what will

 ultimately happen to t

 ex



 Strategies for register_allocation and assignment are discussed in

 Section ra-sect Section ershov-sect shows that for

 certain classes of machines we can construct code sequences that

 evaluate expressions using as few registers as possible



 Evaluation Order



 The order in which computations are performed can affect the

 efficiency of the target code As we_shall see some computation

 orders require fewer registers to hold intermediate results than

 others However picking a best order in the general case is a

 difficult NP-complete problem Initially we_shall avoid the

 problem by generating code for the three-address_statements in the

 order in which they have_been produced_by the intermediate_code

 generator In Chapter 10 we_shall study code scheduling for

 pipelined machines that can execute several operations in a single

 clock cycle

 Iteration Spaces

 it-space-sect



 In this_section we_shall begin to develop the idea of linear spaces

 and how they represent the behavior of a program There_are several

 different spaces that we need to learn about Later we_shall look_at a

 space whose dimensions are the indexes of an array_access Another space

 of importance has dimensions whose values locate a processor in the

 space Still another space has dimensions that represent constraints on

 the relative timing of steps in a program Here however we begin_with

 what is the simplest and most fundamental space the iteration

 space whose dimensions correspond to the indexes belonging to each of

 several nested_loops



 Basic Assumptions



 We_shall now begin a careful study of the most_common form of loops

 involving arrays those where



 enumerate



 The accesses to arrays involve linear functions of the loop_indexes

 such_as A2i1 These accesses will be referred to as

 affine the latter is simply a fancy term of linear_algebra

 for multiply by

 constants and add constants



 The possible combinations of values for the indexes of several nested

 loops can be described by linear_inequalities

 These inequalities describe the iteration_space



 enumerate

 The motivation for this study is to exploit the techniques that in

 simple settings like matrix_multiplication as in Section ch11mm

 were quite straightforward In the more general setting the same

 techniques apply but they are far less intuitive But by_applying some

 linear_algebra we can make everything work in the general setting



 Often the iteration_space is rectangular as in the

 matrix-multiplication example of Section mm-basic-fig There

 each of the nested_loops had a lower_bound of 0 and an upper_bound of

 However in more_complicated but still quite realistic loop

 nests the upper andor lower_bounds on one loop_index can depend_on the

 values of the indexes of the outer loops We_shall see an example

 shortly



 Constructing Iteration Spaces from Loop Nests



 To_begin let_us describe the sort of loop_nests (sets of nested

 loops) that can be handled by the techniques to be developed

 Each loop has a single loop_index which we assume is incremented

 by 1 in each iteration That assumption is without loss of generality

 since if the incrementation is by integer we can always replace

 uses of the index by uses of for some positive or negative

 constant and then increment by 1 in the loop



 ex

 Consider the loop



 verbatim

 for (i2 i100 ii3)

 Ai 0

 verbatim

 which increments by 3 each time around the loop The effect is to

 set to 0 each of the elements



 We can get the same effect with



 verbatim

 for_(j0 j32 j)

 A3j2 0

 verbatim

 That is we substitute for The lower limit becomes

 (just solve for ) and the upper limit

 becomes (simplify to get and round

 down because

 has to be an integer)

 ex



 Typically we_shall use for-loops in loop_nests A while-loop or

 repeat-loop can be replaced_by a for-loop if there is an index and

 upper and lower_bounds for the index as would be the case in something

 like the loop of Fig while-loops-fig(a) A for-loop like for_(i0 i100_i) serves exactly the same purpose



 figurehtfb



 verbatim

 i 0

 while (i100)

 some statements not involving i

 i_i1



 verbatim



 center

 (a) A while-loop with obvious limits

 center



 verbatim

 i 0

 while (1)

 some statements

 i_i1



 verbatim



 center

 (b) It is unclear when or if this loop terminates

 center



 verbatim

 i 0

 while (in)

 some statements not involving i or n

 i_i1



 verbatim



 center

 (c) We don't_know the value of so we don't_know when this loop terminates

 center



 Some while-loops

 while-loops-fig



 figure



 However some while- or repeat-loops have no obvious limit

 For_example Fig while-loops-fig(b) may or may not terminate

 but there is no way to tell what condition on in the unseen body of

 the loop causes the loop to break Figure while-loops-fig(c) is

 another problem case Variable might be a parameter of a function

 for example We know the loop iterates times but we don't_know

 what is at_compile time and in fact we may expect that different

 executions of the loop will execute different numbers of times In

 cases like (b) and (c) we must treat the upper limit on as

 infinity



 A -deep_loop nest can be modeled by a -dimensional space The

 dimensions are ordered with the th dimension representing the

 th nested loop counting from the outermost_loop inward

 A point in this space represents values for all

 the loop_indexes the outermost_loop index has value the second

 loop_index has value and so on The innermost_loop index has

 value



 But not all points in this space represent combinations of indexes that

 actually occur during execution of the loop_nest

 Each lower

 and upper loop bound defines an inequality which divides the

 iteration_space into two half spaces those that are iterations in the

 loop (the positive half space)

 and those that are not (the negative half space

 The conjunction (logical AND)

 of all the linear

 equalities represents the intersection of the positive half spaces which

 defines a convex_polyhedron which we call

 the iteration_space A convex_polyhedron has the property

 that if two points are in the polyhedron all points on the line between them

 are also in the polyhedron All the iterations in the loop are represented

 by the points with integer

 coordinates found within the polyhedron described by

 the loop-bound inequalities

 And conversely all integer points within the polyhedron represent

 iterations of the loop_nest at some time



 figurehtfb



 verbatim

 for_(i 0 i 5 i)

 for_(j i_j 7 j)

 Aij 0





 verbatim



 A 2-dimensional loop-nest

 loop-nest-fig



 figure



 ex

 ex2d

 Consider the 2-dimensional loop_nest in Fig loop-nest-fig

 We can model this two-deep loop_nest by the 2-dimensional polyhedron

 shown in Fig figitspace The two axes represent the

 values of the loop_indexes and Index can take on any

 integral value between 0 and 5 index can take on any integral

 value such that

 ex



 figurehtfb

 fileuullmanalsuch11figsiterationeps

 The iteration_space of Example ex2d

 figitspace

 figure



 Execution Order for Loop Nests



 A sequential_execution of a loop_nest sweeps through iterations in its

 iteration_space in an ascending lexicographic order

 A vector is lexicographically

 less_than another vector of the same length

 written

 if and only if

 there_exists an such that

 and





 Iteration Spaces and Array-Accesses

 In the code of Fig loop-nest-fig the iteration_space is also the

 portion of the array that the code accesses That sort of

 access where the array indexes are also loop_indexes in some order is

 very common However we should not confuse the space of iterations

 whose dimensions are loop_indexes with the space of affine array

 accesses as described in Section ch11index where the dimensions

 correspond to the array indexes If we had used in

 Fig loop-nest-fig an array_access like A2iij instead

 of Aij the difference would have_been apparent



 ex

 With as the outer_loop

 the iterations in the loop_nest in Example ex2d are executed in

 the order shown in Fig iter-order-fig

 ex



 figurehtfb





 arrayl

 00 01 02 03 04 05 06 07



 11 12 13 14 15 16 17



 22 23 24 25 26 27



 33 34 35 36 37



 44 45 46 47



 55 56 57



 array





 Iteration order for loop_nest of Fig loop-nest-fig

 iter-order-fig



 figure



 Matrix Formulation of Inequalities

 matrix-bounds-subsect



 The iterations in a -deep_loop can be

 represented mathematically as



 equation

 bib-eq

 i in Zd B i b 0

 equation

 Here



 enumerate



 as is conventional in mathematics represents the set of

 integers - positive negative and zero



 is a integer matrix



 is an

 integer vector of length and



 0 is a vector of 0's



 enumerate



 ex

 ineq-ex

 We can write the inequalities of Example ex2d as in

 Fig ineq-mat-fig

 That is the range of is described by and the range

 of is described by and

 We need to put each of these inequalities in the form

 Then becomes a row of the matrix B in the

 inequality (bib-eq) and becomes the

 corresponding component of the vector b

 For_instance is of this form with and

 This inequality is represented_by the first row of B and top

 element of b in Fig ineq-mat-fig



 figurehtfb







 arrayrr

 1_0



 -1_0



 -1 1



 0 -1



 array





 arrayr

 i



 j



 array







 arrayr

 0



 5



 0



 7



 array





 arrayr

 0



 0



 0



 0



 array







 Matrix-vector multiplication and a vector inequality represents

 the inequalities defining an iteration_space

 ineq-mat-fig



 figure



 As_another example the inequality is equivalent to

 and is represented_by the second row of B

 and b in Fig ineq-mat-fig Also becomes

 and is represented_by the third row Finally

 becomes and is the last_row of the matrix and

 vector

 ex



 Manipulating Inequalities

 To convert inequalities as in Example ineq-ex we can perform

 transformations much as we do for equalities eg adding or

 subtracting from both_sides or multiplying both_sides by a constant

 The only special rule we must remember is that when we multiply both

 sides by a negative number we have to reverse the direction of the

 inequality Thus multiplied by -1 becomes Adding

 5 to both_sides gives which is essentially the second row of

 Fig ineq-mat-fig



 Incorporating Symbolic Constants



 Sometimes we need to optimize a loop_nest that involves certain

 variables that are loop-invariant for all the loops in the nest We

 call such variables symbolic_constants but to describe the

 boundaries of an iteration_space we need to treat_them as variables and

 create an entry for them in the vector of loop_indexes which is

 the vector

 i in the general formulation of inequalities (bib-eq)



 ex

 Consider the simple loop

 verbatim

 for_(i 0 i_n i)





 verbatim

 This loop defines a one-dimensional iteration_space with index

 bounded by and Since is a symbolic_constant we

 need to include it as a variable giving us a vector of loop_indexes



 In matrix-vector form this iteration_space is defined by





 i in Z

 arrayrr

 1_0



 -1 1



 array





 arrayr

 i



 n



 array





 arrayr

 0



 0



 array







 Notice_that although the vector of array indexes has two dimensions

 only the first of these representing is part of the output - the

 set of points lying with the iteration_space

 ex

 Jumping Code for Boolean Expressions

 jumping-java-sect



 Jumping code for a boolean_expression is generated_by method

 jumping which takes two labels t and f as

 parameters called the true and false_exits of respectively

 The code contains a jump to t if evaluates to true and

 a jump to f if evaluates to false By convention the

 special_label 0 means that control falls through to the

 next instruction after the code for



 We begin_with class Constant The constructor Constant on line_4 takes a token tok and a type p as

 parameters It constructs a leaf in the syntax_tree with label

 tok and type p For_convenience the constructor Constant is overloaded (line_5) to create a constant object from

 an_integer



 footnotesize

 flushleft

 1)_package inter_File Constantjava



 2)_import lexer_import symbols



 3)_public class Constant extends_Expr



 4)_public Constant(Token tok Type p) super(tok p)



 5)_public Constant(int i) super(new Num(i) TypeInt)



 6)_public static final Constant



 7) True new Constant(WordTrue TypeBool)



 8) False new Constant(WordFalse TypeBool)



 9)_public void_jumping(int t int_f)



 10) if_( this True t 0 ) emit(goto_L t)



 11) else if_( this False f 0) emit(goto_L f)



 12)



 13)



 flushleft

 footnotesize



 Method jumping (lines 9-12 file Constantjava) takes

 two parameters labels t and f If this constant is

 the static object True (defined on line 7) and t is

 not the special_label 0 then a jump to t is

 generated Otherwise if this is the object False (defined

 on line 8) and f is nonzero then a jump to f is

 generated



 Class Logical provides some common functionality for classes

 Or And and Not expr1 and expr2

 (line_4) correspond to the operands of a logical operator

 (Although class Not implements a unary operator for

 convenience it is a subclass of Logical) The constructor

 Logical(tokab) (lines 5-10) builds a syntax node with

 operator tok and operands a and b In doing_so

 it uses function check to ensure_that both a and b are booleans Method gen will be discussed at the end of

 this_section



 footnotesize

 flushleft

 1)_package inter_File Logicaljava



 2)_import lexer_import symbols



 3)_public class Logical extends_Expr



 4)_public Expr expr1 expr2



 5) Logical(Token tok_Expr x1_Expr x2)



 6) super(tok null) null type to start



 7) expr1 x1 expr2 x2



 8) type check(expr1type expr2type)



 9) if (type null_) error(type_error)



 10)



 11) public_Type check(Type p1 Type p2)



 12) if_( p1 TypeBool p2 TypeBool_) return TypeBool



 13) else_return null



 14)



 15)_public Expr gen()



 16) int f newlabel() int a newlabel()



 17) Temp temp new Temp(type)



 18) thisjumping(0f)



 19) emit(temptoString() true)



 20) emit(goto_L a)



 21) emitlabel(f) emit(temptoString() false)



 22) emitlabel(a)



 23) return temp



 24)



 25) public_String toString()



 26) return expr1toString() optoString() expr2toString()



 27)



 28)



 flushleft

 footnotesize



 In class Or method jumping (lines 5-10) generates

 jumping_code for a boolean_expression For

 the moment suppose that neither the true exit t nor the

 false exit f of is the special_label 0 Since

 is true if is true the true exit of must_be t

 and the false exit corresponds to the first instruction of

 The true and false_exits of are the same as those of



 footnotesize

 flushleft

 1)_package inter_File Orjava



 2)_import lexer_import symbols



 3)_public class Or extends Logical



 4)_public Or(Token tok_Expr x1_Expr x2) super(tok x1_x2)



 5)_public void_jumping(int t int_f)



 6) '_int label t 0 t newlabel()'



 7) expr1jumping(label 0)



 8) expr2jumping(tf)



 9) if( t 0 ) emitlabel(label)



 10)



 11)



 flushleft

 footnotesize



 In the general case t the true exit of can be the

 special_label 0 Variable label (line 6 file Orjava) ensures that the true exit of is set properly to

 the end of the code for If t is 0 then label is set to a new_label that is emitted after code_generation

 for both and



 The code for class And is similar to the code for Or



 footnotesize

 flushleft

 1)_package inter_File Andjava



 2)_import lexer_import symbols



 3)_public class And extends Logical



 4)_public And(Token tok_Expr x1_Expr x2) super(tok x1_x2)



 5)_public void_jumping(int t int_f)



 6) '_int label f 0 f newlabel()'



 7) expr1jumping(0 label)



 8) expr2jumping(tf)



 9) if( f 0 ) emitlabel(label)



 10)



 11)



 flushleft

 footnotesize



 Class Not has enough in common with the other boolean

 operators that we make it a subclass of Logical even_though

 Not implements a unary operator The superclass expects two

 operands so x2 appears twice in the call to super on

 line_4 Only expr2 (declared on line_4 file Logicaljava) is used in the methods on lines 5-6 On line 5

 method jumping simply calls expr2jumping with the true

 and false_exits reversed



 footnotesize

 flushleft

 1)_package inter_File Notjava



 2)_import lexer_import symbols



 3)_public class Not extends Logical



 4)_public Not(Token tok_Expr x2) super(tok x2 x2)



 5)_public void_jumping(int t int_f) expr2jumping(f t)



 6)_public String_toString() return optoString() expr2toString()



 7)



 flushleft

 footnotesize



 Class Rel implements the operators and Function check (lines

 5-9) checks that the two operands have the same type and that they

 are not arrays For_simplicity coercions are not permitted



 footnotesize

 flushleft

 1)_package inter_File Reljava



 2)_import lexer_import symbols



 3)_public class Rel extends Logical



 4)_public Rel(Token tok_Expr x1_Expr x2) super(tok x1_x2)



 5)_public Type_check(Type p1 Type p2)



 6) if_( p1 instanceof_Array p2 instanceof_Array )_return null



 7) else_if( p1 p2 )_return TypeBool



 8) else_return null



 9)



 10)_public void_jumping(int t int_f)



 11) Expr a expr1reduce()



 12) Expr b expr2reduce()



 13) String test atoString() optoString() btoString()



 14) emitjumps(test t f)



 15)



 16)



 flushleft

 footnotesize



 Method jumping (lines 10-15 file Reljava) begins by

 generating code for the subexpressions expr1 and expr2 (lines

 11-12) It then calls method emitjumps defined on lines

 10-18 file Exprjava in Section expr-java-sect If

 neither t nor f is the special_label 0 then

 emitjumps executes the following



 footnotesize

 flushleft

 12) emit(if test goto_L t) File Exprjava



 13) emit(goto_L f)



 flushleft

 footnotesize



 At most one instruction is generated if either t

 or f is the special_label 0 (again from file Exprjava)



 footnotesize

 flushleft

 15) else_if( t 0 ) emit(if test goto_L t)



 16) else_if( f 0 ) emit(iffalse test goto_L f)



 17) else nothing since both t and f fall_through



 flushleft

 footnotesize



 For another use of emitjumps consider the code for class

 Access The source_language allows boolean values to be

 assigned to identifiers and array_elements so a boolean

 expression can be an array_access Class Access has method

 gen for generating normal code and method jumping

 for jumping_code Method jumping (line 11) calls emitjumps after reducing this array_access to a temporary The

 constructor (lines 6-9) is called with a flattened array a

 an index i and the type p of an element in the

 flattened array Type_checking is done during array address

 calculation



 footnotesize

 flushleft

 1)_package inter_File Accessjava



 2)_import lexer_import symbols



 3)_public class Access extends_Op



 4)_public Id array



 5)_public Expr index



 6)_public Access(Id a Expr i Type p) p is element type after



 7) super(new Word( TagINDEX) p) flattening the array



 8) array a index i



 9)



 10)_public Expr gen() return_new Access(array indexreduce() type)



 11) public_void jumping(int tint f) emitjumps(reduce()toString()tf)



 12) public_String toString()



 13) return arraytoString() indextoString()



 14)



 15)



 flushleft

 footnotesize



 Jumping code can also be used to return a boolean value Class

 Logical earlier in this_section has a method gen

 (lines 15-24) that returns a temporary temp whose value is

 determined by the flow of control through the jumping_code for

 this expression At the true exit of this boolean_expression

 temp is assigned true at the false exit temp

 is assigned false The temporary is declared on line 17 Jumping

 code for this expression is generated on line 18 with the true

 exit being the next instruction and the false exit being a new

 label f The next instruction assigns true to temp (line 19) followed_by a jump to a new_label a (line

 20) The code on line 21 emits label f and an instruction

 that assigns false to temp The code_fragment ends

 with label a generated on line 22 Finally gen returns

 temp (line 23)

 Implementing L-Attributed SDD's

 l-att-sect



 Since many translation applications can be addressed using

 L-attributed definitions we_shall consider their implementation

 in more_detail in this_section The following methods do

 translation by traversing a parse_tree



 enumerate



 Build the parse_tree and annotate This method works

 for any noncircular SDD whatsoever We introduced annotated_parse

 trees in Section ann-pt-subsect



 Build the parse_tree add actions and execute the

 actions in preorder This_approach works for any L-attributed

 definition We discussed how to turn an L-attributed_SDD into

 an SDT in Section l-attr-sdt-subsect in particular we

 discussed how to embed actions into productions based_on the

 semantic_rules of such an SDD



 enumerate



 In this_section we discuss the following methods for

 translation during_parsing



 enumerate



 3 rec-desc-item Use a recursive-descent

 parser with one function for each nonterminal The function for

 nonterminal receives the inherited_attributes of as

 arguments and returns the synthesized_attributes of



 4 Generate_code on the fly using a

 recursive-descent_parser



 5 Implement an SDT in conjunction with an

 LL-parser The attributes are kept on the parsing stack and the

 rules fetch the needed attributes from known locations on the

 stack



 6 Implement an SDT in conjunction with an

 LR-parser This method may be surprising since the SDT for an

 L-attributed_SDD typically has actions in the middle of

 productions and we cannot be_sure during an LR parse that we are

 even in that production until its entire body has_been

 constructed We_shall see however that if the underlying_grammar

 is LL we can always handle both the parsing and translation

 bottom-up



 enumerate



 Translation During Recursive-Descent Parsing

 sdt-rec-desc-subsect



 A recursive-descent_parser has a function for each nonterminal

 as discussed in Section rec-desc-subsect We can

 extend the parser into a translator as_follows



 itemize



 a) The arguments of function are the inherited

 attributes of nonterminal



 b) The return-value of function is the collection of

 synthesized_attributes of nonterminal



 itemize

 In the body of function we need to both parse and handle

 attributes



 enumerate



 Decide upon the production used to expand



 Check that each terminal appears on the input when it is

 required We_shall assume that no backtracking is needed but the

 extension to recursive-descent_parsing with backtracking can be done

 by restoring the input position upon failure as discussed in

 Section rec-desc-subsect



 Preserve in local_variables the values of all attributes

 needed to compute inherited_attributes for nonterminals in the

 body or synthesized_attributes for the head nonterminal



 Call functions corresponding to nonterminals in the body of

 the selected production providing them with the proper arguments

 Since the underlying SDD is L-attributed we have_already

 computed these attributes and stored them in local_variables



 enumerate



 ex

 while-rec-desc-ex Let_us consider the SDD and SDT of

 Example while-ex for while-statements A pseudocode

 rendition of the relevant parts of the function appears in

 Fig while-rec-desc-fig



 figurehtfb



 center

 tabularl

 string



 string local_variables holding code fragments



 label the local labels



 if_( current_input token while )



 advance input



 check is next on the input and advance















 check is next on the input and advance







 return(label )







 else other statement types







 tabular

 center



 Implementing while-statements with a recursive-descent_parser

 while-rec-desc-fig



 figure



 We show as storing and returning long strings In_practice it

 would be far more_efficient for functions like and to

 return pointers to records that represent these strings Then the

 return-statement in function would not physically concatenate

 the components shown but rather would construct a record or

 perhaps tree of records expressing the concatenation of the

 strings represented_by and the

 labels and and the two_occurrences of the literal

 string label

 ex



 ex

 tex-rec-desc-ex Now let_us take_up the SDT of

 Fig_tex-box-sdt-fig for typesetting boxes

 First we address parsing since the underlying_grammar in

 Fig_tex-box-sdt-fig is ambiguous The following transformed

 grammar makes juxtaposition and subscripting right associative

 with sub taking precedence_over juxtaposition



 center

 tabularr_c l

















 tabular

 center

 The two new nonterminals and are motivated by terms and

 factors in expressions Here a factor generated_by is

 either a parenthesized box or a text_string

 A term generated

 by is a factor with a sequence of subscripts and a box

 generated_by is a sequence of juxtaposed terms



 The attributes of carry over to and since the new

 nonterminals also denote boxes they_were introduced simply to aid

 parsing Thus both and have an inherited_attribute

 and synthesized_attributes and

 with semantic_actions that can be adapted from the

 SDT in Fig_tex-box-sdt-fig



 The grammar is not_yet ready for top-down_parsing since the

 productions for and have common prefixes Consider

 for instance A top-down parser cannot choose between the two

 productions for by_looking one symbol ahead in the input

 Fortunately we can use a form of left-factoring discussed in

 Section left-factor-subsect to make the grammar ready With

 SDT's the notion of common prefix applies to actions as_well

 Both productions for begin_with the nonterminal inheriting

 attribute from



 The pseudocode in Fig tex-rec-desc-fig for

 folds in the code for After left-factoring is

 applied to there is only one

 call to the pseudocode shows the result of substituting the

 code for in place of this call



 The function will be called as by the function for

 which we do_not show It returns a pair consisting of the

 height and depth of the box generated_by nonterminal in

 practice it would return a record containing the height and

 depth



 figurehtfb



 center

 tabularl

 (float float)



 float locals to hold heights and depths



 start code for



 if_( current_input )



 advance input







 if (current input ) syntax error expected



 advance input







 else if_( current_input text )



 let lexical value textlexval be



 advance input















 else syntax error expected text or



 end code for



 if_( current_input sub )



 advance input







 return







 return







 tabular

 center



 Recursive-descent typesetting of boxes

 tex-rec-desc-fig



 figure



 Function begins by checking for a left_parenthesis in which

 case it must have the production to work with

 It saves whatever the inside the parentheses returns but if

 that is not followed_by a right_parenthesis then there is a

 syntax error which must_be handled in a manner not shown



 Otherwise if the current_input is text then the function

 uses getHt and getDp to determine the height and

 depth of this text



 then decides whether the next box is a subscript and adjusts

 the point size if so We use the actions associated_with the

 production in

 Fig_tex-box-sdt-fig for the height and depth of the larger

 box Otherwise we simply return what would have returned



 ex



 On-The-Fly Code_Generation

 sdt-fly-subsect



 The construction of long strings of code that are attribute

 values as in Example while-rec-desc-ex is undesirable for

 several reasons including the time it could take to copy or move

 long strings In common cases such_as our_running code-generation

 example we can instead incrementally generate pieces of the code

 into an array or output_file by executing actions in an SDT

 The elements we need to make this technique work are



 enumerate



 There is for one or_more nonterminals a main

 attribute For_convenience we_shall assume that the main

 attributes are all string valued In

 Example while-rec-desc-ex the attributes and

 are main_attributes the other attributes are not



 The main_attributes are synthesized



 The rules that evaluate the main attribute(s) ensure_that

 enumerate



 The main attribute is the concatenation of main_attributes

 of nonterminals appearing in the body of the production involved

 perhaps with other elements that are not main_attributes such_as

 the string label or the values of labels and



 The main_attributes of nonterminals appear in the rule in

 the same order as the nonterminals themselves appear in the

 production_body

 enumerate



 enumerate

 As a consequence of the above conditions the main attribute can

 be constructed by emitting the non-main-attribute elements of the

 concatenation We can rely_on the recursive_calls to the

 functions for the nonterminals in a production_body to

 emit the value of their main attribute incrementally



 The Type of Main Attributes Our simplifying

 assumption that main_attributes are of string type is really too

 restrictive The true requirement is that the type of all the main

 attributes must have values that can be constructed by

 concatenation of elements For_instance a list of objects of any

 type would be appropriate as_long as we represent these lists in

 a way that allows elements to be efficiently appended to the end

 of the list Thus if the purpose of the main attribute is to

 represent a sequence of intermediate-code statements we could

 produce the intermediate_code by writing statements to the end of

 an array of objects Of_course the requirements stated in

 Section sdt-fly-subsect still apply to lists for example

 main_attributes must_be assembled from other main_attributes by

 concatenation in order



 ex

 while-fly-ex We can modify the function of

 Fig while-rec-desc-fig to emit elements of the main

 translation instead of saving them for

 concatenation into a return value of The revised

 function appears in Fig while-fly-fig



 figurehtfb



 center

 tabularl

 void



 label the local labels



 if_( current_input token while )



 advance input



 check is next on the input and advance



















 check is next on the input and advance















 else other statement types







 tabular

 center



 On-the-fly recursive-descent code_generation for

 while-statements while-fly-fig



 figure



 In Fig while-fly-fig and now have no return value

 since their only synthesized_attributes are produced_by printing

 Further the position of the print statements is significant The

 order in which output is printed is first label then

 the code for (which is the same as the value of

 in Fig while-rec-desc-fig) then label and

 finally the code from the recursive call to (which is the same

 as in Fig while-rec-desc-fig) Thus the

 code printed by this call to is exactly the same as the return

 value in Fig while-rec-desc-fig

 ex



 Incidentally we can make the same change to the underlying

 SDT turn the construction of a main attribute into actions

 that emit the elements of that attribute In

 Fig while-sdt-fly-fig we see the SDT of

 Fig while-sdt-fig revised to generate code on the fly



 figurehtfb



 center

 tabularl_l l_l

 while to 10pt







 to 10pt







 tabular

 center



 SDT for on-the-fly code_generation for while

 statements while-sdt-fly-fig



 figure



 L-Attributed SDD's and LL Parsing

 l-att-ll-subsect



 Suppose that an L-attributed_SDD is based_on an LL-grammar and

 that we have converted it to an SDT with actions embedded in

 the productions as described in Section l-attr-sdt-subsect

 We can then perform the translation during LL parsing by extending

 the parser stack to hold actions and certain data items needed for

 attribute evaluation Typically the data items are copies of

 attributes



 In_addition to records representing terminals and nonterminals

 the parser stack will hold action-records representing

 actions to be executed and synthesize-records to hold the

 synthesized_attributes for nonterminals We use the following two

 principles to manage attributes on the stack



 itemize



 The inherited_attributes of a nonterminal are placed in

 the stack record that represents that nonterminal The code to

 evaluate these attributes will usually be represented_by an

 action-record immediately above the stack record for in fact

 the conversion of L-attributed SDD's to SDT's ensures that the

 action-record will be immediately above



 The synthesized_attributes for a nonterminal are placed

 in a separate synthesize-record that is immediately below the

 record for on the stack



 itemize

 This strategy places records of several types on the parsing

 stack trusting that these variant record types can be managed

 properly as subclasses of a stack-record class In_practice

 we might combine several records into one but the ideas are

 perhaps best explained by separating data used for different

 purposes into different records



 Action-records contain pointers to code to be executed Actions

 may also appear in synthesize-records these actions typically

 place copies of the synthesized attribute(s) in other records

 further down the stack where the value of that attribute will be

 needed after the synthesize-record and its attributes are popped

 off the stack



 Let_us take a brief look_at LL parsing to see the need to make

 temporary copies of attributes From

 Section nonrec-pp-subsect a table-driven LL parser mimics a

 leftmost_derivation If is the input that has_been matched so

 far then the stack_holds a sequence of grammar_symbols

 such that where is the start_symbol

 When the parser expands by a production it

 replaces on top of the stack by



 Suppose nonterminal has an inherited_attribute With

 the inherited_attribute may depend not only

 on the inherited_attributes of but on all the attributes of

 Thus we may need to process completely before can be

 evaluated We therefore save temporary copies of all the

 attributes needed to evaluate in the action-record that

 evaluates Otherwise when the parser replaces on top of

 the stack by the inherited_attributes of will have

 disappeared along with its stack record



 Since the underlying SDD is L-attributed we can be_sure that

 the values of the inherited_attributes of are available when

 rises to the top of the stack The values will therefore be

 available in time to be copied_into the action-record that

 evaluates the inherited_attributes of Furthermore space for

 the synthesized_attributes of is not a problem since the

 space is in the synthesize-record for which remains on the

 stack below and when the parser expands by





 As is processed we can perform actions (through a record just

 above on the stack) that copy its inherited_attributes for use

 by as needed and after is processed the

 synthesize-record for can copy its synthesized_attributes for

 use by if needed Likewise synthesized_attributes of may

 need temporaries to help compute their value and these can be

 copied to the synthesize-record for as and then are

 processed The principle that makes all this copying of attributes

 work is



 itemize



 All copying takes place among the records that are created

 during one expansion of one nonterminal Thus each of these

 records knows how far below it on the stack each other record is

 and can write values into the records below safely



 itemize



 The next example_illustrates the implementation of inherited

 attributes during LL parsing by diligently copying attribute

 values Shortcuts or optimizations are possible particularly with

 copy rules which simply copy the value of one attribute into

 another Shortcuts are deferred until

 Example while-ll-full-ex which also illustrates

 synthesize-records



 ex

 while-ll-fly-ex

 This example implements the SDT of

 Fig while-sdt-fly-fig which generates code on the fly for

 the while-production This SDT does_not have synthesized

 attributes except for dummy attributes that represent labels



 Figure while-ll1-fig(a) shows the situation as we are about

 to use the while-production to expand presumably because

 the lookahead_symbol on the input is while The record at

 the top of stack is for and it contains only the inherited

 attribute which we suppose has the value

 Since we are now parsing top-down we show the stack top at the

 left according to our usual convention





 figurehtfb



 Expansion of according to the while-statement

 production while-ll1-fig

 figure



 Figure while-ll1-fig(b) shows the situation immediately

 after we have expanded There_are action-records in front of

 the nonterminals and corresponding to the actions in

 the underlying SDT of Fig while-sdt-fly-fig The record

 for has room for inherited_attributes and

 while the record for has room for attribute

 as all -records must We show values for these

 fields as because we do_not yet know their values



 The parser next recognizes while and on the input and

 pops their records off the stack Now the first action is at the

 top and it must_be executed This action-record has a field

 which holds a copy of the inherited_attribute

 When is popped from the stack the value of

 is copied_into the field for use

 during the evaluation of the inherited_attributes for The

 code for the first action generates new values for and

 which we_shall suppose are and respectively The next

 step is to make the value of The assignment

 is written knowing it

 is only executed when this action-record is at the top of stack

 so refers to the record below it - the record for





 The first action-record then copies into field in the

 second action where it will be used to evaluate

 It also copies into a field called

 of the second action this value is needed for that action-record

 to print its output properly Finally the first action-record

 prints label to the output





 figurehtfb

 After

 the action above is performed while-ll2-fig

 figure



 The situation after completing the first action and popping its

 record off the stack is shown in Fig while-ll2-fig The

 values of inherited_attributes in the record for have_been

 filled in properly as have the temporaries and in the

 second action record At this point is expanded and we

 presume that the code to implement its test containing jumps to

 labels and as appropriate is generated When the

 -record is popped from the stack the record for becomes

 top and causes the parser to check for on its input



 With the action above at the top of the stack its code sets

 and emits label When that is done

 the record for becomes the top of stack and as it is

 expanded we presume it correctly generates code that implements

 whatever kind of statement it is and then jump to label

 ex



 ex

 while-ll-full-ex Now let_us consider the same

 while-statement but with a translation that produces the output

 as a synthesized_attribute rather_than by

 on-the-fly generation In order to follow the explanation it is

 useful to bear in mind the following invariant or inductive

 hypothesis which we assume is followed for every nonterminal



 itemize



 Every nonterminal that has code associated_with it leaves

 that code as a string in the synthesize-record just below it on

 the stack

 itemize

 Assuming this statement is true we_shall handle the

 while-production so it maintains this statement as an invariant



 Figure while-ll3-fig(a) shows the situation just_before

 is expanded using the production for while-statements At the top

 of the stack we see the record for it has a field for its

 inherited_attribute as in

 Example while-ll-fly-ex Immediately below that record is

 the synthesize-record for this occurrence of The latter has a

 field for as all synthesize-records for must

 have We also show it with some other fields for local storage and

 actions since the SDT for the while production in

 Fig while-sdt-fig is surely part of a larger SDT





 figurehtfb





 Expansion of with synthesized_attribute constructed

 on the stack while-ll3-fig

 figure



 Our expansion of is based_on the SDT of

 Fig while-sdt-fig and it is shown in

 Fig while-ll3-fig(b) As a shortcut during the expansion

 we assume that the inherited_attribute is assigned

 directly to rather_than being placed in the

 first action and then copied_into the record for



 Let_us examine what each record does when it becomes the top of

 stack First the while record causes the token while

 to be matched with the input which it must or else we would not

 have expanded in this way After while and are

 popped_off the stack the code for the action-record is executed

 It generates values for and and we take the shortcut of

 copying them directly to the inherited_attributes that need them

 and The last two steps of the

 action cause and to be copied_into the record called

 Synthesize



 The synthesize-record for does double duty not only will it

 hold the synthesized_attribute but it will also

 serve as an action-record to complete the evaluation of the

 attributes for the entire production

 In_particular when

 it gets to the top it will compute the synthesized_attribute

 and place its value in the synthesize-record for

 the head



 When becomes the top of the stack it has both its inherited

 attributes computed By the inductive_hypothesis stated above we

 suppose it correctly generates code to execute its condition and

 jump to the proper label We also assume that the actions

 performed during the expansion of correctly place this code in

 the record below as the value of synthesized_attribute





 After is popped the synthesize-record for

 becomes the top Its code is needed in the synthesize-record for

 because that is where we concatenate all the

 code elements to form The synthesize-record for

 therefore has an action to copy

 into the synthesize-record for After doing_so

 the record for token reaches the top of stack and causes a

 check for on the input Assuming that test succeeds the

 record for becomes the top of stack By our inductive

 hypothesis this nonterminal is expanded and the net effect is

 that its code is correctly constructed and placed in the field for

 in the synthesize-record for



 Now all the data fields of the synthesize-record for have

 been filled in so when it becomes the top of stack the action in

 that record can be executed The action causes the labels and code

 from and to be concatenated in

 the proper order The resulting string is placed in the record

 below that is in the synthesize-record for We have now

 correctly computed and when the synthesize-record

 for becomes the top that code is available for placement in

 another record further down the stack where it will_eventually be

 assembled into a larger string of code implementing a program

 element of which this is a part

 ex



 Bottom-Up Parsing of L-Attributed SDD's

 l-att-lr-subsect



 Can We Handle L-Attributed SDD's on LR Grammars

 In Section bus-subsect we saw that every S-attributed

 SDD on an LR grammar can be_implemented during a bottom-up

 parse From Section l-att-ll-subsect every L-attributed

 SDD on an LL grammar can be_parsed top-down Since LL_grammars

 are a proper subset of the LR_grammars and the S-attributed

 SDD's are a proper subset of the L-attributed SDD's can we

 handle every LR grammar and L-attributed_SDD bottom-up



 We cannot as the following intuitive argument shows Suppose we

 have a production in an LR-grammar and there is

 an inherited_attribute that depends_on inherited_attributes

 of When we reduce to we still have not seen the input

 that generates so we cannot be_sure that we have a body of

 production Thus we cannot compute yet

 since we are unsure whether to use the rule_associated with this

 production



 Perhaps we could wait_until we have reduced to and know that

 we must reduce to However even then we do_not know

 the inherited_attributes of because even after reduction we

 may not be_sure of the production_body that contains this We

 could reason that this decision too should be deferred and

 therefore further defer the computation of If we keep

 reasoning this way we soon realize that we cannot make any

 decisions until the entire input is parsed Essentially we have

 reached the strategy of build the parse_tree first and then

 perform the translation



 We can do bottom-up every translation that we can do top-down

 More_precisely given an L-attributed_SDD on an LL grammar

 we can adapt the grammar to compute the same SDD on the new

 grammar during an LR

 parse The trick has three_parts



 enumerate



 Start with the SDT constructed as in

 Section l-attr-sdt-subsect which places embedded actions

 before each nonterminal to compute its inherited_attributes and an

 action at the end of the production to compute synthesized

 attributes



 Introduce into the grammar a marker_nonterminal in place of

 each embedded action Each such place gets a distinct marker and

 there is one production for any marker namely





 Modify the action if marker_nonterminal replaces it

 in some production

 and associate with an action that

 enumerate

 Copies as inherited_attributes of any attributes of or symbols

 of that action needs

 Computes attributes in the same way as but makes those attributes

 be synthesized_attributes of

 enumerate

 This change appears illegal since typically the action_associated

 with production will have to access attributes

 belonging to grammar_symbols that do_not appear in this

 production However we_shall implement the actions on the LR

 parsing stack so the necessary attributes will always be

 available a known number of positions down the stack



 enumerate



 ex

 marker-ex Suppose that there is a production

 in an LL grammar and the inherited_attribute is

 computed from inherited_attribute by some formula

 That is the fragment of an SDT we care_about is



 center



 center

 We introduce marker with inherited_attribute and

 synthesized_attribute The former will be a copy of

 and the latter will be The SDT will be written



 center

 tabularl_l l









 tabular

 center



 Notice_that the rule for does_not have available to it

 but in fact we_shall arrange that every inherited_attribute for a

 nonterminal such_as appears on the stack immediately below

 where the reduction to will later take place Thus when we

 reduce to we_shall find immediately below

 it from where it may be read Also the value of which is

 left on the stack along with is really and properly is

 found right below where the reduction to will later occur

 ex



 Why Markers Work Markers are nonterminals that

 derive only and that appear only once among all the

 bodies of all productions We_shall not give a formal proof that

 when a grammar is LL marker_nonterminals can be added at any

 position in the body and the resulting grammar will still be LR

 The intuition however is as_follows If a grammar is LL then we

 can determine that a string on the input is derived_from

 nonterminal in a derivation that starts with production

 by seeing only the first symbol of (or the

 following symbol if ) Thus if we parse

 bottom-up then the fact that a prefix of must_be reduced to

 and then to is known_as soon_as the beginning of

 appears on the input In_particular if we insert markers anywhere

 in the LR states will incorporate the fact that this

 marker has to be there and will reduce to the marker

 at the appropriate point on the input



 ex

 while-lr-ex Let_us turn the SDT of

 Fig while-sdt-fig into an SDT that can operate with an

 LR parse of the revised grammar We introduce a marker before

 and a marker before so the underlying_grammar

 becomes



 center

 tabularl_l l

 while











 tabular

 center

 Before we discuss the actions that are associated_with markers

 and let_us outline the inductive_hypothesis about where

 attributes are stored



 enumerate



 Below the entire body of the while-production - that is

 below while on the stack - will be the inherited_attribute

 We may not know the nonterminal or parser state

 associated_with this stack record but we can be_sure that it will

 have a field in a fixed position of the record that holds

 before we begin to recognize what is derived_from

 this



 Inherited_attributes and

 will be just below the stack record for Since the grammar is

 presumed to be LL the appearance of while on the input

 assures us that the while-production is the only one that can be

 recognized so we can be_sure that will appear immediately

 below on the stack and 's record will hold the inherited

 attributes of



 Similarly the inherited_attribute must

 appear immediately below on the stack so we may place that

 attribute in the record for



 The synthesized_attribute will appear in the

 record for As always when we have a long string as an

 attribute value we expect that in practice a pointer to (an

 object representing) the string will appear in the record while

 the string itself is outside the stack



 Similarly the synthesized_attribute will

 appear in the record for



 enumerate



 Let_us follow the parsing process for a while-statement Suppose

 that a record holding appears on the top of the

 stack and the next_input is the terminal while We shift

 this terminal onto the stack It is then certain that the

 production being recognized is the while-production so the LR

 parser can shift and determine that its next step must_be

 to reduce to The stack at this time is shown in

 Fig while-lr1-fig We also show in that figure the action

 that is associated_with the reduction to We create values for

 and which live in fields of the -record Also in

 that record are fields for and

 These attributes must_be in the second and third fields of the

 record for consistency with other stack records that might appear

 below in other contexts and also must provide these attributes

 for The action completes by assigning values to

 and one from the just

 generated and the other by reaching down the stack to where we

 know is found



 figurehtfb



 LR

 parsing stack after reduction of to

 while-lr1-fig

 figure



 We presume that the next inputs are properly reduced to The

 synthesized_attribute is therefore placed in the

 record for This change to the stack is shown in

 Fig while-lr2-fig which also_incorporates the next several

 records that are later placed above on the stack



 figurehtfb



 Stack

 just_before reduction of the while-production body to

 while-lr2-fig

 figure



 Continuing with the recognition of the while-statement the parser

 should next find ) on the input which it pushes onto the

 stack in a record of its_own At that point the parser which

 knows it is working on a while-statement because the grammar is

 LL will reduce to The single piece of data

 associated_with is the inherited_attribute

 Note_that this attribute needs to be in the record for because

 that will be just below the record for The code that is

 executed to compute the value of is



 center



 center

 This action reaches three records below which is at the top

 of stack when the code is executed and retrieves the value of





 Next the parser reduces some prefix of the remaining_input to

 which we have consistently referred to as to

 distinguish it from the at the head of the production The

 value of is computed and appears in the stack

 record for This step takes us to the condition that is

 illustrated in Fig while-lr2-fig



 At this point the parser will reduce everything from while to

 to

 The code that is executed during this reduction is



 center

 tabularl

















 tabular

 center

 That is we construct the value of in a variable

 tempCode That code is the usual consisting of the two

 labels and the code for and the code for The

 stack is popped so appears where while was The value

 of the code for is placed in the field of that

 record where it can be interpreted as the synthesized_attribute

 Note_that we do_not show in any of this

 discussion the manipulation of LR states which must also appear

 on the stack in the field that we have populated with grammar

 symbols

 ex



 exer

 Implement each of your_SDD's of Exercise_genl-while-exer

 as a recursive-descent_parser in the style of

 Section sdt-rec-desc-subsect

 exer



 exer

 Implement each of your_SDD's of Exercise_genl-while-exer

 as a recursive-descent_parser in the style of

 Section sdt-fly-subsect

 exer



 exer

 Implement each of your_SDD's of Exercise_genl-while-exer

 with an LL parser in the style of Section l-att-ll-subsect

 with code generated on the fly

 exer



 exer

 Implement each of your_SDD's of Exercise_genl-while-exer

 with an LL parser in the style of Section l-att-ll-subsect

 but with code (or pointers to the code) stored on the stack

 exer



 exer

 Implement each of your_SDD's of Exercise_genl-while-exer

 with an LR_parser in the style of Section l-att-lr-subsect

 exer



 exer

 Implement your SDD of Exercise bit-position-exer in the

 style of Section sdt-rec-desc-subsect Would an

 implementation in the style of Section sdt-fly-subsect be

 any different

 exer

 Lexical Analysis

 lexan-sect



 A lexical_analyzer reads characters from the input and groups them

 into tokens these tokens become the terminal_symbols

 that are input to the parser A sequence of input

 characters that comprises a single token is called a lexeme

 Thus we can say that the lexical_analyzer insulates a parser from the lexeme

 representation of tokens



 The lexical_analyzer in this_section allows numbers identifiers

 and white_space (blanks tabs and newlines)

 to appear within

 expressions It can be used to extend the expression translator of

 the previous section



 Since the expression grammar of Fig_post-scheme2-fig must

 be extended to allow numbers and identifiers we take this

 opportunity to allow multiplication and division as_well The

 extended translation_scheme appears in Fig expr-scheme-fig

 In the semantic_actions for nonterminal factor token num is assumed to have an attribute numvalue which

 gives the integer value corresponding to this occurrence of the

 token num Token id has a string-valued_attribute

 written as idlexeme we assume this string is the

 actual lexeme comprising this instance of the token_id



 figurehtfb

 center

 tabularr_c l_l

 print





 print













 print





 print













 (_)



 num





 id



 tabular

 center

 Actions for translating into_postfix notation

 expr-scheme-fig

 figure



 The pseudocode_fragments used to illustrate the workings of a

 lexical_analyzer will be assembled into Java code at the end of

 this_section Symbol_tables or data_structures for holding

 information_about identifiers are considered in Section symtab-sect





 Removal of White Space and Comments

 white-space-subsect



 The expression translator in Section_postfix-sect sees every

 character in the input so extraneous characters such_as blanks

 will cause it to fail Most_languages allow arbitrary amounts of

 white_space to appear

 between tokens Comments are likewise ignored during_parsing so

 they may also be treated_as white_space



 If white_space is eliminated by the lexical_analyzer the parser

 will never have to consider it The alternative of modifying the

 grammar to incorporate white_space into the syntax is not nearly

 as easy to implement



 The following pseudocode skips white_space by reading input

 characters as_long as it_sees a blank a tab or a newline

 Variable peek_holds the next_input character Line numbers

 are useful within error_messages to help pinpoint errors so the

 code uses variable line to count newline characters in the

 input



 center

 tabularl

 for (_peek next_input character )



 if_( peek is a blank or a tab ) do_nothing



 else if_( peek is a newline ) line line1



 else_break





 tabular

 center



 Reading Ahead

 readahead-subsect



 A lexical_analyzer may need to read ahead some characters before

 it can decide on the token to be returned to the parser For

 example a lexical_analyzer for C or Java must read ahead after it

 sees the character If the next_character is then

 is part of the character sequence the lexeme for

 the token for the greater_than or equal to operator Otherwise

 itself forms the greater_than operator and the

 lexical_analyzer has read one character too_many



 A general approach to reading ahead on the

 input is to maintain an input_buffer from which the

 lexical_analyzer can read and push back characters Input buffers

 can be justified on efficiency grounds alone since fetching a

 block of characters is usually more_efficient than fetching one

 character at a time A pointer keeps_track of the portion of the

 input that has_been analyzed pushing back a character is

 implemented_by moving back the pointer Techniques for input

 buffering are discussed in Section buffer-sect



 One-character read-ahead often suffices so a simple solution is

 to use a variable say peek to hold the next_input

 character The lexical_analyzer in this_section reads ahead one

 character while it collects digits for numbers or characters for

 identifiers eg it reads past 1 to distinguish_between

 1 and 10 and it reads past t to distinguish

 between t and true



 The lexical_analyzer reads ahead only when it must An operator

 like can be identified without reading ahead In such

 cases peek is set to a blank which will be skipped when

 the lexical_analyzer is called to find the next token

 The invariant assertion in this_section is that when the lexical

 analyzer returns a token variable peek either holds the

 character beyond the lexeme for the current token or it holds a

 blank



 Constants

 num-subsect



 Anytime a single digit appears in an expression it seems

 reasonable to allow an arbitrary integer constant in its place

 Since an_integer constant is a sequence of digits integer

 constants can be allowed either by creating a token for such

 constants or by incorporating digit sequences into the grammar for

 expressions The job of collecting digits into integers and computing

 their collective numerical value is

 generally given to a lexical_analyzer so numbers can be treated

 as single units during_parsing and translation



 When a sequence of digits appears in the input stream the lexical

 analyzer passes a token say num to the parser The value

 of the integer is passed along as an attribute

 of the token If we write tokens

 and their attributes as tuples enclosed between

 the input

 center

 31 28 59

 center

 is transformed into the sequence

 center







 center

 The token has no attributes so its tuple is simply



 The pseudocode in Fig group-digits-fig reads

 the digits in an_integer and

 accumulates the value of the integer using variable v



 figurehtfb



 center

 tabularl

 if_( peek_holds a digit )



 v 0



 do



 integer value of digit peek



 peek next_input character



 while_( peek_holds a digit )



 return num with v as an attribute





 tabular

 center



 Grouping digits into integers

 group-digits-fig



 figure



 Recognizing Keywords and Identifiers

 reserved-subsect



 Most_languages use fixed character_strings such_as for do and if as punctuation marks or to identify constructs

 Such character_strings are called keywords



 Character strings are also used as identifiers to name variables

 arrays functions and the like Grammars routinely treat

 identifiers as tokens to simplify the parser which can then

 expect the same token say id each time any identifier

 appears in the input For_example on input



 equation

 count count incrementeqinput-lexan

 equation

 the parser works with the token_stream id_id id

 Token id has an attribute

 that holds the lexeme Writing tokens and their attributes as

 tuples the tuples for the input stream (eqinput-lexan) are



 center

 idcount idcount idincrement

 center



 Keywords generally satisfy the rules for forming identifiers so a

 mechanism is needed for deciding when a lexeme forms a keyword and

 when it forms an_identifier The problem is easier to resolve if

 keywords are reserved ie if they cannot be used as

 identifiers Then a character string forms an_identifier only if

 it is not a keyword



 The lexical_analyzer in this_section solves two problems by using

 a table to hold character_strings

 itemize

 Efficiency References or pointers to strings can be

 manipulated more efficiently than the strings themselves Once a

 string is in the table the phases of the compiler can work with

 references to the string in the table



 Reserved Words Reserved words can be_implemented by

 initializing the string table with the reserved strings and their

 tokens When the lexical_analyzer reads a string or lexeme that

 could form an_identifier it first checks_whether the lexeme is in

 the string table If so it returns the token from the table

 otherwise it returns token_id for an_identifier

 itemize



 In Java a string table can be_implemented as a hash_table using a

 class called Hashtable The declaration



 center

 Hashtable_words new Hashmap()

 center

 sets up words as a default hash_table that maps keys to

 values We_shall use it to map lexemes to tokens The

 pseudocode in Fig res-words-fig

 uses the operation get to look up reserved

 words



 figurehtfb



 center

 tabularl

 if_( peek_holds a letter )



 collect letters or digits into a buffer b



 s string formed from the characters in b



 w token returned by wordsget(s)



 if_( w is not null_) return_w



 else



 Enter token_id into words



 return token_id with lexeme_s as an attribute









 tabular

 center



 Distinguishing keywords from identifiers

 res-words-fig



 figure



 This pseudocode collects from the input a string consisting of letters

 and digits beginning with a letter

 We assume that is made as_long as possible ie the lexical

 analyzer will continue reading from the input as_long as it encounters

 letters and digits

 When something other_than a letter or digit eg white_space is

 encountered the lexeme is copied_into a buffer

 If the table has an entry for

 then the token retrieved by wordsget is

 returned

 Here could be either a keyword with which the words table

 was initially seeded or it could be an_identifier that was previously

 entered into the table

 Otherwise token_id and attribute are installed in the table

 and returned



 A Lexical_Analyzer

 scan-java-subsect



 The pseudocode_fragments so_far in this_section fit together to

 form a function scan that returns token objects as

 follows

 center

 tabularl

 Token_scan()



 skip white_space as in Section white-space-subsect



 handle numbers as in Section num-subsect



 handle reserved_words and identifiers as in Section reserved-subsect



 if we get here treat read-ahead character peek as a token



 Token_t new Token(peek



 peek blank initialization as discussed in Section readahead-subsect



 return





 tabular

 center



 The rest of this_section implements function scan as part

 of a Java package for lexical analysis The package called lexer has classes for tokens and a class_Lexer containing

 function scan



 Tokens and their attributes are grouped into objects referred to

 as token objects since the lexical_analyzer passes token

 objects to the parser The classes for tokens and their fields are

 illustrated in Fig tokens-fig their methods are not shown

 Class Token has a field_tok that is used for parsing

 decisions Subclass Num adds a field value for an

 integer value Subclass Word adds a field lexeme that

 is used for reserved_words and identifiers



 figurehtfb

 Class

 Token and subclasses Num and Word

 tokens-fig

 figure



 Each class is in a file by itself The file for class_Token

 is as_follows







 center

 tabularr_l

 1)_package lexer_File Tokenjava



 2)_public class_Token



 3)_public final_int tok



 4)_public Token(int t) tok t



 5)

 tabular

 center

 Line 1 identifies the package_lexer Field tok is

 declared on line 3 to be final so it cannot be changed once

 it is set The constructor Token on line_4 is used to create

 token objects as in

 center

 new_Token('')

 center

 which creates a new object of class_Token and sets its field

 tok to an_integer representation of '' (For the

 moment we omit the customary method toString which would

 return a string suitable for printing)



 Where the pseudocode had tokens like num and id the

 Java code uses integer constants Class Tag implements such

 constants

 center

 tabularr_l

 1)_package lexer_File Tagjava



 2)_public class_Tag



 3)_public final static_int



 4) NUM256 ID257 TRUE258 FALSE259



 5)

 tabular

 center

 In_addition to the integer-valued fields NUM and ID

 this class defines two additional fields TRUE and FALSE for future use they will be used to illustrate the

 treatment of reserved keywords



 The fields in class_Tag are public so they can be

 used outside the package They are static so there is just

 one instance or copy of these fields The fields are final

 so they can be set just once In effect these fields represent

 constants A similar effect is achieved in C by using

 define-statements to allow names such_as NUM to be used as symbolic

 constants eg



 center

 tabularl

 define NUM 256

 tabular

 center



 The Java code refers to TagNUM and TagID in places_where the

 pseudocode referred to tokens num and id The only

 requirement is that TagNUM and TagID must_be

 initialized_with distinct values that differ from each other and

 from the constants representing

 single-character tokens such_as '' or ''



 figurehtfb

 center

 tabularr_l

 1)_package lexer_File Numjava



 2)_public class Num_extends Token



 3)_public final_int value



 4)_public Num(int v) super(TagNUM) value v



 5)



 1)_package lexer_File Wordjava



 2)_public class_Word extends_Token



 3)_public final String lexeme



 4)_public Word(String s int t)



 5) super(t) lexeme new String(s)



 6)



 7)

 tabular

 center

 Subclasses Num and Word of Token

 tokens-java-fig

 figure



 The code for classes Num and Word appears in

 Fig tokens-java-fig Class Num_extends Token by

 declaring an_integer field value on line 3 The constructor

 Num on line_4 calls super(TagNUM) which sets field

 tok in the superclass Token to TagNUM



 figurehtfb

 center

 tabularr_l

 1)_package lexer_File Lexerjava



 2)_import javaio_import javautil



 3)_public class_Lexer



 4)_public int line 1



 5) private char peek_' '



 6) private Hashtable_words new_Hashtable()



 7) void reserve(Word t) wordsput(tlexeme t)



 8)_public Lexer()



 9) reserve(new Word(true TagTRUE) )



 10) reserve(new Word(false TagFALSE) )



 11)



 12) public Token_scan() throws_IOException



 13) for (_peek (char)Systeminread() )



 14) if_( peek_' ' peek_') continue



 15) else if_( peek_') line line 1



 16) else_break



 17)



 continues in Fig scan-java2-fig



 tabular

 center

 Code for a lexical_analyzer part 1 of 2

 scan-java1-fig

 figure



 Class Word is used for both reserved_words and identifiers

 so the constructor Word on line_4 expects two parameters a

 lexeme and a corresponding integer value for tok An object

 for the reserved word true can be created by executing

 center

 new Word(true TagTRUE)

 center

 which creates a new object with field_tok set to TagTRUE and field lexeme set to the string true



 Class Lexer for lexical analysis appears in

 Figs scan-java1-fig and scan-java2-fig Variable line on line_4 counts input lines and variable peek on line 5

 holds the next_input character



 Reserved words are handled on lines 6 through 11 The table words is

 declared on line 6 The helper function reserve on line 7

 puts a string-word pair in the table Lines 9 and 10 in the

 constructor Lexer initialize the table They use the

 constructor Word to create word objects which are passed to

 the helper function reserve The table is therefore

 initialized_with reserved_words true and false

 before the first call of scan



 The code for scan in

 Fig scan-java1-fig-scan-java2-fig implements the

 pseudocode_fragments in this_section The for-statement on lines

 13 through 17 skips blank tab and newline characters Control leaves the

 for-statement with peek holding a non-white-space character



 figurehtfb

 center

 tabularr_l

 18) if(_CharacterisDigit(peek) )



 19) int_v 0



 20) do



 21) v 10v Characterdigit(peek_10)



 22) peek_(char)Systeminread()



 23) while_( CharacterisDigit(peek)_)



 24) return_new Num(v)



 25)



 26) if( CharacterisLetter(peek) )



 27) StringBuffer b new StringBuffer()



 28) do



 29) bappend(peek)



 30) peek_(char)Systeminread()



 31) while_( CharacterisLetterOrDigit(peek) )



 32) String_s btoString()



 33) Word w (Word)wordsget(s)



 34) if_( w null_) return_w



 35) w new Word(s TagID)



 36) wordsput(s w)



 37) return_w



 38)



 39) Token_t new Token(peek)



 40) peek_' '



 41) return t



 42)



 43)

 tabular

 center

 Code for a lexical_analyzer part 2 of 2

 scan-java2-fig

 figure



 The code for reading a sequence of digits is on lines 18 through 25 The

 function isDigit is from the built-in Java class Character It is used on line 18 to check_whether peek is

 a digit If so the code on lines 19 through 24 accumulates the integer

 value of the sequence of digits in the input and returns a new

 Num object



 Lines 26 through 38 analyze reserved_words and identifiers Keywords true and false have_already been reserved on lines 9 and 10

 Therefore line 35 is reached if string s is not reserved

 so it must_be the lexeme for an_identifier Line 35 therefore

 returns a new word object with lexeme set to s and

 tok set to TagID

 Finally lines 39 through 41 return the current character as a token and

 set peek to a blank that will be stripped the next time scan is called

 Lexical Analysis

 lexan-sect



 A lexical_analyzer reads characters from the input and groups them

 into tokens these tokens become the terminal_symbols

 that are input to the parser A sequence of input

 characters that comprises a single token is called a lexeme

 Thus we can say that the lexical_analyzer insulates a parser from the lexeme

 representation of tokens



 The lexical_analyzer in this_section allows numbers identifiers

 and white_space (blanks tabs and newlines)

 to appear within

 expressions It can be used to extend the expression translator of

 the previous section



 Since the expression grammar of Fig_post-scheme2-fig must

 be extended to allow numbers and identifiers we take this

 opportunity to allow multiplication and division as_well The

 extended translation_scheme appears in Fig expr-scheme-fig

 In the semantic_actions for nonterminal factor token num is assumed to have

 an integer-valued attribute numvalue Presumably the value of this attribute is the integer

 represented_by the lexeme corresponding to this occurrence of the token

 num

 Token id has a string-valued_attribute written as

 idlexeme we assume this string is the actual lexeme

 comprising this instance of the token_id



 figurehtfb

 center

 tabularr_c l_l

 print





 print













 print





 print













 (_)



 num





 id



 tabular

 center

 Actions for translating into_postfix notation

 expr-scheme-fig

 figure



 The pseudocode_fragments used to illustrate the workings of a

 lexical_analyzer will be assembled into Java code at the end of

 this_section Symbol_tables or data_structures for holding

 information_about identifiers are considered in Section symtab-sect





 Removal of White Space and Comments

 white-space-subsect



 The expression translator in Section_postfix-sect sees every

 character in the input so extraneous characters such_as blanks

 will cause it to fail Most_languages allow arbitrary amounts of

 white_space to appear

 between tokens Comments are likewise ignored during_parsing so

 they may also be treated_as white_space



 If white_space is eliminated by the lexical_analyzer the parser

 will never have to consider it The alternative of modifying the

 grammar to incorporate white_space into the syntax is not nearly

 as easy to implement



 The following pseudocode skips white_space by reading input

 characters as_long as it_sees a blank a tab or a newline

 Variable peek_holds the next_input character Line numbers

 are useful within error_messages to help pinpoint errors so the

 code uses variable line to count newline characters in the

 input



 center

 tabularl

 for (_peek next_input character )



 if_( peek is a blank or a tab ) do_nothing



 else if_( peek is a newline ) line line1



 else_break





 tabular

 center



 Reading Ahead

 readahead-subsect



 A lexical_analyzer may need to read ahead some characters before

 it can decide on the token to be returned to the parser For

 example a lexical_analyzer for C or Java must read ahead after it

 sees the character If the next_character is then

 is part of the character sequence which is a lexeme

 forming the

 token for the greater_than or equal to operator Otherwise

 forms the greater_than operator by itself and the lexical

 analyzer has read one character too_many



 A general approach to reading ahead on the

 input is to maintain an input_buffer from which the

 lexical_analyzer can read and push back characters Input buffers

 can be justified on efficiency grounds alone since fetching a

 block of characters is usually more_efficient than fetching one

 character at a time A pointer keeps_track of the portion of the

 input that has_been analyzed pushing back a character is

 implemented_by moving back the pointer Techniques for input

 buffering are discussed in Section buffer-sect



 One-character read-ahead often suffices so a simple solution is

 to use a variable say peek to hold the next_input

 character The lexical_analyzer in this_section reads ahead one

 character while it collects digits for numbers or characters for

 identifiers eg it reads past 1 to distinguish_between

 1 and 10 and it reads past t to distinguish

 between t and true



 The lexical_analyzer reads ahead only when it must An operator

 like can be identified without reading ahead In such

 cases peek is set to a blank which will be skipped when

 the lexical_analyzer is called to find the next token

 The invariant assertion in this_section is that when the lexical

 analyzer returns a token variable peek either holds the

 character beyond the lexeme for the current token or it holds a

 blank



 Constants

 num-subsect



 Anytime a single digit appears in an expression it seems

 reasonable to allow an arbitrary integer constant in its place

 Since an_integer constant is a sequence of digits integer

 constants can be allowed either by creating a token for such

 constants or by incorporating digit sequences into the grammar for

 expressions The job of collecting digits into integers and computing

 their collective numerical value is

 generally given to a lexical_analyzer so numbers can be treated

 as single units during_parsing and translation



 When a sequence of digits appears in the input stream the lexical

 analyzer passes a token say num to the parser The value

 of the integer is passed along as an attribute

 of the token If we write tokens

 and their attributes as tuples enclosed between

 the input

 center

 31 28 59

 center

 is transformed into the sequence

 center







 center

 The token has no attributes so its tuple is simply



 The pseudocode in Fig group-digits-fig reads

 the digits in an_integer and

 accumulates the value of the integer using variable v



 figurehtfb



 center

 tabularl

 if_( peek_holds a digit )



 v 0



 do



 integer value of digit peek



 peek next_input character



 while_( peek_holds a digit )



 return num with v as an attribute





 tabular

 center



 Grouping digits into integers

 group-digits-fig



 figure



 Recognizing Keywords and Identifiers

 reserved-subsect



 Most_languages use fixed character_strings such_as for do and if as punctuation marks or to identify constructs

 Such character_strings are called keywords



 Character strings are also used as identifiers to name variables

 arrays functions and the like Grammars routinely treat

 identifiers as tokens to simplify the parser which can then

 expect the same token say id each time any identifier

 appears in the input For_example on input



 equation

 count count incrementeqinput-lexan

 equation

 the parser works with the token_stream id_id id

 Token id has an attribute

 that holds the lexeme Writing tokens and their attributes as

 tuples the tuples for the input stream (eqinput-lexan) are



 center

 idcount idcount idincrement

 center



 Keywords generally satisfy the rules for forming identifiers so a

 mechanism is needed for deciding when a lexeme forms a keyword and

 when it forms an_identifier The problem is easier to resolve if

 keywords are reserved ie if they cannot be used as

 identifiers Then a character string forms an_identifier only if

 it is not a keyword



 The lexical_analyzer in this_section solves two problems by using

 a table to hold character_strings

 itemize

 Efficiency References or pointers to strings can be

 manipulated more efficiently than the strings themselves Once a

 string is in the table the phases of the compiler can work with

 references to the string in the table



 Reserved Words Reserved words can be_implemented by

 initializing the string table with the reserved strings and their

 tokens When the lexical_analyzer reads a string or lexeme that

 could form an_identifier it first checks_whether the lexeme is in

 the string table If so it returns the token from the table

 otherwise it returns token_id for an_identifier

 itemize



 In Java a string table can be_implemented as a hash_table using a

 class called Hashtable The declaration



 center

 Hashtable_words new Hashmap()

 center

 sets up words as a default hash_table that maps keys to

 values We_shall use it to map lexemes to tokens The

 pseudocode in Fig res-words-fig

 uses the operation get to look up reserved

 words



 figurehtfb



 center

 tabularl

 if_( peek_holds a letter )



 collect letters or digits into a buffer b



 s string formed from the characters in b



 w token returned by wordsget(s)



 if_( w is not null_) return_w



 else



 Enter token_id into words



 return token_id with lexeme_s as an attribute









 tabular

 center



 Distinguishing keywords from identifiers

 res-words-fig



 figure



 This pseudocode collects from the input a string consisting of letters

 and digits beginning with a letter

 We assume that is made as_long as possible ie the lexical

 analyzer will continue reading from the input as_long as it encounters

 letters and digits

 When something other_than a letter or digit eg white_space is

 encountered the lexeme is copied_into a buffer

 If the table has an entry for

 then the token retrieved by wordsget is

 returned

 Here could be either a keyword with which the words table

 was initially seeded or it could be an_identifier that was previously

 entered into the table

 Otherwise token_id and attribute are installed in the table

 and returned



 A Lexical_Analyzer

 scan-java-subsect



 The pseudocode_fragments so_far in this_section fit together to

 form a function scan that returns token objects as

 follows

 center

 tabularl

 Token_scan()



 skip white_space as in Section white-space-subsect



 handle numbers as in Section num-subsect



 handle reserved_words and identifiers as in Section reserved-subsect



 if we get here treat read-ahead character peek as a token



 Token_t new Token(peek



 peek blank initialization as discussed in Section readahead-subsect



 return





 tabular

 center



 The rest of this_section implements function scan as part

 of a Java package for lexical analysis The package called lexer has classes for tokens and a class_Lexer containing

 function scan



 Tokens and their attributes are grouped into objects referred to

 as token objects since the lexical_analyzer passes token

 objects to the parser The classes for tokens and their fields are

 illustrated in Fig tokens-fig their methods are not shown

 Class Token has a field_tok that is used for parsing

 decisions Subclass Num adds a field value for an

 integer value Subclass Word adds a field lexeme that

 is used for reserved_words and identifiers



 figurehtfb

 Class

 Token and subclasses Num and Word

 tokens-fig

 figure



 Each class is in a file by itself The file for class_Token

 is as_follows







 center

 tabularr_l

 1)_package lexer_File Tokenjava



 2)_public class_Token



 3)_public final_int tok



 4)_public Token(int t) tok t



 5)

 tabular

 center

 Line 1 identifies the package_lexer Field tok is

 declared on line 3 to be final so it cannot be changed once

 it is set The constructor Token on line_4 is used to create

 token objects as in

 center

 new_Token('')

 center

 which creates a new object of class_Token and sets its field

 tok to an_integer representation of '' (For the

 moment we omit the customary method toString which would

 return a string suitable for printing)



 Where the pseudocode had tokens like num and id the

 Java code uses integer constants Class Tag implements such

 constants

 center

 tabularr_l

 1)_package lexer_File Tagjava



 2)_public class_Tag



 3)_public final static_int



 4) NUM256 ID257 TRUE258 FALSE259



 5)

 tabular

 center

 In_addition to the integer-valued fields NUM and ID

 this class defines two additional fields TRUE and FALSE for future use they will be used to illustrate the

 treatment of reserved keywords



 The fields in class_Tag are public so they can be

 used outside the package They are static so there is just

 one instance or copy of these fields The fields are final

 so they can be set just once In effect these fields represent

 constants A similar effect is achieved in C by using

 define-statements to allow names such_as NUM to be used as symbolic

 constants eg



 center

 tabularl

 define NUM 256

 tabular

 center



 The Java code refers to TagNUM and TagID in places_where the

 pseudocode referred to tokens num and id The only

 requirement is that TagNUM and TagID must_be

 initialized_with distinct values that differ from each other and

 from the constants representing

 single-character tokens such_as '' or ''



 figurehtfb

 center

 tabularr_l

 1)_package lexer_File Numjava



 2)_public class Num_extends Token



 3)_public final_int value



 4)_public Num(int v) super(TagNUM) value v



 5)



 1)_package lexer_File Wordjava



 2)_public class_Word extends_Token



 3)_public final String lexeme



 4)_public Word(String s int t)



 5) super(t) lexeme new String(s)



 6)



 7)

 tabular

 center

 Subclasses Num and Word of Token

 tokens-java-fig

 figure



 The code for classes Num and Word appears in

 Fig tokens-java-fig Class Num_extends Token by

 declaring an_integer field value on line 3 The constructor

 Num on line_4 calls super(TagNUM) which sets field

 tok in the superclass Token to TagNUM



 figurehtfb

 center

 tabularr_l

 1)_package lexer_File Lexerjava



 2)_import javaio_import javautil



 3)_public class_Lexer



 4)_public int line 1



 5) private char peek_' '



 6) private Hashtable_words new_Hashtable()



 7) void reserve(Word t) wordsput(tlexeme t)



 8)_public Lexer()



 9) reserve(new Word(true TagTRUE) )



 10) reserve(new Word(false TagFALSE) )



 11)



 12) public Token_scan() throws_IOException



 13) for (_peek (char)Systeminread() )



 14) if_( peek_' ' peek_') continue



 15) else if_( peek_') line line 1



 16) else_break



 17)



 continues in Fig scan-java2-fig



 tabular

 center

 Code for a lexical_analyzer part 1 of 2

 scan-java1-fig

 figure



 Class Word is used for both reserved_words and identifiers

 so the constructor Word on line_4 expects two parameters a

 lexeme and a corresponding integer value for tok An object

 for the reserved word true can be created by executing

 center

 new Word(true TagTRUE)

 center

 which creates a new object with field_tok set to TagTRUE and field lexeme set to the string true



 Class Lexer for lexical analysis appears in

 Figs scan-java1-fig and scan-java2-fig Variable line on line_4 counts input lines and variable peek on line 5

 holds the next_input character



 Reserved words are handled on lines 6 through 11 The table words is

 declared on line 6 The helper function reserve on line 7

 puts a string-word pair in the table Lines 9 and 10 in the

 constructor Lexer initialize the table They use the

 constructor Word to create word objects which are passed to

 the helper function reserve The table is therefore

 initialized_with reserved_words true and false

 before the first call of scan



 The code for scan in

 Fig scan-java1-fig-scan-java2-fig implements the

 pseudocode_fragments in this_section The for-statement on lines

 13 through 17 skips blank tab and newline characters Control leaves the

 for-statement with peek holding a non-white-space character



 figurehtfb

 center

 tabularr_l

 18) if(_CharacterisDigit(peek) )



 19) int_v 0



 20) do



 21) v 10v Characterdigit(peek_10)



 22) peek_(char)Systeminread()



 23) while_( CharacterisDigit(peek)_)



 24) return_new Num(v)



 25)



 26) if( CharacterisLetter(peek) )



 27) StringBuffer b new StringBuffer()



 28) do



 29) bappend(peek)



 30) peek_(char)Systeminread()



 31) while_( CharacterisLetterOrDigit(peek) )



 32) String_s btoString()



 33) Word w (Word)wordsget(s)



 34) if_( w null_) return_w



 35) w new Word(s TagID)



 36) wordsput(s w)



 37) return_w



 38)



 39) Token_t new Token(peek)



 40) peek_' '



 41) return t



 42)



 43)

 tabular

 center

 Code for a lexical_analyzer part 2 of 2

 scan-java2-fig

 figure



 The code for reading a sequence of digits is on lines 18 through 25 The

 function isDigit is from the built-in Java class Character It is used on line 18 to check_whether peek is

 a digit If so the code on lines 19 through 24 accumulates the integer

 value of the sequence of digits in the input and returns a new

 Num object



 Lines 26 through 38 analyze reserved_words and identifiers Keywords true and false have_already been reserved on lines 9 and 10

 Therefore line 35 is reached if string s is not reserved

 so it must_be the lexeme for an_identifier Line 35 therefore

 returns a new word object with lexeme set to s and

 tok set to TagID

 Finally lines 39 through 41 return the current character as a token and

 set peek to a blank that will be stripped the next time scan is called

 Design of a Lexical-Analyzer Generator

 lex-arch-sect



 In this_section we_shall apply the techniques presented in

 Section re-fa-sect to see_how a lexical-analyzer generator such_as

 Lex is architected

 We discuss two approaches based_on NFA's and DFA's the latter is

 essentially the implementation of Lex



 The Structure of the Generated Analyzer

 lexan-struct-subsect



 Figure lex-fa-fig overviews the architecture of a

 lexical_analyzer generated_by Lex

 The program that serves as the lexical_analyzer includes a fixed

 program that simulates an automaton at this point we leave open whether

 that automaton is deterministic or nondeterministic

 The rest of the lexical_analyzer consists of components that are created

 from the Lex program by Lex itself



 figurehtfb

 fileuullmanalsuch3figslex-faeps

 A Lex program is turned_into a transition table and

 actions which are used by a finite-automaton simulator

 lex-fa-fig

 figure



 These components are



 enumerate



 A transition table for the automaton



 Those functions that are passed directly through Lex to the output

 (see_Section lex-struct-subsect)



 The actions from the input program which appear as fragments of code to

 be invoked at the appropriate time by the automaton simulator



 enumerate



 To construct the automaton we begin by_taking each regular-expression

 pattern in the

 Lex program and converting it using Algorithm_my-alg to an

 NFA

 We need a single automaton that will recognize lexemes matching any of

 the patterns in the program so we combine all the NFA's into one by

 introducing a new start_state with s to each of the start states

 of the NFA's for pattern

 This construction is shown in Fig lex-nfa-fig



 figurehtfb

 fileuullmanalsuch3figslex-nfaeps

 An NFA constructed from a Lex program

 lex-nfa-fig

 figure



 ex

 lex-nfa-ex

 We_shall illustrate the ideas of this_section with the following simple

 abstract example



 center

 tabularl_l

 action for pattern



 action for pattern



 action for pattern



 tabular

 center

 Note_that these three patterns present some conflicts of the type

 discussed in Section lex-confl-subsect

 In_particular string matches both the second and third patterns

 but we_shall consider it a lexeme for pattern since that pattern

 is listed_first in the above Lex program

 Then input strings such_as have many prefixes that match

 the third pattern

 The Lex rule is to take the longest so we continue reading 's

 until another is met whereupon we report the lexeme to be the

 initial 's followed_by as many 's as there are



 Figure lex-nfa1-fig shows three NFA's that recognize the three

 patterns

 The third is a simplification of what would come out of

 Algorithm_my-alg

 Then Fig lex-nfa2-fig shows these three NFA's combined_into a

 single NFA by the addition of start_state 0 and three s

 ex



 figure



 fileuullmanalsuch3figslex-nfa1eps

 NFA's for and

 lex-nfa1-fig



 fileuullmanalsuch3figslex-nfa2eps

 Combined NFA

 lex-nfa2-fig



 fileuullmanalsuch3figslex-state-seqeps

 Sequence of sets of states entered when processing input



 lex-state-seq-fig



 figure



 Pattern Matching Based on NFA's

 lex-nfa-match-subsect



 If the lexical_analyzer simulates an NFA such_as that of

 Fig lex-nfa2-fig then it must read input beginning at the point

 on its input which we have referred to as lexemeBegin

 As it moves the pointer called forward ahead in the input it

 calculates the set of states it is in at each point following

 Algorithm nfa-sim-alg



 Eventually the NFA simulation reaches a point on the input where there

 are no next states

 At that point there is no hope that any longer prefix of the input

 would ever get the NFA to an accepting_state rather the set of states

 will always be empty

 Thus we are ready to decide on the longest_prefix that is a lexeme

 matching some pattern



 We look backwards in the sequence of sets of states until we find a

 set that includes one or_more accepting_states

 If there are several accepting_states in that set

 pick the one associated_with the

 earliest pattern in the list from the Lex program

 Move the forward pointer back to the end of the lexeme and

 perform the action_associated with pattern



 ex

 lex-nfa-sim-ex

 Suppose we have the patterns of Example lex-nfa-ex and the input

 begins

 Figure lex-state-seq-fig shows the sets of states of the NFA of

 Fig lex-nfa2-fig that we enter starting_with of the

 initial_state 0 which is and proceeding from there

 After reading the fourth input_symbol we are in an empty_set of states

 since in Fig lex-nfa2-fig there are no transitions out of

 state 8 on input



 Thus we need to back up looking for a set of states that includes an

 accepting_state

 Notice_that as indicated in Fig lex-state-seq-fig after_reading

 we are in a set that includes state 2 and therefore indicates that

 the pattern has_been matched

 However after_reading we are in state 8 which indicates that

 has_been matched prefix is the longest_prefix that

 gets us to an accepting_state

 We therefore select as the lexeme and execute action which

 should include a return to the parser indicating that the token whose

 pattern is has_been found

 ex



 DFA's for Lexical Analyzers

 lex-dfa-subsect



 Another architecture resembling the output of Lex is to convert

 the NFA for all the patterns into an equivalent DFA using the subset

 construction of Algorithm subset-cons-alg

 Within each DFA state if there are one or_more accepting NFA_states

 determine the first pattern whose accepting_state is represented and

 make that pattern the output of the DFA state



 ex

 lex-dfa-ex

 Figure lex-dfa-fig shows a transition_diagram based_on the DFA

 that is constructed by the subset_construction from the NFA in

 Fig lex-nfa2-fig

 The accepting_states are labeled by the pattern that is identified by

 that state

 For_instance the state has two accepting_states

 corresponding to patterns and

 Since the former is listed_first that is the pattern associated_with

 state

 ex



 figurehtfb

 fileuullmanalsuch3figslex-dfaeps

 Transition graph for DFA handling the patterns

 and

 lex-dfa-fig

 figure



 We use the DFA in a lexical_analyzer much as we did the NFA

 We simulate the DFA until at some point there is no next state (or

 strictly speaking the next state is the dead_state

 corresponding to the empty_set of NFA states)

 At that point we back up through the sequence of states we entered and

 as_soon as we meet an accepting DFA state we perform the action

 associated_with the pattern for that state



 Dead States in DFA's

 Technically the automaton in Fig lex-dfa-fig is not

 quite a DFA

 The_reason is that a DFA has a transition from every state

 on every input_symbol in its input alphabet

 Here we have omitted transitions to the dead_state and we

 have therefore omitted the transitions from the dead_state to itself on

 every input

 Previous NFA-to-DFA

 examples did_not have a way to get from the start_state to

 but the NFA of Fig lex-nfa2-fig does



 However when we construct a DFA for use in a lexical_analyzer it is

 important that we treat the dead_state differently since we must know

 when there is no_longer any possibility of recognizing a longer lexeme

 Thus we suggest always omitting transitions to the dead_state and

 eliminating the dead_state itself

 In_fact the problem is harder than

 it appears since an NFA-to-DFA construction

 may yield several states that cannot reach any accepting_state and we

 must know when any of these states have_been reached

 Section dfa-min-subsect discusses_how to combine all these states

 into one dead_state so their identification becomes easy

 It is also interesting to note_that if we construct a DFA from a regular

 expression using Algorithms subset-cons-alg and my-alg then

 the DFA will not have any states besides that cannot lead to

 an accepting_state



 ex

 lex-dfa-exec-ex

 Suppose the DFA of Fig lex-dfa-fig is given input

 The sequence of states entered is and at the final

 there is no transition out of state

 Thus we consider the sequence from the end and in this case

 itself is an accepting_state that reports pattern

 ex



 Implementing the Lookahead Operator

 nfa-lookahead-subsect



 Recall from Section lex-lookahead-subsect that the Lex

 lookahead operator

 in a Lex pattern is sometimes necessary because the pattern

 for a particular token may need to describe some trailing context

 in order to correctly identify the actual lexeme

 When converting the pattern to an NFA we treat the as

 if it were so we do_not actually look for a on the input

 However if the NFA recognizes a prefix of the input_buffer as matching

 this regular_expression the end of the lexeme is not where the NFA

 entered its accepting_state

 Rather the end occurs_when the NFA enters a state such that



 itemize

 1

 has an on the (imaginary)



 2

 There is a path from the start_state of the NFA to state

 that spells out



 3

 There is a path from state to the accepting_state that

 spells out



 4

 is as_long as possible for any satisfying conditions 1-3

 itemize



 If there is only one -transition state on the

 imaginary in the NFA

 then the end of the lexeme occurs_when this state is

 entered for the last time as the

 following example_illustrates

 If the NFA has more_than one -transition state

 on the imaginary then the general problem of finding the

 correct state is much more difficult



 ex

 nfa-lookahead-ex

 An NFA for the pattern for the Fortran IF with lookahead from

 Example lex-slash-ex is shown in Fig nfa-lookahead-fig

 Notice_that the from state 2 to state 3 represents the lookahead

 operator

 State 6 indicates the presence of the keyword IF

 However we find

 the lexeme IF by scanning backwards to the last occurrence of

 state 2 whenever state 6 is entered

 ex



 figurehtfb

 fileuullmanalsuch3figsnfa-lookaheadeps

 NFA recognizing the keyword IF

 nfa-lookahead-fig

 figure



 sexer

 lex-arch-exer

 Suppose we have two tokens (1) the keyword if and (2) identifiers which

 are strings of letters other_than if

 Show



 itemize



 a) The NFA for these tokens and

 b) The DFA for these tokens



 itemize

 sexer



 exer

 Repeat_Exercise lex-arch-exer for tokens consisting of (1) the

 keyword while (2) the keyword when and (3) identifiers

 consisting of strings of letters and digits beginning with a letter

 exer



 hexer

 Suppose we were to revise the definition of a DFA to allow zero_or one

 transition out of each state on each input_symbol (rather than exactly

 one such transition as in the standard DFA definition) Some regular

 expressions would then have smaller DFA's than they do under the

 standard definition of a DFA Give an example of one such regular

 expression

 hexer



 vhexer

 Design an algorithm to recognize Lex-lookahead

 patterns of the form where and

 are regular_expressions

 Show_how your algorithm works on the following inputs



 itemize

 a)

 b)

 c)

 itemize



 vhexer



 Lexical_Analyzer

 lexer-java-sect



 Package lexer is an extension of the code for the lexical

 analyzer in Section_scan-java-subsect Class Tag

 defines constants for tokens



 footnotesize

 flushleft

 1)_package lexer_File Tagjava



 2)_public class_Tag



 3)_public final static_int



 4) AND 256 BASIC 257 BREAK 258 DO 259 ELSE 260



 5) EQ 261 FALSE 262 GE 263 ID 264 IF 265



 6) INDEX 266 LE 267 MINUS 268 NE 269 NUM 270



 7) OR 271 REAL 272 TEMP 273 TRUE 274 WHILE 275



 8)



 flushleft

 footnotesize



 Three of the constants INDEX MINUS and

 TEMP are not lexical tokens they will be used in syntax

 trees



 Classes Token and Num are as in

 Section_scan-java-subsect with method toString added



 footnotesize

 flushleft

 1)_package lexer_File Tokenjava



 2)_public class_Token



 3)_public final_int tag



 4)_public Token(int t) tag t



 5)_public String_toString() return (char)tag



 6)



 flushleft

 footnotesize



 footnotesize

 flushleft

 1)_package lexer_File Numjava



 2)_public class Num_extends Token



 3)_public final_int value



 4)_public Num(int v) super(TagNUM) value v



 5)_public String_toString() return value



 6)



 flushleft

 footnotesize



 Class Word manages lexemes for reserved_words identifiers

 and composite tokens like It is also useful for

 managing the written form of operators in the intermediate_code

 like unary_minus for example the source text -2 has the

 intermediate form minus 2



 footnotesize

 flushleft

 1)_package lexer_File Wordjava



 2)_public class_Word extends_Token



 3)_public String lexeme



 4)_public Word(String s int tag) super(tag) lexeme_s



 5)_public String_toString() return lexeme



 6)_public static final Word



 7) and new_Word( TagAND ) or new_Word( TagOR )



 8) eq new_Word( TagEQ ) ne new_Word( TagNE )



 9) le new_Word( TagLE ) ge new_Word( TagGE )



 10) minus new_Word( minus TagMINUS )



 11) True new_Word( true TagTRUE )



 12) False new_Word( false TagFALSE )



 13) temp new_Word( t TagTEMP )



 14)



 flushleft

 footnotesize



 Class Real is for floating_point numbers



 footnotesize

 flushleft

 1)_package lexer_File Realjava



 2)_public class Real extends_Token



 3)_public final float value



 4)_public Real(float v) super(TagREAL) value v



 5)_public String_toString() return value



 6)



 flushleft

 footnotesize



 The main method in class_Lexer function scan recognizes

 numbers identifiers and reserved_words as discussed in

 Section_scan-java-subsect



 Lines 9-13 in class_Lexer reserve selected keywords Lines

 14-16 reserve lexemes for objects defined elsewhere Objects WordTrue and WordFalse are defined in class_Word

 Objects for the basic types int char bool and

 float are defined in class Type a subclass of Word Class Type is from package_symbols



 footnotesize

 flushleft

 1)_package lexer_File Lexerjava



 2)_import javaio_import javautil import_symbols



 3)_public class_Lexer



 4)_public static_int line 1



 5) char peek_' '



 6) Hashtable_words new_Hashtable()



 7) void reserve(Word w) wordsput(wlexeme w)



 8)_public Lexer()



 9) reserve(_new Word(if TagIF) )



 10) reserve(_new Word(else TagELSE) )



 11) reserve(_new Word(while TagWHILE) )



 12) reserve(_new Word(do TagDO) )



 13) reserve(_new Word(break TagBREAK) )



 14) reserve( WordTrue ) reserve( WordFalse )



 15) reserve( TypeInt ) reserve( TypeChar )



 16) reserve( TypeBool_) reserve( TypeFloat )



 17)



 flushleft

 footnotesize



 Function readch() (line 18) is used to read the next_input

 character into variable peek The name readch is

 reused or overloaded (lines 19-24) to help recognize composite

 tokens For_example once input is seen the call readch('') reads the next_character into peek and checks

 whether it is



 footnotesize

 flushleft

 18) void readch() throws_IOException peek_(char)Systeminread()



 19) boolean readch(char c) throws_IOException



 20) readch()



 21) if(_peek c )_return false



 22) peek_' '



 23) return true



 24)



 flushleft

 footnotesize



 Function scan begins by skipping white_space (lines 26-30)

 It recognizes composite tokens like (lines 31-44) and

 numbers like 365 and 314 (lines 45-58) before

 collecting words (lines 59-70)



 footnotesize

 flushleft

 25) public Token_scan() throws_IOException



 26) for( readch() )



 27) if(_peek '_' peek_') continue



 28) else_if( peek_') line line 1



 29) else_break



 30)



 31) switch( peek )



 32) case_''



 33) if(_readch('') )_return Wordand else_return new_Token('')



 34) case_''



 35) if(_readch('') )_return Wordor else_return new_Token('')



 36) case_''



 37) if(_readch('') )_return Wordeq else_return new_Token('')



 38) case_''



 39) if(_readch('') )_return Wordne else_return new_Token('')



 40) case_''



 41) if(_readch('') )_return Wordle else_return new_Token('')



 42) case_''



 43) if(_readch('') )_return Wordge else_return new_Token('')



 44)



 45) if(_CharacterisDigit(peek) )



 46) int_v 0



 47) do



 48) v 10v Characterdigit(peek_10) readch()



 49) while( CharacterisDigit(peek)_)



 50) if(_peek ''_) return_new Num(v)



 51) float_x v float d 10



 52) for()



 53) readch()



 54) if(_CharacterisDigit(peek) ) break



 55) x x Characterdigit(peek_10) d d d10



 56)



 57) return_new Real(x)



 58)



 59) if( CharacterisLetter(peek) )



 60) StringBuffer b new StringBuffer()



 61) do



 62) bappend(peek) readch()



 63) while( CharacterisLetterOrDigit(peek) )



 64) String_s btoString()



 65) Word w (Word)wordsget(s)



 66) if( w null_) return_w



 67) w new Word(s TagID)



 68) wordsput(s w)



 69) return_w



 70)



 flushleft

 footnotesize



 Finally any remaining characters are returned as tokens (lines

 71-72)



 footnotesize

 flushleft

 71) Token_tok new Token(peek) peek_' '



 72) return tok



 73)



 74)



 flushleft

 footnotesize

 The Lexical-Analyzer Generator Lex

 lex-sect



 In this_section we introduce a tool called Lex or in a more

 recent implementation Flex that allows one to specify a lexical

 analyzer by specifying regular_expressions to describe patterns for tokens

 The input notation for the Lex tool is referred to as the Lex

 language and the tool itself is the Lex compiler

 Behind the scenes the Lex compiler transforms the input patterns into a

 transition_diagram and generates code in a file called

 lexyyc

 that simulates this transition_diagram

 The mechanics of how this translation from regular_expressions to

 transition_diagrams occurs is the subject of the next sections here we

 only learn the Lex language



 Use of Lex

 lex-use-subsect



 Figure lex-use-fig suggests how Lex is used

 An input file which we call lexl is written in the Lex language

 and describes the lexical_analyzer to be generated

 The Lex compiler transforms lexl to a C program in a file that

 is always named lexyyc

 The latter file is compiled by the C compiler into a file called aout as always

 The C-compiler output is a working lexical_analyzer that can take a

 stream of input_characters and produce a stream of tokens



 figurehtfb

 fileuullmanalsuch3figslex-useeps

 Creating a lexical_analyzer with Lex

 lex-use-fig

 figure



 The normal use of the compiled C program referred to as aout in

 Fig lex-use-fig is as a subroutine of the parser

 It is a C function that returns an_integer which is a code for one of

 the possible token names

 The attribute value whether it be another numeric code a pointer to

 the symbol_table or nothing is placed in a global variable yylvalfootnoteIncidentally the yy that appears in yylval and lexyyc refers to the Yacc parser-generator

 which we_shall describe in Section yacc-sect and which is

 commonly_used in conjunction with Lexfootnote

 which is shared between the lexical_analyzer and parser

 thereby making it simple to return both the name and an attribute value

 of a token



 Structure of Lex Programs

 lex-struct-subsect



 A Lex program has the following form



 center

 tabularl

 declarations







 translation rules







 auxiliary functions



 tabular

 center

 The declarations section includes declarations of variables manifest_constants (identifiers declared to stand_for a constant eg

 the name of a token) and regular definitions in the style of

 Section reg-def-subsect



 The translation rules each have the form



 center

 Pattern Action

 center

 Each pattern is a regular_expression which may use the regular

 definitions of the declaration section

 The actions are fragments of code typically written in C although

 many variants of Lex using other languages have_been created



 The third section holds whatever additional functions are used in the

 actions

 Alternatively these functions can be compiled separately and loaded

 with the lexical_analyzer



 The lexical_analyzer created by Lex behaves in concert with the

 parser as_follows

 When called by the parser the lexical_analyzer begins reading its

 remaining_input one character at a time until it finds the longest

 prefix of the input that matches one of the patterns

 It then executes the associated action

 Typically will return to the parser but if it does_not (eg

 because describes whitespace or

 comments) then the lexical_analyzer proceeds to find

 additional lexemes until one of the corresponding actions causes a

 return to the parser

 The lexical_analyzer returns a single value the token_name to the parser

 but uses the shared integer variable yylval to pass additional

 information_about the lexeme found if needed



 ex

 lex-prog-ex

 Figure lex-prog-fig is a Lex program that recognizes the

 tokens of Fig if-exp-token-fig and returns the token found

 A few observations about this code will introduce us to many of the

 important features of Lex



 figure



 verbatim



 definitions of manifest_constants

 LT LE EQ NE GT GE

 IF THEN ELSE ID NUMBER RELOP





 regular definitions

 delim

 ws delim

 letter A-Za-z

 digit 0-9

 id letter(letterdigit)

 number digit(digit)(E-digit)







 ws no action and no return

 if return(IF)

 then return(THEN)

 else return(ELSE)

 id yylval (int) installID() return(ID)

 number yylval (int) installNum() return(NUMBER)

 yylval LT return(RELOP)

 yylval LE return(RELOP)

 yylval EQ return(RELOP)

 yylval NE return(RELOP)

 yylval GT return(RELOP)

 yylval GE return(RELOP)







 int installID() function to install the lexeme whose

 first character is pointed to by yytext

 and whose length is yyleng into the

 symbol_table and return a pointer

 thereto





 int installNum() similar to installID but puts numer-

 ical constants into a separate table



 verbatim



 Lex program for the tokens of Fig if-exp-token-fig

 lex-prog-fig



 figure



 In the declarations section we see a pair of special brackets

 123 and 125

 Anything within these brackets is copied directly to the file lexyyc and is not treated_as a regular definition

 It is common to place there the definitions of the manifest_constants

 using C define statements to associate unique integer codes with

 each of the manifest_constants

 In our example we have listed in a comment the names of the manifest

 constants LT IF and so on but have not shown them

 defined to be particular integersfootnoteIf Lex is used

 along with Yacc then it would be normal to define the manifest

 constants in the Yacc program and use them without definition in

 the Lex program

 Since lexyyc is compiled with the Yacc output the

 constants thus will be available to the actions in the Lex

 programfootnote



 Also in the declarations section is a sequence of regular definitions

 These use the extended notation for regular_expressions described in

 Section re-short-subsect

 Regular definitions that are used in later definitions or in the

 patterns of the translation rules are surrounded_by curly_braces

 Thus for instance delim is defined to be a shorthand for the

 character class consisting of the blank the tab and the newline the

 latter two are represented as in all UNIX commands by backslash

 followed_by t or n respectively

 Then ws is defined to be one or_more delimiters by the regular

 expression 123delim125



 Notice_that in the definition of id and number parentheses

 are used as grouping metasymbols and do_not stand_for themselves

 In_contrast E in the definition of number stands_for

 itself

 If we_wish to use one of the Lex metasymbols such_as any of the

 parentheses or to stand_for themselves we may

 precede them with a backslash

 For_instance we see 92 in the definition of number

 to represent the dot since that character is a metasymbol representing

 any character as usual in UNIX regular_expressions



 In the auxiliary-function section we see two such functions installID() and installNum()

 Like the portion of the declaration section that appears between

 123125 everything in the auxiliary section is

 copied directly to file lexyyc but may be used in the actions



 Finally let_us examine some of the patterns and rules in the middle

 section of Fig lex-prog-fig

 First ws an_identifier declared in the first section has an

 associated empty action

 If we find whitespace we do_not return to the parser but look for

 another lexeme

 The second token has the simple regular_expression pattern if

 Should we see the two letters if on the input and

 they are not followed_by another

 letter or digit (which would cause the lexical_analyzer to find a longer

 prefix of the input matching the pattern for id) then the

 lexical_analyzer consumes these two letters from the input and returns

 the token_name IF that is the integer for which the

 manifest constant IF stands

 Keywords then and else are treated similarly



 The fifth token has the pattern defined by id

 Note_that although keywords_like if match this pattern as_well as

 an earlier pattern Lex chooses whichever pattern is listed_first

 in situations_where the longest matching prefix matches two or_more

 patterns

 The action taken when id is matched is threefold



 enumerate



 Function installID() is called to place the lexeme found in the

 symbol_table



 This function returns a pointer to the symbol_table which is placed in

 global variable yylval where it can be used by the parser or a

 later component of the compiler

 Note_that installID() has available to it two variables that are

 set automatically by the lexical_analyzer that Lex generates

 enumerate

 yytext is a pointer to the beginning of the lexeme analogous to

 lexemeBegin in Fig buffer1-fig

 yyleng is the length of the lexeme found

 enumerate



 The token_name ID is returned to the parser



 enumerate

 The action taken when a lexeme matching the pattern number is

 similar using the auxiliary function installNum()

 ex



 Conflict Resolution in Lex

 lex-confl-subsect



 We have alluded to the two rules that Lex uses to decide on the

 proper lexeme to select when several prefixes of the input match one or

 more patterns



 enumerate



 Always prefer a longer prefix to a shorter prefix



 If the longest possible prefix matches two or_more patterns prefer the

 pattern listed_first in the Lex program



 enumerate



 ex

 lex-confl-ex

 The first rule tells_us to continue reading letters and digits to find

 the longest_prefix of these characters to group as an_identifier

 It also tells_us to treat as a single lexeme rather_than

 selecting as one lexeme and as the next lexeme

 The second rule makes keywords reserved if we list the keywords before

 id in the program

 For_instance if then is determined to be the longest_prefix of

 the input that matches any pattern and the pattern then precedes

 123id125 as it does in Fig lex-prog-fig then

 the token THEN is returned rather_than ID

 ex



 The Lookahead Operator

 lex-lookahead-subsect



 Lex automatically reads one character ahead of the last character

 that forms the selected lexeme and then retracts the input so only the

 lexeme itself is consumed from the input

 However sometimes we_want a certain pattern to be matched to the input

 only when it is followed_by a certain other characters

 If so we may use the slash in a pattern to indicate the end of the part

 of the pattern that matches the lexeme

 What follows is additional pattern that must_be matched before we

 can decide that the token in question was seen but what matches this

 second pattern is not part of the lexeme



 ex

 lex-slash-ex

 In Fortran and some other languages keywords are not reserved

 That situation creates problems such_as a statement



 verbatim

 IF(IJ) 3

 verbatim

 where IF is the name of an array not a keyword

 This statement contrasts with statements of the form



 verbatim

 IF( condition ) THEN

 verbatim

 where IF is a keyword

 Fortunately we can be_sure that the keyword IF is always followed

 by a left_parenthesis some text - the condition - that may contain

 parentheses a right_parenthesis and a letter

 Thus we could write a Lex rule for the keyword IF like



 verbatim

 IF letter

 verbatim

 This rule_says that the pattern the lexeme matches is just the two

 letters IF

 The slash says_that additional pattern follows but does_not match the

 lexeme

 In this pattern the first character is the left parentheses

 Since that character is a Lex metasymbol it must_be preceded by a

 backslash to indicate that it has its literal meaning

 The dot and star match any string without a newline

 Note_that the dot is a Lex metasymbol meaning any character

 except newline

 It is followed_by a right_parenthesis again with a backslash to give

 that character its literal meaning

 The additional pattern is followed_by the symbol letter which is

 a regular definition representing the character class of all letters



 Note_that in order for this pattern to be foolproof we must preprocess

 the input to delete whitespace

 We have in the pattern neither

 provision for whitespace nor can we deal_with the possibility that the

 condition extends over lines since the dot will not match a newline

 character



 For_instance suppose this pattern is asked to match a prefix of input



 verbatim

 IF(A(BC)D)THEN

 verbatim

 the first two characters match IF the next_character matches

 92( the next nine characters match and the next

 two match 92) and letter

 Note the fact that the first right_parenthesis (after C) is not

 followed_by a letter is irrelevant we only need to find some way of

 matching the input to the pattern

 We_conclude that the letters IF constitute the lexeme and they

 are an instance of token if

 ex



 exer

 Describe how to make the following modifications to the Lex

 program of Fig lex-prog-fig



 itemize



 a) Add the keyword while

 b) Change the comparison_operators to be the C operators of that

 kind

 c) Allow the underscore () as an additional letter

 d) Add a new pattern with token STRING The pattern

 consists of a double-quote () any string of characters and a

 final double-quote However if a double-quote appears in the string

 it must_be escaped by preceding it with a backslash (92) and

 therefore a backslash in the string must_be represented_by two

 backslashes The lexical value which is the string without the

 surrounding double-quotes and with backslashes used to escape a

 character removed Strings are to be installed in a table of strings



 itemize

 exer



 exer

 Write a Lex program that copies a file replacing each nonempty

 sequence of white_space by a single blank

 exer



 exer

 Write a Lex program that copies a C program replacing each

 instance of the keyword float by double

 exer



 hexer

 Write a Lex program that converts a file to Pig latin

 Specifically assume the file is a sequence of words (groups of letters)

 separated_by whitespace Every time you encounter a word



 enumerate



 If the first letter is a consonant move it to the end of the word and

 then add ay



 If the first letter is a vowel just add ay to the end of the

 word



 enumerate

 All nonletters are copied intact to the output

 hexer



 hsexer

 In SQL keywords and identifiers are case-insensitive

 Write a Lex program that recognizes the keywords SELECT

 FROM and WHERE (in any combination of capital and

 lower-case letters) and token ID which for the purposes of this

 exercise you may take to be any sequence of letters and digits

 beginning with a letter You need not install identifiers in a symbol

 table but tell how the install function would differ from that

 described for case-sensitive identifiers as in Fig lex-prog-fig

 hsexer

 Loops in Flow_Graphs

 loops-sect



 In our_discussion so_far loops are not handled specially but are treated

 just like any other kind of

 control_flow However loops are important because

 programs_spend most of their time executing

 them and optimizations that improve the performance of loops can

 have a significant impact As_discussed in

 Section secprog-abs some of these analyses need to be

 iteration-sensitive that is they need to distinguish_among

 different iterations of a loop Thus it is essential that we

 identify loops and treat_them specially Loops also affect the running

 time of program analyses If a program does

 not contain any loops we can obtain the answers to data-flow_problems

 by making just one

 pass_through the program For

 example a forward_data-flow problem

 can be_solved by visiting all the

 nodes once in topological_order



 In this_section we introduce the following concepts dominators

 depth-first_ordering

 back

 edges graph depth and reducibility

 Each of these are needed for our

 subsequent discussions on finding loops and the speed of

 convergence of

 iterative data flow analysis



 Dominators

 secdominance

 We_say node of a flow_graph

 dominates

 node written

 if every_path from the initial_node of the flow_graph to goes_through

 Note_that under this definition every node_dominates itself



 ex

 Consider the flow_graph of Fig_figdom-fg with initial_node 1 The

 initial_node dominates every node Node 2 dominates only

 itself since control can reach any other node along a path that

 begins_with



 Node 3 dominates all but 1 and 2 Node_4 dominates all but 1_2 and 3

 since all paths from 1 must begin_with

 or



 Nodes 5 and 6 dominate only themselves since flow of control can

 skip around either by going_through the other Finally 7 dominates

 7_8 9 and 10 8 dominates 8_9 and 10_9 and 10 dominate only

 themselves

 ex



 figurehtb

 figureuullmanalsuch9figsdom-fgeps

 A flow_graph

 figdom-fg

 figure



 A useful way of presenting

 dominator information is in a tree

 called the

 dominator_tree

 in which the initial_node is the root and each node

 dominates only its descendants in the tree

 For_example Fig figdom-tree shows the dominator_tree

 for the flow_graph of Fig_figdom-fg

 figurehtb

 figureuullmanalsuch9figsdom-treeeps

 Dominator tree for flow_graph of Fig_figdom-fg

 figdom-tree

 figure



 The existence of dominator trees follows from a property

 of dominators

 each node has a unique

 immediate_dominator

 that is the last dominator of on any path from

 the initial_node to

 In terms of the relation

 the immediate_dominator

 has that property that

 if and

 then



 Properties of the dom Relation

 A key observation about dominators is that if we take any_acyclic path

 from the entry to node then all the dominators of appear along

 this path and moreover they must appear in the same order along

 any such path

 To_see why suppose there were one acyclic_path to along which

 dominators and appeared in that order and another such path

 along which preceded

 Then we could follow to and to thereby avoiding

 altogether



 This reasoning allows_us to prove that dom is transitive if

 and then

 Also dom is antisymmetric it is never possible that both

 and hold

 Moreover if and are two dominators of then either

 or must hold

 Finally we see_why each node except the entry must have a unique

 immediate_dominator - that dominator which appears closest to

 along any_acyclic path from the entry to



 We_shall give a simple algorithm for computing the dominators of every

 node in a flow_graph based_on the principle that if

 are all the predecessors of and then

 if and only if for each This problem can

 be formulated as a forward_data-flow analysis The data-flow_values

 are sets of basic_blocks A node's set of dominators other_than

 itself is the

 intersection of the dominators of all its_predecessors thus the meet

 operator is set_intersection The transfer_function for block

 simply adds itself to the set of input nodes

 The boundary

 condition is that the entry_node dominates itself Finally

 the initialization of the interior_nodes is the universal_set



 alg

 algdom

 Finding_dominators



 A flow_graph with set of nodes set of edges and

 initial_node entry



 The relation



 Find the solution to the data flow problems whose parameters are shown

 in Fig figdom

 Finding_dominators using this data-flow algorithm is

 efficient Nodes in the graph need to be visited only a few

 times as will be discussed in

 Section secconvergence-speed









 alg



 figurehtb

 center

 tabularl_l

 Dominators



 Domain sets of basic_blocks



 Direction forwards









 Transfer_function



 Boundary





 Meet_()





 Initialization



 tabular

 center

 A data-flow algorithm for computing dominators

 figdom

 figure



 ex

 exdom-ex

 Let_us return to the flow_graph of Fig_figdom-fg

 and suppose the for-loop of lines_(4) through_(6) in Fig figdf-alg

 visits the nodes

 in numerical order

 Let be the set of nodes in

 Since 1 is the entry_node

 was assigned at line_(1)

 Node 2 has only 1 for a predecessor

 so

 Thus is set to 1_2

 Then node 3 with predecessors 1_2 4 and 8 is considered

 Since all the interior_nodes are initialized_with the universal_set





 D(3) 3 (11212 10

 12 10) 13



 The remaining calculations are shown in Fig dom-comp-fig

 Since these values do_not change in the second iteration through the

 outer_loop in lines_(3) through_(6) in Fig_figdf-alg(a)

 they are the final answers to the dominator problem

 ex



 figurehtfb

 align

 D(4) 4 (D(3)D(7)) 4 (13 12 10)134



 D(5) 5 D(4) 5 134 1345



 D(6) 6 D(4) 6 134 1346



 D(7) 7 (D(5)D(6)D(10))



 7 (1345134612 10) 1347



 D(8) 8 D(7) 8 1347 13478



 D(9) 9 D(8) 9 13478 134789



 D(10) 10 D(8) 10 13478 1347810

 align



 Completion of the dominator calculation for Example exdom-ex

 dom-comp-fig



 figure



 Depth-First Ordering and Graph Reducibility

 secdfs





 A depth-first_search of a graph visits all the nodes in the

 graph once by starting_at the initial_node and visiting the nodes as

 far away from the initial_node as quickly as possible The route of

 the search in a depth-first_search forms a depth-first_spanning

 tree (DFST)



 A preorder_traversal visits a node before

 visiting any of its_children which it then visits recursively in

 left-to-right_order while a postorder_traversal visits a

 node's children recursively in left-to-right_order

 before_visiting the node itself There is one more variant ordering

 that is important for flow-graph analysis a depth-first

 ordering is the reverse of a postorder_traversal That is in a

 depth-first_ordering we visit a node then traverse its rightmost

 child the child to its left and so on

 Before we give

 the algorithm for depth-first_ordering let_us consider an example



 ex

 exdfs

 One possible depth-first_search

 of the flow_graph in Fig_figdom-fg

 is illustrated in Fig_figdepth-first

 Solid edges form the tree dashed edges

 are the other edges of the flow_graph

 A depth-first_traversal of the tree is given by



 then back to 8

 then to 9

 We go back to 8 once more retreating to 7 6 and 4

 and then forward to 5

 We retreat from 5 back to 4 then back to 3 and 1

 From 1 we go to 2 then retreat from 2 back to 1 and we have

 traversed the entire tree



 The preorder sequence for the traversal is thus



 1 3 4 6_7 8 10_9 5 2





 The postorder_sequence for the traversal of the tree in

 Fig_figdepth-first is



 10_9 8 7 6 5 4 3 2 1





 The depth-first_ordering which is the reverse of the postorder

 sequence is



 1_2 3 4_5 6_7 8_9 10



 ex



 figurehtb

 figureuullmanalsuch9figsdepth-firsteps

 A depth-first_presentation of the flow_graph in Fig_figdom-fg

 figdepth-first

 figure



 An Algorithm to Compute Depth-First Ordering



 We_now give an algorithm that finds a depth-first_spanning

 tree and a depth-first_ordering of a graph

 It is this algorithm that finds the

 DFST in Fig_figdepth-first from Fig_figdom-fg



 alg

 algdfst

 Depth-first spanning_tree and depth-first_ordering



 A flow_graph



 A DFST of and an ordering of the nodes of



 We use the recursive_procedure ) of Fig_figdfs-alg

 the algorithm initializes all nodes of to unvisited then

 calls ) where is the entry

 When we call ) we first mark visited to avoid

 adding to the tree twice

 We use to count from the number of nodes of down to 1

 assigning depth-first numbers to nodes as we go

 The set of edges forms the depth-first_spanning

 tree for

 alg



 figurehtb

 center

 tabularl

 void



 mark visited



 for ( each successor of )



 if_( is unvisited )



 add_edge to































 set of edges (



 for ( each node of )



 mark unvisited



 number of nodes of



 )





 tabular

 center

 Depth-first search algorithm

 figdfs-alg

 figure



 ex

 exdfs-alg

 For the flow_graph in Fig_figdepth-first

 Algorithm_algdfst sets

 to 10 and begins the search by calling (1)

 The rest of the execution sequence is

 shown in

 Fig figdfs-execution

 ex



 figure

 tabularl p35in

 Call

 Node 1 has two_successors Suppose is considered first

 add_edge to





 Call

 Add edge to





 Call

 Node_4 has two_successors 4 and 6

 Suppose is considered first add

 edge to





 Call

 Add to





 Call

 Node 7 has two_successors 4 and 8

 But 4 is already_marked visited by so do_nothing

 when

 For add_edge to





 Call

 Node 8 has two_successors 9 and 10

 Suppose is considered first

 add_edge





 Call

 10 has a successor 7 but 7 is already_marked visited Thus

 completes by setting

 and





 Return to

 Set and add_edge to





 Call

 The only successor of 9 node 1 is already visited so set

 and





 Return to

 The last successor of 8 node 3 is visited so do

 nothing for

 At this point all successors of 8 have_been considered so set

 and





 Return to

 All of 7's successors have_been considered so set and







 Return to

 Similarly 6's successors have_been considered so set and







 Return to

 Successor 3 of 4 has_been visited but 5 has not so

 add to the tree





 Call

 Successor 7 of 5 has_been visited

 thus set and





 Return to

 All successors of 4 have_been considered set and





 Return to

 Set and





 Return to

 2 has not been_visited yet so add to





 Call

 Set





 Return to

 Set and

 tabular

 Execution of Algorithm_algdfst on the flow_graph

 in Fig_figdfs-alg

 figdfs-execution

 figure



 Edges in a Depth-First Presentation of a Flow Graph

 When we construct a DFST for a flow_graph the edges of the

 flow_graph fall into three categories

 enumerate

 There_are edges called advancing_edges that go from a node

 to a proper descendant of in the tree All edges in the DFST

 itself are advancing_edges There_are no other advancing_edges in

 Fig_figdepth-first but for example if were an edge it would

 be in this category



 There_are edges that go from a node to an_ancestor of in the

 tree (possibly to itself) These edges we_shall term retreating_edges For_example





 and are the

 retreating_edges in Fig_figdepth-first



 There_are edges such that neither_nor is an_ancestor of

 the other in the DFST

 Edges and are the only such examples in

 Fig_figdepth-first We call these edges

 cross_edges

 An_important property of cross_edges is that if we draw the

 DFST so children of a node are drawn from left to right

 in the order in which they_were added to the tree

 then all cross_edges travel from right to left

 enumerate



 It should be noted that is a retreating_edge if and only

 if To_see why note_that if is a descendant of in the

 DFST then terminates_before so

 Conversely if

 then terminates_before

 or But must

 have begun before if

 there is an edge

 or else the fact that is a

 successor of would have made a descendant of in the DFST

 Thus the time is active is a subinterval of the time

 is active from which it follows that is an

 ancestor of in the DFST



 Back_Edges and Reducibility



 A back_edge is an edge whose head_dominates its_tail



 For any flow_graph every back_edge is retreating but not every

 retreating_edge is a back_edge A flow_graph is said to be reducible

 if all its retreating_edges in any depth-first_spanning tree are also back

 edges In other_words if a graph is reducible then all the DFST's

 have the same set of retreating_edges and those are all the back_edges in

 the graph If the graph is nonreducible (not reducible)

 however all the back_edges

 are retreating_edges in any DFST but each DFST may have additional

 retreating_edges that are not back_edges These retreating_edges may

 be different from one DFST to another Thus if we remove all the

 back_edges of a flow_graph and the remaining graph is cyclic then the

 graph is nonreducible and vice_versa



 Why Are Back_Edges Retreating Edges

 Suppose is a back_edge ie its head_dominates its_tail

 The sequence of calls of the function in Fig_figdfs-alg

 that lead to node must_be a path in the flow_graph

 This path must of course include any dominator of

 It follows that

 a call to must_be open when is called

 Therefore is already in the tree when is added to the tree and

 is added as a descendant of

 Therefore must_be a retreating_edge



 Flow graphs that occur in practice are almost_always reducible

 Exclusive use of structured flow-of-control_statements such_as

 if-then-else while-do continue and break statements

 produces programs whose flow_graphs are always reducible

 Even programs_written using goto statements often turn out to be

 reducible as the programmer logically thinks in terms of loops and

 branches



 ex

 exreducible

 The flow_graph of Fig_figdom-fg is reducible The retreating

 edges in the graph are all back_edges that is their heads

 all dominate_their respective tails

 ex



 ex

 exback-edge

 Consider the flow_graph of Fig_figirreducible whose initial

 node is 1

 Node 1 dominates nodes 2 and 3 but 2 does_not dominate 3 nor vice-versa

 Thus this flow_graph has no back_edges since no

 head of any edge dominates its_tail

 There_are two possible depth-first_spanning trees depending_on whether

 we choose to call or first from

 In the first case edge is a retreating_edge but not a back

 edge in the second case is the retreating-but-not-back

 edge

 Intuitively the reason this flow

 graph is not reducible is that the cycle 2-3 can be entered at

 two different places nodes 2 and 3

 ex



 figurehtb

 figureuullmanalsuch9figsirreducibleeps

 A nonreducible_flow graph

 figirreducible

 figure



 Depth of a Flow Graph

 There is an important parameter of flow_graphs called its

 depth

 Given a depth-first_spanning tree for the graph

 the depth is the largest_number of retreating_edges on any cycle-free_path



 ex

 exdeepest-path

 In Fig_figdepth-first

 the depth is 3 since there is a

 path



 10 7 4 3



 with three retreating_edges

 but no cycle-free_path with four or_more retreating_edges

 It is a coincidence that the deepest_path here has only

 retreating_edges in general we may have a mixture of retreating

 advancing and cross_edges in a deepest_path

 ex



 We can prove the depth is never_greater than what one would

 intuitively call the depth of loop nesting in the flow_graph If a

 flow_graph is reducible we may replace retreating by back in

 the definition of depth since the retreating_edges in any DFST

 are exactly the back_edges The notion of depth then becomes

 independent of the DFST actually chosen and we may truly speak of the

 depth of a flow_graph rather_than the depth of a

 flow_graph in connection_with one of its depth-first_spanning trees



 Natural Loops

 secnatural-loops



 Loops can be specified in a source_program in many different_ways

 they can be written as for loops while loops or repeat

 loops they can even be defined using labels and goto statements From a

 program-analysis point of view it does_not matter_how the loops appear

 in the source code

 What matters is whether they have the properties that enable

 easy optimization In_particular we care_about whether a loop

 has a single entry_node if it does compiler analyses can assume

 certain initial conditions to hold at the beginning of each iteration

 through the loop

 This opportunity

 motivates the need for the definition of a natural_loop



 A natural_loop is defined by two essential properties



 enumerate



 It must have a single entry_node

 called the header

 This entry_node dominates all nodes in the loop

 or it would not be the sole entry to the loop



 There must_be a back_edge that enters the loop header Otherwise

 it is not possible for the flow of control to return to the header and

 we do_not really have a loop



 enumerate



 Given a back_edge we define the

 natural_loop

 of the edge to be plus the set of nodes

 that can reach without_going

 through

 Node is the

 header

 of the loop



 alg

 algloops

 Constructing the natural_loop of a back_edge



 A flow_graph and a back_edge



 The set consisting of all nodes in the natural_loop

 of



 Let be Mark as visited so that the

 search does_not reach beyond Perform a

 depth-first_search on the reverse control-flow_graph starting_with node

 Insert all the nodes visited in this search into This procedure

 finds all

 the nodes that reach without_going through

 alg



 ex

 In Fig_figdom-fg there are five back_edges those

 whose heads_dominate their tails







 and



 Note_that these

 are exactly the edges that one would think of as forming loops

 in the flow_graph



 Back_edge has natural_loop since 8 and 10 are

 the only nodes that can reach 10 without_going through 7

 Back_edge has a natural_loop consisting of

 and therefore contains the loop of

 We thus assume the latter is an inner_loop contained inside the former



 The natural_loops

 of back_edges and have the same header node 3

 and

 they also happen to have the same set of nodes

 We_shall therefore combine these two

 loops as one

 This loop contains the two smaller loops discovered earlier



 Finally the edge has as its natural_loop the entire_flow

 graph and therefore is the outermost_loop

 In this example the four loops are nested_within one another

 It is typical however for one loop to have two loops nested

 within neither of which is a subset of the other

 ex



 In reducible_flow graphs since all retreating_edges are back_edges

 we can associate a natural_loop with each retreating_edge That is

 not the case with nonreducible graphs The nonreducible_flow graph in

 Fig_figirreducible has a cycle consisting of nodes 2 and 3

 None of the edges in the cycle are back_edges so this cycle does_not

 fit the definition of a natural_loop We do_not identify the cycle as

 a natural_loop and it is not optimized as such This situation is

 acceptable because now all our loop analyses can be made simpler by assuming

 that all loops have single entry nodes

 and nonreducible programs are rare in practice anyway



 By considering only natural_loops as

 loops we have the useful property that unless two loops have the

 same header they are either_disjoint or one is

 nested_within the other Thus we have a natural notion of innermost_loops loops that contain no other loops



 When two natural_loops have the same header as in

 Fig_figcommon-header it is hard to tell which is the inner

 loop Thus we_shall assume that when two natural_loops have the same

 header and neither is properly_contained within the other

 they are combined and

 treated_as a single loop



 figurehtb

 figureuullmanalsuch9figscommon-headereps

 Two loops with the same header

 figcommon-header

 figure



 ex

 The natural_loops of the back_edges and

 in Fig_figcommon-header are and

 respectively

 We_shall combine them into a single loop



 However were there another back_edge in

 Fig_figcommon-header its natural_loop would be a

 third loop with header 1

 This set of nodes is properly_contained within so it

 would not be_combined with the other natural_loops but rather treated

 as an inner_loop nested_within

 ex



 Speed of Convergence of Iterative Data-FlowAlgorithms

 secconvergence-speed



 We are now_ready to discuss the speed of convergence of iterative

 algorithms As_discussed in Section df-iterative the maximum

 number of iterations the algorithm may take is the product of the

 height of the lattice and the number of nodes in the flow_graph For

 many data-flow_analyses it is possible to order the evaluation such

 that the algorithm_converges in a much_smaller number of iterations

 The property of interest is whether all events of significance at a node

 will be propagated to that node along some_acyclic path Among the data

 flow analyses discussed so_far reaching_definitions available

 expressions and live_variables have this property but constant

 propagation does_not More_specifically



 itemize



 If a definition is in then there is some

 acyclic_path from the block containing to such that

 is in the 's and 's all along that path



 If an expression xy is not available at the entrance to

 block then there is some_acyclic path that demonstrates that fact

 either the path is from the entry_node and includes no statement that

 kills or generates xy or the path is from a block that

 kills xy and along the path there is no_subsequent generation of xy



 For live_variables if x is live_on exit from block

 then there is an acyclic_path from to a use of x along which there

 are no definitions of x



 itemize

 You_should check that in each of these cases paths with

 cycles add nothing

 For_example if a use of x is reached from the end of block

 along a path with a cycle we can eliminate that cycle to find

 a shorter path along which the use of x is still reached from





 In_contrast constant_propagation does_not have this property

 Consider a simple program that has one loop containing a basic_block

 with statements

 verbatim

 L a b

 b_c

 c 1

 goto_L

 verbatim

 The first time the basic_block is visited c is found to have

 constant value 1 but both a and b are undefined

 Visiting the basic_block the second time we find that b and

 c have constant values 1 It takes three visits of the basic

 block for the constant value 1 assigned to c to reach a



 If all useful information propagates along acyclic_paths we have an

 opportunity to tailor the order in which we visit nodes in iterative

 data-flow algorithms so that after relatively few passes_through the

 nodes we can be_sure information has passed along all the acyclic

 paths



 Recalling our_discussion of the depth-first_spanning tree

 in Section secdfs

 we note_that if is an edge then the

 depth-first number of is less_than that of only when the

 edge is a retreating_edge

 For forward_data-flow problems it is desirable to visit the nodes

 according to the depth-first_ordering Specifically we modify the

 algorithm in Fig_figdf-alg(a) by_replacing line_(4) which visits

 the basic_blocks in the flow_graph with



 center

 for ( each block other_than entry in depth-first_order

 )

 center



 ex

 Suppose we have a path along which a definition propagates such_as



 3_5 19_35 16_23 45

 4 10_17



 where integers represent the depth-first numbers of the blocks

 along the path

 Then the first time through the loop of lines_(4) through_(6) in the

 algorithm in Fig_figdf-alg(a)

 will propagate from to to and so on up to



 It will not reach on that round

 because as 16 precedes 35 we had already_computed by the time

 was put in

 However the next time we run through the loop of lines (7) through (12)

 when we compute will be included because it is in

 Definition will also propagate to

 and so on up to

 where it must_wait because was already_computed on this round

 On the third_pass travels to and

 so after three passes we establish

 that reaches block 17

 ex



 It should not be hard to extract the general principle from this example

 If we use depth-first_order in Fig_figdf-alg(a)

 then the number of passes

 needed to propagate any reaching definition along any_acyclic path

 is no more_than one greater_than the number of edges along that path

 that go

 from a higher numbered block to a lower numbered block

 Those edges are exactly the retreating_edges so the number of passes needed

 is one plus the depth

 Of_course Algorithm_algreaching-definitions

 does_not detect the fact that all definitions

 have reached wherever they can reach until one more pass has yielded

 no changes

 Therefore the upper_bound

 on the number of passes taken by that algorithm with depth-first block

 ordering is actually two plus the depth

 Statistics gathered by Don KnuthfootnoteD E Knuth An

 empirical study of FORTRAN programs Software - Practice and

 Experience 12 (1971) pp 105-133footnote

 show that typical

 flow_graphs have an average depth around 275

 Thus the algorithm_converges very quickly



 In the case of backward-flow problems like live_variables

 we visit the nodes in the reverse of the depth-first_order

 Thus we may propagate a use of a variable in block 17

 backwards along the path



 3_5 19_35 16_23 45_4 10_17



 in one pass to where we must_wait for the next pass to in order reach



 On the second pass it reaches and on the third_pass it

 goes from to



 In_general one plus the depth passes suffices to carry

 the use of a variable backward along

 any_acyclic path

 However we must choose the reverse of depth-first_order to

 visit the nodes in a pass because then uses propagate along any

 decreasing sequence in a single pass



 A Reason for Irreducible Flow_Graphs

 There is one place where we cannot generally expect a flow_graph to be

 reducible

 If we reverse the edges of a program flow_graph as we did in

 Algorithm algloops to find natural_loops then we may not get a

 reducible_flow graph

 The intuitive reason is that while typical_programs have loops with

 single entries those loops sometimes have several exits which become

 entries when we reverse the edges



 The bound described so_far is an upper_bound on all problems where

 cyclic paths add no information to the analysis In special problems

 such_as dominators the algorithm_converges even faster In the case

 where the input flow_graph is reducible the correct set of dominators

 for each node is obtained in the

 first iteration of a data-flow algorithm that visits the nodes in

 depth-first_ordering If we do_not know that the input is reducible

 ahead of time it takes an extra iteration to determine that convergence

 has_occurred

 Loops in Flow_Graphs

 loops-sect



 In our_discussion so_far loops have not been handled

 differently they have_been treated

 just like any other kind of

 control_flow However loops are important because

 programs_spend most of their time executing

 them and optimizations that improve the performance of loops can

 have a significant impact

 Thus it is essential that we

 identify loops and treat_them specially



 Loops also affect the running

 time of program analyses If a program does

 not contain any loops we can obtain the answers to data-flow_problems

 by making just one

 pass_through the program For

 example a forward_data-flow problem

 can be_solved by visiting all the

 nodes once in topological_order



 In this_section we introduce the following concepts dominators

 depth-first_ordering

 back

 edges graph depth and reducibility

 Each of these is needed for our

 subsequent discussions on finding loops and the speed of

 convergence of

 iterative_data-flow analysis



 Dominators

 secdominance



 We_say node of a flow_graph

 dominates

 node written

 if every_path from the entry_node of the flow_graph to goes_through

 Note_that under this definition every node_dominates itself



 ex

 Consider the flow_graph of Fig_figdom-fg with entry_node 1 The

 entry_node dominates every node (this statement is true for every flow

 graph) Node 2 dominates only

 itself since control can reach any other node along a path that

 begins_with



 Node 3 dominates all but 1 and 2 Node_4 dominates all but 1_2 and 3

 since all paths from 1 must begin_with

 or



 Nodes 5 and 6 dominate only themselves since flow of control can

 skip around either by going_through the other Finally 7 dominates

 7_8 9 and 10 8 dominates 8_9 and 10_9 and 10 dominate only

 themselves

 ex



 figurehtb

 figureuullmanalsuch9figsdom-fgeps

 A flow_graph

 figdom-fg

 figure



 A useful way of presenting

 dominator information is in a tree

 called the

 dominator_tree

 in which the entry_node is the root and each node

 dominates only its descendants in the tree

 For_example Fig figdom-tree shows the dominator_tree

 for the flow_graph of Fig_figdom-fg

 figurehtb

 figureuullmanalsuch9figsdom-treeeps

 Dominator tree for flow_graph of Fig_figdom-fg

 figdom-tree

 figure



 The existence of dominator trees follows from a property

 of dominators

 each node has a unique

 immediate_dominator

 that is the last dominator of on any path from

 the entry_node to

 In terms of the relation

 the immediate_dominator

 has that property that

 if and

 then



 We_shall give a simple algorithm for computing the dominators of every

 node in a flow_graph based_on the principle that if

 are all the predecessors of and then

 if and only if for each This problem can

 be formulated as a forward_data-flow analysis The data-flow_values

 are sets of basic_blocks A node's set of dominators other_than

 itself is the

 intersection of the dominators of all its_predecessors thus the meet

 operator is set_intersection The transfer_function for block

 simply adds itself to the set of input nodes

 The boundary

 condition is that the entry_node dominates itself Finally

 the initialization of the interior_nodes is the universal_set

 that is the set of all nodes



 alg

 algdom

 Finding_dominators



 A flow_graph with set of nodes set of edges and

 entry_node entry



 the set of nodes that dominate node for all

 nodes in



 Find the solution to the data-flow_problem whose parameters are shown

 in Fig figdom

 The basic_blocks are the nodes

 for all in

 alg



 Finding_dominators using this data-flow algorithm is

 efficient Nodes in the graph need to be visited only a few

 times as we_shall see in

 Section secconvergence-speed











 figurehtb

 center

 tabularl_l

 Dominators



 Domain The power set of



 Direction_Forwards



 Transfer_function



 Boundary



 Meet_()



 Equations







 Initialization



 tabular

 center

 A data-flow algorithm for computing dominators

 figdom

 figure



 Properties of the dom Relation

 A key observation about dominators is that if we take any_acyclic path

 from the entry to node then all the dominators of appear along

 this path and moreover they must appear in the same order along

 any such path

 To_see why suppose there were one acyclic_path to along which

 dominators and appeared in that order and another path

 to along which preceded

 Then we could follow to and to thereby avoiding

 altogether Thus would not really dominate



 This reasoning allows_us to prove that dom is transitive if

 and then

 Also dom is antisymmetric it is never possible that both

 and hold if

 Moreover if and are two dominators of then either

 or must hold

 Finally it follows that each node except the entry must have a unique

 immediate_dominator - the dominator that appears closest to

 along any_acyclic path from the entry to



 ex

 exdom-ex

 Let_us return to the flow_graph of Fig_figdom-fg

 and suppose the for-loop of lines_(4) through_(6) in Fig figdf-alg

 visits the nodes

 in numerical order

 Let be the set of nodes in

 Since 1 is the entry_node

 was assigned at line_(1)

 Node 2 has only 1 for a predecessor

 so

 Thus is set to 1_2

 Then node 3 with predecessors 1_2 4 and 8 is considered

 Since all the interior_nodes are initialized_with the universal_set





 D(3) 3 (11212 10

 12 10) 13



 The remaining calculations are shown in Fig dom-comp-fig

 Since these values do_not change in the second iteration through the

 outer_loop of lines_(3) through_(6) in Fig_figdf-alg(a)

 they are the final answers to the dominator problem

 ex



 figurehtfb

 align

 D(4) 4 (D(3)D(7)) 4 (13 12 10)134



 D(5) 5 D(4) 5 134 1345



 D(6) 6 D(4) 6 134 1346



 D(7) 7 (D(5)D(6)D(10))



 7 (1345134612 10) 1347



 D(8) 8 D(7) 8 1347 13478



 D(9) 9 D(8) 9 13478 134789



 D(10) 10 D(8) 10 13478 1347810

 align



 Completion of the dominator calculation for Example exdom-ex

 dom-comp-fig



 figure



 Depth-First Ordering

 secdfs



 As introduced in Section dfs-subsect

 a depth-first_search of a graph visits all the nodes in the

 graph once by starting_at the entry_node and visiting the nodes as

 far away from the entry_node as quickly as possible The route of

 the search in a depth-first_search forms a depth-first_spanning

 tree (DFST) Recall from Section dfs-subsect

 that a preorder_traversal visits a node before

 visiting any of its_children which it then visits recursively in

 left-to-right_order Also a postorder_traversal visits a

 node's children recursively in left-to-right_order

 before_visiting the node itself



 There is one more variant ordering

 that is important for flow-graph analysis a depth-first

 ordering is the reverse of a postorder_traversal That is in a

 depth-first_ordering we visit a node then traverse its rightmost

 child the child to its left and so on

 However before we build the tree for the flow_graph we have choices as

 to which successor of a node becomes the rightmost child in the tree

 which node becomes the next child and so on

 Before we give

 the algorithm for depth-first_ordering let_us consider an example



 ex

 exdfs

 One possible depth-first_presentation

 of the flow_graph in Fig_figdom-fg

 is illustrated in Fig_figdepth-first

 Solid edges form the tree dashed edges

 are the other edges of the flow_graph

 A depth-first_traversal of the tree is given by



 then back to 8

 then to 9

 We go back to 8 once more retreating to 7 6 and 4

 and then forward to 5

 We retreat from 5 back to 4 then back to 3 and 1

 From 1 we go to 2 then retreat from 2 back to 1 and we have

 traversed the entire tree



 The preorder sequence for the traversal is thus



 1 3 4 6_7 8 10_9 5 2





 The postorder_sequence for the traversal of the tree in

 Fig_figdepth-first is



 10_9 8 7 6 5 4 3 2 1





 The depth-first_ordering which is the reverse of the postorder

 sequence is



 1_2 3 4_5 6_7 8_9 10



 ex



 figurehtb

 figureuullmanalsuch9figsdepth-firsteps

 A depth-first_presentation of the flow_graph in Fig_figdom-fg

 figdepth-first

 figure



 We_now give an algorithm that finds a depth-first_spanning

 tree and a depth-first_ordering of a graph

 It is this algorithm that finds the

 DFST in Fig_figdepth-first from Fig_figdom-fg



 alg

 algdfst

 Depth-first spanning_tree and depth-first_ordering



 A flow_graph



 A DFST of and an ordering of the nodes of



 We use the recursive_procedure ) of Fig_figdfs-alg

 The algorithm initializes all nodes of to unvisited then

 calls ) where is the entry

 When it calls ) it first marks visited to avoid

 adding to the tree twice

 It uses to count from the number of nodes of down to 1

 assigning depth-first numbers to nodes as we go

 The set of edges forms the depth-first_spanning

 tree for

 alg



 figurehtb

 center

 tabularl

 void



 mark visited



 for_(each successor of )



 if_( is unvisited)



 add_edge to































 set of edges



 for_(each node of )



 mark unvisited



 number of nodes of



 )





 tabular

 center

 Depth-first search algorithm

 figdfs-alg

 figure



 ex

 exdfs-alg

 For the flow_graph in Fig_figdepth-first

 Algorithm_algdfst sets

 to 10 and begins the search by calling (1)

 The rest of the execution sequence is

 shown in

 Fig figdfs-execution

 ex



 figure

 tabularl p32in

 Call

 Node 1 has two_successors Suppose is considered first

 add_edge to





 Call

 Add edge to





 Call

 Node_4 has two_successors 4 and 6

 Suppose is considered first add

 edge to





 Call

 Add to





 Call

 Node 7 has two_successors 4 and 8

 But 4 is already_marked visited by so do_nothing

 when

 For add_edge to





 Call

 Node 8 has two unvisited successors 9 and 10

 Suppose is considered first

 add_edge





 Call

 10 has a successor 7 but 7 is already_marked visited Thus

 completes by setting

 and





 Return to

 Set and add_edge to





 Call

 The only successor of 9 node 1 is already visited so set

 and





 Return to

 The last successor of 8 node 3 is visited so do

 nothing for

 At this point all successors of 8 have_been considered so set

 and





 Return to

 All of 7's successors have_been considered so set and







 Return to

 Similarly 6's successors have_been considered so set and







 Return to

 Successor 3 of 4 has_been visited but 5 has not so

 add to the tree





 Call

 Successor 7 of 5 has_been visited

 thus set and





 Return to

 All successors of 4 have_been considered set and





 Return to

 Set and





 Return to

 2 has not been_visited yet so add to





 Call

 Set





 Return to

 Set and

 tabular

 Execution of Algorithm_algdfst on the flow_graph

 in Fig_figdepth-first

 figdfs-execution

 figure



 Edges in a Depth-First Spanning Tree

 dfs-edges-subsect



 When we construct a DFST for a flow_graph the edges of the

 flow_graph fall into three categories

 enumerate

 There_are edges called advancing_edges that go from a node

 to a proper descendant of in the tree All edges in the DFST

 itself are advancing_edges There_are no other advancing_edges in

 Fig_figdepth-first but for example if were an edge it would

 be in this category



 There_are edges that go from a node to an_ancestor of in the

 tree (possibly to itself) These edges we_shall term retreating_edges For_example





 and are the

 retreating_edges in Fig_figdepth-first



 There_are edges such that neither_nor is an_ancestor of

 the other in the DFST

 Edges and are the only such examples in

 Fig_figdepth-first We call these edges

 cross_edges

 An_important property of cross_edges is that if we draw the

 DFST so children of a node are drawn from left to right

 in the order in which they_were added to the tree

 then all cross_edges travel from right to left

 enumerate



 It should be noted that is a retreating_edge if and only

 if To_see why note_that if is a descendant of in the

 DFST then terminates_before so

 Conversely if

 then terminates_before

 or But must

 have begun before if

 there is an edge

 or else the fact that is a

 successor of would have made a descendant of in the DFST

 Thus the time is active is a subinterval of the time

 is active from which it follows that is an

 ancestor of in the DFST



 Back_Edges and Reducibility



 A back_edge is an edge whose head_dominates its_tail



 For any flow_graph every back_edge is retreating but not every

 retreating_edge is a back_edge A flow_graph is said to be reducible

 if all its retreating_edges in any depth-first_spanning tree are also back

 edges In other_words if a graph is reducible then all the DFST's

 have the same set of retreating_edges and those are exactly the back_edges in

 the graph If the graph is nonreducible (not reducible)

 however all the back_edges

 are retreating_edges in any DFST but each DFST may have additional

 retreating_edges that are not back_edges These retreating_edges may

 be different from one DFST to another Thus if we remove all the

 back_edges of a flow_graph and the remaining graph is cyclic then the

 graph is nonreducible and conversely



 Why Are Back_Edges Retreating Edges

 Suppose is a back_edge ie its head_dominates its_tail

 The sequence of calls of the function in Fig_figdfs-alg

 that lead to node must_be a path in the flow_graph

 This path must of course include any dominator of

 It follows that

 a call to must_be open when is called

 Therefore is already in the tree when is added to the tree and

 is added as a descendant of

 Therefore must_be a retreating_edge



 Flow graphs that occur in practice are almost_always reducible

 Exclusive use of structured flow-of-control_statements such_as

 if-then-else while-do continue and break statements

 produces programs whose flow_graphs are always reducible

 Even programs_written using goto statements often turn out to be

 reducible as the programmer logically thinks in terms of loops and

 branches



 ex

 exreducible

 The flow_graph of Fig_figdom-fg is reducible The retreating

 edges in the graph are all back_edges that is their heads

 dominate_their respective tails

 ex



 ex

 exback-edge

 Consider the flow_graph of Fig_figirreducible whose initial

 node is 1

 Node 1 dominates nodes 2 and 3 but 2 does_not dominate 3 nor vice-versa

 Thus this flow_graph has no back_edges since no

 head of any edge dominates its_tail

 There_are two possible depth-first_spanning trees depending_on whether

 we choose to call or first from

 In the first case edge is a retreating_edge but not a back

 edge in the second case is the retreating-but-not-back

 edge

 Intuitively the reason this flow

 graph is not reducible is that the cycle 2-3 can be entered at

 two different places nodes 2 and 3

 ex



 figurehtb

 figureuullmanalsuch9figsirreducibleeps

 The canonical nonreducible_flow graph

 figirreducible

 figure



 Depth of a Flow Graph

 fg-depth-subsect



 Given a depth-first_spanning tree for the graph

 the depth is the largest_number of retreating_edges on any cycle-free_path

 We can prove the depth is never_greater than what one would

 intuitively call the depth of loop nesting in the flow_graph If a

 flow_graph is reducible we may replace retreating by back in

 the definition of depth since the retreating_edges in any DFST

 are exactly the back_edges The notion of depth then becomes

 independent of the DFST actually chosen and we may truly speak of the

 depth of a flow_graph rather_than the depth of a

 flow_graph in connection_with one of its depth-first_spanning trees



 ex

 exdeepest-path

 In Fig_figdepth-first

 the depth is 3 since there is a

 path



 10 7 4 3



 with three retreating_edges

 but no cycle-free_path with four or_more retreating_edges

 It is a coincidence that the deepest_path here has only

 retreating_edges in general we may have a mixture of retreating

 advancing and cross_edges in a deepest_path

 ex



 Natural Loops

 secnatural-loops



 Loops can be specified in a source_program in many different_ways

 they can be written as for-loops while-loops or

 repeat-loops they can even be defined using labels and goto statements From a

 program-analysis point of view it does_not matter_how the loops appear

 in the source code

 What matters is whether they have the properties that enable

 easy optimization In_particular we care_about whether a loop

 has a single-entry node if it does compiler analyses can assume

 certain initial conditions to hold at the beginning of each iteration

 through the loop

 This opportunity

 motivates the need for the definition of a natural_loop



 A natural_loop is defined by two essential properties



 enumerate



 It must have a single-entry node

 called the header

 This entry_node dominates all nodes in the loop

 or it would not be the sole entry to the loop



 There must_be a back_edge that enters the loop header Otherwise

 it is not possible for the flow of control to return to the header

 directly from the loop ie there really is no loop



 enumerate



 Given a back_edge we define the

 natural_loop

 of the edge to be plus the set of nodes

 that can reach without_going

 through

 Node is the

 header

 of the loop



 alg

 algloops

 Constructing the natural_loop of a back_edge



 A flow_graph and a back_edge



 The set consisting of all nodes in the natural_loop

 of



 Let be Mark as visited so that the

 search does_not reach beyond Perform a

 depth-first_search on the reverse control-flow_graph starting_with node

 Insert all the nodes visited in this search into This procedure

 finds all

 the nodes that reach without_going through

 alg



 ex

 In Fig_figdom-fg there are five back_edges those

 whose heads_dominate their tails







 and



 Note_that these

 are exactly the edges that one would think of as forming loops

 in the flow_graph



 Back_edge has natural_loop since 8 and 10 are

 the only nodes that can reach 10 without_going through 7

 Back_edge has a natural_loop consisting of

 and therefore contains the loop of

 We thus assume the latter is an inner_loop contained inside the former



 The natural_loops

 of back_edges and have the same header node 3

 and

 they also happen to have the same set of nodes

 We_shall therefore combine these two

 loops as one

 This loop contains the two smaller loops discovered earlier



 Finally the edge has as its natural_loop the entire_flow

 graph and therefore is the outermost_loop

 In this example the four loops are nested_within one another

 It is typical however to have two loops

 neither of which is a subset of the other

 ex



 In reducible_flow graphs since all retreating_edges are back_edges

 we can associate a natural_loop with each retreating_edge That statement

 does_not hold for nonreducible graphs For_instance

 the nonreducible_flow graph in

 Fig_figirreducible has a cycle consisting of nodes 2 and 3

 Neither of the edges in the cycle is a back_edge so this cycle does_not

 fit the definition of a natural_loop We do_not identify the cycle as

 a natural_loop and it is not optimized as such This situation is

 acceptable because our loop analyses can be made simpler by assuming

 that all loops have single-entry nodes

 and nonreducible programs are rare in practice anyway



 By considering only natural_loops as

 loops we have the useful property that unless two loops have the

 same header they are either_disjoint or one is

 nested_within the other Thus we have a natural notion of innermost_loops loops that contain no other loops



 When two natural_loops have the same header as in

 Fig_figcommon-header it is hard to tell which is the inner

 loop Thus we_shall assume that when two natural_loops have the same

 header and neither is properly_contained within the other

 they are combined and

 treated_as a single loop



 figurehtb

 figureuullmanalsuch9figscommon-headereps

 Two loops with the same header

 figcommon-header

 figure



 ex

 The natural_loops of the back_edges and

 in Fig_figcommon-header are and

 respectively

 We_shall combine them into a single loop



 However were there another back_edge in

 Fig_figcommon-header its natural_loop would be a

 third loop with header 1

 This set of nodes is properly_contained within so it

 would not be_combined with the other natural_loops but rather treated

 as an inner_loop nested_within

 ex



 Speed of Convergence of Iterative Data-FlowAlgorithms

 secconvergence-speed



 We are now_ready to discuss the speed of convergence of iterative

 algorithms As_discussed in Section df-iterative the maximum

 number of iterations the algorithm may take is the product of the

 height of the lattice and the number of nodes in the flow_graph For

 many data-flow_analyses it is possible to order the evaluation such

 that the algorithm_converges in a much_smaller number of iterations

 The property of interest is whether all events of significance at a node

 will be propagated to that node along some_acyclic path Among the

 data-flow_analyses discussed so_far reaching_definitions available

 expressions and live_variables have this property but constant

 propagation does_not More_specifically



 itemize



 If a definition is in then there is some

 acyclic_path from the block containing to such that

 is in the 's and 's all along that path



 If an expression is not available at the entrance to

 block then there is some_acyclic path that demonstrates that

 either the path is from the entry_node and includes no statement that

 kills or generates or the path is from a block that

 kills and along the path there is no_subsequent generation of



 If is live_on exit from block

 then there is an acyclic_path from to a use of along which there

 are no definitions of



 itemize

 We should check that in each of these cases paths with

 cycles add nothing

 For_example if a use of is reached from the end of block

 along a path with a cycle we can eliminate that cycle to find

 a shorter path along which the use of is still reached from





 In_contrast constant_propagation does_not have this property

 Consider a simple program that has one loop containing a basic_block

 with statements

 verbatim

 L a b

 b_c

 c 1

 goto_L

 verbatim

 The first time the basic_block is visited is found to have

 constant value 1 but both and are undefined

 Visiting the basic_block the second time we find that and

 have constant values 1 It takes three visits of the basic

 block for the constant value 1 assigned to to reach



 If all useful information propagates along acyclic_paths we have an

 opportunity to tailor the order in which we visit nodes in iterative

 data-flow algorithms so that after relatively few passes_through the

 nodes we can be_sure information has passed along all the acyclic

 paths



 Recall from Section dfs-edges-subsect

 that if is an edge then the

 depth-first number of is less_than that of only when the

 edge is a retreating_edge

 For forward_data-flow problems it is desirable to visit the nodes

 according to the depth-first_ordering Specifically we modify the

 algorithm in Fig_figdf-alg(a) by_replacing line_(4) which visits

 the basic_blocks in the flow_graph with



 center

 for_(each block other_than entry in depth-first order)



 center



 ex

 Suppose we have a path along which a definition propagates such_as



 3_5 19_35 16_23 45

 4 10_17



 where integers represent the depth-first numbers of the blocks

 along the path

 Then the first time through the loop of lines_(4) through_(6) in the

 algorithm in Fig_figdf-alg(a)

 will propagate from to to and so on up to



 It will not reach on that round

 because as 16 precedes 35 we had already_computed by the time

 was put in

 However the next time we run through the loop of lines_(4) through_(6)

 when we compute will be included because it is in

 Definition will also propagate to

 and so on up to

 where it must_wait because was already_computed on this round

 On the third_pass travels to and

 so after three passes we establish

 that reaches block 17

 ex



 It should not be hard to extract the general principle from this example

 If we use depth-first_order in Fig_figdf-alg(a)

 then the number of passes

 needed to propagate any reaching definition along any_acyclic path

 is no more_than one greater_than the number of edges along that path

 that go

 from a higher numbered block to a lower numbered block

 Those edges are exactly the retreating_edges so the number of passes needed

 is one plus the depth

 Of_course Algorithm_algreaching-definitions

 does_not detect the fact that all definitions

 have reached wherever they can reach until one more pass has yielded

 no changes

 Therefore the upper_bound

 on the number of passes taken by that algorithm with depth-first block

 ordering is actually two plus the depth

 A studyfootnoteD E Knuth An

 empirical study of FORTRAN programs Software - Practice and

 Experience 12 (1971) pp 105-133footnote

 has shown that typical

 flow_graphs have an average depth around 275

 Thus the algorithm_converges very quickly



 In the case of backward-flow problems like live_variables

 we visit the nodes in the reverse of the depth-first_order

 Thus we may propagate a use of a variable in block 17

 backwards along the path



 3_5 19_35 16_23 45_4 10_17



 in one pass to where we must_wait for the next pass in order

 to reach

 On the second pass it reaches and on the third_pass it

 goes from to



 In_general one-plus-the-depth passes suffice to carry

 the use of a variable backward along

 any_acyclic path

 However we must choose the reverse of depth-first_order to

 visit the nodes in a pass because then uses propagate along any

 decreasing sequence in a single pass



 A Reason for Nonreducible Flow_Graphs

 There is one place where we cannot generally expect a flow_graph to be

 reducible

 If we reverse the edges of a program flow_graph as we did in

 Algorithm algloops to find natural_loops then we may not get a

 reducible_flow graph

 The intuitive reason is that while typical_programs have loops with

 single entries those loops sometimes have several exits which become

 entries when we reverse the edges



 The bound described so_far is an upper_bound on all problems where

 cyclic paths add no information to the analysis In special problems

 such_as dominators the algorithm_converges even faster In the case

 where the input flow_graph is reducible the correct set of dominators

 for each node is obtained in the

 first iteration of a data-flow algorithm that visits the nodes in

 depth-first_ordering If we do_not know that the input is reducible

 ahead of time it takes an extra iteration to determine that convergence

 has_occurred



 sexer

 dom-exer

 For the flow_graph of Fig_fg1-fig (see the exercises for

 Section_secopt-sources)



 itemize



 Compute the dominator relation

 Find the immediate_dominator of each node

 Construct the dominator_tree

 Find one depth-first_ordering for the flow_graph

 Indicate the advancing retreating cross and tree edges for your

 answer to

 Is the flow_graph reducible

 Compute the depth of the flow_graph

 Find the natural_loops of the flow_graph



 itemize

 sexer



 exer

 Repeat_Exercise dom-exer on the following flow_graphs



 itemize



 a)

 Fig_figqs-fg



 b)

 Fig_identity-fg-fig



 c)

 Your_flow graph from Exercise mm-code-exer



 d)

 Your_flow graph from Exercise primes-code-exer



 itemize

 exer



 hexer

 Prove the following about the dom relation



 itemize



 a)

 If and then (transitivity)



 b)

 It is never possible that both

 and hold if (antisymmetry)



 c)

 If and are two dominators of then either

 or must hold



 d)

 Each node except the entry has a unique

 immediate_dominator - the dominator that appears closest to

 along any_acyclic path from the entry to



 itemize

 hexer



 hexer

 Figure figdepth-first is one depth-first_presentation of the flow

 graph of Fig_figdom-fg How many other depth-first

 presentations of this flow_graph are there Remember order of children

 matters in distinguishing depth-first presentations

 hexer



 vhexer

 Prove that a flow_graph is reducible if and only if when we remove all

 the back_edges (those whose heads_dominate their tails) the resulting

 flow_graph is acyclic

 vhexer



 hsexer

 A complete flow_graph on nodes has arcs between

 any two nodes and (in both directions)

 For what values of is this graph reducible

 hsexer



 hexer

 A complete acyclic flow_graph on nodes

 has arcs

 for all nodes and such that Node 1 is the entry



 itemize



 a)

 For what values of is this graph reducible



 b)

 Does your answer to (a) change if you add self-loops for

 all nodes



 itemize

 hexer



 hsexer

 The natural_loop of a back_edge was defined to be plus

 the set of nodes that can reach without_going through Show

 that

 dominates all the nodes in the natural_loop of

 hsexer



 vhexer

 We claimed that the flow_graph of Fig_figirreducible is

 nonreducible If the arcs were replaced_by paths of disjoint_sets of nodes

 (except for the endpoints of course)

 then the flow_graph would still be nonreducible In_fact node 1 need

 not be the entry it can be any node reachable from the entry along a

 path whose intermediate nodes are not part of any of the four explicitly

 shown paths Prove the converse

 that every nonreducible_flow graph has a subgraph like

 Fig_figirreducible but with arcs possibly replaced_by node-disjoint

 paths and node 1 being any node reachable from the entry by a path that

 is node-disjoint from the four other paths

 vhexer



 vhexer

 Show that every depth-first_presentation for every

 nonreducible_flow graph has a retreating_edge that is

 not a back_edge

 vhexer



 vhexer

 Show that if the following condition



 f(a)g(a)a f(g(a))



 holds for all functions and and value then the general

 iterative_algorithm

 Algorithm_algiterative with iteration following a depth-first

 ordering converges within 2-plus-the-depth passes

 vhexer



 hexer

 Find a nonreducible_flow graph with two different DFST's that have

 different depths

 hexer



 hexer

 Prove the following



 itemize



 a)

 If a definition is in then there is some

 acyclic_path from the block containing to such that

 is in the 's and 's all along that path



 b)

 If an expression is not available at the entrance to

 block then there is some_acyclic path that demonstrates that fact

 either the path is from the entry_node and includes no statement that

 kills or generates or the path is from a block that

 kills and along the path there is no_subsequent generation of



 c)

 If is live_on exit from block

 then there is an acyclic_path from to a use of along which there

 are no definitions of



 itemize

 hexer

 Language Processors



 Simply stated a compiler is a program that can read a program in

 one language - the source_language - and translate it

 into an equivalent program in another language - the target language see Fig compiler-fig An_important role

 of the compiler is to report any errors in the source_program that it detects

 during the translation process



 figurehtfb





 A compiler compiler-fig

 figure



 If the target program is an executable machine-language program

 it can then be called by the user to process inputs and produce

 outputs see Fig target-fig



 figurehtfb





 Running the target program target-fig

 figure



 An interpreter is another common kind of language processor

 Instead of producing a target program as a translation an

 interpreter appears to directly execute the operations specified

 in the source_program on inputs supplied_by the user as shown in

 Fig interpreter-fig



 figurehtfb





 An interpreter interpreter-fig

 figure



 The machine-language target program produced_by a compiler is

 usually much faster_than an interpreter at mapping inputs to

 outputs An interpreter however can usually give better error

 diagnostics than a compiler because it executes the source

 program statement by statement



 exjava-compiler-ex

 Java language processors combine compilation and interpretation

 as shown in Fig hybrid-fig A Java source_program may first

 be compiled into an intermediate form called bytecodes The

 bytecodes are then interpreted by a virtual machine A benefit of

 this arrangement is that bytecodes compiled on one machine can be

 interpreted on another machine perhaps across a network



 figurehtfb





 A hybrid compiler hybrid-fig

 figure



 In order to achieve faster processing of inputs to outputs some

 Java compilers called just-in-time compilers translate the

 bytecodes into machine language immediately_before they run the

 intermediate program to process the input

 ex



 In_addition to a compiler several other programs may be required

 to create an executable target program as shown in

 Fig lp-system-fig A source_program may be divided_into

 modules stored in separate files The task of collecting the

 source_program is sometimes entrusted to a separate program

 called a preprocessor The preprocessor may also expand

 shorthands called macros into source_language statements



 The modified source_program is then fed to a compiler The

 compiler may produce an assembly-language program as its output

 because assembly language is easier to produce as output and is

 easier than machine code to debug The assembly language is then processed by a

 program called an assembler that produces relocatable

 machine code as its output



 Large programs are often compiled in pieces so the relocatable

 machine code may have to be linked together_with other relocatable

 object files and library files into the code that actually runs on

 the machine The linker resolves external memory addresses

 where the code in one file may refer to a location in another

 file The loader then puts together all of the executable

 object files into memory for execution



 figurehtfb





 A language-processing system lp-system-fig

 figure



 sexer

 What is the difference_between a compiler and an interpreter

 sexer



 exer

 What are the advantages of (a) a compiler over an interpreter

 (b) an interpreter over a compiler

 exer



 exer

 What advantages are there to a language-processing system

 in which the compiler produces assembly language rather

 than machine language

 exer



 exer

 A compiler that translates a high-level_language into

 another high-level_language is called a source-to-source

 translator What advantages are there to using C

 as a target language for a compiler

 exer



 exer

 Describe some of the tasks that an assembler needs to perform

 exer

 LR Parsers

 lr-parsers-sect



 This_section presents an efficient bottom-up syntax analysis

 technique that can be used to parse a large class of context-free

 grammars The technique is called LR() parsing the L is for

 left-to-right scanning of the input the R for constructing a

 rightmost_derivation in reverse and the for the number of

 input symbols of lookahead that are used in making parsing

 decisions The cases or are of practical interest and

 we_shall only consider LR_parsers with here When ()

 is omitted is assumed to be 1



 A grammar for which we can construct a parsing_table using one of

 the methods in this_section is said to be an LR grammar

 Intuitively in order for a grammar to be LR it is sufficient that

 a left-to-right shift-reduce_parser be_able to recognize handles

 when they appear on top of the stack



 LR_parsing is attractive for a variety of reasons



 itemize





 LR_parsers can be constructed to recognize virtually all

 programming-language constructs for which context-free_grammars

 can be written There_are context-free_grammars that are not LR

 but these can generally be avoided for typical

 programming-language constructs





 The LR_parsing method is the most general non-backtracking

 shift-reduce_parsing method known yet it can be_implemented as

 efficiently as other shift-reduce methods





 An LR_parser can detect a syntactic error as_soon as it is

 possible to do so on a left-to-right_scan of the input





 The class of grammars that can be_parsed using LR methods is a

 proper superset of the class of grammars that can be_parsed with

 predictive or LL methods For a grammar to be LR() we must_be

 able to recognize the occurrence of the right_side of a

 production having seen all of what is derived_from that right

 side with input symbols of lookahead This requirement is far

 less stringent than that for LL() grammars where we must_be

 able to recognize the use of a production seeing only the first

 symbols of what its right_side derives Thus LR_grammars can

 describe more languages than LL_grammars



 itemize



 The principal drawback of the method is that it is too_much work

 to construct an LR_parser by hand for a typical

 programming-language grammar A specialized tool an LR_parser

 generator is needed Fortunately many such generators are

 available and we_shall discuss the design and use of one Yacc

 in Section yacc-sect Such a generator takes a context-free

 grammar and automatically produces a parser for that grammar If

 the grammar contains ambiguities or other constructs that are

 difficult to parse in a left-to-right_scan of the input then the

 parser_generator can locate these constructs and provide detailed

 diagnostic messages



 After discussing the operation of an LR_parser we present three

 methods for constructing an LR_parsing table for a grammar



 The first method builds an SLR (or simple LR) parsing_table using

 concepts like items and the function introduced in Section

 bottom-up-sect The SLR_method is the easiest to implement

 but the least powerful of the three It may fail to produce a

 parsing_table for certain grammars on which the other methods

 succeed The second method called canonical_LR is the most

 powerful and the most expensive The third method called

 lookahead LR (LALR for short) is intermediate in power and cost

 between the other two The LALR method will work on most

 programming-language grammars and with some effort can be

 implemented efficiently Some techniques for compressing the size

 of an LR_parsing table are considered later in this_section



 The LR_Parsing Algorithm



 The schematic form of an LR_parser is shown in

 Fig lr-parser-model-fig It consists of an input an

 output a stack a driver program and a parsing_table that has

 two_parts ( and ) The driver program is the same

 for all LR_parsers only the parsing_table changes from one parser

 to another The parsing program reads characters from an input

 buffer one at a time Where a shift-reduce_parser would shift a

 symbol an LR_parser shifts a state Each state summarizes

 the information contained in the stack below it



 figurehtfb

 center



 Model of an LR_parser

 lr-parser-model-fig

 center

 figure



 The LR program stack_holds a sequence of states

 where is on top In the SLR_method - the

 canonical_LR and LALR methods are similar - the stack_holds

 states from the LR(0)_automaton By construction each state has a

 corresponding grammar symbol Recall that states correspond to

 sets of items and that there is a transition from state to

 state if All transitions to state

 must_be for the same grammar symbol Thus each state except

 the start_state has a unique grammar symbol associated_with

 it (The converse need not hold that is more_than one state may

 have the same grammar symbol See for example states and

 in the LR(0)_automaton in Fig_lr-states-fig which are both

 entered by transitions on E or states and which

 are both entered by transitions on T)



 The parsing_table consists of two_parts a parsing_action function

 and a goto function The program driving the LR

 parser behaves as_follows It determines the state

 currently on top of the stack and the current_input

 symbol It then consults the parsing_action

 table entry for state and input which can have one of

 four values



 enumerate





 shift where is a state





 reduce by a grammar production





 accept and





 error

 enumerate



 The function for the SLR_method is as discussed in Section

 bottom-up-sect It takes a state and a grammar symbol as

 arguments and produces a state



 The and functions ensure_that the states on the

 stack correspond to viable_prefixes Recall that the viable

 prefixes of are those prefixes of right-sentential_forms that

 can appear on the stack of a shift-reduce_parser because they do

 not extend past the rightmost handle



 A configuration of an LR_parser is a pair whose first

 component is the stack_contents and whose second_component is the

 unexpended input

 disp





 disp

 This configuration represents the right-sentential_form

 disp



 disp

 in essentially the same way as a shift-reduce_parser would the

 only_difference is that instead of grammar_symbols the stack

 holds states from which grammar_symbols can be recovered



 The next move of the parser is determined by reading the

 current_input symbol and the state on top of the stack

 and then consulting the entry in the parsing

 action table The configurations resulting after each of the four

 types of move are as_follows



 enumerate



 If shift the parser executes a shift

 move entering the configuration

 disp





 disp

 Here the parser has shifted the next state which is given in

 onto the stack The symbol need not be

 held on the stack since it can be recovered from The

 current_input symbol is now





 If reduce then the

 parser executes a reduce move entering the configuration

 disp





 disp

 where and is the length of

 the right_side of the production Here the parser first

 popped state symbols off the stack exposing state

 The parser then pushed the entry for

 onto the stack The current_input symbol is not changed in a

 reduce move For the LR_parsers we_shall construct

 the sequence of grammar_symbols corresponding to the

 states popped_off the stack will always match the right

 side of the reducing production



 The output of an LR_parser is generated after a reduce move by

 executing the semantic_action associated_with the reducing

 production For the time being we_shall assume the output

 consists of just printing the reducing production





 If accept parsing is completed





 If error the parser has discovered an

 error and calls an error_recovery routine

 enumerate



 The LR_parsing algorithm is summarized below All LR_parsers

 behave in this fashion the only_difference between one LR_parser

 and another is the information in the parsing_action and goto

 fields of the parsing_table



 alglr-parsing-alg LR_parsing algorithm



 An input_string and an LR_parsing table with functions

 and for a grammar



 If is in a bottom-up_parse for otherwise

 an error indication



 Initially the parser has on its stack where

 is the initial_state and in the input_buffer The parser

 then executes the program in Fig lr-alg-fig until an accept

 or error action is encountered

 alg



 figurehtfb

 tabbing



 set ip to point to the first symbol of



 do forever



 let be the state on top of the stack



 let be the symbol pointed to by ip



 if shift then



 push_onto the stack



 advance ip to the next_input symbol









 bf else if reduce

 then



 pop symbols off the stack



 let be the state now on top of the stack



 push on top of the stack



 output the production







 else if accept then



 return



 else





 tabbing

 LR_parsing programlr-alg-fig

 figure



 exaction-goto-ex

 Figure action-goto-fig shows the parsing_action and goto

 functions of an LR_parsing table for the expression grammar

 (expr-gram-display) repeated_here with the productions

 numbered

 disp

 tabbing

 (1) E_E T



 (2)ET



 (3)TT F



 (4)TF



 (5)F( E )



 (6)Fid

 tabbing

 disp

 The codes for the actions are



 figurehtfb

 center

 tabularcc_c c_c c_c cc c_c



 0pt0ptSTATE 6c 3c



 2-11

 -2ptid (_) E_T F



 1pt

 0 s5_s4 1_2 3



 1 s6 acc



 2 r2 s7 r2_r2



 3_r4 r4_r4 r4



 4 s5_s4 8 2_3



 5 r6_r6 r6_r6



 6 s5_s4 9 3



 7 s5_s4 10



 8 s6 s11



 9 r1 s7 r1_r1



 10 r3_r3 r3_r3



 11 r5_r5 r5_r5





 tabular

 Parsing_table for expression grammar

 action-goto-fig

 center

 figure



 enumerate



 means shift and stack state



 means reduce by production numbered

 acc

 means accept



 blank means error

 enumerate

 Note_that the value of for terminal is found in

 the action field connected with the shift_action on input for

 state The goto field gives for nonterminals

 Although we have not_yet explained how the entries for

 Fig_action-goto-fig were selected we_shall deal_with this

 issue shortly



 On input ididid the sequence of

 stack and input contents is shown in

 Fig lr-parser-moves-fig Also shown for clarity are the

 sequences of grammar_symbols corresponding to the states held on

 the stack For_example at line_(1) the LR_parser is in state 0

 the initial_state with no grammar symbol and with id the

 first input_symbol The action in row 0 and column id of the

 action field of Fig_action-goto-fig is s5 meaning shift by

 pushing state 5 That is what has happened at line (2) the state

 symbol 5 has_been pushed_onto the stack and id has_been

 removed_from the input



 Then becomes the current_input symbol and the action of

 state 5 on input is to reduce by F

 id One state symbol is popped_off the stack State 0 is then

 exposed Since the goto of state 0 on F is 3 state 3 is

 pushed_onto the stack We_now have the configuration in line_(3)

 Each of the remaining moves is determined similarly

 ex



 figurehtfb

 center

 tabularr_l l_r l



 STACK_SYMBOLS INPUT_ACTION





 1pt(1) 0 id_id id shift



 (2) 0 5 id_id id reduce by Fid



 (3) 0 3 F_id id reduce by TF



 (4) 0 2 T id_id shift



 (5) 0 2 7 T id_id shift



 (6) 0 2 7 5 T id_id reduce by Fid



 (7) 0 2 7 10 T_F id reduce by TTF



 (8) 0 2 T id reduce by ET



 (9) 0_1 E_id shift



 (10) 0_1 6 E_id shift



 (11) 0_1 6 5 E_id reduce by Fid



 (12) 0_1 6 3 E F reduce by TF



 (13) 0_1 6 9 E_T EET



 (14) 0_1 E accept





 tabular

 Moves of an LR_parser on ididid

 lr-parser-moves-fig

 center

 figure



 Constructing SLR Parsing_Tables



 The SLR_method for constructing parsing_tables is a good starting

 point for studying LR_parsing We_shall refer to the parsing_table

 constructed by this method as an SLR table and to an LR_parser

 using an SLR_parsing table as an SLR_parser The other two methods

 augment the SLR_method with lookahead information



 The SLR_method begins_with LR(0)_items and LR(0)_automata

 introduced in Section bottom-up-sect That is given a

 grammar we augment to produce with a new start

 symbol From we construct the canonical_collection

 of sets of items for together_with the function



 The and entries in the parsing_table are then

 constructed using the following algorithm It requires us to know

 () for each nonterminal of a grammar (see_Section

 top-down-sect)



 algslr-table-alg Constructing an SLR_parsing table



 An_augmented grammar



 The SLR_parsing table functions and for







 enumerate





 Construct the collection of

 sets of LR(0)_items for





 State is constructed from The parsing actions for state

 are determined as_follows



 enumerate





 If is in and

 then set to shift

 Here must_be a terminal





 If is in then set

 to reduce for all

 in () here may not be





 If is in then set

 to accept

 enumerate

 If any conflicting actions result from the above rules we say the

 grammar is not SLR(1) The algorithm_fails to produce a parser in

 this case





 The goto_transitions for state are constructed for all

 nonterminals using the rule If

 then





 All entries not defined by rules (2) and (3) are made error





 The initial_state of the parser is the one constructed from the

 set of items containing



 enumerate

 alg



 The parsing_table consisting of the parsing_action and goto

 functions determined by Algorithm slr-table-alg is called

 the SLR(1) table for G An LR_parser using the SLR(1) table

 for is called the SLR(1) parser for and a grammar having

 an SLR(1) parsing_table is said to be SLR(1) We usually

 omit the (1) after the SLR since we_shall not deal here

 with parsers having more_than one symbol of lookahead



 ex

 Let_us construct the SLR table for the augmented expression

 grammar The canonical_collection of sets of LR(0)_items for the

 grammar was shown in Fig_lr-states-fig First consider the

 set of items

 tabbing

 E'_E



 E_ET



 E_T



 T TF



 T_F



 F (E)



 F_id

 tabbing



 The item F(E)

 gives rise to the entry ( shift 4 the

 item Fid to the entry

 id shift 5 Other items in yield no

 actions Now_consider

 tabbing

 E'_E



 E_ET

 tabbing

 The first item yields accept the second

 yields shift 6 Next consider

 tabbing

 E'_E

 ET



 T TF

 tabbing

 Since (E) ) the first

 item makes

 ) reduce ET The

 second item makes shift 7 Continuing

 in this fashion we obtain the parsing_action and goto tables that

 were shown in Fig_lr-states-fig In that figure the

 numbers of productions in reduce actions are the same as the order

 in which they appear in the original grammar

 (expr-gram-display) That is E

 ET is number 1 ET is 2

 and so on

 ex



 exnot-slr-ex

 Every SLR(1) grammar is unambiguous but there are many

 unambiguous_grammars that are not SLR(1) Consider the grammar

 with productions



 displaylvalue-display

 317ptsminipage4in

 tabbing













 id





 tabbing

 minipage

 (lvalue-display)

 display

 Think of and as standing for l-value and

 r-value respectively and as an operator indicating

 contents of (As in Section OLD28 an l-value

 designates a location and an r-value is a value that can be

 stored in a location) The canonical_collection of sets of LR(0)

 items for grammar_(lvalue-display) is shown in

 Fig not-slr-fig



 figurehtfb

 center

 tabularr_l p r_l







































































 tabular

 Canonical LR(0)_collection for grammar_(lvalue-display)

 not-slr-fig

 center

 figure



 Consider the set of items The first item in this set makes

 be shift 6 Since ()

 contains (to see_why consider

 ) the second item sets

 to reduce Thus

 entry is multiply defined Since there is

 both a shift and a reduce entry in state

 2 has a shiftreduce_conflict on input_symbol



 Grammar (lvalue-display) is not ambiguous This shiftreduce

 conflict arises from the fact that the SLR_parser construction

 method is not powerful enough to remember enough left context to

 decide what action the parser should take on input having

 seen a string reducible to The canonical and LALR methods to

 be discussed next will succeed on a larger collection of

 grammars including grammar_(lvalue-display) Note_however

 that there are unambiguous_grammars for which every LR_parser

 construction method will produce a parsing_action table with

 parsing_action conflicts Fortunately such grammars can generally

 be avoided in programming_language applications

 ex



 Constructing Canonical LR_Parsing Tables



 We_shall now present the most general technique for constructing

 an LR_parsing table from a grammar Recall that in the SLR

 method state calls for reduction by if

 the set of items contains item

 and is in () In some situations however when

 state appears on top of the stack the viable_prefix

 on the stack is such that cannot be followed_by

 in a right-sentential_form Thus the reduction by

 would be invalid on input



 exlvalue-lr-ex

 Let_us reconsider Example_not-slr-ex where in state 2 we

 had item which could correspond to

 above and could be the sign

 which is in () Thus the SLR_parser calls for reduction

 by in state 2 with as the next_input

 (the shift_action is also called for because of item

 in state 2) However there is no

 right-sentential_form of the grammar in Example_not-slr-ex

 that begins Thus state 2 which is the state

 corresponding to viable_prefix only should not really call

 for reduction of that to

 ex



 It is possible to carry more information in the state that will

 allow_us to rule out some of these invalid reductions by

 By splitting states when necessary we can

 arrange to have each state of an LR_parser indicate exactly which

 input symbols can follow a handle for which there is a

 possible reduction to



 The extra information is incorporated into the state by redefining

 items to include a terminal symbol as a second_component The

 general form of an item becomes

 where is a production

 and is a terminal or the right_endmarker We call such an

 object an LR(1)_item The 1 refers to the length of the

 second_component called the lookahead of the

 item(Lookaheads that are strings of length greater_than

 one are possible of course but we_shall not consider such

 lookaheads here) The lookahead has no effect in an item of the

 form where is

 not but an item of the form

 calls for a reduction by only

 if the next_input symbol is Thus we are compelled to reduce

 by only on those input symbols for

 which is an LR(1)_item in the

 state on top of the stack The set of such 's will always be a

 subset of () but it could be a proper subset as in

 Example lvalue-lr-ex



 Formally we say LR(1)_item

 is valid for a viable_prefix if there is a

 derivation

 where

 enumerate





 and





 either is the first symbol of or is and

 is

 enumerate



 ex

 Let_us consider the grammar

 disp

 tabularl_c l







 tabular

 disp

 There is a rightmost_derivation

 We see that item

 is valid for a viable_prefix by letting

 and

 in the above definition



 There is also a rightmost_derivation

 From this derivation we see that item

 is valid for viable_prefix

 ex



 The method for building the collection of sets of valid LR(1)

 items is essentially the same as the one for building the

 canonical_collection of sets of LR(0)_items We only need to

 modify the two procedures and



 figurehtfb

 tabbing



 function



 do



 for each item in



 for each production in



 for each terminal in ()



 add to set



 while items are being added to



 return











 function



 initialize to be the empty_set



 for each item in



 add item to set



 return











 procedure



 initialize to



 do



 for each set of items in



 for each grammar symbol



 if is not empty and not in then



 add to



 while sets of items are being added to





 tabbing

 Sets of LR(1)_items construction for grammar lr1-alg-fig

 figure



 To appreciate the new definition of the operation

 consider an item of the form

 in the set of items_valid for some viable_prefix

 Then there is a rightmost_derivation

 where

 Suppose derives terminal string

 Then for each production of the form

 for some we have derivation

 Thus

 is valid for Note_that can be the first

 terminal derived_from or it is possible that

 derives in the derivation

 and can therefore be To summarize

 both possibilities we say that can be any terminal in (

 ) where is the function from Section

 top-down-sect Note_that cannot contain the first

 terminal of so () () We

 now_give the LR(1) sets of items construction



 figurehtfb

 center



 The graph for grammar_(lr1-gram-display)

 lr1-items-fig

 center

 figure



 alglr1-itemsConstruction of the sets of LR(1)_items





 An_augmented grammar



 The sets of LR(1)_items that are the set of items_valid for

 one or_more viable_prefixes of



 The procedures and and the main routine

 for constructing the sets of items are shown in

 Fig lr1-alg-fig

 alg



 exlr1-items-ex

 Consider the following augmented_grammar

 displaylr1-gram-display

 317ptstabularl_c l











 tabular

 (lr1-gram-display)

 display



 We begin by computing the closure of

 To close we match the item

 with the item

 in the procedure That is

 and

 Function tells_us to add

 for each production and terminal

 in () In terms of the present grammar

 must_be and since

 is and is may only be Thus we add





 We continue to compute the closure by_adding all items

 for in () That

 is matching against

 we have

 and Since does

 not derive the empty_string () () Since

 () contains terminals and we add items



 and None of

 the new items has a nonterminal immediately to the right of the

 dot so we have completed our first set of LR(1)_items The

 initial set of items is

 disp

 tabularp_l c_l















 tabular

 disp

 The brackets have_been omitted for notational_convenience and we

 use the notation as a shorthand

 for the two items and





 Now we compute ) for the various values of For

 we must close the item

 No additional closure is possible since the dot is at the right

 end Thus we have the next set of items

 disp

 tabularp_l c_l



 tabular

 disp

 For we close We add the

 -productions with second_component and then can add no

 more yielding

 disp

 tabularp_l c_l











 tabular

 disp

 Next let We must close

 We add the -productions with second_component

 yielding

 disp

 tabularp_l c_l











 tabular

 disp

 Finally let and we wind_up with the set of items

 disp

 tabularp_l c_l



 tabular

 disp

 We have finished considering on We get no new sets

 from but has 's on and On we

 get

 disp

 tabularp_l c_l



 tabular

 disp

 no closure being needed On we take the closure of

 to obtain

 disp

 tabularp_l c_l











 tabular

 disp

 Note_that differs_from only in second components We

 shall_see that it is common for several sets of LR(1)_items for a

 grammar to have the same first components and differ in their

 second components When we construct the collection of sets of

 LR(0)_items for the same grammar each set of LR(0)_items will

 coincide with the set of first components of one or_more sets of

 LR(1)_items We_shall have more to say about this phenomenon when

 we discuss LALR_parsing



 Continuing with the function for ) is

 seen to be

 disp

 tabularp_l c_l



 tabular

 disp

 Turning now to the 's of on and are

 and respectively and ) is

 disp

 tabularp_l c_l



 tabular

 disp

 and have no 's The 's of on and

 are and respectively and ) is

 disp

 tabularp_l c_l



 tabular

 disp



 The remaining sets of items yield no 's so we are done

 Figure lr1-items-fig shows the ten sets of items with their

 's

 ex



 We_now give the rules for constructing the LR(1) parsing_action

 and goto functions from the sets of LR(1)_items The action and

 goto functions are represented_by a table as before The only

 difference is in the values of the entries



 algcanonical-lr-alg Construction of canonical

 LR_parsing tables



 An_augmented grammar



 The canonical_LR parsing_table functions and

 for





 enumerate



 Construct the collection

 of sets of LR(1)_items for





 State of the parser is constructed from The parsing

 action for state is determined as_follows

 enumerate



 If is in and

 then set to shift

 Here must_be a terminal



 If is in

 then set to reduce







 If is in then set

 to accept

 enumerate

 If any conflicting actions result from the above rules we say the

 grammar is not LR(1) The algorithm_fails to produce a parser in

 this case





 The goto_transitions for state are constructed for all

 nonterminals using the rule If

 then





 All entries not defined by rules (2) and (3) are made error





 The initial_state of the parser is the one constructed from the

 set of items containing



 enumerate

 alg



 The table formed from the parsing_action and goto functions

 produced_by Algorithm lr-parsing-alg is called the

 canonical LR(1) parsing_table An LR_parser using this table is

 called a canonical LR(1) parser If the parsing_action function

 has no multiply-defined entries then the given grammar is called

 an LR(1) grammar As before we omit the (1) if it is

 understood



 ex

 The canonical parsing_table for grammar_(lr1-gram-display)

 is shown in Fig_lr1-table-fig Productions 1_2 and 3 are

 and

 ex



 figurehtfb

 center

 tabularc_c c_c c_c c

 -7pt0pt0ptstate

 3c

 2c



 2-7

 -2pt





 1pt0 s3 s4 1_2



 1 acc



 2 s6 s7 5



 3 s3 s4 8



 4 r3_r3



 5 r1



 6 s6 s7 9



 7 r3



 8 r2_r2



 9 r2





 tabular

 center

 Canonical parsing_table for grammar

 (lr1-gram-display)lr1-table-fig

 figure



 Every SLR(1) grammar is an LR(1) grammar but for an SLR(1)

 grammar the canonical_LR parser may have more states than the SLR

 parser for the same grammar The grammar of the previous examples

 is SLR and has an SLR_parser with seven states compared with the

 ten of Fig_lr1-table-fig



 Constructing LALR Parsing_Tables



 We_now introduce our last parser construction method the LALR

 (lookahead-LR) technique This method is often used in

 practice because the tables obtained_by it are considerably

 smaller_than the canonical_LR tables yet most_common syntactic

 constructs of programming_languages can be_expressed conveniently

 by an LALR grammar The same is almost true for SLR grammars but

 there are a few constructs that cannot be conveniently handled by

 SLR techniques (see Example_not-slr-ex for example)



 For a comparison of parser size the SLR and LALR tables for a

 grammar always have the same number of states and this number is

 typically several hundred states for a language like C The

 canonical_LR table would typically have several thousand states

 for the same size language Thus it is much_easier and more

 economical to construct SLR and LALR tables than the canonical_LR

 tables



 By way of introduction let_us again consider grammar

 (lr1-gram-display) whose sets of LR(1)_items were shown in

 Fig lr1-items-fig Take a pair of similar looking states

 such_as and Each of these states has only items with

 first component In the

 lookaheads are or in is the only lookahead



 To_see the difference_between the roles of and in the

 parser note_that the grammar generates the regular set

 When reading an input the parser

 shifts the first group of 's and their following onto the

 stack entering state 4 after_reading the The parser then

 calls for a reduction by provided the next

 input_symbol is or The requirement that or follow

 makes_sense since these are the symbols that could begin strings

 in If follows the first we have an input like

 which is not in the language and state 4 correctly

 declares an error if is the next_input



 The parser enters state_7 after_reading the second Then the

 parser must see on the input or it started with a string

 not of the form It thus makes_sense that state_7

 should reduce by on input and declare

 error on inputs or



 Let_us now replace and by the union of

 and consisting of the set of three items represented_by

 The goto's on to or

 from and now enter The

 action of state 47 is to reduce on any input The revised parser

 behaves essentially like the original although it might reduce

 to in circumstances where the original would declare

 error for example on input like or The error will

 eventually be caught in fact it will be caught before any more

 input symbols are shifted



 More_generally we can look for sets of LR(1)_items having the

 same_core that is set of first components and we may

 merge these sets with common_cores into one set of items For

 example in Fig lr1-items-fig and form such a

 pair with core Similarly

 and form another pair with core



 There is one more pair and with core

 Note_that in general a core is a set

 of LR(0)_items for the grammar at hand and that an LR(1) grammar

 may produce more_than two sets of items with the same_core



 Since the core of ) depends only on the core of

 the goto's of merged sets can themselves be merged Thus there is

 no problem revising the goto function as we merge sets of items

 The action functions are modified to reflect the non-error actions

 of all sets of items in the merger



 Suppose we have an LR(1) grammar that is one whose sets of LR(1)

 items produce no parsing_action conflicts If we replace all

 states having the same_core with their_union it is possible that

 the resulting union will have a conflict but it is unlikely for

 the following reason Suppose in the union there is a conflict on

 lookahead because there is an item

 calling for a reduction by and

 there is another item

 calling for a shift Then some set of items from which the union

 was formed has item and since

 the cores of all these states are the same it must have an item

 for some But then

 this state has the same shiftreduce_conflict on and the

 grammar was not LR(1) as we assumed Thus the merging of states

 with common_cores can never produce a shiftreduce_conflict that

 was not present in one of the original states because shift

 actions depend only on the core not the lookahead



 It is possible however that a merger will produce a

 reducereduce_conflict as the following example shows



 exreduce-reduce-ex

 Consider the grammar

 disp

 tabularl_c l















 tabular

 disp

 which generates the four strings and

 The reader can check that the grammar is LR(1) by constructing the

 sets of items Upon doing_so we find the set of items



 valid for viable_prefix and

 valid for Neither of

 these sets generates a conflict and their cores are the same

 However their_union which is

 disp

 tabularl_c l







 tabular

 disp

 generates a reducereduce_conflict since reductions by both

 and are called for on inputs

 and

 ex



 We are now prepared to give the first of two LALR table

 construction algorithms The general_idea is to construct the sets

 of LR(1)_items and if no conflicts arise merge sets with common

 cores We then construct the parsing_table from the collection of

 merged sets of items The method we are about to describe serves

 primarily as a definition of LALR(1) grammars Constructing the

 entire collection of LR(1) sets of items requires too_much space

 and time to be useful in practice



 alg1st-lalr-alg An easy but space-consuming LALR table construction



 An_augmented grammar



 The LALR_parsing table functions and for







 enumerate





 Construct the collection

 of sets of LR(1)_items





 For each core present among the set of LR(1)_items find all sets

 having that core and replace these sets by their_union





 Let be the resulting sets

 of LR(1)_items The parsing actions for state are constructed

 from in the same manner as in Algorithm

 canonical-lr-alg If there is a parsing_action conflict the

 algorithm_fails to produce a parser and the grammar is said not

 to be LALR(1)





 The table is constructed as_follows If is the union of

 one or_more sets of LR(1)_items that is

 then the cores of

 are the same since

 all have the same_core Let be the union

 of all sets of items having the same_core as Then





 enumerate

 alg



 The table produced_by Algorithm 1st-lalr-alg is called the

 LALR_parsing table for If there are no parsing_action

 conflicts then the given grammar is said to be an LALR(1)

 grammar The collection of sets of items constructed in step (3)

 is called the LALR(1)_collection



 ex

 Again consider grammar_(lr1-gram-display) whose graph

 was shown in Fig lr1-items-fig As we mentioned there are

 three pairs of sets of items that can be merged and

 are replaced_by their_union

 disp

 tabularp_l c_l











 tabular

 disp

 and are replaced_by their_union

 disp

 tabularp_l c_l



 tabular

 disp

 and and are replaced_by their_union

 disp

 tabularp_l c_l



 tabular

 disp

 The LALR action and goto functions for the condensed sets of items

 are shown in Fig_lalr-table-fig



 figurehtfb

 center

 tabularc_c c_c c_c c

 0pt0pt

 state 3c

 2c



 2-7

 -2pt





 1pt0 s36_s47 1_2



 1 acc



 2 s36_s47 5



 36 s36_s47 89



 47 r3_r3 r3



 5 r1



 89 r2_r2 r2





 tabular

 center

 LALR_parsing table for the grammar of Example

 lr1-items-exlalr-table-fig

 figure



 To_see how the goto's are computed consider

 In the original set of LR(1)_items

 and is now part of so we make )

 be We could have arrived at the same conclusion if we

 considered the other part of That is

 and is now part of For another

 example consider an entry that is exercised

 after the shift_action of on input In the original sets

 of LR(1)_items Since is now

 part of becomes Thus the

 entry in Fig_lalr-table-fig for state 2 and input is

 made s36 meaning shift and push_state 36 onto the stack

 ex



 When presented with a string from the language both

 the LR_parser of Fig_lr1-table-fig and the LALR_parser of

 Fig_lalr-table-fig make exactly the same sequence of shifts

 and reductions although the names of the states on the stack may

 differ ie if the LR_parser puts or on the stack

 the LALR_parser will put on the stack This relationship

 holds in general for an LALR grammar The LR and LALR_parsers will

 mimic one another on correct inputs



 However when presented with erroneous_input the LALR_parser may

 proceed to do some reductions after the LR_parser has declared an

 error although the LALR_parser will never shift another symbol

 after the LR_parser declares an error For_example on input

 followed_by the LR_parser of Fig_lr1-table-fig

 will put

 disp



 disp

 on the stack and in state 4 will discover an error because

 is the next_input symbol and state 4 has action error on

 In_contrast the LALR_parser of Fig_lalr-table-fig will

 make the corresponding moves putting

 disp



 disp

 on the stack But state 47 on input has action reduce

 The LALR_parser will thus change its stack to

 disp



 disp

 Now the action of state 89 on input is reduce

 The stack becomes

 disp



 disp

 whereupon a similar reduction is called for obtaining stack

 disp



 disp

 Finally state 2 has action error on input so the error

 is now discovered



 Efficient Construction of LALR Parsing_Tables



 There_are several modifications we can make to Algorithm

 1st-lalr-alg to avoid constructing the full collection of

 sets of LR(1)_items in the process of creating an LALR(1) parsing

 table The first observation is that we can represent a set of

 items by its kernel that is by those items that are either

 the initial item or that have

 the dot somewhere other_than at the beginning of the right_side



 Second we can compute the parsing actions generated_by from

 the kernel alone Any item calling for a reduction by

 will be in the kernel unless

 Reduction by is called for on

 input if and only if there is a kernel_item

 such that

 for some and is in

 The set of nonterminals such that

 can be precomputed for each nonterminal



 The shift actions generated_by can be determined from the

 kernel of as_follows We shift on input if there is a

 kernel_item where

 in a derivation in which the last step

 does_not use an -production The set of such 's can

 also be precomputed for each



 Here is how the goto_transitions for can be computed from the

 kernel If is in the

 kernel of then is

 in the kernel of Item

 is also in the kernel of if there is an item

 in the kernel of

 and for some If we

 precompute for each pair of nonterminals and whether

 for some then computing sets of

 items from kernels only is just slightly less efficient than doing

 so with closed sets of items



 To_compute the LALR(1) sets of items for an augmented_grammar

 we start with the kernel of the

 initial set of items Then we compute the kernels of the

 goto_transitions from as outlined above We continue

 computing the goto_transitions for each new kernel generated until

 we have the kernels of the entire collection of sets of LR(0)

 items



 ex

 Let_us again consider the augmented_grammar

 disp

 tabularl_c l









 id





 tabular

 disp

 The kernels of the sets of LR(0)_items for this grammar are shown

 in Fig lr0-kernels-fig

 ex



 figurehtfb

 center

 minipaget2in

 tabularp_c c_l























 tabular

 minipage

 minipaget15in

 tabularp_c c_l

 id

















 tabular



 minipage

 Kernels of the sets of LR(0)_items for grammar

 (lvalue-display)lr0-kernels-fig

 center

 figure



 Now we expand the kernels by_attaching to each LR(0)_item the

 proper lookahead symbols (second components) To_see how lookahead

 symbols propagate from a set of items to

 consider an LR(0)_item in

 the kernel of Suppose for some

 (perhaps and ) and

 is a production Then LR(0)_item

 is in



 Suppose now that we are computing not LR(0)_items but LR(1)

 items and is in the

 set Then for what values of will

 be in Certainly if some is in

 then the derivation

 tells_us that must_be

 in In this case the value of is irrelevant and

 we say that as a lookahead for

 is generated_spontaneously By definition is

 generated_spontaneously as a lookahead for the item

 in the initial set of items



 But there is another source of lookaheads for item

 If then

 will also be in

 We_say in this case that lookaheads_propagate from

 to

 A simple method to determine when an LR(1)_item in

 generates a lookahead in spontaneously and when

 lookaheads_propagate is contained in the next algorithm



 alglookaheads-alg Determining lookaheads



 The kernel of a set of LR(0)_items and a grammar

 symbol



 The lookaheads spontaneously_generated by items in for

 kernel_items in and the items in from which

 lookaheads are propagated to kernel_items in



 The algorithm is given in Fig lookaheads-fig It

 uses a dummy lookahead_symbol to detect situations in which

 lookaheads_propagate

 alg



 figurehtfb

 tabbing





 for each item in



 )



 if is in where is not then



 lookahead is generated_spontaneously for item



 in



 if is in then



 lookaheads_propagate from in to



 in





 tabbing

 Discovering propagated and spontaneous

 lookaheadslookaheads-fig

 figure



 Now let_us consider how we go about finding the lookaheads

 associated_with the items in the kernels of the sets of LR(0)

 items First we know that is a lookahead for

 in the initial set of LR(0)_items Algorithm

 lookaheads-alg gives_us all the lookaheads generated

 spontaneously After listing all those lookaheads we must allow

 them to propagate until_no further propagation is possible There

 are many different approaches all of which in some sense keep

 track of new lookaheads that have propagated to an item but

 which have not_yet propagated out The next algorithm describes

 one technique to propagate lookaheads to all items



 algpropagate-alg Efficient computation of the kernels of the LALR(1)

 collection of sets of items



 An_augmented grammar



 The kernels of the LALR(1)_collection of sets of items for







 enumerate



 Using the method outlined above construct the kernels of the sets

 of LR(0)_items for





 Apply_Algorithm lookaheads-alg to the kernel of each set of

 LR(0)_items and grammar symbol to determine which lookaheads

 are spontaneously_generated for kernel_items in and

 from which items in lookaheads are propagated to kernel_items

 in





 Initialize a table that gives for each kernel_item in each set of

 items the associated lookaheads Initially each item has

 associated_with it only those lookaheads that we determined in (2)

 were generated_spontaneously





 Make repeated_passes over the kernel_items in all sets When we

 visit an item we look up the kernel_items to which

 propagates its lookaheads using information tabulated in (2) The

 current set of lookaheads for is added to those already

 associated_with each of the items to which propagates its

 lookaheads We continue making passes_over the kernel_items until

 no more new lookaheads are propagated

 enumerate

 alg



 exlalr-items-ex

 Let_us construct the kernels of the LALR(1) items for the grammar

 in the previous example The kernels of the LR(0)_items were shown

 in Fig lr0-kernels-fig When we apply_Algorithm

 lookaheads-alg to the kernel of set of items we

 compute which is



 disp

 tabularc_c l

















 id





 tabular

 disp

 Two items in this closure cause lookaheads to be generated

 spontaneously Item

 causes lookahead to be spontaneously_generated for

 kernel_item in and item

 id causes to

 be spontaneously_generated for kernel_item

 in



 figurehtfb

 center

 tabularr_l r_l



 -3ptfrom to

























 id















 id

















 id













 tabular

 center

 Propagation of lookaheadspropagation-fig

 figure



 The pattern of propagation of lookaheads among the kernel_items

 determined in step_(2) of Algorithm_propagate-alg is

 summarized in Fig propagation-fig For_example the gotos

 of on symbols and are

 respectively and For we

 computed only the closure of the lone kernel_item

 Thus

 propagates its lookahead to each kernel_item in through





 In Fig kernel-lookaheads-fig we show steps (3) and (4) of

 Algorithm_propagate-alg The column labeled

 initshows the spontaneously_generated lookaheads for

 each kernel_item On the first pass the lookahead propagates

 from in to the six items listed in

 Fig propagation-fig The lookahead propagates from

 in to items

 in and

 in It also propagates to itself and to

 id in but these lookaheads

 are already present In the second and third passes the only new

 lookahead propagated is discovered for the successors of

 and on pass 2 and for the successor of on pass

 3 No new lookaheads are propagated on pass 4 so the final set of

 lookaheads is shown in the rightmost column of

 Fig kernel-lookaheads-fig



 figurehtfb

 center

 tabularr_l c_c c_c

 -7pt0pt0ptset 0pt0ptitem 4c0pt6ptlookaheads



 3-6

 -2pt init pass 1 pass 2 pass 3



 1pt





 2pt





 2pt









 2pt





 2pt





 2pt

 id



 2pt





 2pt





 2pt





 2pt







 tabular

 center

 Computation of lookaheadskernel-lookaheads-fig

 figure



 Note_that the shiftreduce_conflict found in Example

 not-slr-ex using the SLR_method has disappeared with the

 LALR technique The_reason is that only lookahead is

 associated_with in so there is no

 conflict with the parsing_action of shift on generated_by

 item in

 ex



 Compaction of LR_Parsing Tables



 A_typical programming_language grammar with 50 to 100 terminals

 and 100 productions may have an LALR_parsing table with several

 hundred states The action function may easily have 20000

 entries each requiring at_least 8 bits to encode Clearly a more

 efficient encoding than a two-dimensional_array may be important

 We_shall briefly mention a few techniques that have_been used to

 compress the action and goto fields of an LR_parsing table



 One useful technique for compacting the action field is to

 recognize that usually many rows of the action table are

 identical For_example in Fig_lr1-table-fig states 0 and

 3 have identical action entries and so do 2 and 6 We can

 therefore save considerable space at little cost in time if we

 create a pointer for each state into a one-dimensional array

 Pointers for states with the same actions point to the same

 location To access information from this array we assign each

 terminal a number from zero to one less_than the number of

 terminals and we use this integer as an offset from the pointer

 value for each state In a given state the parsing_action for the

 th terminal will be found locations past the pointer value

 for that state



 Further space efficiency can be achieved at the expense of a

 somewhat slower parser (generally considered a reasonable trade

 since an LR-like parser consumes only a small_fraction of the

 total compilation time) by creating a list for the actions of each

 state The list consists of (terminal-symbol action) pairs The

 most frequent action for a state can be placed_at the end of the

 list and in place of a terminal we may use the notation

 any meaning that if the current_input symbol has not been_found

 so_far on the list we should do that action no_matter what the

 input is Moreover error_entries can safely be replaced_by reduce

 actions for further uniformity along a row The errors will be

 detected later before a shift_move



 excompaction-ex

 Consider the parsing_table of Fig_action-goto-fig First

 note_that the actions for states 0 4 6 and 7 agree We can

 represent them all by the list

 disp

 tabularp_c

 symbol action



 id s5



 ( s4



 any error

 tabular

 disp

 State 1 has a similar list

 disp

 tabularp_c

 s6



 acc



 any error

 tabular

 disp

 In state 2 we can replace the error_entries by r2 so reduction

 by production 2 will occur on any input but Thus the list

 for state 2 is

 disp

 tabularp_c

 s7



 acc



 any r2

 tabular

 disp



 State 3 has only error and r4 entries We can replace the former

 by the latter so the list for state 3 consists of only the pair

 (any r4) States 5 10 and 11 can be treated similarly

 The list for state 8 is

 disp

 tabularp_c

 s6



 ) s11



 acc



 any error

 tabular

 disp

 and for state 9

 disp

 tabularp_c

 s7



 ) s11



 acc



 any r1

 tabular

 disp

 ex



 We can also encode the table by a list but here it appears

 more_efficient to make a list of pairs for each nonterminal

 Each pair on the list for is of the form

 indicating

 disp



 disp

 This technique is useful because there tend to be rather few

 states in any one column of the table The_reason is that

 the goto on nonterminal can only be a state derivable_from a

 set of items in which some items have immediately to the left

 of a dot No set has items with and immediately to the

 left of a dot if Thus each state appears in at most

 one column



 For more space reduction we note_that the error_entries in the

 goto table are never consulted We can therefore replace each

 error entry by the most_common non-error entry in its column This

 entry becomes the default it is represented in the list for each

 column by one pair with any in place of



 exerror-entries-ex

 Consider Fig_action-goto-fig again The column for F

 has entry 10 for state_7 and all other entries are either 3 or

 error We may replace error by 3 and create for column F the

 list

 disp

 tabularp_c





 7 10



 any 3

 tabular

 disp

 Similarly a suitable list for column is

 disp

 tabularp_c

 6 9



 any 2

 tabular

 disp

 For column E we may choose either 1 or 8 to be the default

 two entries are necessary in either case For_example we might

 create for column E the list

 disp

 tabularp_c

 4 8



 any 1

 tabular

 disp

 ex



 This space savings in these small examples may be misleading

 because the total_number of entries in the lists created in this

 example and the previous one together_with the pointers from

 states to action lists and from nonterminals to next-state lists

 result in unimpressive space savings over the matrix

 implementation of Fig_action-goto-fig For practical

 grammars the space needed for the list representation is

 typically less_than ten percent of that needed for the matrix

 representation



 The table-compression methods for finite_automata that were

 discussed in Section OLD39 can also be used to represent LR

 parsing_tables Application of these methods is discussed in the

 exercises

 More Powerful LR Parsers

 lr-parsers-sect



 In this_section we_shall extend the previous LR_parsing techniques to

 use one symbol of lookahead on the input

 There_are two different methods



 enumerate



 The canonical-LR or just LR method which makes full use of the

 lookahead symbol(s)

 This method uses a large set of items called the LR(1)_items



 The lookahead-LR or LALR method which is based_on the LR(0)

 sets of items and has many fewer states than typical parsers based_on

 the LR(1)_items

 By carefully introducing lookaheads into the LR(0)_items we can handle

 many more grammars with the LALR method than with the SLR_method and

 build parsing_tables that are no bigger than the SLR tables

 LALR is the method of choice in most situations



 enumerate

 After introducing both these methods we conclude with a discussion of

 how to compact LR_parsing tables for environments with limited memory



 Canonical LR(1) Items

 lr1-items-subsect



 We_shall now present the most general technique for constructing

 an LR_parsing table from a grammar Recall that in the SLR

 method state calls for reduction by if

 the set of items contains item

 and input_symbol

 is in () In some situations however when

 state appears on top of the stack the viable_prefix

 on the stack is such that cannot be followed_by

 in any right-sentential_form Thus the reduction by

 should be invalid on input



 exlvalue-lr-ex

 Let_us reconsider Example_not-slr-ex where in state 2 we

 had item which could correspond to

 above and could be the sign

 which is in () Thus the SLR_parser calls for reduction

 by in state 2 with as the next_input

 (the shift_action is also called for because of item

 in state 2) However there is no

 right-sentential_form of the grammar in Example_not-slr-ex

 that begins Thus state 2 which is the state

 corresponding to viable_prefix only should not really call

 for reduction of that to

 ex



 It is possible to carry more information in the state that will

 allow_us to rule out some of these invalid reductions by

 By splitting states when necessary we can

 arrange to have each state of an LR_parser indicate exactly which

 input symbols can follow a handle for which there is a

 possible reduction to



 The extra information is incorporated into the state by redefining

 items to include a terminal symbol as a second_component The

 general form of an item becomes

 where is a production

 and is a terminal or the right_endmarker We call such an

 object an LR(1)_item The 1 refers to the length of the

 second_component called the lookahead of the

 item(Lookaheads that are strings of length greater_than

 one are possible of course but we_shall not consider such

 lookaheads here) The lookahead has no effect in an item of the

 form where is

 not but an item of the form

 calls for a reduction by only

 if the next_input symbol is Thus we are compelled to reduce

 by only on those input symbols for

 which is an LR(1)_item in the

 state on top of the stack The set of such 's will always be a

 subset of () but it could be a proper subset as in

 Example lvalue-lr-ex



 Formally we say LR(1)_item

 is valid for a viable_prefix if there is a

 derivation

 where



 enumerate



 and



 Either is the first symbol of or is and

 is

 enumerate



 ex

 Let_us consider the grammar



 center

 tabularl









 tabular

 center

 There is a rightmost_derivation

 We see that item

 is valid for a viable_prefix by letting

 and

 in the above definition

 There is also a rightmost_derivation

 From this derivation we see that item

 is valid for viable_prefix

 ex



 Constructing LR(1) Sets of Items

 lr1-sets-subsect



 The method for building the collection of sets of valid LR(1)

 items is essentially the same as the one for building the

 canonical_collection of sets of LR(0)_items We need only to

 modify the two procedures and



 figurehtfb



 center

 tabularl

 SetOfItems



 repeat



 for ( each item in

 )



 for ( each production in )



 for ( each terminal in )



 add to set



 until_no more items are added to



 return











 SetOfItems



 initialize to be the empty_set



 for ( each item in

 )



 add item to set



 return











 void



 initialize to



 repeat



 for ( each set of items in )



 for ( each grammar symbol )



 if_( is not empty and not in )



 add to



 until_no new sets of items are added to







 tabular

 center



 Sets-of-LR(1)-items construction for grammar

 lr1-alg-fig



 figure



 To appreciate the new definition of the operation

 in particular why must_be in

 consider an item of the form

 in the set of items_valid for some viable_prefix

 Then there is a rightmost_derivation

 where

 Suppose derives terminal string

 Then for each production of the form

 for some we have derivation

 Thus

 is valid for Note_that can be the first

 terminal derived_from or it is possible that

 derives in the derivation

 and can therefore be To summarize

 both possibilities we say that can be any terminal in (

 ) where is the function from Section

 top-down-sect Note_that cannot contain the first

 terminal of so () () We

 now_give the LR(1) sets of items construction



 figurehtfb

 center



 The graph for grammar_(lr1-gram-display)

 lr1-items-fig

 center

 figure



 alg

 lr1-items

 Construction of the sets of LR(1)_items



 An_augmented grammar



 The sets of LR(1)_items that are the set of items_valid for

 one or_more viable_prefixes of



 The procedures and and the main routine

 for constructing the sets of items were shown in

 Fig lr1-alg-fig

 alg



 exlr1-items-ex

 Consider the following augmented_grammar



 displaylr1-gram-display

 317ptstabularr c_l













 tabular

 (lr1-gram-display)

 display



 We begin by computing the closure of

 To close we match the item

 with the item in the

 procedure That is

 and Function tells

 us to add for each production

 and terminal in () In terms

 of the present grammar must_be

 and since is and is may only

 be Thus we add



 We continue to compute the closure by_adding all items

 for in () That

 is matching against

 we have

 and Since does

 not derive the empty_string () () Since

 () contains terminals and we add items



 and None of

 the new items has a nonterminal immediately to the right of the

 dot so we have completed our first set of LR(1)_items The

 initial set of items is



 center

 tabularl_l

















 tabular

 center

 The brackets have_been omitted for notational_convenience and we

 use the notation as a shorthand

 for the two items and





 Now we compute ) for the various values of For

 we must close the item

 No additional closure is possible since the dot is at the right

 end Thus we have the next set of items



 center

 tabularl_l



 tabular

 center

 For we close We add the

 -productions with second_component and then can add no

 more yielding



 center

 tabularl_l













 tabular

 center

 Next let We must close

 We add the -productions with second_component

 yielding



 center

 tabularl_l











 tabular

 center

 Finally let and we wind_up with the set of items



 center

 tabularl_l



 tabular

 center

 We have finished considering on We get no new sets

 from but has goto's on and

 For we get



 center

 tabularl_l



 tabular

 center

 no closure being needed

 To_compute we take the closure of

 to obtain



 center

 tabularl_l











 tabular

 center

 Note_that differs_from only in second components We

 shall_see that it is common for several sets of LR(1)_items for a

 grammar to have the same first components and differ in their

 second components When we construct the collection of sets of

 LR(0)_items for the same grammar each set of LR(0)_items will

 coincide with the set of first components of one or_more sets of

 LR(1)_items We_shall have more to say about this phenomenon when

 we discuss LALR_parsing



 Continuing with the function for ) is

 seen to be



 center

 tabularl_l



 tabular

 center

 Turning now to the 's of on and are

 and respectively and ) is



 center

 tabularl_l



 tabular

 center

 and have no 's since all items have their dots at

 the right_end The 's of on and

 are and respectively and ) is



 center

 tabularl_l



 tabular

 center



 The remaining sets of items yield no 's so we are done

 Figure lr1-items-fig shows the ten sets of items with their

 goto's

 ex



 Canonical LR(1) Parsing_Tables

 can-lr1-table-subsect



 We_now give the rules for constructing the LR(1) and

 functions from the sets of LR(1)_items These

 functions are represented_by a table as before The only

 difference is in the values of the entries



 alg

 canonical-lr-alg

 Construction of

 canonical-LR parsing_tables



 An_augmented grammar



 The canonical-LR parsing_table functions and

 for



 enumerate

 Construct the collection

 of sets of LR(1)_items for



 State of the parser is constructed from The parsing

 action for state is determined as_follows

 enumerate

 If is in and

 then set to shift

 Here must_be a terminal

 If is in

 then set to reduce





 If is in then set

 to accept

 enumerate

 If any conflicting actions result from the above rules we say the

 grammar is not LR(1) The algorithm_fails to produce a parser in

 this case



 The goto_transitions for state are constructed for all

 nonterminals using the rule If

 then



 All entries not defined by rules (2) and (3) are made error



 The initial_state of the parser is the one constructed from the

 set of items containing



 enumerate

 alg



 The table formed from the parsing_action and goto functions

 produced_by Algorithm canonical-lr-alg is called the canonical LR(1) parsing_table An LR_parser using this table is

 called a canonical-LR(1) parser If the parsing_action function

 has no multiply defined entries then the given grammar is called

 an LR(1) grammar As before we omit the (1) if it is

 understood



 ex

 The canonical parsing_table for grammar_(lr1-gram-display)

 is shown in Fig_lr1-table-fig Productions 1_2 and 3 are

 and

 respectively

 ex



 figurehtfb

 center

 tabularc_c c_l c_c c

 -7pt0pt0ptstate

 3c

 2c



 2-7

 -2pt



 1pt0 s3 s4 1_2



 1 acc



 2 s6 s7 5



 3 s3 s4 8



 4 r3_r3



 5 r1



 6 s6 s7 9



 7 r3



 8 r2_r2



 9 r2



 tabular

 center

 Canonical parsing_table for grammar

 (lr1-gram-display)lr1-table-fig

 figure



 Every SLR(1) grammar is an LR(1) grammar but for an SLR(1)

 grammar the canonical_LR parser may have more states than the SLR

 parser for the same grammar The grammar of the previous examples

 is SLR and has an SLR_parser with seven states compared with the

 ten of Fig_lr1-table-fig



 Constructing LALR Parsing_Tables



 We_now introduce our last parser construction method the LALR

 (lookahead-LR) technique This method is often used in

 practice because the tables obtained_by it are considerably

 smaller_than the canonical_LR tables yet most_common syntactic

 constructs of programming_languages can be_expressed conveniently

 by an LALR grammar The same is almost true for SLR grammars but

 there are a few constructs that cannot be conveniently handled by

 SLR techniques (see Example_not-slr-ex for example)



 For a comparison of parser size the SLR and LALR tables for a

 grammar always have the same number of states and this number is

 typically several hundred states for a language like C The

 canonical_LR table would typically have several thousand states

 for the same-size language Thus it is much_easier and more

 economical to construct SLR and LALR tables than the canonical_LR

 tables



 By way of introduction let_us again consider grammar

 (lr1-gram-display) whose sets of LR(1)_items were shown in

 Fig lr1-items-fig Take a pair of similar looking states

 such_as and Each of these states has only items with

 first component In the

 lookaheads are or in is the only lookahead



 To_see the difference_between the roles of and in the

 parser note_that the grammar generates the regular language



 When reading an input the parser

 shifts the first group of 's and their following onto the

 stack entering state 4 after_reading the The parser then

 calls for a reduction by provided the next

 input_symbol is or The requirement that or follow

 makes_sense since these are the symbols that could begin strings

 in If follows the first we have an input like

 which is not in the language and state 4 correctly

 declares an error if is the next_input



 The parser enters state_7 after_reading the second Then the

 parser must see on the input or it started with a string

 not of the form

 It thus makes_sense that state_7

 should reduce by on input and declare

 error on inputs or



 Let_us now replace and by the union of

 and consisting of the set of three items represented_by

 The goto's on to or

 from and now enter The

 action of state 47 is to reduce on any input The revised parser

 behaves essentially like the original although it might reduce

 to in circumstances where the original would declare

 error for example on input like or The error will

 eventually be caught in fact it will be caught before any more

 input symbols are shifted



 More_generally we can look for sets of LR(1)_items having the

 same_core that is set of first components and we may

 merge these sets with common_cores into one set of items For

 example in Fig lr1-items-fig and form such a

 pair with core Similarly and

 form another pair with core

 There is one more

 pair and with common core

 Note_that in general a core is a set of LR(0)_items for the

 grammar at hand and that an LR(1) grammar may produce more_than

 two sets of items with the same_core



 Since the core of ) depends only on the core of

 the goto's of merged sets can themselves be merged Thus there is

 no problem revising the goto function as we merge sets of items

 The action functions are modified to reflect the non-error actions

 of all sets of items in the merger



 Suppose we have an LR(1) grammar that is one whose sets of LR(1)

 items produce no parsing-action_conflicts If we replace all

 states having the same_core with their_union it is possible that

 the resulting union will have a conflict but it is unlikely for

 the following reason Suppose in the union there is a conflict on

 lookahead because there is an item

 calling for a reduction by and

 there is another item

 calling for a shift Then some set of items from which the union

 was formed has item and since

 the cores of all these states are the same it must have an item

 for some But then

 this state has the same shiftreduce_conflict on and the

 grammar was not LR(1) as we assumed Thus the merging of states

 with common_cores can never produce a shiftreduce_conflict that

 was not present in one of the original states because shift

 actions depend only on the core not the lookahead



 It is possible however that a merger will produce a

 reducereduce_conflict as the following example shows



 exreduce-reduce-ex

 Consider the grammar



 center

 tabularr_c l

















 tabular

 center

 which generates the four strings and

 The reader can check that the grammar is LR(1) by constructing the

 sets of items Upon doing_so we find the set of items

 valid for

 viable_prefix and

 valid for Neither of these sets has a

 conflict and their cores are the same However their_union

 which is



 center

 tabularl









 tabular

 center

 generates a reducereduce_conflict since reductions by both

 and are called for on inputs

 and

 ex



 We are now prepared to give the first of two LALR

 table-construction algorithms The general_idea is to construct the sets

 of LR(1)_items and if no conflicts arise merge sets with common

 cores We then construct the parsing_table from the collection of

 merged sets of items The method we are about to describe serves

 primarily as a definition of LALR(1) grammars Constructing the

 entire collection of LR(1) sets of items requires too_much space

 and time to be useful in practice



 alg

 1st-lalr-alg

 An easy but space-consuming LALR table construction



 An_augmented grammar



 The LALR parsing-table functions and for





 enumerate



 Construct

 the collection of sets of LR(1)_items



 For each core present among the set of LR(1)_items find all sets

 having that core and replace these sets by their_union



 Let be the

 resulting sets of LR(1)_items The parsing actions for state

 are constructed from in the same manner as in Algorithm

 canonical-lr-alg If there is a parsing_action conflict the

 algorithm_fails to produce a parser and the grammar is said not

 to be LALR(1)



 The table is constructed as_follows If is the union of

 one or_more sets of LR(1)_items that is

 then the cores of

 are the same since

 all have the same_core Let be the union

 of all sets of items having the same_core as Then





 enumerate

 alg



 The table produced_by Algorithm 1st-lalr-alg is called the

 LALR_parsing table for If there are no parsing_action

 conflicts then the given grammar is said to be an LALR(1)

 grammar The collection of sets of items constructed in step (3)

 is called the LALR(1)_collection



 ex

 Again consider grammar_(lr1-gram-display) whose graph

 was shown in Fig lr1-items-fig As we mentioned there are

 three pairs of sets of items that can be merged and

 are replaced_by their_union



 center

 tabularl_l











 tabular

 center

 and are replaced_by their_union



 center

 tabularl_l





 tabular

 center

 and and are replaced_by their_union

 center

 tabularl_l



 tabular

 center

 The LALR action and goto functions for the condensed sets of items

 are shown in Fig_lalr-table-fig



 figurehtfb

 center

 tabularr_l l_l c_c r

 0pt0ptstate 3c

 2c



 2-7

 -2pt



 1pt0 s36_s47 1_2



 1 acc



 2 s36_s47 5



 36 s36_s47 89



 47 r3_r3 r3



 5 r1



 89 r2_r2 r2



 tabular

 center

 LALR_parsing table for the grammar of Example

 lr1-items-exlalr-table-fig

 figure



 To_see how the are computed consider

 In the original set of LR(1)_items

 and is now part of so we make )

 be We could have arrived at the same conclusion if we

 considered the other part of That is

 and is now part of For another

 example consider an entry that is exercised

 after the shift_action of on input In the original sets

 of LR(1)_items Since is now

 part of becomes Thus the

 entry in Fig_lalr-table-fig for state 2 and input is

 made s36 meaning shift and push_state 36 onto the stack

 ex



 When presented with a string from the language

 both

 the LR_parser of Fig_lr1-table-fig and the LALR_parser of

 Fig_lalr-table-fig make exactly the same sequence of shifts

 and reductions although the names of the states on the stack may

 differ For_instance if the LR_parser puts or on the stack

 the LALR_parser will put on the stack This relationship

 holds in general for an LALR grammar The LR and LALR_parsers will

 mimic one another on correct inputs



 When presented with erroneous_input the LALR_parser may

 proceed to do some reductions after the LR_parser has declared an

 error

 However the LALR_parser will never shift another symbol

 after the LR_parser declares an error For_example on input

 followed_by the LR_parser of Fig_lr1-table-fig

 will put

 center



 center

 on the stack and in state 4 will discover an error because

 is the next_input symbol and state 4 has action error on

 In_contrast the LALR_parser of Fig_lalr-table-fig will

 make the corresponding moves putting

 center



 center

 on the stack But state 47 on input has action reduce

 The LALR_parser will thus change its stack to

 center



 center

 Now the action of state 89 on input is reduce

 The stack becomes

 center



 center

 whereupon a similar reduction is called for obtaining stack

 center



 center

 Finally state 2 has action error on input so the error

 is now discovered



 Efficient Construction of LALR Parsing_Tables



 There_are several modifications we can make to Algorithm

 1st-lalr-alg to avoid constructing the full collection of

 sets of LR(1)_items in the process of creating an LALR(1) parsing

 table



 itemize



 First we can represent any set of

 LR(0) or LR(1)_items

 by its kernel that is by those items that are either

 the initial item - or

 - or that have

 the dot somewhere other_than at the beginning of the production_body



 We can construct the LALR(1)-item kernels from the LR(0)-item kernels by

 a process of propagation and spontaneous generation of lookaheads that

 we_shall describe shortly



 If we have the LALR(1) kernels we can generate the LALR(1) parsing

 table by closing each kernel using the function of

 Fig lr1-alg-fig and then computing table entries by

 Algorithm canonical-lr-alg as if the LALR(1) sets of items were

 canonical LR(1) sets of items



 itemize



 ex

 lalr-grammar-ex

 We_shall use as an example of the efficient LALR(1) table-construction

 method the non-SLR grammar from Example_not-slr-ex which we

 reproduce below in its augmented form



 center

 tabularr_c l

















 tabular

 center

 The complete sets of LR(0)_items for this grammar were shown in

 Fig not-slr-fig

 The kernels of these items are shown

 in Fig lr0-kernels-fig

 ex



 figurehtfb

 center

 tabularr_l p r_l









































 tabular

 center



 Kernels of the sets of LR(0)_items for grammar

 (lvalue-display)

 lr0-kernels-fig

 figure



 Now we must attach the proper lookaheads to the LR(0)_items in the

 kernels to create the kernels of the sets of LALR(1) items

 There_are two ways a lookahead can get attached to an LR(0)_item

 in some set of LALR(1) items



 enumerate



 There is a set of items with a kernel_item

 and and the

 construction of



 as given in Fig lr1-alg-fig contains

 regardless of Such a

 lookahead is said to be generated_spontaneously for



 As a special_case lookahead is generated_spontaneously for the

 item in the initial set of items



 All is as in (1) but and



 as given in Fig lr1-alg-fig contains

 only because

 has as one of its associated

 lookaheads In such a case we say that lookaheads_propagate

 from in the kernel of to

 in the kernel of Note_that

 propagation does_not depend_on the particular lookahead_symbol

 either all lookaheads_propagate from one item to another or none

 do



 enumerate



 We need to determine the spontaneously_generated lookaheads for

 each set of LR(0)_items and also to determine which items

 propagate lookaheads from which The test is actually quite

 simple Let be a symbol not in the grammar at hand Let

 be a kernel LR(0)_item in set

 Compute for each



 For each kernel_item in we examine its set of lookaheads If

 is a lookahead then lookaheads_propagate to that item from

 Any other lookahead is spontaneously

 generated These ideas are made precise in the following

 algorithm which also makes use of the fact that the only kernel

 items in must have immediately to the left of the dot

 that is they must_be of the form



 alg

 lookaheads-alg

 Determining lookaheads



 The kernel of a set of LR(0)_items and a grammar

 symbol



 The lookaheads spontaneously_generated by items in for

 kernel_items in and the items in from which

 lookaheads are propagated to kernel_items in



 The algorithm is given in Fig lookaheads-fig

 alg



 figurehtfb

 center

 tabularl

 for ( each item in )



 )



 if_( is in and

 is not )



 conclude that lookahead is generated_spontaneously for item



 in



 if_( is in )



 conclude that lookaheads_propagate from in to



 in







 tabular

 center



 Discovering propagated and spontaneous

 lookaheadslookaheads-fig

 figure



 We are now_ready to attach lookaheads to the kernels of the sets

 of LR(0)_items to form the sets of LALR(1) items First we know

 that is a lookahead for in the initial

 set of LR(0)_items Algorithm_lookaheads-alg gives_us all

 the lookaheads generated_spontaneously After listing all those

 lookaheads we must allow them to propagate until_no further

 propagation is possible There_are many different approaches all

 of which in some sense keep_track of new lookaheads that have

 propagated into an item but which have not_yet propagated out The

 next algorithm describes one technique to propagate lookaheads to

 all items



 alg

 propagate-alg

 Efficient computation of the kernels of the LALR(1)

 collection of sets of items



 An_augmented grammar



 The kernels of the LALR(1)_collection of sets of items for





 enumerate



 Construct the kernels of the sets

 of LR(0)_items for

 If space is not at a premium the simplest way is to construct the LR(0)

 sets of items as in Section items-subsect and then remove the

 nonkernel items

 If space is severely constrained we may wish instead to store only the

 kernel_items for each set and compute for a set of items by

 first computing the closure of



 Apply_Algorithm lookaheads-alg to the kernel of each set of

 LR(0)_items and grammar symbol to determine which lookaheads

 are spontaneously_generated for kernel_items in and

 from which items in lookaheads are propagated to kernel_items

 in



 Initialize a table that gives for each kernel_item in each set of

 items the associated lookaheads Initially each item has

 associated_with it only those lookaheads that we determined in step_(2)

 were generated_spontaneously



 Make repeated_passes over the kernel_items in all sets When we

 visit an item we look up the kernel_items to which

 propagates its lookaheads using information tabulated in step_(2) The

 current set of lookaheads for is added to those already

 associated_with each of the items to which propagates its

 lookaheads We continue making passes_over the kernel_items until

 no more new lookaheads are propagated

 enumerate

 alg



 ex

 lalr-items-ex Let_us construct the kernels of the LALR(1)

 items for the grammar of Example lalr-grammar-ex The

 kernels of the LR(0)_items were shown in

 Fig lr0-kernels-fig When we apply_Algorithm

 lookaheads-alg to the kernel of set of items we first

 compute which

 is



 center

 tabularl_l













 tabular

 center

 Among the items in the closure we see two where the lookahead has

 been generated_spontaneously

 The first of these is

 This item with to the right of the dot gives rise to

 That is is a spontaneously_generated

 lookahead for which is in set of items

 Similarly tells_us that is a spontaneously

 generated lookahead for in



 As is a lookahead for all six items in the closure we determine that

 the item in propagates lookaheads to the following

 six items



 center

 tabularl_l

 in in



 in in



 in in



 tabular

 center



 figurehtfb

 center

 tabularr_l r_l

 -3ptfrom to































































 tabular

 center

 Propagation of lookaheadspropagation-fig

 figure



 In Fig kernel-lookaheads-fig we show steps (3) and (4) of

 Algorithm_propagate-alg The column labeled

 initshows the spontaneously_generated lookaheads for

 each kernel_item These are only the two_occurrences of discussed

 earlier and the spontaneous lookahead for the initial item





 On the first pass the lookahead propagates

 from in to the six items listed in

 Fig propagation-fig The lookahead propagates from

 in to items

 in and

 in It also propagates to itself and to

 id in but these lookaheads

 are already present In the second and third passes the only new

 lookahead propagated is discovered for the successors of

 and on pass 2 and for the successor of on pass

 3 No new lookaheads are propagated on pass 4 so the final set of

 lookaheads is shown in the rightmost column of

 Fig kernel-lookaheads-fig



 figurehtfb

 center

 tabularr_l c_c c_c

 -7pt0pt0ptset 0pt0ptitem 4c0pt6ptlookaheads



 3-6

 -2pt init pass 1 pass 2 pass 3



 1pt





 2pt





 2pt









 2pt





 2pt





 2pt





 2pt





 2pt





 2pt





 2pt





 tabular

 center

 Computation of lookaheadskernel-lookaheads-fig

 figure



 Note_that the shiftreduce_conflict found in Example

 not-slr-ex using the SLR_method has disappeared with the

 LALR technique The_reason is that only lookahead is

 associated_with in so there is no

 conflict with the parsing_action of shift on generated_by

 item in

 ex



 Compaction of LR_Parsing Tables



 A_typical programming_language grammar with 50 to 100 terminals

 and 100 productions may have an LALR_parsing table with several

 hundred states The action function may easily have 20000

 entries each requiring at_least 8 bits to encode On small devices

 a more

 efficient encoding than a two-dimensional_array may be important

 We_shall mention briefly a few techniques that have_been used to

 compress the and fields of an LR_parsing table



 One useful technique for compacting the action field is to

 recognize that usually many rows of the action table are

 identical For_example in Fig_lr1-table-fig states 0 and

 3 have identical action entries and so do 2 and 6 We can

 therefore save considerable space at little cost in time if we

 create a pointer for each state into a one-dimensional array

 Pointers for states with the same actions point to the same

 location To access information from this array we assign each

 terminal a number from zero to one less_than the number of

 terminals and we use this integer as an offset from the pointer

 value for each state In a given state the parsing_action for the

 th terminal will be found locations past the pointer value

 for that state



 Further space efficiency can be achieved at the expense of a

 somewhat slower parser

 by creating a list for the actions of each

 state The list consists of (terminal-symbol action) pairs The

 most frequent action for a state can be placed_at the end of the

 list and in place of a terminal we may use the notation any meaning that if the current_input symbol has not been_found

 so_far on the list we should do that action no_matter what the

 input is Moreover error_entries can safely be replaced_by reduce

 actions for further uniformity along a row The errors will be

 detected later before a shift_move



 ex

 compaction-ex

 Consider the parsing_table of Fig_action-goto-fig First

 note_that the actions for states 0 4 6 and 7 agree We can

 represent them all by the list



 center

 tabularp_l

 symbol action



 id s5



 ( s4



 any error



 tabular

 center

 State 1 has a similar list



 center

 tabularp_l

 s6



 acc



 any error



 tabular

 center

 In state 2 we can replace the error_entries by r2 so reduction

 by production 2 will occur on any input but Thus the list

 for state 2 is



 center

 tabularp_l

 s7



 any r2



 tabular

 center



 State 3 has only error and r4 entries We can replace the former

 by the latter so the list for state 3 consists of only the pair

 (any r4) States 5 10 and 11 can be treated similarly

 The list for state 8 is



 center

 tabularp_l

 s6



 ) s11



 any error



 tabular

 center

 and for state 9



 center

 tabularp_l

 s7



 any r1



 tabular

 center

 ex



 We can also encode the table by a list but here it appears

 more_efficient to make a list of pairs for each nonterminal

 Each pair on the list for is of the form

 indicating

 center



 center

 This technique is useful because there tend to be rather few

 states in any one column of the table The_reason is that

 the on nonterminal can only be a state derivable_from a

 set of items in which some items have immediately to the left

 of a dot No set has items with and immediately to the

 left of a dot if Thus each state appears in at most

 one column



 For more space reduction we note_that the error_entries in the

 goto table are never consulted We can therefore replace each

 error entry by the most_common non-error entry in its column This

 entry becomes the default it is represented in the list for each

 column by one pair with any in place of



 exerror-entries-ex

 Consider Fig_action-goto-fig again The column for F

 has entry 10 for state_7 and all other entries are either 3 or

 error We may replace error by 3 and create for column F the

 list

 center

 tabularc_c

 Currentstate Nextstate



 to 20pt7 10



 to 20ptany 3



 tabular

 center

 Similarly a suitable list for column is



 center

 tabularp_l

 6 9



 any 2



 tabular

 center

 For column E we may choose either 1 or 8 to be the default

 two entries are necessary in either case For_example we might

 create for column E the list



 center

 tabularp_l

 4 8



 any 1



 tabular

 center

 ex



 This space savings in these small examples may be misleading

 because the total_number of entries in the lists created in this

 example and the previous one together_with the pointers from

 states to action lists and from nonterminals to next-state lists

 result in unimpressive space savings over the matrix

 implementation of Fig_action-goto-fig For practical

 grammars the space needed for the list representation is

 typically less_than ten percent of that needed for the matrix

 representation

 The table-compression methods for finite_automata that were

 discussed in Section ts-tradeoff-subsect can also be used to represent

 LR_parsing tables



 sexer

 first-lr-exer

 Construct the



 itemize



 a) canonical_LR and

 b) LALR



 itemize

 sets of items for

 the grammar of Exercise_cfg-exer

 sexer



 exer

 Repeat_Exercise first-lr-exer

 for each of the (augmented) grammars of

 Exercise more-cfgs-exer(a)-(g)

 exer



 hexer

 For the grammar of Exercise first-lr-exer

 use Algorithm_propagate-alg to compute the collection of LALR sets

 of items from the kernels of the LR(0) sets of items

 hexer



 hexer

 Show that the following grammar



 centertabularl_l l









 tabularcenter

 is LALR(1) but not SLR(1)



 hexer



 hexer

 Show that the following grammar



 centertabularl_l l













 tabularcenter

 is LR(1) but not LALR(1)



 hexer

 Processor Architectures

 secarch



 When we think of instruction-level_parallelism we usually imagine a

 processor issuing several operations in a single clock cycle In_fact it

 is possible for a machine to issue just one operation per clock(We

 shall_refer to a clock_tick or clock cycle simply as a clock

 when the intent is clear)

 and yet achieve instruction-level_parallelism using the concept of pipelining In the following we_shall first explain pipelining then

 discuss multiple-instruction issue



 Instruction Pipelines and Branch Delays



 Practically every processor be it a high-performance supercomputer or

 a standard machine uses an instruction pipeline With an

 instruction pipeline a new instruction can be fetched every clock

 while preceding instructions are still going_through the pipeline

 Shown in Fig figinstruction-pipeline is a simple 5-stage

 instruction pipeline it first fetches the instruction (IF) decodes

 it (ID) executes the operation (EX) accesses the memory (MEM) and

 writes back the result (WB) The figure shows_how instructions

 and can execute at the same

 time Each row corresponds to a clock_tick and each column in the

 figure specifies the stage each instruction occupies at each clock_tick



 figurehtb



 center

 tabularl_l l_l l_l









 1 IF



 2 ID IF



 3 EX ID IF



 4 MEM EX ID IF



 5 WB MEM EX ID IF



 6 WB MEM EX ID



 7 WB MEM EX



 8 WB MEM



 9 WB



 tabular

 center



 Five consecutive instructions in a 5-stage instruction pipeline

 figinstruction-pipeline

 figure



 If the result from an instruction is available by the time the

 succeeding instruction needs the data the processor can issue an

 instruction every clock Branch instructions are especially problematic

 because until they are fetched decoded and executed the processor

 does_not know which instruction will execute next Many processors

 speculatively fetch and decode the immediately succeeding instructions

 in case a branch is not taken When a branch is found to be taken

 the instruction pipeline is emptied and the branch target is fetched

 Thus taken branches introduce a delay in the fetch of the branch

 target and introduce hiccups in the instruction pipeline Advanced

 processors use hardware to predict the outcomes of branches based_on

 their execution history and to prefetch from the predicted target

 locations Branch delays are nonetheless observed if branches are

 mispredicted



 Pipelined Execution

 pipe-exec-subsect



 Some instructions take several clocks to execute One common example

 is the memory-load operation Even when a memory access hits in the

 cache it usually takes several clocks for the cache to

 return the data We_say that the execution of an instruction is

 pipelined if succeeding instructions not dependent on the result

 are allowed to proceed Thus even if a processor can issue only one

 operation per clock several operations might be in their execution

 stages at the same time If the deepest execution pipeline has

 stages potentially operations can be in flight at the same time

 Note_that not all instructions are fully pipelined While

 floating-point adds and multiplies often are fully pipelined

 floating-point divides being more_complex and less frequently

 executed often are not



 Most general-purpose processors dynamically detect dependences_between

 consecutive instructions and automatically stall the execution of

 instructions if their operands are not available Some processors

 especially those embedded in hand-held devices leave the dependence

 checking to the software in order to keep the hardware simple and power

 consumption low In this case the compiler is responsible_for

 inserting no-op instructions in the

 code if necessary to assure that the results are available when needed



 Multiple Instruction Issue



 By issuing several operations per clock processors can keep even

 more operations in flight The largest_number of operations that can

 be executed simultaneously can be computed by multiplying the

 instruction issue width by the average_number of stages in the

 execution pipeline



 Like pipelining parallelism on multiple-issue machines can be

 managed either by software or hardware Machines that rely_on software to

 manage their parallelism are known_as VLIW

 (Very-Long-Instruction-Word) machines while those that manage their

 parallelism with hardware are known_as superscalar machines

 VLIW machines as their name implies have wider than normal instruction

 words that encode the operations to be issued in a single clock The

 compiler decides which operations are to be issued in parallel and

 encodes the information in the machine code explicitly Superscalar

 machines on the other_hand have a regular instruction set with an

 ordinary sequential-execution semantics Superscalar machines

 automatically detect dependences among instructions and issue

 them as their operands become available

 Some processors include both VLIW and superscalar functionality



 Simple hardware schedulers execute instructions in the order in which

 they are

 fetched If a scheduler comes across a dependent instruction it and

 all instructions that follow must_wait until the dependences are

 resolved (ie the needed results are

 available) Such machines obviously can benefit from having a static

 scheduler that places independent operations next to each other in the

 order of execution



 More sophisticated schedulers can execute instructions out of

 order Operations are independently stalled and not allowed to

 execute until all the values they depend_on have_been produced Even

 these schedulers benefit from static scheduling because

 hardware schedulers

 have only a limited space in which to buffer operations that must_be

 stalled Static scheduling can place independent operations close

 together to allow better hardware utilization More_importantly

 regardless how sophisticated a dynamic_scheduler is it cannot

 execute instructions it has not fetched When the processor has to

 take an unexpected branch it can only find parallelism among the

 newly fetched instructions The compiler can enhance the performance

 of the dynamic_scheduler by ensuring that these newly fetched

 instructions can execute in parallel

 theoremTheorem

 algorithmtheoremAlgorithm





 Input

 Output

 Method

 Main

 main-java-sect



 Execution begins in method main in class Main Method

 main creates a lexical_analyzer and a parser and then calls

 method program in the parser



 footnotesize

 flushleft

 1)_package main File Mainjava



 2)_import javaio_import lexer_import parser



 3)_public class Main



 4)_public static void main(String args) throws_IOException



 5) Lexer lex new Lexer()



 6) Parser parse new Parser(lex)



 7) parseprogram()



 8) Systemoutwrite('9)



 10)



 flushleft

 footnotesize

 Matrix Multiply An In-Depth Example

 ch11mm



 We_shall introduce many of the techniques used by parallel compilers in

 an extended example

 In this_section we explore the

 familiar matrix-multiplication_algorithm to show that it is

 nontrivial to optimize even a simple and easily parallelizable

 program

 We_shall see_how rewriting the code can improve data_locality that is

 processors are able to do their work with far less communication (with

 global memory or with other processors depending_on the architecture)

 than if the straightforward program is chosen

 We_shall also discuss_how cognizance of the existence of cache_lines

 that hold several consecutive data elements can improve the running_time

 of programs such_as matrix_multiplication



 The Matrix-Multiplication Algorithm



 In Fig_mm-basic-fig we see a typical matrix-multiplication

 program(In programs

 of this_chapter we_shall generally use C

 syntax but to make multidimensional

 array_accesses - the central issue for most of the

 chapter - easier to read we_shall use Fortran-style array

 references that is instead of )

 It takes two matrices and and produces

 their product in a third matrix

 Recall that - the element of matrix in row and column

 - must become



 figurehtfb



 verbatim

 for_(i 0 i_n i)

 for_(j 0 j_n j)

 Zij 00

 for (k 0 k n k)

 Zij Zij XikYkj



 verbatim



 The basic matrix-multiplication_algorithm

 mm-basic-fig



 figure



 The code of Fig_mm-basic-fig

 generates results each of which is an inner

 product between one row and one column of the two matrix operands

 Clearly the calculations of each of the elements of are independent

 and can be executed in parallel



 The larger is the more times the algorithm touches each element

 That is there are

 locations among the three

 matrices but the algorithm performs operations each of which

 multiplies an element of by an element of and adds the product

 to an element of Thus

 the algorithm is computation-intensive and memory accesses should not

 in principle constitute a bottleneck



 Serial Execution of the Matrix Multiplication

 serial-mm-subsect



 Let_us first consider how this program behaves when run sequentially

 on a uniprocessor The innermost_loop reads and writes the same

 element of and uses a row of and a column of

 can easily be stored in a register and requires no memory accesses

 Assume without loss of generality that the matrix is laid_out in

 row-major_order

 and that is the number of array_elements in a cache_line



 figurehtfb

 fileuullmanalsuch11figsmmeps

 The data access pattern in matrix multiply

 figmm

 figure



 Figure figmm suggests the access pattern as we execute one

 iteration of the outer_loop of

 Fig_mm-basic-fig In_particular the picture shows the first

 iteration with Each time we move from one element of the

 first row of to the next we visit each element in a single column

 of We see in Fig figmm the assumed organization of the

 matrices into cache_lines That is each small rectangle represents a

 cache_line holding four array_elements (ie and in the picture)



 Accessing puts little burden on the cache

 One row of is spread among only cache_lines Assuming these

 all fit in the cache only cache_misses occur for a fixed value of

 index and the total

 number of misses for all of is the minimum possible (we

 assume is divisible by for convenience)



 However while using one row of the matrix-multiplication_algorithm

 accesses all the elements of column by column

 That is when the inner_loop brings to the cache the entire first

 column of Notice_that the elements of that column are stored among

 different cache_lines

 If the cache is big enough (or small enough) to hold cache_lines

 and no other uses of the cache force some of these cache_lines to be

 expelled then the column for will still be in the cache when we

 need the second column of In that case there will not be another

 cache_misses reading until at which time we need to

 bring into the cache an entirely different set of cache_lines for

 Thus to complete the first iteration of the outer_loop (with )

 requires between and cache_misses depending_on whether

 columns of cache_lines can survive from one iteration of the second loop

 to the next



 Moreover as we complete the outer_loop for and so on we may

 have many additional cache_misses as we read or none at all If

 the cache is big enough that all cache_lines holding can

 reside together in the cache then we need no more cache_misses The

 total_number of cache_misses is thus half for and half for

 However if the cache can hold one column of but not all of

 then we need to bring all of into cache again each time we

 perform an iteration of the outer_loop That is the number of cache

 misses is the first term is for and the second is for

 Worst if we cannot even hold one column of in the cache then

 we have cache_misses per iteration of the outer_loop and a total of

 cache_misses



 Row-by-Row Parallelization

 r-r-par-subsect



 Now let_us consider how we could use some number of processors say

 processors to speed_up the execution of Fig_mm-basic-fig

 An obvious approach to parallelizing matrix_multiplication is to

 assign different rows of to different_processors A processor is

 responsible_for consecutive rows

 (we assume is divisible by for convenience)

 With this division of labor each processor needs to

 access rows of matrices and but the entire matrix

 One processor will compute elements of performing

 multiply-and-add operations to do so



 While the computation time thus decreases in proportion to

 the communication cost actually rises in proportion to

 That is each of processors has to read elements of but

 all elements of

 The total_number of cache_lines that must_be delivered to the caches of

 the processors is at_least the two terms are for delivering

 and copies of respectively

 As approaches the computation time becomes while the

 communication cost is That is the bus on which data is moved

 between memory and the processors' caches becomes the bottleneck

 Thus with the proposed data layout

 using a large_number of processors to share the computation can actually

 slow down the computation rather_than speed it up



 Optimizations

 loop-reorder-subsect



 The matrix-multiplication_algorithm of Fig_mm-basic-fig shows

 that even_though an algorithm may reuse the same data it may

 have poor data_locality A reuse of data results in a cache hit

 only if the reuse happens soon enough before the data is displaced

 from the cache In this case multiply-add operations separate

 the reuse of the same data element in matrix so locality is poor

 In_fact operations separate the reuse of the same cache_line in

 In_addition on a multiprocessor reuse may result in a cache

 hit only if the data is reused by the same processor When we

 considered a parallel implementation in Section r-r-par-subsect

 we saw that elements of had to be used by every processor

 Thus the reuse of is not turned_into locality



 Changing Data Layout

 One_way to improve the locality of a program is to change the

 layout of its data_structures For_example storing in column-major

 order would have improved the reuse of cache_lines for matrix

 The applicability of this approach is limited because the same matrix

 normally is used in different operations

 If played the role of in another matrix_multiplication then it

 would suffer from being stored in column-major_order since the first

 matrix in a multiplication is better stored in row-major_order



 Blocking



 It is sometimes possible

 to change the execution order of the instructions to improve data

 locality The technique of interchanging loops however does_not

 improve the matrix-multiplication routine Suppose the routine were

 written to generate a column of matrix at a time instead of a row

 at a time That is make the -loop the outer_loop and the -loop

 the second loop Assuming matrices are still stored in row-major

 order matrix enjoys better spatial and temporal_locality but

 only at the expense of matrix



 Blocking is another_way of reordering iterations in a loop that

 can greatly improve the locality of a program Instead of computing

 the result a row or a column at a time we divide the matrix up into

 submatrices or blocks as suggested by

 Fig block-matrix-fig and we order operations so an entire

 block is used over a short period of time Typically the blocks are

 squares with a side of length If evenly_divides then

 all the blocks are square If does_not evenly divide then

 the blocks on the lower and right edges will have one or both_sides of

 length less_than



 figurehtfb



 fileuullmanalsuch11figsblock-matrixeps

 A matrix divided_into blocks of side

 block-matrix-fig

 figure



 Figure mm-block-fig shows a version of the basic

 matrix-multiplication_algorithm where all three matrices have_been

 blocked into squares of side

 As in Fig_mm-basic-fig is assumed to have_been initialized

 to all 0's

 We assume that divides if not then we need to modify line_(4)

 so the upper limit is and similarly for lines (5) and

 (6)



 figurehtfb



 verbatim

 1) for (ii 0 ii n ii iiB)

 2) for (jj 0 jj n jj jjB)

 3) for (kk 0 kk n kk kkB)

 4) for_(i ii_i iiB i)

 5) for_(j jj j jjB j)

 6) for (k kk k kkB k)

 7) Zij Zij XikYkj

 verbatim



 Matrix multiplication with blocking

 mm-block-fig



 figure



 The outer three loops lines_(1) through (3) use indexes

 and which are always incremented by and therefore always mark

 the left or upper edge of some blocks With fixed values of

 and lines_(4) through_(7) enable the blocks with upper-left

 corners and to make all possible contributions to

 the block with upper-left corner



 Another View of Block-Based Matrix Multiplication

 We can imagine that the matrices and of

 Fig mm-block-fig are not matrices of floating-point

 numbers but

 rather matrices whose elements are themselves

 matrices of floating-point_numbers

 Lines (1) through (3) of Fig mm-block-fig are then like the three

 loops of the basic algorithm in Fig_mm-basic-fig but with

 as the size of the matrices rather_than

 We can then think of lines_(4) through_(7) of Fig mm-block-fig as

 implementing a single multiply-and-add operation of Fig_mm-basic-fig

 Notice_that in this operation the single multiply step is a

 matrix-multiply step and it uses the basic algorithm of

 Fig_mm-basic-fig on the floating-point_numbers

 that are elements of the two

 matrices involved The matrix addition is element-wise addition

 of floating-point_numbers



 If we pick properly we can significantly decrease the number of

 cache_misses compared with the basic algorithm when all of

 or cannot fit in the cache Choose such that it is possible to

 fit one block from each of the matrices in the cache Because of the

 order of the loops we actually need each block of in cache only once

 so (as in the analysis of the basic algorithm

 in Section serial-mm-subsect)

 we_shall not count the cache_misses due to



 To bring a block of or to the cache takes cache

 misses recall is the number of elements in a cache_line

 However with fixed blocks from and we perform

 multiply-and-add operations in lines_(4) through_(7) of

 Fig mm-block-fig Since the entire matrix-multiplication

 requires multiply-and-add operations the number of times we need

 to bring a pair of blocks to the cache is As we require

 cache_misses each time we do the total_number of cache

 misses is



 It is interesting to compare this figure with the estimates

 given in Section serial-mm-subsect There we said that if entire

 matrices can fit in the cache then cache_misses suffice

 However in that case we can pick ie make each matrix be a

 single block We again get as our estimate of cache_misses

 On the other_hand we observed that if entire matrices will not fit in

 cache we require cache_misses or even cache

 misses In that case assuming that we can still pick a significantly

 large (eg could be 200 and we could still fit three blocks

 of 8-byte numbers in a one-megabyte cache) there is a great advantage

 to using blocking in matrix_multiplication



 The blocking technique

 can be reapplied for each level of the memory hierarchy For

 example we may wish to optimize register usage by holding the

 operands of a matrix_multiplication in registers We

 choose successively bigger block sizes for the different_levels of

 caches and physical_memory



 Similarly we can distribute blocks

 between processors to minimize data_traffic Experiments showed that

 such optimizations can improve the performance of a

 uniprocessor by a factor of 3 and the speed_up on a

 multiprocessor is close to linear with_respect to the number of

 processors used



 Cache Interference



 Unfortunately there is somewhat more to the story of cache

 utilization

 Most caches are not

 fully associative (see_Section secmemoryhierarchy)

 In a direct-mapped cache if is a

 multiple of the cache size then all the elements in the same column of

 an array will be competing for the same cache location

 In that case bringing in the second element of a column will throw

 away the cache_line of the first even_though the cache has the

 capacity to keep both of these lines at the same time

 This situation is referred to as cache interference



 There_are

 various solutions to this problem The first is to rearrange the data

 once and for all so that the data_accessed is laid_out in consecutive

 data locations The second is to

 embed the array in a larger

 array where is chosen to minimize the interference problem

 Third in some_cases we can choose a block size that is guaranteed to avoid

 interference



 exer

 The block-based matrix-multiplication_algorithm of

 Fig mm-block-fig does_not have the initialization of the matrix

 to zero as the code of Fig_mm-basic-fig does Add the steps

 that initialize to all zeros in Fig mm-block-fig

 exer

 OUT

 IN

 OUT

 IN

 MFP

 MOP

 IDEAL



 Machine-Independent Optimizations



 As_discussed in Chapter ch1 high-level_language constructs can

 introduce substantial run-time overhead if we naively translate each

 construct independently into machine code This_chapter discusses_how

 to eliminate many of these redundancies

 Elimination of unnecessary instructions in object code or the replacement of

 one sequence of instructions by faster sequences of instructions that do

 the same_thing is usually called code_improvement or code

 optimization

 We begin in Section_secopt-sources with a discussion of the

 principal opportunities for code_improvement

 We use the terms

 local code_optimization to refer to

 transformations applied to individual basic_blocks and

 global code_optimization to refer to

 improvements that take into_account what_happens across basic_blocks





 The exploitation of these opportunities in local code_optimization

 (within a basic block) is in Section secbb







 The balance of the chapter_deals with global code_optimization

 where improvements take into_account what_happens in several basic

 blocks

 Most_global optimizations are based_on

 data-flow_analyses which are processes to gather approximate

 information_about what a program does

 Typically data-flow_analysis examines the intermediate_code but does

 not execute the code

 Rather we classify all possible_executions of the program into a

 limited collection of classes

 The simplest form of data-flow_analysis groups (incomplete)

 executions according to

 the location of the intermediate-code step most_recently executed

 More complex forms also take into_account such information as the

 contents of the run-time_stack





 The balance of the chapter_deals with global code_optimization

 Most_global optimizations are based_on data-flow

 analyses which are algorithms to gather approximate information

 about what a program does The results of data flow analyses all have

 the same form namely for each instruction in the program they

 specify some property that is held every time that instruction is

 executed The analyses differ in the properties they compute For

 example a constant_propagation analysis computes for each

 instruction in the program if variables hold the same constant value

 after executing the instruction This information may be used to

 replace variable references with constant values instead

 A liveness_analysis determines for

 each instruction if the values held by variables after executing that

 instruction are always overwritten_before they are read If so we do

 not need to keep these values in registers



 We introduce data-flow_analysis in Section_secdf-intro including

 several important_examples of the kind of information we gather globally

 and then use to improve the code

 Section secdf-foundation introduces the general_idea of a

 data-flow_framework of which the data-flow_analyses that we discuss are

 special_cases

 We can use the same algorithms for all these instances of data-flow

 analysis and we can

 measure the performance of these algorithms and show their correctness

 on all instances as_well

 Section_const-prop-sect is an example of the general framework

 that does more_powerful analysis than the previous examples

 Then in Section_secpre we consider a powerful technique called

 partial_redundancy elimination for

 optimizing the placement of each expression evaluation in the program

 The solution to this problem requires the solution of

 a variety of different data-flow_problems

 In Section loops-sect we take_up the discovery and analysis of

 loops in programs





 The identification of loops leads to another family of algorithms for

 solving_data-flow problems that is based_on the hierarchical_structure

 of the loops of a well formed (reducible) program

 This_approach to data-flow_analysis is covered in

 Section_region-sect

 Finally Section secinduction uses hierarchical analysis to

 eliminate

 induction_variables (essentially variables that count the number of

 times_around a loop)

 This code_improvement is one of the most_important we can make for

 programs_written in commonly_used programming_languages























 OUT

 IN

 OUT

 IN

 MFP

 MOP

 IDEAL



 Machine-Independent Optimizations



 As_discussed in Chapter ch1 high-level_language constructs can

 introduce substantial run-time overhead if we naively translate each

 construct independently into machine code This_chapter discusses_how

 to eliminate many of these redundancies

 Elimination of unnecessary instructions in object code or the replacement of

 one sequence of instructions by faster sequences of instructions that do

 the same_thing is usually called code_improvement or code

 optimization

 We begin in Section_secopt-sources with a discussion of the

 principal opportunities for code_improvement

 We use the terms

 local code_optimization to refer to

 transformations applied to individual basic_blocks and

 global code_optimization to refer to

 improvements that take into_account what_happens across basic_blocks





 The exploitation of these opportunities in local code_optimization

 (within a basic block) is in Section secbb







 The balance of the chapter_deals with global code_optimization

 where improvements take into_account what_happens in several basic

 blocks

 Most_global optimizations are based_on

 data-flow_analyses which are processes to gather approximate

 information_about what a program does

 Typically data-flow_analysis examines the intermediate_code but does

 not execute the code

 Rather we classify all possible_executions of the program into a

 limited collection of classes

 The simplest form of data-flow_analysis groups (incomplete)

 executions according to

 the location of the intermediate-code step most_recently executed

 More complex forms also take into_account such information as the

 contents of the run-time_stack





 The balance of the chapter_deals with global code_optimization

 Most_global optimizations are based_on data-flow

 analyses which are algorithms to gather approximate information

 about what a program does The results of data flow analyses all have

 the same form namely for each instruction in the program they

 specify some property that is held every time that instruction is

 executed The analyses differ in the properties they compute For

 example a constant_propagation analysis computes for each

 instruction in the program if variables hold the same constant value

 after executing the instruction This information may be used to

 replace variable references with constant values instead

 A liveness_analysis determines for

 each instruction if the values held by variables after executing that

 instruction are always overwritten_before they are read If so we do

 not need to keep these values in registers



 We introduce data-flow_analysis in Section_secdf-intro including

 several important_examples of the kind of information we gather globally

 and then use to improve the code

 Section secdf-foundation introduces the general_idea of a

 data-flow_framework of which the data-flow_analyses that we discuss are

 special_cases

 We can use the same algorithms for all these instances of data-flow

 analysis and we can

 measure the performance of these algorithms and show their correctness

 on all instances as_well

 Section_const-prop-sect is an example of the general framework

 that does more_powerful analysis than the previous examples

 Then in Section_secpre we consider a powerful technique called

 partial_redundancy elimination for

 optimizing the placement of each expression evaluation in the program

 The solution to this problem requires the solution of

 a variety of different data-flow_problems



 In Section loops-sect we take_up the discovery and analysis of

 loops in programs

 The identification of loops leads to another family of algorithms for

 solving_data-flow problems that is based_on the hierarchical_structure

 of the loops of a well formed (reducible) program

 This_approach to data-flow_analysis is covered in

 Section_region-sect

 Finally Section secinduction uses hierarchical analysis to

 eliminate

 induction_variables (essentially variables that count the number of

 times_around a loop)

 This code_improvement is one of the most_important we can make for

 programs_written in commonly_used programming_languages





















 The early development of context-free_grammars was summarized in

 the bibliographic notes for Chapter simple-ch

 The phenomenon of ambiguity was observed first by Cantor can62

 and Floyd floyd62

 Chomsky Normal Form (Exercise chomsky-exer) is from

 chomsky59

 The theory of context-free_grammars is summarized in hmu01



 One of the earliest techniques for building parsers involved the use of

 the precedence of operators by Floyd floyd63 The idea was

 generalized to parts of the language that do_not involve operators by

 Wirth and Weber ww66 These techniques are rarely used today

 but can be seen as leading in a chain of improvements to LR_parsing



 Recursive-descent parsing was the method of choice for early compiler-writing

 systems such_as META schorre64 and TMG mcclure65

 LL_grammars were_introduced by Lewis and Stearns ls68

 Exercise birman-exer the linear-time simulation of

 recursive-descent is from bu73



 LR_parsers were_introduced by Knuth knuth65 and the

 canonical-LR parsing_tables originated there This_approach was not

 considered practical because of the size of the parsing_tables (which

 were larger_than the main memories of typical computers of the day)

 until Korenjak korenjak69 gave a method for producing

 reasonably sized parsing_tables for typical programming_languages

 DeRemer developed the LALR deremer69 and SLR deremer71

 methods that are in use today The construction of LR_parsing tables

 for ambiguous_grammars came from aju75 and earley75



 Johnson's Yacc very quickly demonstrated the practicality

 of generating parsers with an LALR_parser generator for

 production compilers

 The manual for the Yacc parser_generator is found in

 yacc The open-source version Bison is described in

 bison A similar LALR-based parser_generator called CUP

 cup supports actions written in Java

 There_are also top-down parser_generators available today Antlr

 antlr

 is a recursive-descent_parser generator that accepts actions in C

 Java or C

 LLGen llgen is an LL(1)-based generator



 dain91 is a bibliography of error correction associated_with

 various types of parsers



 The general-purpose dynamic-programming parsing algorithm described in

 Exercise cyk-exer was invented independently by J Cocke

 (unpublished) by Younger younger67 and Kasami kasami65

 hence the CYK algorithm There is a more_complex general-purpose

 algorithm due to Earley earley70 that tabulates LR-items for

 each substring of the given input this algorithm while also

 in general is only on unambiguous_grammars



 enumerate



 aju75

 Aho_A V S_C Johnson and J_D Ullman

 Deterministic parsing of ambiguous_grammars

 Comm_ACM 188 (Aug 1975) pp 441-452



 bu73

 Birman A and J_D Ullman

 Parsing algorithms with backtrack

 Information and Control 231 (1973)_pp 1-34



 can62

 Cantor D C On the ambiguity problem of Backus systems

 J_ACM 94 (1962) pp 477-479



 chomsky59

 Chomsky N

 On certain formal properties of grammars

 Information and Control 22 (1959) pp 137-167



 dain91

 Dain J

 Bibliography on Syntax Error Handling in Language Translation

 Systems 1991 Available on the Web as

 httpcompilersiecc comcomparcharticle91-04-050



 deremer69

 DeRemer F

 Practical Translators for LR() Languages

 PhD thesis MIT Cambridge MA 1969



 deremer71

 DeRemer F

 Simple LR() grammars

 Comm_ACM 147 (July 1971) pp 453-460



 bison

 Donnelly C and R Stallman

 Bison The YACC-compatible Parser_Generator httpwwwgnuorgmanualbisonhtmlmonobison html



 earley70

 Earley J

 An efficient context-free parsing algorithm

 Comm_ACM 132 (Feb 1970) pp 94-102



 earley75

 Earley J Ambiguity and precedence in syntax description

 Acta Informatica 42 (1975) pp 183-192



 floyd62

 Floyd R W On ambiguity in phrase-structure languages

 Comm_ACM 510 (Oct 1962) pp 526-534



 floyd63

 Floyd R W

 Syntactic analysis and operator precedence

 J_ACM 103 (1963) pp 316-333



 llgen

 Grune D and CJH Jacobs A programmer-friendly LL(1) parser

 generator Software Practice and Experience 181 (Jan

 1988) pp 29-38 See also httpwwwcsvunl126cerielLLgenhtml



 hmu01

 Hopcroft J E R Motwani and J_D Ullman Introduction

 to Automata Theory Languages and Computation Addison-Wesley

 Boston MA 2001



 cup

 Hudson S E et_al CUP LALR Parser_Generator for Java

 http wwwcsprincetonedu126appelmodernjavaCUP



 yacc

 Johnson S_C

 Yacc - Yet Another Compiler Compiler

 Computing Science Technical Report 32 Bell_Laboratories Murray_Hill

 NJ 1975 Available on the Web at httpdinosaurcompilertoolsnetyacc



 kasami65

 Kasami T

 An efficient recognition and syntax analysis algorithm for

 context-free languages AFCRL-65-758 Air Force Cambridge Research

 Laboratory Bedford MA 1965



 knuth65

 Knuth D E

 On the translation of languages from left to right

 Information and Control 86 (1965) pp 607-639



 korenjak69

 Korenjak A J

 A practical method for constructing LR() processors

 Comm_ACM 1211 (Nov 1969) pp 613-623



 ls68

 Lewis P M II and R E Stearns

 syntax-directed transduction

 J_ACM 153 (1968) pp 465-488



 mcclure65

 McClure R_M

 TMG - a syntax-directed compiler

 proc 20th ACM Natl Conf (1965) pp 262-274



 antlr

 Parr T ANTLR httpwwwantlrorg



 schorre64

 Schorre D V

 Meta-II a syntax-oriented compiler writing language

 Proc 19th ACM Natl Conf (1964) pp D13-1-D13-11



 ww66

 Wirth N and H Weber

 Euler a generalization of Algol and its formal definition Part I

 Comm_ACM 91 (Jan 1966) pp 13-23



 younger67

 Younger DH

 Recognition and parsing of context-free languages in time

 Information and Control 102 (1967) pp 189-208



 enumerate

 OUT

 IN

 OUT

 IN

 MFP

 MOP

 IDEAL

 1

 1

 1



 Machine-independent Optimizations

 code-op-ch



 As_discussed in Chapter chapter1

 high-level_language constructs can

 introduce substantial run-time overhead if we naively translate each

 construct independently into machine code This_chapter discusses_how

 to eliminate many of these inefficiencies

 Elimination of unnecessary instructions in object code or the replacement of

 one sequence of instructions by a faster sequence of instructions that does

 the same_thing is usually called code_improvement or code

 optimization



 Local code_optimization

 (code improvement

 within a basic block) was_introduced in Section secbb

 This_chapter deals_with global code_optimization

 where improvements take into_account what_happens across basic

 blocks

 We begin in Section_secopt-sources with a discussion of the

 principal opportunities for code_improvement



 Most_global optimizations are based_on data-flow

 analyses which are algorithms to gather approximate information

 about what a program does The results of data-flow_analyses all have

 the same form for each instruction in the program they

 specify some property that is held every time that instruction is

 executed The analyses differ in the properties they compute For

 example a constant-propagation analysis computes for each

 point in the program and for each variable used by the program whether

 that variable has a unique constant value at that point

 This information may be used to

 replace variable references by constant values for instance

 As_another example a liveness_analysis determines for

 each point whether the value held by a particular

 variable at that point is sure to be

 overwritten_before it is read If so we do

 not need to preserve that value either in a register or in a memory

 location



 We introduce data-flow_analysis in Section_secdf-intro including

 several important_examples of the kind of information we gather globally

 and then use to improve the code

 Section secdf-foundation introduces the general_idea of a

 data-flow_framework of which the data-flow_analyses in

 Section_secdf-intro are

 special_cases

 We can use essentially

 the same algorithms for all these instances of data-flow

 analysis and we can

 measure the performance of these algorithms and show their correctness

 on all instances as_well

 Section_const-prop-sect is an example of the general framework

 that does more_powerful analysis than the earlier examples

 Then in Section_secpre we consider a powerful technique called

 partial_redundancy elimination for

 optimizing the placement of each expression evaluation in the program

 The solution to this problem requires the solution of

 a variety of different data-flow_problems



 In Section loops-sect we take_up the discovery and analysis of

 loops in programs

 The identification of loops leads to another family of algorithms for

 solving_data-flow problems that is based_on the hierarchical_structure

 of the loops of a well-formed (reducible) program

 This_approach to data-flow_analysis is covered in

 Section_region-sect

 Finally Section secinduction uses hierarchical analysis to

 eliminate

 induction_variables (essentially variables that count the number of

 times_around a loop)

 This code_improvement is one of the most_important we can make for

 programs_written in commonly_used programming_languages





















 Constant Propagation



 All the data-flow_schemas discussed in Section are

 actually simple examples of distributive frameworks with finite_height

 Thus the iterative Algorithm applies to them in

 either its forward or backward version and produces the solution

 in each case

 In this_section we_shall examine in detail a useful data-flow

 framework with more interesting properties



 Recall that constant_propagation or constant_folding

 replaces expressions that

 evaluate to the same constant every time executed by that constant

 The constant-propagation framework described below

 is different from all the data-flow

 problems discussed so_far in that it







 a)

 Has an_unbounded set of possible

 data-flow_values even for a fixed flow_graph and



 b)

 Is not_distributive





 Constant propagation is a forward_data-flow problem The semilattice

 representing the data-flow_values and the family of transfer_functions

 are presented next



 Data-Flow Values for the Constant-PropagationFramework



 The set of data-flow_values is a product_lattice with one component

 for

 each variable in a program

 The lattice for a single variable consists of the following







 All constants appropriate for the type of the variable



 The value which stands_for not-a-constant A variable is

 mapped to this value if it is determined not to have a constant

 value The variable may have_been assigned an input value or derived_from a

 variable that is not a constant or assigned different constants along

 different paths that lead to the same program point



 The value which stands_for undefined

 A variable is assigned this value if nothing may yet be asserted

 presumably no definition of the variable has

 been discovered to reach the point in question





 Note_that and are not the same they are essentially

 opposites

 says we have_seen so many_ways a variable could be defined

 that we know it is not constant

 says we have_seen so little about the variable that we

 cannot say anything at all



 The semilattice for a typical integer-valued variable is shown in

 Fig Here the top_element is

 and the bottom_element is That is the greatest

 value in the partial_order is and the least is



 The constant

 values are unordered but they are all

 less_than and greater_than

 As_discussed in Section the

 meet of two values is their greatest_lower bound Thus

 for all values







 For any constant





 and given two distinct constants and







 figureuullmanalsuch9figsconstant-latticeeps

 Semilattice representing the possible values of a single

 integer variable



 A data-flow value for this framework

 is a map from each variable in the program to one of

 the values in the constant semilattice The value of a variable

 in a map is denoted_by



 The Meet for the Constant-Propagation Framework



 The semilattice of

 data-flow_values

 is simply the product of the semilattices like

 Fig one

 for each variable Thus if and only if

 for all variables we have



 Put_another way if for

 all variables



 Transfer_Functions for the Constant-PropagationFramework



 We assume in the following that a basic_block contains only one

 statement Transfer_functions for basic_blocks containing

 several statements can be constructed by_composing the functions

 corresponding to individual_statements

 The set consists of certain

 transfer_functions that accept a map of variables to values in the

 constant lattice

 and return another such map



 contains the identity_function which takes a map as input and

 returns the same map

 also contains

 the constant transfer_function for the entry_node

 This transfer_function given any input map

 returns a map where for all variables

 This boundary_condition makes_sense because before executing any

 program statements there are no definitions for any variables



 In_general

 let be the transfer_function of statement and let and

 represent data-flow_values

 such that

 We_shall describe in terms of the relationship_between and









 If is not an assignment_statement then is simply the

 identity_function



 If is an assignment to variable then

 for all variables





 If the right-hand-side (RHS) of the statement is a constant then





 If the RHS is of the form thenAs usual

 represents a generic_operator not_necessarily addition





















 If the RHS is any other expression (eg a function call or assignment

 through a pointer) then







 Monotonicity of the Constant-Propagation Framework



 Let_us show that the constant_propagation framework is monotone First

 we can consider the effect of a function on a single variable

 In all

 but case 2(b) either does_not change the value of or

 it changes the map to return a constant In these cases

 must surely be

 monotone



 For case 2(b) the effect of is tabulated in

 Fig The first and second columns represent

 the possible input values of and the last

 represents the output value of The values are

 ordered from the greatest to the smallest in each column or subcolumn

 To show that the function is monotone we check that for each

 possible input value of the value of

 does_not get bigger as the value of gets smaller

 For_example in

 the case_where

 has a constant value as the value of varies from

 to to the value of varies from

 to and then to respectively

 We can repeat this procedure for all the possible values of

 Because of symmetry we do_not even need to repeat the procedure for

 the second operand before we conclude that the output value cannot get

 larger as the input gets smaller











 The constant-propagation transfer_function for x_yz





 (Nondistributivity of the Constant-PropagationFramework



 The constant-propagation framework as defined is monotone but not

 distributive That is the iterative solution

 is safe but may be smaller_than the

 solution An_example will prove that the

 framework is not_distributive





 figureuullmanalsuch9figsnon-distributiveeps

 An_example demonstrating that the constant_propagation

 framework is not_distributive





 In the program in Fig and

 are set to 2 and 3 in block and to 3 and 2 respectively in

 block We know that regardless of which path is taken the

 value of at the end of block is 5 The iterative

 algorithm does_not discover this fact

 however Rather it applies

 the meet_operator at the entry of getting as the values

 of and Since adding two yields a the

 output produced_by Algorithm is that at the exit

 of the program This result is safe but imprecise

 Algorithm is

 imprecise because it does_not keep_track of the correlation that

 whenever is 2 is 3 and vice-versa

 It is possible but significantly more_expensive to use a more_complex

 framework that tracks all the possible equalities that hold among

 pairs of expressions_involving the variables in the program this

 approach is discussed in Exercise



 Theoretically we can attribute this loss of precision to the

 nondistributivity of the constant_propagation framework

 Let and be the transfer_functions representing blocks

 and respectively

 As shown in Fig









 rendering the framework nondistributive









 Example of nondistributive transfer_functions



 Interpretation of the Results



 The value is used in the iterative_algorithm for two purposes

 to initialize the entry_node and to initialize

 the interior_points of the program before the iterations The meaning

 is slightly_different in the two cases The first says_that variables

 are undefined at the beginning of the program execution the second says_that

 for lack of information at the beginning of the iterative process we

 approximate the solution with the top_element At the end of

 the iterative process the variables at the exit of the entry

 node will still hold the value since

 never changes



 It is possible that may

 show up at some other program points When they do it means that no

 definitions have_been observed for that variable along any of the

 paths_leading up to that program point Notice_that with the

 way we define the meet_operator as_long as there_exists a path that

 defines a variable reaching a program point the variable will not have an

 value If all the definitions_reaching a

 program point have the same constant value

 the variable is considered a constant

 even_though it may not be defined along some program path



 By assuming that the program is correct the algorithm can find more

 constants than it otherwise would That is the algorithm

 conveniently chooses some values for those possibly

 undefined_variables in order to make the program more_efficient This change

 is legal

 in most programming_languages since undefined_variables are

 allowed to take on any value If the language semantics requires that

 all undefined_variables be given some specific value then we must

 change our problem formulation accordingly And if instead we

 are_interested in finding possibly undefined_variables in a program

 we can formulate a different data-flow_analysis to

 provide that result (see Exercise )





 figureuullmanalsuch9figscorr-patheps

 Meet of and a constant





 In Fig the values of are 10 and

 at the exit of basic_blocks and respectively Since

 the value of is 10 on entry to block

 Thus block where is used can

 be optimized by_replacing by 10

 Had the path executed

 been

 the value of reaching basic_block

 would have_been undefined So it appears incorrect to replace the

 use of by 10



 However if it is impossible for predicate to be false while is

 true

 then this execution

 path never occurs While the programmer may be aware of that

 fact it may well be beyond the capability of any data-flow

 analysis to determine

 Thus if we assume that the program is correct and that all the variables

 are defined before they are used it is indeed correct that the value

 of at the beginning of basic_block can only be 10

 And if the program is incorrect to begin_with then

 choosing 10 as the value

 of cannot be worse than allowing to assume some random

 value





 Suppose we_wish to detect all possibility of a variable being uninitialized

 along any path to a point where it is used How_would you modify the

 framework of this_section to detect such situations





 An interesting and powerful data-flow-analysis framework is obtained_by

 imagining the domain to be all possible partitions of expressions so

 that two expressions are in the same class if and only if they are

 certain to have the same value along any path to the point in question

 To_avoid having to list an infinity of expressions we can represent

 by listing only the minimal pairs of equivalent expressions For

 example if we execute the statements





 then the minimal set of equivalences is

 From these follow other equivalences such_as and

 but there is no need to list these explicitly







 a)

 What is the appropriate meet_operator for this framework



 b)

 Give a data_structure to represent domain values and an algorithm to

 implement the meet_operator



 c)

 What are the appropriate functions to associate with statements

 Explain the effect that a statement such_as a bc should have on

 a partition of expressions (ie on a value in )



 d)

 Is this framework monotone Distributive



 Context Sensitivity



 By differentiating between the contexts in which a function is called

 a context-sensitive_analysis produces much more_precise results but

 is also much more_expensive A calling_context of a function is

 defined by the invocation_sites on the call stack at the time the

 function is executed Since there can be an_unbounded number of

 possible calling_contexts in the presence of recursive_cycles an

 abstraction on the calling_contexts to reduce them to a finite number



 Various abstractions have_been used in the past

 Boils down a finite number of contexts

 An expanded_call graph



 Given a method a context a call_site and other info such_as this pointer

 - target a context







 Abstraction on Calling Contexts





 We create a call_multigraph where is the set of

 methods in the program and there_exists an edge

 for every method called by at each call_site in



 A dynamic calling_context of a method is simply given by the sequence

 of call_sites pushed_onto its call stack We can represent every

 context of a method as a path from the root node of the call

 multigraph and terminating at the method The sequence of methods

 visited along the path corresponds to the sequence of methods pushed

 on an execution instance's call stack



 If no recursive cycle is present in the program the call_multigraph

 is acyclic Every possible calling_context corresponds to an acyclic

 path originating from the root of the graph



 If a recursive cycle is present however the following abstraction is

 used We find all the strongly computed components (SCC) in the call

 multigraph Each strongly_connected component has a number of entry

 points From each entry point there are infinitely many possible

 paths that lead to any method in the strongly_connected component

 Abstracting the paths taken we combine for each method all its

 instances reached from the same entry point That is if there are a

 total of calling_contexts reaching the entry points of a strongly

 connected_component then results will be computed for each method

 in the component



 A Context-Sensitive BDD-Based Analysis





 Domain

 description

 Context C C is the context domain

 description



 Program Relations

 description

 Context-sensitive invocation_edges cIE C_I

 C_M

 description



 Computed Relations

 description

 Context-sensitive arguments and return values cA C

 V_C V



 Context-sensitive variable points-to cvP C_V C_H



 hPointsTo - Context-sensitive heap points-to (C

 H_F C H)

 description



 Inference_Rules



 4

 ( vPfilterrule)

 ( cArule0)

 ( crule0)

 ( crule1) ( crule2) ( crule3)







 equation

 (v h) vP0 (ch h cnew new) cIE

 (ch v cnew h) cvP

 rule0

 equation



 equation

 (ci i cm m) cIE (m z v1) formal (i z v2) actual

 (ci v1 cm v2) cA

 cArule

 equation



 equation

 (cv1_v1 cv2 v2) cA (cv2_v2 ch h) cvP (v1 h) vPfilter

 (cv1_v1 ch h) cvP

 crule1

 equation



 equation

 (v1_f v2) S (cv1_v1 ch1 h1) cvP

 (cv2_v2 ch2_h2) cvP

 (ch1 h1 f ch2_h2) chP

 crule2

 equation



 equation

 (v1_f v2) L (cv1_v1 ch1 h1) cvP

 (ch1 h1 f ch2_h2) chP (v2 h2) vPfilter

 (cv2_v2 ch2_h2) cvP

 crule3

 equation

















 Context numbering









 figurehtb

 tabularl

 NumberContexts( rootSet)



 foreach rootSet



 Contexts



 foreach SCC in reverse post order



 0_0



 foreach







 if is important



 Contexts



 max( Contexts



 Contexts



 foreach such that







 tabular

 The algorithm to number contexts

 figcontextno

 figure























 Introduction to Data-Flow_Analysis

 secdf-intro



 All the optimizations introduced in Section_secopt-sources can

 benefit from data-flow_analysis Data-flow_analysis

 refers to a body of

 techniques that derive information_about the flow

 of data along control paths For_example one way to implement global

 common-subexpression_elimination requires us to determine_whether two textually

 identical expressions evaluate to the same value along any possible

 execution_path of the program

 As_another example if the result

 of an assignment is not used along any subsequent execution_path

 then we can eliminate the

 assignment as dead_code

 These and many other important questions can be addressed

 by data-flow_analysis



 The Data-Flow Abstraction



 Following Section two-stage-subsect a

 program execution can be_viewed as a series of transformations of the

 program state which consists of the values of

 all the variables in the program including those associated

 with stack frames below the top of the run-time_stack Each execution of

 an intermediate-code

 statement transforms an input state to a new output state The input

 state is associated_with the program point before the statement

 and the output state is associated_with the program point after the

 statement



 When we analyze the behavior of a program we must consider all the

 possible sequences of program points (paths) through a flow_graph

 that the program execution can take

 We then extract from the possible program states at each point the

 information we need for the particular data-flow_analysis problem we aim

 to solve

 In more_complex analyses we must consider paths that jump among the

 flow_graphs for various procedures as calls and returns are

 executed

 However to begin our study we_shall concentrate_on the paths through a

 single flow_graph for a single procedure



 Let_us see what the flow_graph tells_us about the possible_execution

 paths



 itemize



 Within one basic_block the program point after a statement is the same

 as the program point before the next statement



 If there is an edge from block to block then the

 program point after the last statement of

 may be followed immediately by

 the program point before the first statement of





 itemize

 Thus we may define

 an execution_path (or just path)

 from to to be a sequence of points

 such that for each either



 enumerate



 is the point_immediately preceding a statement and

 is the point_immediately following that

 same statement or



 is the end of some block and

 is the beginning

 of a successor block



 enumerate



 In_general there is an_infinite

 number of possible_execution paths through a program

 and there is no finite upper_bound on the length of an execution_path

 Program analyses summarize all the possible program states that can

 occur at a point in

 the program with a finite set of facts

 Different analyses may choose to abstract

 out different information and in general no analysis is a perfect

 representation of the state



 ex

 Even the simple program in Fig figdf-abs describes an

 unbounded_number of execution_paths

 Not entering the loop at all

 the shortest complete

 execution_path consists of the program points

 The next shortest path executes one iteration of the loop

 and consists of the points



 We know that for example

 the first time program point_(5) is executed

 the value of is 1 due to definition We_say that

 reaches point_(5) in the first iteration In subsequent

 iterations reaches point_(5) and the value of is 243



 figurehtb

 figureuullmanalsuch9figsdf-abseps

 Example program illustrating the data-flow_abstraction

 figdf-abs

 figure



 In_general it is not possible to

 keep_track of all the program states for all

 possible paths In data-flow_analysis we do_not distinguish

 among the paths taken to reach a program point

 Moreover we do_not keep_track of entire states rather we abstract_out

 certain details keeping only the data we need for the purpose of the

 analysis

 Two examples will illustrate_how the same program states may lead to

 different information abstracted at a point



 enumerate



 To help users debug their programs we may wish

 to find out what are all the values a variable may have at a program

 point

 and where these values may be defined For_instance we may

 summarize all the program states at point_(5) by

 saying that the value of is one of and that it

 may be defined by one of The definitions that

 may reach a program point along some path are known_as reaching_definitions



 Suppose instead we are_interested in implementing constant_folding

 If a use of the variable is reached by only one definition and that

 definition assigns a constant to then we can simply replace by

 the constant

 If on the other_hand several definitions of may

 reach a single program point then we cannot perform constant_folding on



 Thus for constant_folding we_wish to find those definitions that

 that are the unique definition of their variable

 to reach a given program point no_matter which execution_path is

 taken For point_(5) of Fig figdf-abs there is no definition

 that must_be the definition of at that point so this set is

 empty for at point_(5)

 Even_if a variable has a unique definition at a point that definition

 must assign a constant to the variable

 Thus we may simply describe certain

 variables as not a constant instead of collecting all their

 possible values or all their possible definitions



 enumerate

 Thus we see that the same information may be

 summarized differently depending_on the purpose of the analysis

 ex



 The Data-Flow_Analysis Schema

 secdf-schema



 In each application of

 data-flow_analysis we associate with every program point a data-flow value that represents an abstraction of the set of all

 possible program states that can be observed for that

 point The set of possible data-flow_values is the domain for

 this application

 For_example the domain of data-flow_values

 for reaching_definitions

 is the set of all subsets of definitions in the program

 A particular data-flow value is a set of definitions and we_want to

 associate with each point in the program the exact set of definitions

 that can reach that point

 As_discussed above the choice of abstraction depends_on the goal of the

 analysis to be efficient we only keep_track of information that is

 relevant



 We denote the data-flow_values before and after each statement by

 and respectively The data-flow_problem is

 to find a solution to a set of constraints on the

 's and 's for all statements There_are two sets of

 constraints those based_on the semantics of the statements (transfer

 functions) and those

 based_on the flow of control



 Transfer_Functions



 The data-flow_values before and after a statement are constrained by

 the semantics of the statement For_example suppose our data-flow

 analysis involves determining the constant value of variables at points

 If variable has

 value before executing statement b a then both and

 will have the value after the statement This relationship

 between the data-flow_values before and after the assignment_statement is

 known_as a transfer_function



 Transfer

 functions come in two flavors information may propagate

 forward along execution_paths or it may flow

 backwards up the execution_paths In a forward-flow problem the

 transfer_function of a statement which we_shall usually

 denote takes the data-flow

 value before the statement and produces a new data-flow value after the

 statement That is



 s_fs(s)



 Conversely in a backward-flow problem

 the transfer_function for statement

 converts a data-flow value after the statement to a new

 data-flow value before the statement That is



 s_fs(s)





 Control-Flow Constraints



 The second set of constraints on data-flow_values

 is derived_from the flow of control

 Within a basic_block control_flow is simple

 If a block consists of statements in that

 order then the control-flow value out of is the same as the

 control-flow value into

 That is



 center

 for all

 center



 However control-flow edges between basic_blocks

 create more_complex constraints between the last statement of

 one basic_block and the first statement of the following block

 For_example if we are_interested in collecting

 all the definitions that may reach a program point then the set of

 definitions_reaching the leader statement of a basic_block is the union of

 the definitions after the last statements of each of the predecessor

 blocks The next section gives the details of how data flows among the

 blocks



 Data Flow Schemas on Basic_Blocks



 While a data-flow_schema technically involves data-flow_values at each

 point in the program we can save time and space by recognizing that

 what goes on inside a block is usually quite_simple

 Control flows from

 the beginning to the end of the block without interruption or

 branching

 Thus we can restate the schema in terms of data-flow_values entering and

 leaving the blocks

 We denote the data-flow_values immediately_before

 and immediately

 after each basic_block by and respectively

 The constraints involving and can be derived_from

 those involving

 and for the various statements in as_follows



 Suppose block consists of statements in that

 order

 If is the first statement of basic_block then

 Similarly if is the last statement of basic_block

 then The transfer_function of a basic_block

 which we denote can be derived by_composing the transfer_functions of the

 statements in the block That is let be the

 transfer_function of statement

 Then

 The relationship_between the beginning and end of the block is



 B_fB(B)





 The constraints

 due to control_flow between basic_blocks can easily be_rewritten by

 substituting and for and

 respectively

 For_instance if data-flow_values are information_about the sets of

 constants that may be assigned to a variable then



 B_P a predecessor of B_P





 When the data-flow is backwards the equations are similar but with the

 roles of the 's and 's reversed

 That is



 align

 B_fB(B)



 B_S a successor of B

 S



 align



 Unlike linear arithmetic equations the data-flow_equations usually do_not

 have a unique solution

 Our_goal is to find the most_precise solution that satisfies

 the two sets of constraints control-flow and transfer constraints

 That is we need a solution that encourages valid code improvements but

 does_not justify unsafe transformations - those that change_what the

 program computes

 This issue is discussed briefly here in the box on Conservatism and

 more extensively in Section df-semantics-subsect

 In the following subsections we discuss some of the most_important

 examples of problems that can be_solved by data-flow_analysis



 Reaching_Definitions

 secrd-df



 Reaching_definitions is one of the most

 common and useful data-flow_schemas

 By knowing where in the program each variable may have_been defined

 when control_reaches each point we can tell many things about

 For just two examples

 a compiler then knows whether is a constant at point and a

 debugger can tell_whether it is possible for to be an undefined

 variable should be used at



 Detecting Possible Uses Before Definition

 Here is how we use a solution to the reaching-definitions_problem to

 detect uses before definition

 The trick is to introduce

 a dummy definition for each variable in the entry to the flow_graph

 If the dummy definition of reaches a point where might be used

 then there might be an opportunity to use before definition Note

 that we can never be absolutely certain that the program has a bug

 since there may be some reason possibly involving a complex logical

 argument why the path along which is reached

 without a real definition of can never be taken



 We_say a definition_reaches a point if there is a path

 from the point_immediately following to such that is not

 killed along that path

 We kill a definition of a variable if

 there is any other definition of anywhere

 along the pathfootnoteNote that the path may have loops so we

 could come to another occurrence of along the path which does_not

 kill footnote

 Intuitively if a definition of some

 variable reaches point then might be the place at

 which the value of used at might last have_been defined



 A definition of a variable is a statement that assigns or may

 assign a value to

 Procedure parameters array_accesses and indirect

 references all may have aliases and it is not easy to tell if a statement

 is referring to a particular variable Program analysis

 must_be conservative if we do_not know whether a statement is assigning a

 value to we must assume that it may assign to it that

 is variable after statement may have either its original

 value before or the new value created by For the sake of

 simplicity the rest of the chapter assumes that we

 are dealing only with variables that have no_aliases This class of

 variables includes

 all local_scalar variables in most languages in the case of C and

 C local_variables whose addresses have_been computed at some point

 are

 excluded



 Conservatism in Data-Flow_Analysis

 Since all data-flow_schemas compute approximations to the ground truth

 (as_defined by all possible_execution paths of the program) we are

 obliged to assure that any errors are in the safe_direction

 A policy decision is safe (or conservative)

 if it never allows_us to

 change_what the program computes

 Safe policies may unfortunately cause_us to miss some code

 improvements that would retain the meaning of the program but in

 essentially all code-optimizations there is no safe policy that misses

 nothing

 It would generally be unacceptable to use an unsafe policy - one that

 sped up the code at the expense of changing what the program computes



 Thus when designing a data-flow_schema we must_be conscious of how the

 information will be used and make_sure that any approximations we make

 are in the conservative or safe_direction

 Each schema and application must_be considered independently

 For_instance if we use reaching_definitions for constant_folding it is

 safe to think a definition_reaches when it doesn't (we_might think

 is not a constant when in fact it is and could have_been folded) but

 not safe to think a definition doesn't reach when it does (we_might

 replace by a constant when the program would at times have a value

 for other_than that constant)



 ex

 Shown in Fig_figreach-def is a flow_graph with seven

 definitions Let_us focus_on the definitions_reaching block

 All the definitions in block

 reach the beginning of block The

 definition j_j-1 in block also reaches the

 beginning of block because no other definitions of can

 be found in the loop leading back to This definition however

 kills the definition j_n preventing it from reaching

 or The statement i_i1 in does_not reach

 the beginning of though because the variable is

 always redefined by i u3 Finally the definition

 a u2 also reaches the beginning of block

 ex



 figurehtb

 figureuullmanalsuch9figsreach-defeps

 Flow_graph for illustrating reaching_definitions

 figreach-def

 figure



 By defining reaching_definitions as we have we sometimes allow

 inaccuracies

 However they are all in the safe or conservative direction

 For_example

 notice our_assumption that all edges

 of a flow_graph can be traversed

 This assumption may not be true in practice

 For_example for no values of and

 can control actually reach statement 2

 in the following program_fragment



 center

 tabularl

 if (a_b) statement 1

 else if (a_b) statement 2

 tabular

 center



 To decide in general whether each path in a flow_graph can

 be taken is an undecidable problem

 Thus we simply assume that every_path in the flow_graph can be followed

 in some execution of the program

 In most applications of reaching_definitions it

 is conservative to assume that a

 definition can reach a point even if it might not

 Thus we may allow paths that are never be traversed in any execution

 of the program and we may allow definitions to pass

 through ambiguous definitions of the same variable safely



 Transfer Equations for Reaching_Definitions



 We_shall now set up the constraints for the reaching

 definitions problem We start by_examining the details of a single

 statement Consider a definition



 center

 u vw

 center

 Here and frequently in what_follows

 is used as a generic binary_operator



 This statement generates a definition

 of variable

 and kills all the other definitions in the program that

 define variable

 while leaving the remaining incoming definitions unaffected

 The transfer_function of definition thus can be_expressed as



 equation

 gen-kill-eq

 fd(x) gend (x_- killd)

 equation

 where the set of definitions generated_by the statement

 and is the set of all other definitions of in the program



 As_discussed in Section secdf-schema the transfer_function of a

 basic_block can be found by_composing the transfer_functions of the

 statements contained therein

 The composition of functions of the form (gen-kill-eq) which we

 shall_refer to as - form is also of

 that form as we can see as_follows

 Suppose there are two functions and



 Then



 align

 f2(f1(x))

 gen2(gen1(x-kill1)-kill2)



 (gen2(gen1-kill2))(x-(kill1kill2))



 align



 This rule extends to a block consisting of any number of statements

 Suppose block has statements with transfer_functions

 for

 Then the transfer_function for block may be written as



 fB(x) genB (x_- killB)



 where



 killB kill1kill2killn



 and

 align

 genB

 genn(genn-1-killn)(genn-2-killn-1-killn)



 (gen1-kill2-kill3-align



 Thus like a statement a basic_block also generates a set of

 definitions and kills a set of definitions The set contains

 all the definitions inside the block

 that are visible immediately_after the block - we refer to

 them as downwards-exposed A definition is downwards-exposed in

 a basic_block only if it is not_killed by a

 subsequent definition to the same variable inside the same basic

 block A basic block's set is simply the union of all the

 definitions killed by the individual_statements Notice_that a

 definition may appear in both the and set of a basic

 block

 If so the fact that it is in takes_precedence

 because in - form the set is applied before the

 set



 ex

 gen-kill-ex

 The set for the following basic_block



 center

 tabularr_l

 a 3



 a 4



 tabular

 center



 is since is not downwards-exposed The set

 contains both and since kills and vice_versa

 Nonetheless since the subtraction of the set precedes

 the union operation with the set the result of the transfer

 function for this block always includes definition

 ex



 Control-Flow Equations



 Next we consider the set of constraints derived_from the control_flow

 between basic_blocks Since a definition_reaches a program point as

 long_as there_exists at_least one path along which the definition

 reaches

 whenever there is a control-flow edge

 from to

 However since a definition cannot reach a point unless there is a path

 along which it reaches

 needs to be no larger_than the

 union of the reaching_definitions of all the predecessor blocks

 That is it is safe to assume



 B_P a predecessor of B_P





 We refer to union as the meet_operator for

 reaching_definitions

 In any data-flow_schema the meet_operator is the one

 we use to create a summary of the

 contributions from different paths at the confluence of those

 paths



 Iterative_Algorithm for Reaching_Definitions



 We assume that every control-flow_graph has two empty basic_blocks an

 entry_node which represents the starting point of the graph

 and an exit node to which all exits_out of the graph go

 Since no definitions reach the beginning of the graph

 the transfer_function for the entry block is a simple

 constant function that returns as an answer That is





 The reaching_definitions problem is defined by the following

 equations





 entry

 For all basic_blocks other_than entry



 B genB (B_- killB)





 B_P a predecessor of B_P



 These_equations can be_solved using

 the following algorithm

 The result of the algorithm is the least fixedpoint of the

 equations ie the solution whose

 assigned values to the 's and 's is contained in the

 corresponding values for any other solution to the equations

 The result of the algorithm below is acceptable since any definition in

 one of the sets or surely must reach the point described

 It is a desirable solution since it does_not include any definitions

 that we can be_sure do_not reach



 alg

 algreaching-definitions

 Reaching_definitions



 A flow_graph for which and have_been computed for

 each block



 and the set of definitions

 reaching the entry and exit of each block of the flow_graph



 We use an iterative_approach in which we start with the estimate

 for all and converge to the

 desired values of and

 As we must iterate_until the 's (and hence the 's) converge

 we could use a boolean variable to record on each pass_through the

 blocks whether any has changed

 However in this and in similar algorithms described later we assume

 that the exact mechanism for keeping_track of changes is understood and

 we elide those details



 The algorithm is sketched in Fig_figreach-def-alg

 The first two lines initialize certain data-flow

 valuesfootnoteThe observant reader will notice that we could

 easily combine lines_(1) and (2) However in similar data-flow

 algorithms it may be necessary to initialize the entry or

 exit node differently from the way we initialize the other nodes

 Thus we follow a pattern in all iterative_algorithms of applying a

 boundary_condition like line_(1) separately from the initialization

 of line (2)footnote

 Line (3) starts the loop in which we iterate_until convergence and the

 inner_loop of lines_(4) through_(6) applies the data-flow_equations to

 every block other_than the entry

 alg



 figurehtb

 center

 tabularr_l

 1)



 2) for_(each basic_block other_than entry)





 3)_while (changes to any_occur)



 4) for_(each basic_block other_than entry)





 5)





 6)







 tabular

 center



 Iterative_algorithm to compute reaching_definitions

 figreach-def-alg



 figure



 Intuitively Algorithm_algreaching-definitions

 propagates definitions as far as they

 will go without being killed thus

 simulating all possible_executions

 of the program

 Algorithm_algreaching-definitions will_eventually halt

 because for every

 never shrinks once a definition is

 added it stays there forever

 (See Exercise rd-ind-exer)

 Since the set of all definitions is finite eventually there must

 be a pass of the while-loop during which nothing is added to any

 and the algorithm then terminates

 We are safe terminating then because if the 's have not changed

 the 's will not change on the next pass

 And if the 's do_not change the 's cannot so on

 all subsequent_passes there can be no changes



 The number of nodes in the flow_graph is

 an upper_bound on the number of times

 around the while-loop

 The_reason is that if a definition_reaches a point

 it can do so along a cycle-free_path and the number

 of nodes in a flow_graph is an upper_bound on the number

 of nodes in a cycle-free_path

 Each time around the while-loop each definition progresses

 by at_least one node along the path in question and it often progresses

 by more_than one node depending_on the order in which the nodes are

 visited



 In_fact if we properly order the blocks in the for-loop of line

 (5) there is empirical evidence that the average_number

 of iterations of the while-loop is under 5

 (see_Section secconvergence-speed)

 Since sets of definitions can be represented_by bit_vectors

 and the operations on these sets can be_implemented by

 logical operations on the bit_vectors

 Algorithm_algreaching-definitions is surprisingly efficient in practice



 ex

 exreaching-definitions

 We_shall represent the seven_definitions

 in the flow_graph of Fig_figreach-def

 by

 bit_vectors where bit from the left represents

 definition

 The union of sets is computed by_taking the logical_OR of the

 corresponding bit_vectors and the difference of two sets

 is computed by

 complementing the bit_vector of and then taking the logical

 AND of that complement with the bit_vector for



 Shown in

 the table of Fig_figreach-def-comp are the values taken on by

 the and sets in Algorithm_algreaching-definitions

 The initial

 values indicated by a superscript 0 as in are assigned by

 the loop of line (2)

 of Fig_figreach-def-alg

 They are each the empty_set represented_by bit

 vector 000_0000

 The values of subsequent_passes of the algorithm are also indicated by

 superscripts and

 labeled and for the first pass and

 and for the second



 figurehtb

 center

 tabularc_c c_c c_c

 Block



 000_0000 000_0000 111_0000 000_0000 111_0000



 000_0000 111_0000 001_1100 111 0111_001 1110



 000_0000 001_1100 000_1110 001_1110 000_1110



 000_0000 001_1110 001_0111 001_1110 001_0111



 000_0000 001_0111 001_0111 001_0111 001_0111



 tabular

 center

 Computation of and

 figreach-def-comp

 figure



 Suppose the for-loop of lines_(4) through_(6) is executed with



 B B1_B2 B3 B4 exit



 in that order

 With since

 is the empty_set and is

 This value differs_from the previous value so we

 now know there is a change on the first round (and will proceed to a

 second round)



 Then we consider and compute



 align

 B21 B11 B40



 111_0000 000_0000 111_0000



 B21 genB2 (B21 - killB2)



 000 1100 (111 0000 - 110 0001) 001_1100



 align



 This computation is summarized in Fig_figreach-def-comp

 For_instance at the end of the first pass

 reflecting the fact that and are generated in while

 reaches the beginning of

 and is not_killed in



 Notice_that after the second round has changed to reflect

 the fact that also reaches the beginning of and is not

 killed by We did_not learn that fact on the first pass because

 the path from to the end of which is

 is not traversed in that order by a single pass That is by the

 time we learn that reaches the end of we have_already

 computed and on the first pass



 There_are no changes in any of the

 sets after the second pass Thus after a third_pass the

 algorithm_terminates with the 's and 's as in the final

 two columns of Fig_figreach-def-comp

 ex



 Live-Variable Analysis

 live-var-subsect



 Some code-improving_transformations

 depend_on information computed in the direction

 opposite to the flow of control in a program

 we_shall examine one such example now

 In live-variable_analysis

 we_wish to know for variable

 and

 point whether the value of

 at

 could be used along some path in the flow

 graph starting_at

 If so we say

 is

 live

 at otherwise

 is

 dead

 at



 An_important use for live-variable information

 is register_allocation for basic_blocks

 Aspects of this issue were_introduced in Sections dag-bb-subsect

 and simple-cg-alg-subsect

 After a value is computed in a register and

 presumably used within a block it is not

 necessary to store that value if it is dead at

 the end of the block

 Also if all registers are full and we need another

 register we should favor using a register with a dead

 value since that value does_not have to be stored



 Here we define the data-flow_equations directly in terms of

 and which represent the set of variables live at

 the points immediately_before and after block respectively

 These_equations can also be derived by first defining the transfer

 functions of individual_statements and composing them to create the

 transfer_function of a basic_block

 Define



 enumerate



 is the set of variables defined (ie

 definitely assigned values) in prior to any use of that

 variable in and



 is the

 set of variables whose values may be

 used in prior to any definition of the variable



 enumerate



 ex

 For_instance block in Fig_figreach-def definitely uses

 It also uses before any redefinition of

 unless it is possible that and are

 aliases of one another Assuming there are no_aliases among the

 variables in Fig_figreach-def then

 Also clearly defines and Assuming there are no_aliases

 as_well

 ex



 As a consequence of the

 definitions any variable in must_be considered live_on entrance to

 block while definitions of variables

 in definitely are dead

 at the beginning of

 In effect membership in kills any opportunity

 for a variable to be live because of paths that begin at



 Thus the equations relating and

 to the

 unknowns and are





 exit

 and for all basic_blocks other_than exit



 align

 B useB (B_- defB)



 B_S a successor of B_S



 align

 The first equation specifies the boundary_condition which is that no

 variables are live_on exit from the program The second

 equation_says that

 a variable is live_coming into a block if

 either it is used before redefinition in the block or it is live

 coming_out of the block and is not redefined in the block

 The third equation_says that a

 variable is live_coming out of a block

 if and only if it is live_coming into one of its successors



 The relationship_between the equations for liveness and

 the reaching-definitions equations

 should be noticed



 itemize



 Both sets of equations have union as the meet_operator

 The_reason is that in each data-flow_schema we propagate information

 along paths and we care only about_whether any path with desired

 properties exist rather_than whether_something is true along all

 paths



 However information flow for liveness travels backward opposite to

 the direction of control_flow because in this problem we_want to make

 sure that the use of a variable at a point

 is transmitted to all points prior to in an execution_path so that

 we may know at the prior point that will have its value used



 itemize



 To solve a backward

 problem instead of initializing we initialize



 Sets and have their roles interchanged and and

 substitute for and respectively

 As for reaching_definitions the solution to the liveness equations

 is not_necessarily

 unique and we_want the solution with the smallest sets of

 live_variables

 The algorithm used is

 essentially a backwards version of Algorithm_algreaching-definitions



 alg

 algliveness

 Live variable analysis



 A flow_graph with def and computed for each block



 and the set of variables

 live_on entry and exit of each block of the flow_graph



 Execute the program in Fig figliveness

 alg



 figurehtb

 center

 tabularl





 for_(each basic_block other_than exit)





 while_(changes to any_occur)



 for_(each basic_block other_than exit)















 tabular

 center



 Iterative_algorithm to compute live_variables

 figliveness



 figure



 Available_Expressions

 ae-subsect



 An expression is

 available

 at a point if every_path from

 the entry_node to evaluates

 and after the last

 such evaluation prior to reaching there are no_subsequent

 assignments to or footnoteNote

 that as usual in this_chapter we use the operator as a

 generic_operator not_necessarily standing for additionfootnote

 For the available-expressions data-flow_schema we say that a block

 kills

 expression

 if it assigns (or may assign)

 or and does_not

 subsequently recompute

 A block

 generates

 expression

 if it definitely evaluates

 and

 does_not subsequently define or



 Note_that the notion of killing or generating an available

 expression is not exactly the same as that for reaching

 definitions

 Nevertheless these notions of kill and generate behave

 essentially

 as they do for reaching_definitions



 The primary use of available-expression information is for

 detecting global_common subexpressions

 For_example in Fig figcse(a) the expression

 in block will be a common_subexpression if

 is available at the entry point of block

 It will be available if

 is not assigned a new value in block or if as

 in Fig figcse(b)

 is recomputed after

 is assigned in



 figurehtb

 figureuullmanalsuch9figscseeps

 Potential common_subexpressions across blocks

 figcse

 figure



 We can compute the set of generated expressions for each point in a block

 working from beginning to end of the block

 At the point prior to the block no expressions are generated

 If at point set of expressions is available

 and is the point after with statement x_yz between

 them then we form the set of expressions

 available at by the following two steps



 enumerate



 Add to the expression



 Delete from any expression involving variable



 enumerate



 Note the steps must_be done in the correct order as could be

 the same as or

 After we reach the end of the block is the set of generated

 expressions for the block

 The set of killed expressions is all expressions say

 such that either or is defined in the block and

 is not generated_by the block

 Note_that could be both generated and killed but the -

 form of transfer_function causes generation to take precedence



 ex

 exavail-exp

 Consider the four statements of Fig figavail-exp-comp

 After the first is available

 After the second statement

 becomes available but is no_longer

 available because has_been redefined

 The third statement

 does_not make available again because the value

 of is immediately changed

 After the last statement is no_longer available because

 has changed

 Thus no expressions are generated and all expressions_involving

 or are killed

 ex



 figurehtb

 center

 tabularc_c

 Statement Available_Expressions







 a b_c







 b a - d







 c b_c







 d a - d







 tabular

 center



 Computation of available_expressions

 figavail-exp-comp



 figure



 We can find available_expressions in a manner reminiscent of the

 way reaching_definitions are computed

 Suppose is the universal_set of all expressions

 appearing on the right of one or_more statements of the program

 For each block let be the set of expressions in

 that are available at the point just_before the beginning of

 Let be the same for the point following the end

 of

 Define to be the expressions generated_by

 and to be the set of expressions in killed

 in

 Note_that and can all be

 represented_by bit_vectors

 The following equations relate the unknowns and to

 each other and the known quantities and



 entry

 For all basic_blocks other_than entry



 align

 B egenB (B_- ekillB)



 B_P a predecessor of B_P



 align



 The above equations look almost identical to the equations for

 reaching_definitions

 Like reaching_definitions the boundary_condition is

 because

 at the exit of the entry_node there are no available

 expressions

 The most_important difference is that the meet_operator

 is intersection rather_than union

 This operator is the proper one because an expression

 is available at the beginning of a block only if it is available

 at the end of all its_predecessors

 In_contrast a definition_reaches the beginning of a block

 whenever it reaches the end of any one or_more of its_predecessors



 The use of rather_than makes the available-expression

 equations behave

 differently from those of reaching_definitions

 While neither set has a unique solution for reaching_definitions it is the

 solution with the smallest sets

 that corresponds to the definition of reaching and we

 obtained that solution by starting_with the assumption

 that nothing reached anywhere and building

 up to the solution

 In that way we never assumed that a definition

 could reach a point unless

 an actual path propagating to could be found

 In_contrast for available expression equations we_want the solution

 with the

 largest sets of available_expressions

 so we start with an approximation that is too large and work down



 It may not be obvious that by starting_with the assumption

 everything (ie the set ) is available everywhere except at the

 end of

 the entry block

 and eliminating

 only those expressions for which we can discover a path along which it

 is not available we do reach a set of truly_available expressions

 In the case of available_expressions it is conservative to produce a subset

 of the exact set of available_expressions

 The argument for subsets being conservative is that our intended use of the

 information is to replace the computation of an available expression

 by a previously_computed value

 Not knowing an expression is available only inhibits

 us from improving the code while believing an expression is available

 when it is not could cause_us to change_what the program computes



 figurehtb

 figureuullmanalsuch9figsavail-expeps

 Initializing the sets to is too

 restrictive

 figavail-exp-ex

 figure



 ex

 exavail-exp-comp

 We_shall concentrate_on a single block in Fig figavail-exp-ex

 to illustrate the effect of the initial approximation of

 on

 Let and abbreviate and

 respectively

 The data-flow_equations for block are



 B2 B1_B2





 B2 G (B2 -_K)



 These_equations may be_rewritten as recurrences

 with and being the th approximations of

 and respectively

 Ij1 B1 Oj

 Oj1 G (Ij1 -_K)



 Starting with



 we get

 However if we start with

 then we get

 as we should

 Intuitively the solution obtained starting_with



 is more desirable because it correctly reflects the fact

 that expressions in

 that are not_killed by are available at

 the end of

 ex



 alg

 algavailable-expr

 Available expressions



 A flow_graph with and

 computed for each block

 The initial block is



 and the set of

 expressions available at

 the entry and exit of each block of the flow_graph





 Execute the algorithm of Fig figavail-exp-alg

 The explanation of the steps is similar to that for Fig_figreach-def-alg

 alg



 figurehtb

 center

 tabularl





 for_(each basic_block other_than entry)





 while_(changes to any_occur)



 for_(each basic_block other_than entry)















 tabular

 center

 Iterative_algorithm to compute available_expressions

 figavail-exp-alg

 figure





 Definition-Use Chains

 A calculation done in virtually the same manner as

 live-variable_analysis is

 definition-use chaining

 (du-chaining)

 We_say a variable is

 used

 at statement if its -value may be required

 For_example

 b

 and

 c

 (but_not

 a

 are used in

 each of the statements

 a bc

 and

 ab c

 The du-chaining

 problem is to compute for a point the set of uses

 of a variable say

 x

 such that there is a path from

 to that does_not redefine

 x



 The data-flow_values in the du-chaining_problem are sets of pairs

 such that is a statement that uses variable

 As with live_variables

 if we can compute the set of uses reachable from

 the end of block then we can compute

 the definitions reached from any point within block by

 scanning the portion of block that follows

 In_particular if there is a definition of variable

 x

 in the block we can determine the

 du-chain

 for that definition the list of all possible uses of that definition



 The equations for computing du-chaining information look

 exactly like those for liveness with substitution for and

 In place of we have the set of pairs_such

 that is a statement in which uses variable

 x

 and such

 that no prior definition of

 x

 occurs in

 Instead of we have

 the set of pairs_such

 that is a statement which uses

 x

 is not in and

 has a definition of x

 These_equations are solved by the obvious analog of

 Algorithm_algliveness





 Summary



 In the above we have discussed three instances of data-flow_problems reaching

 definitions live_variables and available_expressions

 As summarized in Fig figframework

 the definition of each problem

 is given by the domain of the data-flow_values

 the direction of the data flow the

 family of transfer_functions the boundary_condition and the meet

 operator We refer to the meet_operator generically as



 The last_row shows the initial values used in the iterative

 algorithm These values are chosen so that the iterative_algorithm

 will find the most_precise

 solution to the equations This choice is not strictly a part of

 the definition of the data-flow_problem since it is an artifact needed

 for the iterative_algorithm There_are other ways of solving the

 problem For_example we saw how the transfer_function of a basic

 block can be derived by_composing the transfer_functions of the individual

 statements in the block a similar compositional approach may be used

 to compute a transfer_function for the entire procedure or transfer

 functions from the entry of the procedure to any program point

 We_shall discuss such an approach in Section_region-sect



 figurehtb

 center

 tabularl_l l_l

 Reaching_Definitions Live Variables Available_Expressions



 Domain_Sets of definitions Sets of variables Sets of expressions





 Direction_Forwards Backwards_Forwards



 Transfer







 function



 Boundary







 Meet_()



 Equations















 Initialize



 tabular

 center



 Summary of three data-flow_problems

 figframework

 figure



 Why the Available-Expressions Algorithm Works

 We need to explain why starting all 's except that for the entry

 block with the set of all expressions leads to a conservative

 solution to the data-flow_equations that is all expressions found to

 be available really are available

 First because intersection is the meet_operation in this data-flow

 schema any reason that an expression is found not to be available

 at a point

 will propagate_forward in the flow_graph along all possible paths

 until is recomputed and becomes available again

 Second there are only two_reasons could be unavailable



 enumerate



 is killed in block because or is defined without a

 subsequent computation of

 In this case the first time we apply the transfer_function

 will be removed_from



 is never computed along some path

 Since is never in and it is never

 generated along the path in question we can show by induction on the

 length of the path that is eventually removed_from 's and

 's along that path



 enumerate

 Thus after changes subside the solution provided by the interative

 algorithm of Fig figavail-exp-alg will include only truly

 available_expressions



 sexer

 For the flow_graph of Fig_fg1-fig (see the exercises for

 Section_secopt-sources) compute the following for each block



 itemize



 a) The gen and kill_sets for each block



 b) The and sets for each block



 itemize

 sexer



 exer

 For the flow_graph of Fig_fg1-fig compute the

 and sets for available_expressions

 exer



 exer

 For the flow_graph of Fig_fg1-fig compute the

 and sets for live variable analysis

 exer



 hexer

 We claimed that if a block consists of statements and the th

 statement has gen and kill_sets and then the

 transfer_function for block has gen and kill_sets and

 given by



 killB kill1kill2killn



 align

 genB

 genn(genn-1-killn)(genn-2-killn-1-killn)



 (gen1-kill2-kill3-align

 Prove this claim by induction on

 hexer



 hexer

 rd-ind-exer

 Prove by induction on the number of iterations of the for-loop of lines

 (4)_through (6) of Algorithm_algreaching-definitions

 that none of the 's or 's ever shrinks

 That is once a definition is placed in one of these sets on some round

 it never disappears on a subsequent round

 hexer



 hexer

 Show the correctness of Algorithm_algreaching-definitions That

 is show that



 itemize



 a)

 If definition is put in or

 then there is a path from to the beginning or end of block

 respectively along which the variable defined by might not be

 redefined



 b)

 If definition is not put in or

 then there is no path from to the beginning or end of block

 respectively along which the variable defined by might not be

 redefined



 itemize

 hexer



 hexer

 Prove the following about Algorithm_algliveness



 itemize



 a)

 The 's and 's never shrink



 b)

 If variable is put in or

 then there is a path from the beginning or end of block

 respectively along which might be used



 c)

 If variable is not put in or

 then there is no path from the beginning or end of block

 respectively along which might be used



 itemize

 hexer



 hexer

 Prove the following about Algorithm algavailable-expr



 itemize



 a)

 The 's and 's never grow that is successive values of

 these sets are subsets (not_necessarily proper) of their previous

 values



 b)

 If expression is removed_from or

 then there is a path from the entry of the flow_graph to the

 beginning or end of block

 respectively along which is either never computed or after its

 last computaton one of its arguments might be redefined



 c)

 If expression remains in or

 then along every_path from the entry of the flow_graph to the

 beginning or end of block

 respectively is computed and after the last computation no

 argument of could be redefined



 itemize

 hexer



 hexer

 The astute reader will notice that in

 Algorithm_algreaching-definitions we could have saved some time

 by initializing to for all blocks Likewise in

 Algorithm_algliveness we could have initialized to

 We did_not do so for uniformity in the treatment of the

 subject as we_shall see in Algorithm_algiterative However is

 it possible to initialize to in

 Algorithm algavailable-expr Why or why not

 hexer



 hexer

 Our data-flow_analyses so_far do_not take_advantage of the semantics of

 conditionals Suppose we find at the end of a basic_block a test such

 as



 verbatim

 if (x 10) goto

 verbatim

 How could we use our understanding of what the test means to

 improve our knowledge of reaching_definitions Remember improve

 here means that we eliminate certain reaching_definitions that really

 cannot ever reach a certain program point

 hexer

 Foundations of Data-Flow_Analysis

 secdf-foundation



 Having shown several useful examples

 of the data-flow_abstraction we now study the family of

 data-flow_schemas as a whole abstractly We_shall answer

 several basic questions about data-flow algorithms formally



 enumerate



 Under what circumstances is

 the iterative_algorithm used in data-flow_analysis correct



 How precise is the solution obtained_by the iterative_algorithm



 Will the iterative_algorithm converge



 What is the meaning of the solution to the equations



 enumerate



 In Section_secdf-intro we addressed each of the questions

 above informally when describing the reaching-definitions_problem

 Instead of answering the same questions for each subsequent problem

 from scratch we relied on analogies with the problems we have_already

 discussed to explain the new problems

 Here we present a general approach that answers all

 these questions once and for all rigorously and for a large family of

 data-flow_problems We first identify the properties

 desired of data-flow_schemas and prove the implications of these

 properties on the correctness precision and convergence of the

 data-flow algorithm as_well as the meaning of the solution Thus to

 understand old algorithms or formulate new ones we simply show that

 the proposed data-flow_problem definitions have certain properties

 and the answers to all the above difficult questions are available

 immediately



 The concept of having a common theoretical framework for a class of

 schemas also has practical implications The framework

 helps us identify the reusable components of the algorithm in our

 software design Not_only is coding effort reduced but

 programming_errors are reduced by not having to recode

 similar details several_times



 A data-flow_analysis framework consists of



 enumerate



 A direction of the data flow which is either forwards or

 backwards



 Optionally a semilattice (See Section secsemilattice for the

 definition) which includes a domain of values

 and a meet_operator



 A family of transfer_functions

 from to This family must include functions suitable for the

 boundary conditions which are

 constant transfer_functions for the special nodes

 entry and exit in

 any flow_graph



 enumerate



 Semilattices

 secsemilattice



 A semilattice is a set and a binary meet_operator

 such that for all and in



 enumerate



 (meet is idempotent)





 (meet is commutative)





 (meet is associative)



 enumerate



 Optionally a

 semilattice may have a top_element denoted such that



 center

 for all in

 center

 Similarly a semilattice may have a bottom_element denoted such that

 center

 for all in

 center



 Partial Orders



 As we_shall see

 the meet_operator of a semilattice defines a partial_order on

 the values of the domain A relation is a partial_order on

 a set if for all and in



 enumerate





 (the_partial order is reflexive)



 and imply

 (the_partial order is antisymmetric)



 and imply

 (the_partial order is transitive)

 enumerate

 The pair

 is called a poset or partially-ordered set

 It is also convenient to have a relation for a poset defined as



 center

 if and only if and

 center



 The Partial Order for a Semilattice



 It is extremely useful to define a partial_order for a

 semilattice

 The definition of is



 center

 if and only if

 center

 Because the meet_operator is

 idempotent commutative and associative the order as defined is

 reflexive antisymmetric and transitive

 To_see why observe that



 itemize



 Reflexivity for all



 The proof is that since

 meet is idempotent



 Antisymmetry

 if and then

 In proof means and means

 By commutativity of



 Transitivity

 if and then

 In proof

 and mean that and

 Then



 using associativity of meet

 Since has_been shown we have proving

 transitivity



 itemize



 ex

 The meet operators used in the examples in Section_secdf-intro

 are set union and set_intersection They are

 both idempotent commutative and associative

 For set union the

 top_element is and

 the bottom_element is the universal_set since

 for all in

 and

 For set_intersection

 is and

 is



 For all and in

 implies therefore

 the partial_order imposed_by set union is

 Correspondingly the partial_order imposed_by set_intersection is



 That is for set_intersection

 sets with fewer elements are considered to be smaller in the

 partial_order

 However for set union

 sets with more elements are considered to be smaller in the

 partial_order To say that sets larger in size are

 smaller in the partial_order is counterintuitive however this situation

 is an unavoidable consequence of the definitions(And if we

 defined the partial_order to be instead of then the problem

 would surface when the meet was intersection although not for union)



 As_discussed in Section_secdf-intro there are

 usually many solutions

 to a set of data-flow_equations with the greatest solution (in the

 sense of the partial_order ) being the most

 precise For_example in reaching_definitions the most_precise

 among all the solutions to the data-flow_equations is the one with the

 smallest number of definitions

 which corresponds to the greatest element in the

 partial_order defined by the meet_operation union In available

 expressions the most_precise solution is the one with the largest

 number of expressions Again it is the greatest solution in the

 partial_order defined by intersection as the meet_operation

 ex



 Greatest Lower Bounds



 There is another useful relationship_between the meet

 operation and the partial ordering it imposes

 Suppose is a semilattice

 A greatest_lower bound

 (or glb) of domain elements and is an element such

 that



 enumerate











 If is any element such that and then





 enumerate

 It_turns out that the meet of and is their only greatest_lower

 bound

 To_see why let Observe that



 itemize



 because

 The proof involves

 simple uses of associativity commutativity and idempotence

 That is



 align

 gx ((xy)x)

 (x(yx))



 (x(xy))

 ((xx)y)



 (xy) g



 align



 by a similar argument



 Suppose is any element such that and

 We claim and therefore cannot be a glb of and

 unless it is also

 In proof



 Since we know so

 Since we know and therefore

 We have proven and conclude is the only glb of and





 itemize



 Joins Lub's and Lattices

 In symmetry to the glb operation on elements of a poset we may define

 the least upper_bound (or lub) of elements and to be

 that element such that and if is any

 element such that and then

 One can show that there is at most one such element if it exists



 In a true lattice there are two operations on domain elements

 the meet which we have_seen and the operator join

 denoted which gives the lub of two elements (which therefore

 must always exist in the lattice)

 We have_been discussing only semi lattices where only one of the

 meet and join operators exist

 That is our semilattices are meet semilattices

 One could also speak of join semilattices where only the join

 operator exists and in fact some literature on program analysis does

 use the notation of join semilattices

 Since traditional data-flow literature speaks of meet semilattices we

 shall do so in this_book



 Lattice Diagrams



 It often helps to draw the domain as a

 lattice_diagram

 which is a graph whose nodes are the elements of and whose

 edges are directed downward_from to if

 For_example Fig_figlattice shows the set for a reaching-definitions

 data-flow_schema where there are three definitions

 and

 Since is an edge is directed downward_from any

 subset of these three definitions to each of its supersets

 Since is transitive we conventionally omit the edge from

 to as_long as there is another path from to left in the diagram

 Thus although

 we do_not draw this edge since it is represented_by the path through

 for example



 figurehtfb

 figureuullmanalsuch9figslatticeeps

 Lattice of subsets of definitions

 figlattice

 figure



 It is also useful to note_that we can read the meet off such diagrams

 Since is the glb it is

 always the highest for which there are paths

 downward to from both and

 For_example if is and is then

 in Fig_figlattice is which makes_sense because

 the meet_operator is union

 The top_element will appear at the top of the

 lattice_diagram that is there is a path downward_from to

 each element

 Likewise the bottom_element will appear at the bottom with a path

 downward_from every element to



 Product Lattices



 While

 Fig_figlattice involves only three definitions

 the lattice_diagram of a typical program can

 be quite large

 The set of data-flow_values is the power set of

 the definitions which therefore

 contains elements if there are

 definitions in the program However whether a

 definition_reaches a program is independent of the reachability of the

 other definitions

 We may thus express the latticefootnoteIn this discussion and

 subsequently we_shall often drop the semi since lattices like the

 one under discussion do have a join or lub operator even if we do_not

 make use of itfootnote

 of definitions in terms of a product_lattice built from one simple

 lattice for each definition

 That is if there were only one definition in the program then the

 lattice would have two elements the empty_set which is the top

 element and which is the bottom_element



 Formally we may build product lattices as_follows

 Suppose and are (semi)lattices

 The product_lattice for these two lattices is defined as_follows



 enumerate



 The domain of the product_lattice is



 The meet for the product_lattice is defined as_follows

 If and are domain elements of the product_lattice

 then



 enumerate

 displayeqprod-lat-meet

 317ptstabularl





 tabular

 (eqprod-lat-meet)

 display



 It is simple to express the partial_order for the product_lattice

 in terms of the partial orders and for and

 displayeqprod-lat-poset

 317ptstabularl

 if and only if and



 tabular

 (eqprod-lat-poset)

 display

 To_see why (eqprod-lat-poset) follows from

 (eqprod-lat-meet) observe that



 (ab)(a'b')(aAa'bBb')



 So we must ask under

 what circumstances does

 That happens exactly when and

 But these two conditions are the same as and



 The product of lattices is an associative operation so one could show

 that the rules (eqprod-lat-meet) and (eqprod-lat-poset)

 extend to any number of

 lattices

 That is if we are given lattices for

 then the product of all lattices in this order has domain

 a meet_operator defined by



 center





 center

 and a partial_order defined by



 center

 if and only if

 for all

 center

 For_instance the lattice_diagram in Fig_figlattice is

 the product of three lattices each based_on a single definition

 for 2 or 3



 Height of a Semilattice



 We may learn something about the rate of convergence of a data-flow

 analysis algorithm by studying the height

 of the associated semilattice An ascending chain in a poset

 is a sequence where



 The height of a

 semilattice is the largest_number of relations in

 any ascending chain that is the height is one less_than the

 number of elements in the chain For_example the height of the reaching

 definitions semilattice for a program with definitions is



 Showing convergence of an iterative_data-flow algorithm is much

 easier if the semilattice has finite_height Clearly a lattice

 consisting of a finite set of values will have a finite_height it is

 also possible for a lattice with an_infinite number of values to have

 a finite_height The lattice used in the constant_propagation

 algorithm is one such example that we_shall examine closely in

 Section_const-prop-sect



 Transfer_Functions



 The family of transfer_functions in a data-flow_framework

 has the following properties



 enumerate



 has an identity_function such that for

 all in



 is closed_under composition that is for any two functions

 and in the function defined by

 is in



 enumerate



 ex

 In reaching_definitions

 has the identity the function where and are

 both the empty_set

 Closure under_composition was actually shown in

 Section secrd-df we repeat the argument succinctly here

 Suppose we have two functions



 center

 and



 center

 Then



 f2 (f1 (x)) G2 ((G1 (x-K1 ))-K2 )





 The right_side of the above is algebraically equivalent to



 (G2 (G1 -K2 )) (x-(K1 K2 ))



 If we let and



 then we have shown that the composition of and

 which is is of the form that makes it a member of

 If we consider available_expressions the same arguments

 used for reaching_definitions also show that

 has an identity and is closed_under composition

 ex



 Monotone Frameworks



 To make an iterative_algorithm for data-flow_analysis work we need

 for the data-flow_framework to satisfy one more condition

 We_say that a framework is monotone if when we apply

 any transfer_function

 in to two members of the first being no_greater than the

 second then the first result is no_greater than the second result



 Formally a data-flow_framework is monotone

 if



 displayeqmono1

 317ptstabularl

 tabular

 For all and in and in implies





 (eqmono1)

 display

 Equivalently monotonicity can be defined as



 displayeqmono2

 317ptstabularl

 For all and in and in





 tabular

 (eqmono2)

 display

 Equation (eqmono2)

 says_that if we take the meet of two values and then apply the result

 is never_greater than what is obtained_by applying to the values

 individually first and then meeting the results

 Because the two definitions of monotonicity seem so

 different

 they are both

 useful You will find one or the other more useful under different

 circumstances We_shall sketch a proof to show that they are indeed

 equivalent



 We_shall first assume (eqmono1) and show that

 (eqmono2) holds

 Since is the greatest_lower bound of and we know



 x_y x and x_y y



 Thus by (eqmono1)



 f(_x y_) f(_x ) and f(_x y_) f(_y )



 Since is the greatest_lower bound of and

 we have (eqmono2)



 Conversely let_us assume (eqmono2) and prove

 (eqmono1)

 We suppose and use (eqmono2) to conclude



 thus proving (eqmono1)

 Equation (eqmono2) tells_us



 f(_x y_) f(_x )_f( y_)



 But since is assumed by definition

 Thus (eqmono2) says



 f(_x )_f( x )_f( y_)



 Since is the glb of

 and we know

 Thus



 f(_x )_f( x )_f( y_) f(y)



 and (eqmono2) implies (eqmono1)



 Distributive Frameworks



 Often a framework obeys a condition stronger than (eqmono2)

 which we call the distributivity condition



 f(_x y_) f(_x )_f( y_)



 for all and in and in

 Certainly if then by idempotence

 so

 Thus distributivity implies monotonicity although the converse is not

 true



 ex

 Let and be sets

 of definitions

 in the reaching-definitions_framework

 Let be a function defined by

 for some sets of definitions and

 We can verify that the reaching-definitions_framework satisfies

 the distributivity condition by checking that



 G((y z)-K) (G(y-K))(G(z-K))



 While the equation above appears formidable consider first those

 definitions in

 These definitions are surely in the sets defined by both the left and

 right_sides

 Thus we have only to consider definitions that are not in

 In that case we can eliminate everywhere and verify the equality



 (yz)-K(y-K)(z-K)



 The latter equality is easily checked using

 a Venn diagram

 ex



 The Iterative_Algorithm for General Frameworks

 df-iterative



 We can generalize

 Algorithm_algreaching-definitions to make it work for

 a large variety of data-flow_problems



 alg

 algiterative

 Iterative solution to general data-flow_frameworks



 Each of the following



 enumerate



 A data-flow graph with specially labeled and

 nodes



 A direction of the data-flow



 A set of values



 A meet_operator



 A set of functions

 where in is the transfer_function for block

 and



 A constant value or in

 representing the boundary_condition for forward and backward

 frameworks respectively



 enumerate



 and in for each basic_block



 The algorithms for solving forward and backward_data-flow problems are

 shown in Fig_figdf-alg(a) and figdf-alg(b) respectively

 As with the familiar iterative_data-flow algorithms from

 Section_secdf-intro we compute

 and for each node by successive approximation

 alg



 figurehtfb



 center

 tabularr_l

 1)



 2) for_(each basic_block other_than entry)





 3)_while (changes to any_occur)



 4) for_(each basic_block other_than entry)



 5)



 6)







 tabular

 center



 center

 (a) Iterative_algorithm for forward_data-flow problems

 center



 center

 tabularr_l

 1)



 2) for_(each basic_block other_than exit)





 3)_while (changes to any_occur)



 4) for_(each basic_block other_than exit)



 5)



 6)







 tabular

 center



 center

 (b) Iterative_algorithm for backward_data-flow problems

 center



 Forward and backward versions of the iterative_algorithm

 figdf-alg

 figure



 It is possible to write the forward and backward versions of

 Algorithm_algiterative so that a function implementing

 the meet_operation is a parameter as is a function that implements the

 transfer_function for each block

 The flow_graph itself and the boundary value are also

 parameters

 In this way the compiler implementor can avoid recoding the basic

 iterative_algorithm for each data-flow_framework used by the

 optimization_phase of the compiler



 We can use the abstract framework discussed so_far to prove a number of

 useful properties of the iterative_algorithm



 enumerate



 If Algorithm_algiterative converges the result is a

 solution to the data-flow_equations



 If the framework is monotone then the solution found is the maximum

 fixedpoint (MFP) of the data-flow_equations

 A maximum_fixedpoint is a solution with the property that in any

 other solution the values of and are the

 corresponding values of the MFP



 If the semilattice of the framework is monotone and of finite_height then

 the algorithm is guaranteed to converge



 enumerate



 We_shall argue these points assuming that the framework is forward

 The case of backwards frameworks is essentially the same

 The first property is easy to show

 If the equations are not

 satisfied by the time the while-loop ends

 then there will be at_least one change to an (in the forward

 case) or (in the backward case) and we must go_around the loop

 again



 To prove the second property we first show that the values taken on

 by and for any can only decrease (in the

 sense of the relationship for lattices) as the algorithm

 iterates This claim can be proven by induction



 The base case is to show

 that the value of and after the first iteration is

 not greater_than the initialized value This statement is trivial because

 the and for all

 are initialized_with



 Assume that after the th

 iteration the values are all no_greater than those after the st

 iteration

 and show the same for iteration compared with iteration

 In line_(5) of Fig_figdf-alg(a)



 B_P a predecessor of B_P



 Let_us use the notation

 and to

 denote the values and after iteration

 Assuming

 we know that

 because of the properties of the meet_operator

 Next examine line_(6)



 B_fB(B)



 Since

 we have



 by monotonicity



 Note_that every change observed for values of and is

 necessary to satisfy the equation The meet operators return the

 greatest_lower bound of their inputs and the transfer_functions return

 the only solution that is consistent_with the block itself and

 its given input Thus if the iterative

 algorithm_terminates the result must have values that are at_least as

 great as the corresponding values in any other solution that is the

 result of Algorithm_algiterative is the MFP of the equations



 Finally consider the third point

 where the data-flow_framework has finite_height

 Since the values of every and decrease

 with each change and the algorithm stops if at some round nothing changes

 the algorithm is guaranteed to converge after a number of rounds no

 greater_than the product of the height of the framework and the number

 of nodes of the flow_graph



 Meaning of a Data-Flow Solution

 df-semantics-subsect



 We_now know that the solution found using the iterative_algorithm is

 the maximum_fixedpoint but what does the result represent from a

 program-semantics point of view

 To understand the solution of a data-flow_framework

 let_us first describe what an ideal_solution to the

 framework would be We show that the ideal cannot be obtained in

 general but that Algorithm_algiterative

 approximates the

 ideal conservatively



 The Ideal Solution



 Without loss of generality we_shall assume for now that the data-flow

 framework of interest is a forward-flowing problem Consider the

 entry point of a basic_block The ideal_solution begins by finding

 all

 the possible_execution paths_leading from the program entry to

 the beginning of A path is possible only if there is some

 computation of the program that follows exactly that path

 The ideal_solution would then compute the data-flow value at

 the end of each possible

 path and apply the meet_operator to these values

 to find their greatest_lower bound Then

 no execution of the program can produce a

 smaller value for that program point In_addition the bound is

 tight there is no_greater data-flow value that is a glb for the value

 computed along every possible path to in the flow_graph



 We_now try to define the ideal_solution more formally

 For each block in a flow_graph

 let be the transfer_function for

 Consider any path



 P entry B1B2Bk-1 Bk



 from the initial_node to some block The

 program path may have cycles so one basic_block may appear several

 times on the path Define the

 transfer_function for



 to be the composition of

 Note_that is not part of the composition reflecting

 the fact that this path is taken to reach the beginning of block

 not its end The data-flow value created by executing this

 path is thus where is the

 result of the constant transfer_function representing the initial_node

 The ideal result for block is thus



 B_P a possible path from

 entry to B fP (ventry)



 We claim that in terms of the lattice-theoretic partial_order

 for the framework in question



 itemize



 Any answer that is greater_than is incorrect



 Any value smaller_than or equal to the ideal is conservative ie

 safe



 itemize

 Intuitively the closer the

 value to the ideal the more_precise it isfootnoteNote that in

 forward problems the value is what we

 would like

 to be

 In backward problems which we do_not discuss here we would define

 to be the ideal value of

 footnote

 To_see why solutions must_be the ideal_solution note_that

 any solution greater_than for any block could be obtained_by

 ignoring some execution_path that the program could take and we cannot

 be_sure that there is not some effect along that path to invalidate any

 program improvement we might make based_on the greater solution

 Conversely any solution less_than can be_viewed as including

 certain paths that either do_not exist in the flow_graph or that exist

 but that the program can never follow

 This lesser solution will allow only transformations that are correct

 for all possible_executions of the program but may forbid some

 transformations that would permit



 The Meet-Over-Paths Solution



 However as discussed in Section_secopt-sources finding all

 possible_execution paths is undecidable We must therefore

 approximate In the data-flow_abstraction we assume that

 every_path in the flow_graph can be taken

 Thus we can define the

 meet-over-paths solution for to be



 B_P a path from

 entry to B fP (ventry)



 Note_that as for the solution gives values for

 in forward-flow frameworks

 If we were to consider backward-flow frameworks then we would think of

 as

 a value for



 The paths considered in the solution are a superset of all the

 paths that are possibly executed Thus the solution meets

 together not only the data-flow_values of all the executable

 paths but also additional values associated_with the paths that

 cannot possibly be executed Taking the meet of the ideal_solution plus

 additional terms cannot create a solution larger_than the ideal

 Thus for all we have or we

 say simply that



 The Maximum Fixedpoint Versus the Solution



 Notice_that in the solution

 the number of paths considered is still unbounded

 if the flow_graph contains cycles Thus the definition does_not lend

 itself to a direct algorithm

 The iterative_algorithm certainly

 does_not first find all the paths

 leading to a basic_block before applying the meet_operator

 Rather



 enumerate



 The iterative_algorithm visits basic_blocks not_necessarily

 in the order of

 execution



 At each confluence point

 the algorithm applies the meet_operator

 to the data-flow_values obtained so_far Some of these values

 used were_introduced artificially in the initialization process not

 representing the result of any execution from the beginning of the

 program



 enumerate

 So what is the relationship_between the solution and the

 solution produced_by Algorithm_algiterative



 We first discuss the order in which the nodes are visited In

 an iteration we

 may visit a basic_block before having visited its

 predecessors If the predecessor is the node

 would have_already been initialized_with the

 proper constant value Otherwise it has_been initialized to

 a value no smaller_than the final answer By monotonicity the result

 obtained_by using as input is no smaller_than the desired

 solution In a sense we can think of as representing no

 information



 figurehtb



 figureuullmanalsuch9figsmop-fgeps



 Flow_graph illustrating the effect of early meet-over-paths

 mop-fg-fig



 figure



 What is the effect of applying the meet_operator early

 Consider the simple example of Fig mop-fg-fig and

 suppose we are_interested in the value of

 By the definition of



 B4 ((fB3 fB1) (fB3 fB2))(ventry)



 In the iterative_algorithm if we visit the nodes in the order

 then



 B4 fB3 ((fB1(ventry)fB2(ventry)))



 While the meet_operator is applied at the end in the definition

 of the iterative

 algorithm applies it early The answer is the same only if the data-flow

 framework is distributive If the data-flow_framework is

 monotone but not_distributive we still have

 Recall that in general a solution is safe (conservative)

 if for all blocks Surely





 We_now provide a quick sketch of why in general the solution provided

 by the iterative_algorithm is

 always safe An easy induction on shows that

 the values obtained after iterations

 are smaller_than or equal to the meet_over all paths of length or less

 But the iterative_algorithm terminates only if it arrives at the

 same answer as would be obtained_by iterating an_unbounded number of times

 Thus the

 result is no_greater than the solution

 Since and we know that

 and therefore the solution provided by the iterative_algorithm is

 safe



 hsexer

 In Section df-iterative we argued that if the framework has finite

 height then the iterative_algorithm converges Here is an example

 where the framework does_not have finite

 height and the iterative_algorithm does_not converge

 Let the set of values be the nonnegative real numbers and let the

 meet_operator be the minimum There_are three transfer_functions



 itemize



 The identity



 half that is the function



 one that is the function

 itemize

 The set of transfer_functions is these three plus the functions

 formed_by composing them in all possible ways



 itemize



 a)

 Describe the set



 b)

 What is the relationship for this framework



 c)

 Give an example of a flow_graph with assigned transfer_functions such

 that Algorithm_algiterative does_not converge



 d)

 Is this framework monotone Is it distributive



 itemize

 hsexer



 hexer

 We argued that Algorithm_algiterative converges if the framework

 is monotone and of finite_height Here is an example of a framework

 that shows monotonicity is essential finite_height is not enough

 The domain is

 the meet_operator is min and

 the set of functions is only the identity () and the switch

 function () that swaps 1 and 2



 itemize



 a)

 Show that this framework is of finite_height but not monotone



 b)

 Give an example of a flow_graph and assignment of transfer_functions so

 that Algorithm_algiterative does_not converge



 itemize

 hexer



 hexer

 Let be the meet_over all paths of length or less from

 the entry to block Prove that after iterations of

 Algorithm_algiterative Also show that

 as a consequence if Algorithm_algiterative converges then it

 converges to something that is the solution

 hexer



 hexer

 Suppose the set of functions for a framework are all of gen-kill

 form

 That is the domain is the power set of some set and

 for some sets and

 Prove that if the meet_operator is either (a) union or (b) intersection

 then the framework is distributive

 hexer

 Introduction

 secintro



 TODO this_section is old and needs rewriting



 Points-to analysis is a fundamental technique whose results are useful

 for program optimizations program comprehension tools and software

 engineering aids For many applications such_as auditing software for

 programmer errors or security vulnerabilities it is of great

 importance that accurate analysis that work on large_programs be

 available



 Algorithms that have_been demonstrated to scale to large_programs are

 mostly flow-insensitive and context-insensitive The first algorithms

 demonstrated to be scalable were unification-basedsteensgaard96pointsto where pointers are

 either unaliased or pointing to the same set of locations A number

 of scalable inclusion-based algorithms were proposed

 recentlyheintze01ultrafastwhaley02saslhotak03berndl03 In

 inclusion-based analyses two aliased pointers may point to

 overlapping but different sets of locations In_particular Berndl et

 al for Java programs showed that inclusion-based_points-to analysis

 can be computed efficiently using binary_decision diagrams

 (BDDs)berndl03 They show that by_taking advantage of the

 built-in optimizations in available BDD the points-to_analysis can be

 expressed simply and directly



 Inspired by the work by Berndl et_al we have developed a context-sensitive mostly flow-insensitive inclusion-based_pointer

 alias_analysis based_on BDDs



 Context-Sensitive_Analysis

 By differentiating between the contexts in which a function is called

 a context-sensitive_analysis produces much more_precise results but

 is also much more_expensive A calling_context of a function is

 defined by the invocation_sites on the call stack at the time the

 function is executed Since there can be an_unbounded number of

 possible calling_contexts in the presence of recursive_cycles static

 analysis imposes an abstraction on the calling_contexts to reduce them

 to a finite number



 Shivers proposed the concept of k-limiting where calling_contexts are

 distinguished by only the last call_sites pushed on the

 stackshivers91 Emami et_al proposed a more_precise model

 based_on invocation graphsMEmami94 In the absence of

 recursive_calls the analysis computes the points-to_relations for

 each function under all possible calling_contexts where the calling

 context of an function instance is defined as the sequence of call

 sites pushed_onto its call stack Recursive cycles are handled by

 collapsing them to a single_node and modeling it by its fixed point

 solution Due to the exponentially_many contexts present in the

 solution results have_been obtained only for small programs



 More recently introduced context-sensitive_pointer alias_analyses are

 mostly elimination-based

 algorithmsfahndrich00scalableOOPSLA99WhaleyRWilson95 The

 main idea is to first compute for each function a summary of its

 effects on points-to_relations This is performed in a bottom-up

 manner callees' summaries are applied to compute the callers'

 summaries and fixed-point computation is used to compute summaries of

 functions in each strongly_connected component of a call_graph After

 the summaries are computed the points-to result of given contexts can

 be computed on demand This_approach avoids the need of storing the

 result of all the exponentially_many contexts all at once however

 storing just the summaries has demonstrated still to be too_expensive



 Our Approach



 Reduced order binary_decision diagrams (ROBDDs) have_been used

 extensively in model checking (In the rest of the paper we will

 simply refer to ROBDDs as BDDs) Berndl also demonstrated that

 BDDs are efficient for computing and storing context-insensitive

 points-to relationsberndl03



 The goal of our research is to investigate if BDDs can be used to

 perform context-sensitive point alias_analysis as_well as store the

 context-sensitive_results in a convenient form that supports further

 analysis Queries such_as find all functions and contexts that

 access a certain object require that we examine every function under

 every possible context and can be expensive in an elimination-based

 approach that computes context-sensitive answers on demand Thus we

 have chosen to explore using_BDDs to compute and represent

 all the results explicitly



 Since there are exponentially_many contexts it is clear that this

 approach cannot scale to arbitrary programs indefinitely On the

 other_hand we also note_that programmers cannot produce arbitrarily

 large_programs either The size of programs currently no larger_than

 tens of millions of lines of code will continue to grow but much

 more slowly than the exponential growth in computational power we witness

 in commodity computers Furthermore large systems are built out of

 modular components each of which should have a relative simple and

 clean interface We believe that it suffices to perform detailed

 precise analysis on these (large) modules independently and summarize

 its interfaces



 We show in this_paper that the representational power of BDD using

 today's computers is capable of handling programs with hundreds of

 thousands of lines of code This is powerful enough to handle many

 existing programs and components in very_large programs By

 composing the analysis results from different components and by using

 faster computers we expect that this basic approach can

 be used to handle all programs in the future



 Algorithm Overview



 Our algorithm starts by using a couple of pre-processing steps to

 prepare for the context-sensitive_pointer analysis These steps are

 performed with the purpose of improving the precision and efficiency

 of the analysis We first analyze local pointers in a method

 flow-sensitively and create an intraprocedural summary of the effect

 of each method on points-to_relations This is especially important

 for Java because aside from the primitive types all local_variables

 in Java are pointers to heap_objects and the same stack locations are

 used in the bytecode to point to different objects within the same

 procedure Second we use a context-insensitive_pointer alias

 analysis based also on BDDs to first compute a call_graph of the

 program Much more_precise than class_hierarchy analysis this step

 greatly improves the precision of the points-to_analysis results

 The call_graph so obtained is then used in our context-sensitive_analysis



 In our context-sensitive_points-to analysis functions are given

 context IDs one for each context under which they will be

 evaluated Essentially we reduce the context-sensitive_analysis of

 the original graph call to a context-insensitive_analysis for an

 expanded_call graph where each function in the source_program is

 represented as many_times as it has contexts The context IDs are

 chosen such that multiple contexts of the same function can be

 analyzed efficiently and their results can be stored compactly This

 also requires the introduction of a new BDD primitive operation



 Contributions

 The contributions of this_paper include

 itemize

 This_paper presents the first context-sensitive_inclusion-based

 points-to_analysis that scales to hundreds of thousands of

 Java_bytecodes It is also the first context-sensitive_pointer analysis

 based_on BDDs



 Unlike most other context-sensitive_analysis our algorithm produces

 the context-sensitive_results all at once and encode them

 compactly in BDDs The analysis is fast due to our optimizations on

 numbering contexts and numbering variables



 We demonstrate the usefulness of having all the context-sensitive

 results available by implementing an escape_analysis in just a small

 number of BDD queries





 We have implemented the algorithm presented in this_paper and applied

 it to 11 java programs Over two hundred thousands bytecodes were

 analyzed for the larger of the applications J2EE server and javafig

 For the largest program over contexts were

 analyzed in total but the analysis took less_than 6 minutes on a 2GHz

 Pentium 4 machine with 2GB of memory

 itemize



 Paper Organization

 The rest of the paper is organized as_follows

 Section secbddpointsto first provides the background on how

 BDDs can be used to compute context-insensitive_points-to analysis

 Section seccallgraph describes the preprocessing steps in our

 algorithm to prepare for the context-sensitive_analysis We describe

 how we model the intraprocedural effects of a method on points-to

 relations represent them in BDDs and describe_how we find a call

 graph for a program using a context-insensitive_pointer alias

 analysis Section secinvocationgraph then explains the

 abstraction we use to model calling_contexts

 Section secalgorithm describes our context-sensitive_pointer

 alias_analysis Besides showing the algorithm we also

 discuss_how we can use BDD queries efficiently to perform further

 analysis based_on the pointer_alias analysis results We discuss

 related work in Section secrelated and conclude in

 Section secconclusion

 Loops in Flow_Graphs

 loops-sect



 In our_discussion so_far loops are not handled specially but are treated

 just like any other kind of

 control_flow However loops are important because

 programs_spend most of their time executing

 them and optimizations that improve the performance of loops can

 have a significant impact

 Thus it is essential that we

 identify loops and treat_them specially



 Loops also affect the running

 time of program analyses If a program does

 not contain any loops we can obtain the answers to data-flow_problems

 by making just one

 pass_through the program For

 example a forward_data-flow problem

 can be_solved by visiting all the

 nodes once in topological_order



 In this_section we introduce the following concepts dominators

 depth-first_ordering

 back

 edges graph depth and reducibility

 Each of these is needed for our

 subsequent discussions on finding loops and the speed of

 convergence of

 iterative data flow analysis



 Dominators

 secdominance



 We_say node of a flow_graph

 dominates

 node written

 if every_path from the entry_node of the flow_graph to goes_through

 Note_that under this definition every node_dominates itself



 ex

 Consider the flow_graph of Fig_figdom-fg with entry_node 1 The

 entry_node dominates every node (this statement is true for every flow

 graph) Node 2 dominates only

 itself since control can reach any other node along a path that

 begins_with



 Node 3 dominates all but 1 and 2 Node_4 dominates all but 1_2 and 3

 since all paths from 1 must begin_with

 or



 Nodes 5 and 6 dominate only themselves since flow of control can

 skip around either by going_through the other Finally 7 dominates

 7_8 9 and 10 8 dominates 8_9 and 10_9 and 10 dominate only

 themselves

 ex



 figurehtb

 figureuullmanalsuch9figsdom-fgeps

 A flow_graph

 figdom-fg

 figure



 A useful way of presenting

 dominator information is in a tree

 called the

 dominator_tree

 in which the entry_node is the root and each node

 dominates only its descendants in the tree

 For_example Fig figdom-tree shows the dominator_tree

 for the flow_graph of Fig_figdom-fg

 figurehtb

 figureuullmanalsuch9figsdom-treeeps

 Dominator tree for flow_graph of Fig_figdom-fg

 figdom-tree

 figure



 The existence of dominator trees follows from a property

 of dominators

 each node has a unique

 immediate_dominator

 that is the last dominator of on any path from

 the entry_node to

 In terms of the relation

 the immediate_dominator

 has that property that

 if and

 then



 We_shall give a simple algorithm for computing the dominators of every

 node in a flow_graph based_on the principle that if

 are all the predecessors of and then

 if and only if for each This problem can

 be formulated as a forward_data-flow analysis The data-flow_values

 are sets of basic_blocks A node's set of dominators other_than

 itself is the

 intersection of the dominators of all its_predecessors thus the meet

 operator is set_intersection The transfer_function for block

 simply adds itself to the set of input nodes

 The boundary

 condition is that the entry_node dominates itself Finally

 the initialization of the interior_nodes is the universal_set



 alg

 algdom

 Finding_dominators



 A flow_graph with set of nodes set of edges and

 entry_node entry



 The relation



 Find the solution to the data flow problems whose parameters are shown

 in Fig figdom

 Finding_dominators using this data-flow algorithm is

 efficient Nodes in the graph need to be visited only a few

 times as will be discussed in

 Section secconvergence-speed









 alg



 figurehtb

 center

 tabularl_l

 Dominators



 Domain sets of basic_blocks



 Direction_Forwards



 Transfer_function



 Boundary



 Meet_()



 Equations







 Initialization



 tabular

 center

 A data-flow algorithm for computing dominators

 figdom

 figure



 Properties of the dom Relation

 A key observation about dominators is that if we take any_acyclic path

 from the entry to node then all the dominators of appear along

 this path and moreover they must appear in the same order along

 any such path

 To_see why suppose there were one acyclic_path to along which

 dominators and appeared in that order and another path

 to along which preceded

 Then we could follow to and to thereby avoiding

 altogether Thus would not really dominate



 This reasoning allows_us to prove that dom is transitive if

 and then

 Also dom is antisymmetric it is never possible that both

 and hold if

 Moreover if and are two dominators of then either

 or must hold

 Finally it follows that each node except the entry must have a unique

 immediate_dominator - the dominator that appears closest to

 along any_acyclic path from the entry to



 ex

 exdom-ex

 Let_us return to the flow_graph of Fig_figdom-fg

 and suppose the for-loop of lines_(4) through_(6) in Fig figdf-alg

 visits the nodes

 in numerical order

 Let be the set of nodes in

 Since 1 is the entry_node

 was assigned at line_(1)

 Node 2 has only 1 for a predecessor

 so

 Thus is set to 1_2

 Then node 3 with predecessors 1_2 4 and 8 is considered

 Since all the interior_nodes are initialized_with the universal_set





 D(3) 3 (11212 10

 12 10) 13



 The remaining calculations are shown in Fig dom-comp-fig

 Since these values do_not change in the second iteration through the

 outer_loop of lines_(3) through_(6) in Fig_figdf-alg(a)

 they are the final answers to the dominator problem

 ex



 figurehtfb

 align

 D(4) 4 (D(3)D(7)) 4 (13 12 10)134



 D(5) 5 D(4) 5 134 1345



 D(6) 6 D(4) 6 134 1346



 D(7) 7 (D(5)D(6)D(10))



 7 (1345134612 10) 1347



 D(8) 8 D(7) 8 1347 13478



 D(9) 9 D(8) 9 13478 134789



 D(10) 10 D(8) 10 13478 1347810

 align



 Completion of the dominator calculation for Example exdom-ex

 dom-comp-fig



 figure



 Depth-First Ordering

 secdfs



 As introduced in Section dfs-subsect

 a depth-first_search of a graph visits all the nodes in the

 graph once by starting_at the entry_node and visiting the nodes as

 far away from the entry_node as quickly as possible The route of

 the search in a depth-first_search forms a depth-first_spanning

 tree (DFST) Recall from Section dfs-subsect

 that a preorder_traversal visits a node before

 visiting any of its_children which it then visits recursively in

 left-to-right_order Also a postorder_traversal visits a

 node's children recursively in left-to-right_order

 before_visiting the node itself



 There is one more variant ordering

 that is important for flow-graph analysis a depth-first

 ordering is the reverse of a postorder_traversal That is in a

 depth-first_ordering we visit a node then traverse its rightmost

 child the child to its left and so on

 However before we build the tree for the flow_graph we have choices as

 to which successor of a node becomes the rightmost child in the tree

 which node becomes the next child and so on

 Before we give

 the algorithm for depth-first_ordering let_us consider an example



 ex

 exdfs

 One possible depth-first_presentation

 of the flow_graph in Fig_figdom-fg

 is illustrated in Fig_figdepth-first

 Solid edges form the tree dashed edges

 are the other edges of the flow_graph

 A depth-first_traversal of the tree is given by



 then back to 8

 then to 9

 We go back to 8 once more retreating to 7 6 and 4

 and then forward to 5

 We retreat from 5 back to 4 then back to 3 and 1

 From 1 we go to 2 then retreat from 2 back to 1 and we have

 traversed the entire tree



 The preorder sequence for the traversal is thus



 1 3 4 6_7 8 10_9 5 2





 The postorder_sequence for the traversal of the tree in

 Fig_figdepth-first is



 10_9 8 7 6 5 4 3 2 1





 The depth-first_ordering which is the reverse of the postorder

 sequence is



 1_2 3 4_5 6_7 8_9 10



 ex



 figurehtb

 figureuullmanalsuch9figsdepth-firsteps

 A depth-first_presentation of the flow_graph in Fig_figdom-fg

 figdepth-first

 figure



 We_now give an algorithm that finds a depth-first_spanning

 tree and a depth-first_ordering of a graph

 It is this algorithm that finds the

 DFST in Fig_figdepth-first from Fig_figdom-fg



 alg

 algdfst

 Depth-first spanning_tree and depth-first_ordering



 A flow_graph



 A DFST of and an ordering of the nodes of



 We use the recursive_procedure ) of Fig_figdfs-alg

 The algorithm initializes all nodes of to unvisited then

 calls ) where is the entry

 When we call ) we first mark visited to avoid

 adding to the tree twice

 We use to count from the number of nodes of down to 1

 assigning depth-first numbers to nodes as we go

 The set of edges forms the depth-first_spanning

 tree for

 alg



 figurehtb

 center

 tabularl

 void



 mark visited



 for_(each successor of )



 if_( is unvisited)



 add_edge to































 set of edges



 for_(each node of )



 mark unvisited



 number of nodes of



 )





 tabular

 center

 Depth-first search algorithm

 figdfs-alg

 figure



 ex

 exdfs-alg

 For the flow_graph in Fig_figdepth-first

 Algorithm_algdfst sets

 to 10 and begins the search by calling (1)

 The rest of the execution sequence is

 shown in

 Fig figdfs-execution

 ex



 figure

 tabularl p32in

 Call

 Node 1 has two_successors Suppose is considered first

 add_edge to





 Call

 Add edge to





 Call

 Node_4 has two_successors 4 and 6

 Suppose is considered first add

 edge to





 Call

 Add to





 Call

 Node 7 has two_successors 4 and 8

 But 4 is already_marked visited by so do_nothing

 when

 For add_edge to





 Call

 Node 8 has two_successors 9 and 10

 Suppose is considered first

 add_edge





 Call

 10 has a successor 7 but 7 is already_marked visited Thus

 completes by setting

 and





 Return to

 Set and add_edge to





 Call

 The only successor of 9 node 1 is already visited so set

 and





 Return to

 The last successor of 8 node 3 is visited so do

 nothing for

 At this point all successors of 8 have_been considered so set

 and





 Return to

 All of 7's successors have_been considered so set and







 Return to

 Similarly 6's successors have_been considered so set and







 Return to

 Successor 3 of 4 has_been visited but 5 has not so

 add to the tree





 Call

 Successor 7 of 5 has_been visited

 thus set and





 Return to

 All successors of 4 have_been considered set and





 Return to

 Set and





 Return to

 2 has not been_visited yet so add to





 Call

 Set





 Return to

 Set and

 tabular

 Execution of Algorithm_algdfst on the flow_graph

 in Fig_figdfs-alg

 figdfs-execution

 figure



 Edges in a Depth-First Spanning Tree

 dfs-edges-subsect



 When we construct a DFST for a flow_graph the edges of the

 flow_graph fall into three categories

 enumerate

 There_are edges called advancing_edges that go from a node

 to a proper descendant of in the tree All edges in the DFST

 itself are advancing_edges There_are no other advancing_edges in

 Fig_figdepth-first but for example if were an edge it would

 be in this category



 There_are edges that go from a node to an_ancestor of in the

 tree (possibly to itself) These edges we_shall term retreating_edges For_example





 and are the

 retreating_edges in Fig_figdepth-first



 There_are edges such that neither_nor is an_ancestor of

 the other in the DFST

 Edges and are the only such examples in

 Fig_figdepth-first We call these edges

 cross_edges

 An_important property of cross_edges is that if we draw the

 DFST so children of a node are drawn from left to right

 in the order in which they_were added to the tree

 then all cross_edges travel from right to left

 enumerate



 It should be noted that is a retreating_edge if and only

 if To_see why note_that if is a descendant of in the

 DFST then terminates_before so

 Conversely if

 then terminates_before

 or But must

 have begun before if

 there is an edge

 or else the fact that is a

 successor of would have made a descendant of in the DFST

 Thus the time is active is a subinterval of the time

 is active from which it follows that is an

 ancestor of in the DFST



 Back_Edges and Reducibility



 A back_edge is an edge whose head_dominates its_tail



 For any flow_graph every back_edge is retreating but not every

 retreating_edge is a back_edge A flow_graph is said to be reducible

 if all its retreating_edges in any depth-first_spanning tree are also back

 edges In other_words if a graph is reducible then all the DFST's

 have the same set of retreating_edges and those are exactly the back_edges in

 the graph If the graph is nonreducible (not reducible)

 however all the back_edges

 are retreating_edges in any DFST but each DFST may have additional

 retreating_edges that are not back_edges These retreating_edges may

 be different from one DFST to another Thus if we remove all the

 back_edges of a flow_graph and the remaining graph is cyclic then the

 graph is nonreducible and conversely



 Why Are Back_Edges Retreating Edges

 Suppose is a back_edge ie its head_dominates its_tail

 The sequence of calls of the function in Fig_figdfs-alg

 that lead to node must_be a path in the flow_graph

 This path must of course include any dominator of

 It follows that

 a call to must_be open when is called

 Therefore is already in the tree when is added to the tree and

 is added as a descendant of

 Therefore must_be a retreating_edge



 Flow graphs that occur in practice are almost_always reducible

 Exclusive use of structured flow-of-control_statements such_as

 if-then-else while-do continue and break statements

 produces programs whose flow_graphs are always reducible

 Even programs_written using goto statements often turn out to be

 reducible as the programmer logically thinks in terms of loops and

 branches



 ex

 exreducible

 The flow_graph of Fig_figdom-fg is reducible The retreating

 edges in the graph are all back_edges that is their heads

 dominate_their respective tails

 ex



 ex

 exback-edge

 Consider the flow_graph of Fig_figirreducible whose initial

 node is 1

 Node 1 dominates nodes 2 and 3 but 2 does_not dominate 3 nor vice-versa

 Thus this flow_graph has no back_edges since no

 head of any edge dominates its_tail

 There_are two possible depth-first_spanning trees depending_on whether

 we choose to call or first from

 In the first case edge is a retreating_edge but not a back

 edge in the second case is the retreating-but-not-back

 edge

 Intuitively the reason this flow

 graph is not reducible is that the cycle 2-3 can be entered at

 two different places nodes 2 and 3

 ex



 figurehtb

 figureuullmanalsuch9figsirreducibleeps

 A nonreducible_flow graph

 figirreducible

 figure



 Depth of a Flow Graph

 fg-depth-subsect



 Given a depth-first_spanning tree for the graph

 the depth is the largest_number of retreating_edges on any cycle-free_path

 We can prove the depth is never_greater than what one would

 intuitively call the depth of loop nesting in the flow_graph If a

 flow_graph is reducible we may replace retreating by back in

 the definition of depth since the retreating_edges in any DFST

 are exactly the back_edges The notion of depth then becomes

 independent of the DFST actually chosen and we may truly speak of the

 depth of a flow_graph rather_than the depth of a

 flow_graph in connection_with one of its depth-first_spanning trees



 ex

 exdeepest-path

 In Fig_figdepth-first

 the depth is 3 since there is a

 path



 10 7 4 3



 with three retreating_edges

 but no cycle-free_path with four or_more retreating_edges

 It is a coincidence that the deepest_path here has only

 retreating_edges in general we may have a mixture of retreating

 advancing and cross_edges in a deepest_path

 ex



 Natural Loops

 secnatural-loops



 Loops can be specified in a source_program in many different_ways

 they can be written as for-loops while-loops or

 repeat-loops they can even be defined using labels and goto statements From a

 program-analysis point of view it does_not matter_how the loops appear

 in the source code

 What matters is whether they have the properties that enable

 easy optimization In_particular we care_about whether a loop

 has a single entry_node if it does compiler analyses can assume

 certain initial conditions to hold at the beginning of each iteration

 through the loop

 This opportunity

 motivates the need for the definition of a natural_loop



 A natural_loop is defined by two essential properties



 enumerate



 It must have a single entry_node

 called the header

 This entry_node dominates all nodes in the loop

 or it would not be the sole entry to the loop



 There must_be a back_edge that enters the loop header Otherwise

 it is not possible for the flow of control to return to the header

 directly from the loop ie there really is no loop



 enumerate



 Given a back_edge we define the

 natural_loop

 of the edge to be plus the set of nodes

 that can reach without_going

 through

 Node is the

 header

 of the loop



 alg

 algloops

 Constructing the natural_loop of a back_edge



 A flow_graph and a back_edge



 The set consisting of all nodes in the natural_loop

 of



 Let be Mark as visited so that the

 search does_not reach beyond Perform a

 depth-first_search on the reverse control-flow_graph starting_with node

 Insert all the nodes visited in this search into This procedure

 finds all

 the nodes that reach without_going through

 alg



 ex

 In Fig_figdom-fg there are five back_edges those

 whose heads_dominate their tails







 and



 Note_that these

 are exactly the edges that one would think of as forming loops

 in the flow_graph



 Back_edge has natural_loop since 8 and 10 are

 the only nodes that can reach 10 without_going through 7

 Back_edge has a natural_loop consisting of

 and therefore contains the loop of

 We thus assume the latter is an inner_loop contained inside the former



 The natural_loops

 of back_edges and have the same header node 3

 and

 they also happen to have the same set of nodes

 We_shall therefore combine these two

 loops as one

 This loop contains the two smaller loops discovered earlier



 Finally the edge has as its natural_loop the entire_flow

 graph and therefore is the outermost_loop

 In this example the four loops are nested_within one another

 It is typical however to have two loops

 neither of which is a subset of the other

 ex



 In reducible_flow graphs since all retreating_edges are back_edges

 we can associate a natural_loop with each retreating_edge That statement

 does_not hold for nonreducible graphs For_instance

 the nonreducible_flow graph in

 Fig_figirreducible has a cycle consisting of nodes 2 and 3

 Neither of the edges in the cycle is a back_edge so this cycle does_not

 fit the definition of a natural_loop We do_not identify the cycle as

 a natural_loop and it is not optimized as such This situation is

 acceptable because our loop analyses can be made simpler by assuming

 that all loops have single entry nodes

 and nonreducible programs are rare in practice anyway



 By considering only natural_loops as

 loops we have the useful property that unless two loops have the

 same header they are either_disjoint or one is

 nested_within the other Thus we have a natural notion of innermost_loops loops that contain no other loops



 When two natural_loops have the same header as in

 Fig_figcommon-header it is hard to tell which is the inner

 loop Thus we_shall assume that when two natural_loops have the same

 header and neither is properly_contained within the other

 they are combined and

 treated_as a single loop



 figurehtb

 figureuullmanalsuch9figscommon-headereps

 Two loops with the same header

 figcommon-header

 figure



 ex

 The natural_loops of the back_edges and

 in Fig_figcommon-header are and

 respectively

 We_shall combine them into a single loop



 However were there another back_edge in

 Fig_figcommon-header its natural_loop would be a

 third loop with header 1

 This set of nodes is properly_contained within so it

 would not be_combined with the other natural_loops but rather treated

 as an inner_loop nested_within

 ex



 Speed of Convergence of Iterative Data-FlowAlgorithms

 secconvergence-speed



 We are now_ready to discuss the speed of convergence of iterative

 algorithms As_discussed in Section df-iterative the maximum

 number of iterations the algorithm may take is the product of the

 height of the lattice and the number of nodes in the flow_graph For

 many data-flow_analyses it is possible to order the evaluation such

 that the algorithm_converges in a much_smaller number of iterations

 The property of interest is whether all events of significance at a node

 will be propagated to that node along some_acyclic path Among the data

 flow analyses discussed so_far reaching_definitions available

 expressions and live_variables have this property but constant

 propagation does_not More_specifically



 itemize



 If a definition is in then there is some

 acyclic_path from the block containing to such that

 is in the 's and 's all along that path



 If an expression is not available at the entrance to

 block then there is some_acyclic path that demonstrates that fact

 either the path is from the entry_node and includes no statement that

 kills or generates or the path is from a block that

 kills and along the path there is no_subsequent generation of



 If is live_on exit from block

 then there is an acyclic_path from to a use of along which there

 are no definitions of



 itemize

 You_should check that in each of these cases paths with

 cycles add nothing

 For_example if a use of is reached from the end of block

 along a path with a cycle we can eliminate that cycle to find

 a shorter path along which the use of is still reached from





 In_contrast constant_propagation does_not have this property

 Consider a simple program that has one loop containing a basic_block

 with statements

 verbatim

 L a b

 b_c

 c 1

 goto_L

 verbatim

 The first time the basic_block is visited is found to have

 constant value 1 but both and are undefined

 Visiting the basic_block the second time we find that and

 have constant values 1 It takes three visits of the basic

 block for the constant value 1 assigned to to reach



 If all useful information propagates along acyclic_paths we have an

 opportunity to tailor the order in which we visit nodes in iterative

 data-flow algorithms so that after relatively few passes_through the

 nodes we can be_sure information has passed along all the acyclic

 paths



 Recall from Section dfs-edges-subsect

 that if is an edge then the

 depth-first number of is less_than that of only when the

 edge is a retreating_edge

 For forward_data-flow problems it is desirable to visit the nodes

 according to the depth-first_ordering Specifically we modify the

 algorithm in Fig_figdf-alg(a) by_replacing line_(4) which visits

 the basic_blocks in the flow_graph with



 center

 for_(each block other_than entry in depth-first order)



 center



 ex

 Suppose we have a path along which a definition propagates such_as



 3_5 19_35 16_23 45

 4 10_17



 where integers represent the depth-first numbers of the blocks

 along the path

 Then the first time through the loop of lines_(4) through_(6) in the

 algorithm in Fig_figdf-alg(a)

 will propagate from to to and so on up to



 It will not reach on that round

 because as 16 precedes 35 we had already_computed by the time

 was put in

 However the next time we run through the loop of lines (7) through (12)

 when we compute will be included because it is in

 Definition will also propagate to

 and so on up to

 where it must_wait because was already_computed on this round

 On the third_pass travels to and

 so after three passes we establish

 that reaches block 17

 ex



 It should not be hard to extract the general principle from this example

 If we use depth-first_order in Fig_figdf-alg(a)

 then the number of passes

 needed to propagate any reaching definition along any_acyclic path

 is no more_than one greater_than the number of edges along that path

 that go

 from a higher numbered block to a lower numbered block

 Those edges are exactly the retreating_edges so the number of passes needed

 is one plus the depth

 Of_course Algorithm_algreaching-definitions

 does_not detect the fact that all definitions

 have reached wherever they can reach until one more pass has yielded

 no changes

 Therefore the upper_bound

 on the number of passes taken by that algorithm with depth-first block

 ordering is actually two plus the depth

 A studyfootnoteD E Knuth An

 empirical study of FORTRAN programs Software - Practice and

 Experience 12 (1971) pp 105-133footnote

 has shown that typical

 flow_graphs have an average depth around 275

 Thus the algorithm_converges very quickly



 In the case of backward-flow problems like live_variables

 we visit the nodes in the reverse of the depth-first_order

 Thus we may propagate a use of a variable in block 17

 backwards along the path



 3_5 19_35 16_23 45_4 10_17



 in one pass to where we must_wait for the next pass to in order reach



 On the second pass it reaches and on the third_pass it

 goes from to



 In_general one-plus-the-depth passes suffices to carry

 the use of a variable backward along

 any_acyclic path

 However we must choose the reverse of depth-first_order to

 visit the nodes in a pass because then uses propagate along any

 decreasing sequence in a single pass



 A Reason for Irreducible Flow_Graphs

 There is one place where we cannot generally expect a flow_graph to be

 reducible

 If we reverse the edges of a program flow_graph as we did in

 Algorithm algloops to find natural_loops then we may not get a

 reducible_flow graph

 The intuitive reason is that while typical_programs have loops with

 single entries those loops sometimes have several exits which become

 entries when we reverse the edges



 The bound described so_far is an upper_bound on all problems where

 cyclic paths add no information to the analysis In special problems

 such_as dominators the algorithm_converges even faster In the case

 where the input flow_graph is reducible the correct set of dominators

 for each node is obtained in the

 first iteration of a data-flow algorithm that visits the nodes in

 depth-first_ordering If we do_not know that the input is reducible

 ahead of time it takes an extra iteration to determine that convergence

 has_occurred



 sexer

 dom-exer

 For the flow_graph of Figfg1-fig (see the exercises for

 Section_secopt-sources)



 itemize



 Compute the dominator relation

 Find the immediate_dominator of each node

 Construct the dominator_tree

 Find one depth-first_ordering for the flow_graph

 Indicate the advancing retreating cross and tree edges for your

 answer to

 Is the flow_graph reducible

 Compute the depth of the flow_graph

 Find the natural_loops of the flow_graph



 itemize

 sexer



 exer

 Repeat_Exercise dom-exer on the following flow_graphs



 itemize



 a)

 Fig_figqs-fg



 b)

 Fig_identity-fg-fig



 c)

 Your_flow graph from Exercise mm-code-exer



 d)

 Your_flow graph from Exercise primes-code-exer



 itemize

 exer



 hexer

 Prove the following about the dom relation



 itemize



 a)

 If and then (transitivity)



 b)

 It is never possible that both

 and hold if (antisymmetry)



 c)

 If and are two dominators of then either

 or must hold



 d)

 Each node except the entry has a unique

 immediate_dominator - the dominator that appears closest to

 along any_acyclic path from the entry to



 itemize

 hexer



 hexer

 Figure figdepth-first is one depth-first_presentation of the flow

 graph of Fig_figdom-fg How many other depth-first

 presentations of this flow_graph are there Remember order of children

 matters in distinguishing depth-first presentations

 hexer



 vhexer

 Prove that a flow_graph is reducible if and only if when we remove all

 the back_edges (those whose heads_dominate their tails) the resulting

 flow_graph is acyclic

 vhexer



 hsexer

 A complete flow_graph on nodes has arcs between

 any two nodes and (in both directions)

 For what values of is this graph reducible

 hsexer



 hexer

 A complete acyclic flow_graph on nodes

 has arcs

 for all nodes and such that Node 1 is the entry



 itemize



 a)

 For what values of is this graph reducible



 b)

 Does your answer to (a) change if you add self-loops for

 all nodes



 itemize

 hexer



 hsexer

 The natural_loop of a back_edge was defined to be plus

 the set of nodes that can reach without_going through Show

 that

 dominates all the nodes in the natural_loop of

 hsexer



 vhexer

 We claimed that the flow_graph of Fig_figirreducible is

 nonreducible If the arcs were replaced_by paths of disjoint nodes

 (except for the endpoints of course)

 then the flow_graph would still be nonreducible In_fact node 1 need

 not be the entry it can be any node reachable from the entry along a

 path whose intermediate nodes are not part of any of the four explicitly

 shown paths Prove the converse

 that every nonreducible_flow graph has a subgraph like

 Fig_figirreducible but with arcs possibly replaced_by node-disjoint

 paths and node 1 being any node reachable from the entry by a path that

 is node-disjoint from the four other paths

 vhexer



 vhexer

 Show that every depth-first_presentation for every

 nonreducible_flow graph has a retreating_edge that is

 not a back_edge

 vhexer



 vhexer

 Show that if the following condition



 f(a)g(a)a f(g(a))



 holds for all functions and and value then the general

 iterative_algorithm

 Algorithm_algiterative with iteration following a depth-first

 ordering converges within 2-plus-the-depth passes

 vhexer



 hexer

 Find a nonreducible_flow graph with two different DFST's that have

 different depths

 hexer



 hexer

 Prove the following



 itemize



 a)

 If a definition is in then there is some

 acyclic_path from the block containing to such that

 is in the 's and 's all along that path



 b)

 If an expression is not available at the entrance to

 block then there is some_acyclic path that demonstrates that fact

 either the path is from the entry_node and includes no statement that

 kills or generates or the path is from a block that

 kills and along the path there is no_subsequent generation of



 c)

 If is live_on exit from block

 then there is an acyclic_path from to a use of along which there

 are no definitions of



 itemize

 hexer

 The Principal Sources of Optimization

 secopt-sources



 A compiler optimization must preserve the semantics of the

 original_program Except in very special circumstances

 once a programmer chooses and implements a particular

 algorithm the compiler cannot understand enough about the program to

 replace it with a substantially different and more_efficient

 algorithm A compiler knows only how to apply relatively low-level

 semantic transformations using general facts such_as algebraic

 identities like or program semantics such_as the fact that

 performing

 the same operation on the same values yields the same result



 Causes of Redundancy



 There_are many redundant operations in a typical program Sometimes

 the redundancy is available at the source_level

 For_instance a programmer may find

 it more direct and convenient to recalculate some result leaving it to

 the compiler to recognize that only one such calculation is necessary

 But more

 often the redundancy is a side-effect of having written the program

 in a high-level_language In most languages (other_than C or C where

 pointer arithmetic is allowed) programmers have no

 choice but to refer to elements of an array or fields in a structure

 through accesses like or



 Each of these

 high-level data-structure accesses expands into a number of

 low-level_arithmetic operations such_as the computation of the location

 of the th_element of a matrix Accesses to the same data_structure

 often share_many common low-level_operations Programmers are not

 aware of these low-level_operations and cannot eliminate the

 redundancies themselves It is in fact preferable from a

 software-engineering perspective that programmers only access data elements

 by their

 high-level names the programs are easier to write and more

 importantly easier to understand By having a compiler eliminate the

 redundancies we get the best of both worlds the programs are both

 efficient and easy to understand



 A Running Example Quicksort

 secquicksort



 In the following we_shall use a sorting program called quicksort to

 illustrate_how programs are represented and how they can be

 optimized The C program in Fig_figqs is derived_from

 SedgewickfootnoteR Sedgewick Implementing Quicksort

 Programs Comm_ACM 21 pp 847-857footnote

 who discussed the hand-optimization of such a program

 We_shall not discuss all the subtle algorithmic aspects of this

 program here for example the fact that must contain the

 smallest of the sorted elements and the largest



 figurehtb



 verbatim

 void quicksort(int m int n)

 recursively sorts am through an



 int i_j

 int_v x

 if (n m) return

 fragment begins here

 i_m-1 j_n v an

 while (1)

 do i_i1 while (ai v)

 do j_j-1 while (aj v)

 if (i j) break

 x_ai ai_aj aj_x



 x_ai ai an an x

 fragment ends here

 quicksort(mj) quicksort(i1n)



 verbatim



 C code for quicksort

 figqs



 figure



 Before we can optimize away the redundancy in address_calculations

 the address operations in a program first must_be broken_down

 into low-level_arithmetic operations to expose the redundancy

 In the rest of the chapter we assume that the intermediate_code

 consists of three-address_statements where temporary_variables are

 used to hold all the results of intermediate expressions

 Intermediate_code

 for the marked fragment of the program in Fig_figqs is shown

 in Fig_figqs-code



 figurehtb



 verbatim

 (1) i_m-1 (16) t7 4i

 (2) j_n (17) t8 4j

 (3) t1 4n (18) t9 at8

 (4) v at1 (19) at7 t9

 (5) i_i1 (20) t10_4j

 (6) t2 4i (21) at10_x

 (7) t3 at2 (22) goto (5)

 (8) if t3 v_goto (5) (23) t11 4i

 (9) j_j-1 (24) x at11

 (10) t4 4j (25) t12 4i

 (11) t5_at4 (26) t13 4n

 (12) if t5 v_goto (9) (27) t14 at13

 (13) if i_j goto (23) (28) at12 t14

 (14) t6_4i (29) t15 4n

 (15) x at6 (30) at15 x

 verbatim



 Three-address_code for fragment in Fig_figqs

 figqs-code



 figure



 Assuming in this example that integers occupy four_bytes an

 assignment such_as

 x_ai is translated as in Section array-3code-subsect

 into three-address_statements



 verbatim

 t6_4i

 x at6

 verbatim

 as shown in steps (14) and (15) of

 Fig_figqs-code

 Similarly aj_x becomes



 verbatim

 t10_4j

 at10_x

 verbatim

 in steps (20) and (21)

 Notice_that every array

 access in the original_program translates_into a pair of steps

 consisting of a

 multiplication and an array-subscripting operation As a result

 this short program_fragment translates_into a rather long sequence of

 three-address operations



 figure

 figureuullmanalsuch9figsqs-fgeps

 Flow_graph for the quicksort fragment

 figqs-fg

 figure



 Figure figqs-fg is the flow_graph for the program in

 Fig_figqs-code

 is the entry_node

 All conditional and unconditional_jumps to statements in

 Fig_figqs-code have_been replaced in Fig_figqs-fg by jumps

 to the block of which the statements are leaders

 as in Section bb-sect

 In Fig_figqs-fg there are three loops

 and are loops by themselves

 Blocks and together

 form a loop with the only entry point



 Semantics-Preserving Transformations



 There_are a number of ways in which a compiler can

 improve a program without_changing the function it computes

 Common-subexpression elimination

 copy_propagation

 dead-code_elimination

 and constant_folding

 are common examples of such function-preserving (or semantics-preserving) transformations

 we_shall consider each in turn



 Frequently a program will include several calculations of

 the same value such_as an offset in an array

 As_mentioned in Section secquicksort

 some of these duplicate calculations cannot be

 avoided by the programmer because they lie below the level of detail

 accessible within the source_language

 For_example block shown in Fig figlocal-cse(a)

 recalculates and

 although none of these calculations were requested

 explicitly by the programmer



 figurehtb

 figureuullmanalsuch9figslocal-cseeps

 Local common_subexpression elimination

 figlocal-cse

 figure



 Global_Common Subexpressions

 seccse



 An occurrence of an expression is called a

 common_subexpression

 if was previously_computed and the values of

 variables in have not changed since the previous computation

 We avoid recomputing if we can use its

 previously_computed

 value

 that is the variable to which the previous computation of was

 assigned has not changed in the interimfootnoteIf

 has changed it may still be possible to reuse the

 computation of if we assign its value to a new variable as_well as

 to and use the value of in place of a recomputation of

 footnote



 ex

 The assignments to t7 and t10

 in Fig figlocal-cse(a)

 compute the common_subexpressions and

 respectively

 These steps

 have_been eliminated in Fig_figlocal-cse(b) which uses

 t6 instead of t7 and t8 where t10 was used



 This change is what would result if we reconstructed

 the intermediate_code from the dag (directed acyclic graph) for

 the basic_block as in Section old98



 ex



 ex

 exlocal-cse

 Figure figlocal-cse-after

 shows the result of eliminating both

 global and local common_subexpressions

 from blocks and in the flow_graph of Fig_figqs-fg

 We first discuss the transformation of and

 then mention some subtleties involving arrays



 figurehtb

 figureuullmanalsuch9figslocal-cse-aftereps

 and after common_subexpression elimination

 figlocal-cse-after

 figure



 After local common_subexpressions are eliminated

 still evaluates and

 as shown in Fig_figlocal-cse(b)

 Both are

 common_subexpressions in particular the three statements



 verbatim

 t8 4j

 t9 at8

 at8 x

 verbatim

 in can be replaced_by



 verbatim

 t9 at4

 at4 x

 verbatim

 using computed in block

 In Fig_figlocal-cse-after observe that

 as control passes from the evaluation of

 in

 to there is no change to

 and no change to

 so can be used if

 is needed



 Another common_subexpression comes to light in

 after replaces

 The new expression corresponds to the value of

 at the source_level

 Not_only does

 retain its value as control leaves and then

 enters but

 a value computed into a temporary

 does too because there are

 no assignments to elements of the array

 in the interim

 The statements



 verbatim

 t9 at4

 at6 t9

 verbatim

 in therefore can be replaced_by



 verbatim

 at6 t5

 verbatim



 Analogously the value assigned to

 in block of Fig_figlocal-cse(b)

 is seen to be the same as the value assigned to

 in block

 Block in Fig_figlocal-cse-after is the result of eliminating

 common_subexpressions corresponding to the values of the source_level

 expressions and

 from in Fig_figlocal-cse(b)

 A similar series of transformations has_been done to

 in Fig_figlocal-cse-after



 The expression in

 blocks and of Fig_figlocal-cse-after is not

 considered a common_subexpression

 although can be used in both places

 After control leaves and before

 it reaches it can go

 through where there are assignments to

 Hence

 may not have the same value on reaching as it did on

 leaving and it is not safe to treat as

 a common_subexpression

 ex



 Copy Propagation



 Block in Fig_figlocal-cse-after

 can be further improved

 by eliminating

 using two new transformations

 One concerns assignments of the form

 u_v

 called

 copy statements

 or

 copies

 for short

 Had we gone into more_detail in Example exlocal-cse copies would

 have arisen much sooner because the normal algorithm for

 eliminating common_subexpressions introduces them

 as do several other algorithms



 ex

 In order to eliminate the common_subexpression from the statement

 c_de

 in Fig figcopies(a) we must use a new

 variable

 to hold the value of

 Variable instead of the expression

 is assigned to in Fig figcopies(b)

 Since control may reach

 c_de

 either after the assignment to

 or after the assignment to

 it would be incorrect to replace

 c_de

 by either

 c a

 or by

 c b

 ex



 figurehtb

 figureuullmanalsuch9figscopyeps

 Copies introduced during common_subexpression elimination

 figcopies

 figure



 The idea_behind the

 copy-propagation

 transformation is to use for

 wherever possible after the copy_statement

 u_v

 For_example the assignment x t3 in block of

 Fig_figlocal-cse-after is a copy

 Copy propagation applied to

 yields the code in Fig figcopy

 This change may not appear to be an improvement

 but as we_shall see in Section dead-code-subsect

 it gives_us the opportunity

 to eliminate the assignment to



 figurehtfb



 center

 tabularl

 x t3



 at2 t5



 at4 t3



 goto



 tabular

 center



 Basic Block after copy_propagation

 figcopy

 figure



 Dead-Code Elimination

 dead-code-subsect



 A variable is live

 at a point in a program if

 its value can be used subsequently

 otherwise it is

 dead

 at that point

 A related idea is

 dead

 (or

 useless)

 code -

 statements that compute values that never get used

 While the programmer is unlikely to introduce any dead_code

 intentionally it may appear as the result of previous transformations



 ex

 dead-code-ex

 Suppose

 debug

 is set to

 TRUE

 or

 FALSE

 at various points in the program and used in statements like



 verbatim

 if (debug) print

 verbatim

 It may be possible for the compiler to deduce that each time

 the program reaches this statement the value of

 debug

 is

 FALSE

 Usually it is because there is one particular statement



 verbatim

 debug FALSE

 verbatim

 that must_be the last assignment to

 debug

 prior to any tests of the value of debug

 no_matter what sequence of branches the

 program actually takes

 If copy_propagation replaces

 debug

 by FALSE then the print statement is dead because it

 cannot be reached

 We can eliminate both the test and the print operation from the

 object code

 More_generally deducing at_compile time that the value of

 an expression is a constant and using the constant instead

 is known_as

 constant_folding

 ex



 One advantage of copy_propagation is that it often turns

 the copy_statement into dead_code

 For_example

 copy_propagation followed_by dead-code_elimination

 removes the assignment to

 and transforms the code in Fig figcopy

 into



 center

 tabularl

 at2 t5



 at4 t3



 goto



 tabular

 center

 This code is a further improvement of block

 in Fig_figlocal-cse-after



 Code_Motion



 Loops are a very important place

 for optimizations especially the inner_loops

 where programs tend to spend the bulk of their time

 The running_time of a program may be improved if we decrease the

 number of instructions in an inner_loop

 even if we increase the amount of code outside that loop



 An_important modification

 that decreases the amount of code in a loop is

 code_motion

 This transformation takes an expression

 that yields the same result independent of the number

 of times a loop is executed (a

 loop-invariant computation)

 and evaluates the expression before the loop

 Note_that the notion before the loop assumes the

 existence of an entry for the loop that is one basic_block to

 which all jumps from outside the loop go (see

 Section loops-subsect)



 ex

 code-motion-ex

 Evaluation of

 is a loop-invariant computation in the following while-statement



 verbatim

 while (i limit-2) statement does_not change limit

 verbatim

 Code motion will result in the equivalent of

 verbatim

 t limit-2

 while (i t) statement does_not change limit or t

 verbatim

 Now the computation of is performed once before we enter

 the loop

 Previously there would be calculations of

 if we went

 through the body of the loop times

 ex



 Induction_Variables and Reduction in Strength



 Another important optimization is to find induction_variables in loops

 and optimize their computation A variable is said to be an

 induction_variable if there is a positive or negative constant

 such that each time is assigned its value increases by For

 instance and in the loop containing

 of Fig_figlocal-cse-after are all induction_variables

 Induction variables can be computed with a single increment (addition or

 subtraction) per loop

 iteration The transformation of replacing an expensive operation such

 as multiplication by

 a cheaper one such_as addition

 is known_as strength_reduction

 But induction_variables not only allow_us sometimes to perform a

 strength_reduction often it is possible to eliminate all but one of a

 group of induction_variables whose values remain in lock-step as we go

 around the loop



 When processing loops it is useful to work inside-out that is we

 shall start with the inner_loops and proceed to progressively_larger

 surrounding loops

 Thus we_shall see_how this optimization applies to our quicksort

 example by

 beginning with one of the innermost_loops by itself

 Note_that the values of

 and remain in lock-step every time the value of

 decreases by 1 the value of decreases by 4 because

 is assigned to

 These variables and

 thus form a good example of a pair of induction

 variables



 figurehtb

 figureuullmanalsuch9figsstrengtheps

 Strength reduction applied to in block

 figstrength

 figure





 When there are two or_more induction_variables in a loop

 it may be possible to get rid of all but one

 For the inner_loop of in Fig_figlocal-cse-after

 we cannot get rid of either

 or completely is used in and

 is used

 in

 However we can illustrate reduction in strength and

 a part of the process of induction-variable_elimination

 Eventually will be_eliminated when the outer_loop

 consisting of blocks and is considered



 ex

 exinduction

 As the relationship surely

 holds after assignment to in Fig_figlocal-cse-after

 and is not changed elsewhere in the inner_loop around

 it follows that just after the statement

 j_j-1

 the relationship must hold

 We may therefore replace the assignment t4 4j

 by t4 t4-4

 The only problem is that does_not have a value

 when we enter block for the first time

 Since we must maintain the relationship

 on entry to the block we place an initialization

 of at the end of the block where

 itself is initialized shown by the dashed addition to

 block in Fig figstrength

 Although we have added one more instruction which is executed once in

 block the replacement of a multiplication by a subtraction will

 speed_up the object code if multiplication takes more time

 than addition or subtraction

 as is the case on many machines

 ex



 We_conclude this_section with one more instance

 of induction-variable_elimination

 This example treats and

 in the context of

 the outer_loop containing

 and



 ex

 exstrength

 After reduction in strength is applied to the inner_loops around

 and the only use of and

 is to determine the outcome of the test in block

 We know that the values of

 and satisfy the relationship

 while those of

 and satisfy the relationship

 Thus the test can substitute for

 Once this replacement is made

 in block and

 in block become dead variables and the assignments

 to them in these blocks become dead_code that can be_eliminated

 The resulting flow_graph is shown in Fig figinduction

 ex



 figurehtb

 figureuullmanalsuch9figsinductioneps

 Flow_graph after induction-variable_elimination

 figinduction

 figure



 The code-improving_transformations have_been effective

 In Fig figinduction the numbers of instructions in

 blocks and have_been reduced from 4 to 3 compared with

 the original flow_graph in Fig_figqs-fg

 In the number has_been reduced from

 9 to 3

 and in

 from 8 to 3

 True has grown from four instructions to six

 but is executed only once in the fragment

 so the total running_time is barely affected by the

 size of



 exer

 Apply the transformations of this_section to the flow_graphs of



 itemize



 a)

 Fig_identity-fg-fig



 b)

 Your_flow graph from Exercise mm-code-exer



 c)

 Your_flow graph from Exercise primes-code-exer



 itemize

 exer



 figurehtfb

 fileuullmanalsuch9figsfg1eps

 Flow_graph for Exercise fg1-exer

 fg1-fig

 figure



 sexer

 fg1-exer

 For the flow_graph in Fig_fg1-fig



 itemize



 a)

 Identify the loops of the flow_graph



 b)

 Statements (1) and (2) in are both copy statements in which

 and are given constant values For which uses of and can we

 perform copy_propagation and replace these uses of variables by uses of

 a constant Do so wherever possible



 c)

 Identify any global_common subexpressions for each loop



 d)

 Identify any induction_variables for each loop Be sure to take into

 account any constants introduced in (b)



 e)

 Identify any loop-invariant computations for each loop



 itemize

 sexer

 Partial Redundancy Elimination

 secpre



 In this_section we consider in detail how to minimize the

 evaluation of expressions

 Applying the code transformation developed here greatly improves the

 performance of the resulting code since an operation is never applied

 unless it absolutely has to be

 Every optimizing_compiler must implement something_like the

 transformation described_here

 However there is another motivation for discussing the problem

 Finding the right place or

 places in the flow_graph at which to evaluate each

 expression requires four different_kinds of data-flow_analysis

 Thus the study of partial_redundancy elimination as minimizing

 expression evaluation is called will enhance our understanding of the

 role data-flow_analysis plays in a compiler



 Redundancy in programs exists in several forms As_discussed in

 Section seccse it may exist in the form of common

 subexpressions where several

 evaluations of the expression produce the same value

 It may also exist in the

 form of a loop-invariant expression that evaluates to the same value

 in every iteration through the loop Redundancy may also be partial

 if it is found along some of the paths but not_necessarily along all paths

 Common subexpressions and loop-invariant_expressions

 can be_viewed as special_cases

 of partial_redundancy thus a single partial-redundancy_elimination

 algorithm can be devised to eliminate all the various forms of

 redundancy



 The Sources of Redundancy



 In the following we first discuss the different forms of redundancy

 in order to build_up your intuition about the problem

 We then

 describe the generalized redundancy-elimination problem and finally we

 present the algorithm This algorithm is particularly interesting

 because it involves solving multiple data-flow_problems in both

 the forward and backward directions



 Figure figpre-intro illustrates the

 three forms of redundancy common_subexpressions

 loop-invariant_expressions and partially_redundant expressions

 The figure shows the code both before and after each optimization



 figurehtb

 figureuullmanalsuch9figspre-introeps

 Examples of (a) global_common subexpression (b) loop

 invariant code_motion (c) partial-redundancy_elimination

 figpre-intro

 figure



 Global_Common Subexpressions

 In Fig figpre-intro(a) the expression_bc

 executed in block is redundant it

 has already

 been evaluated by the time the flow of control_reaches

 regardless of the path taken to get there As we

 observe in this example the value of the expression may be different

 on different paths We can optimize the code by storing the result of

 the computations of bc in blocks and

 in the same temporary_variable

 say t and then assigning the value of t to the variable

 e in block

 instead of re-evaluating the expression Had there been an

 assignment to either b or c after the last computation of

 bc and before block the expression in block would not

 be redundant



 Formally we say that an expression_bc is (fully) redundant

 at point

 if it is an available expression in the sense of

 Section_ae-subsect at that point

 That is the expression_bc has_been computed along all paths

 reaching and the variables b and c were not

 redefined after the last expression was evaluated

 The latter condition is necessary because even_though the expression

 bc is textually executed before reaching the point the

 value would have_been different because the operands might have

 changed



 Finding Deep Common_Subexpressions

 Using available-expressions analysis to identify

 redundant expressions only works for expressions that are

 textually_identical For_example an application of common_subexpression

 elimination will recognize that t1

 in the code_fragment



 center

 t1 bc a t1d

 center

 has the same value as does t2 in



 center

 t2 bc e t2d

 center

 as_long as the variables b and c have not been_redefined

 in between It does_not however recognize that a and e are also

 the same It is possible to find such deep common_subexpressions by

 re-applying common_subexpression elimination until_no new common

 subexpressions are found on one round

 It is also possible to use a more_complex and expensive

 data-flow_framework in which

 the values propagated are all identities among expressions_involving the

 variables of the program



 Loop-Invariant Expressions



 Fig figpre-intro(b) shows an example of a loop-invariant

 expression The expression_bc is loop_invariant because

 neither the variable b nor c is redefined within the loop

 We can optimize the program by_replacing all the re-executions in a

 loop with a single calculation outside the loop We assign the

 computation to a temporary_variable say t and then replace

 the expression in the loop by t There is one more point we

 need to consider in code_motion We should not execute any

 instruction that would not have executed without the optimization

 For_example if it is possible to exit the loop without executing the

 instruction at all then we should not move the operation out of the

 loop even if it is loop_invariant There_are two_reasons



 enumerate



 If the instruction raises an exception then executing it may throw an

 exception that would not have happened in the original_program



 When the loop exits early the optimized program takes more time

 that the original_program



 enumerate



 A loop-invariant-expression-elimination algorithm needs first to find

 the loop boundaries and then to determine that



 enumerate



 The operands of the expression are

 not redefined in the loop and



 The expression is always executed

 before the loop exits



 enumerate

 This procedure may need to be repeated because

 once a variable is determined to have a loop-invariant value

 expressions using that variable may also become invariant Unlike

 common-subexpression_elimination where a redundant expression

 computation is dropped loop-invariant-expression elimination_requires

 an expression from inside the loop to move outside the loop Thus

 this optimization is generally known_as loop-invariant code_motion



 Partially Redundant Expressions



 An_example of a partially_redundant expression is shown in

 Fig figpre-intro(c) The expression_bc in block

 is redundant on the path but not on the

 path We can eliminate the redundancy

 on the former path by placing a computation of bc in block

 All the results of bc are written into a temporary

 variable t and the calculation in block is replaced with

 t Thus like code_motion partial-redundancy_elimination

 requires the placement of new expression computations



 The Lazy-Code-Motion Problem

 seclcm-conditions



 Partial-redundancy elimination is the task of eliminating any

 redundant calculation of the same expression along any path without

 introducing any expression computations that would otherwise not be

 executed Thus this problem statement includes not just the

 elimination of partially_redundant expressions but generalizes to

 fully_redundant common expressions and loop-invariant_expressions as_well

 The solution to this problem consists of a set of placements of the

 expression calculations the results of the calculation of one

 expression are written to

 the same new variable which is then used to replace the

 redundant_computations



 There may be many placements that can fulfill the

 requirements above We may choose to compute an expression as_soon as

 it is ready to be computed ie as_soon as its operand variables have

 received their correct values Or we may

 choose to delay the expression calculation to the last possible time

 as_long as it is computed before it is

 used computed before its operands' values change and placed so that

 no redundancy is created



 It is preferable that the

 expression placement be delayed as much as possible because doing_so

 shortens the lifetime of an expression which is defined to be

 the time

 between when the expression is computed and when it is last used

 The shorter the lifetime the easier it is to keep the expression's

 value in

 a register We refer to the problem of finding the latest_placements

 as lazy_code motion



 We are now_ready to define the conditions desired of a lazy_code

 motion algorithm



 enumerate



 The optimized code does_not execute any operations that would not have

 been executed in the original_program



 No expressions are re-executed along any path if the operands have

 not been_redefined



 Operations are performed at the latest possible time

 so the lifetime of

 a register holding the result of the computation is not unduly

 lengthened



 Assignments to temporary_variables are eliminated if they are

 not used more_than once



 enumerate

 The last item eliminates unnecessary copy operations that would be

 introduced otherwise



 Critical Edges





 We don't necessarily eliminate all redundancies with just fixing the critical

 edges For_example

 a bc (S1)

 do while x

 d bc (S2)

 d 10



 d bc is redundant on the path (S1-S2)



 The first question we_wish to address is whether it is always possible

 to eliminate redundancies along every_path The answer is no

 unless we are allowed to change the flow_graph

 However if we are permitted to introduce basic_blocks then we can in

 fact eliminate all redundancies





 First we would like to show that if we are able to change the flow

 graph we can eliminate more redundancy in the program



 ex

 excritical-edge

 In the example shown in Fig figcritical-edge(a) the expression

 of bc is executed redundantly in basic_block if the

 program follows the execution_path

 However we cannot simply move the computation of bc to block

 because doing_so would create an extra computation of bc

 had the path been taken



 What we would like to do is to insert the computation of bc

 only along the edge from block to block We can do so by

 placing the instruction in a new basic_block say making the

 flow of control from go_through before it reaches

 The transformation is shown in Fig figcritical-edge(b)

 ex

 figurehtb

 figureuullmanalsuch9figscriticaleps

 A critical_edge

 figcritical-edge

 figure



 We define a critical_edge of a flow_graph to be any edge leading

 from a node with multiple successors to a node with multiple

 predecessors By introducing new basic

 blocks along critical_edges we can always find a basic_block to

 accommodate the desired expression placement

 For_instance the edge from to in

 Fig figcritical-edge(a) is critical because has two

 successors and has two predecessors



 When we introduce basic_blocks along critical_edges we eliminate all

 critical_edges from the flow_graph

 For_instance was_introduced along the critical_edge

 in Fig figcritical-edge(b)

 Since introduced blocks have only one_predecessor and only one

 successor neither the edge nor the edge

 is critical



 The Lazy Code_Motion Algorithm



 For_convenience

 our algorithm assumes that every statement is a basic_block of

 its_own

 Also to keep the algorithm simple we only introduce new computations

 of expressions

 at the beginning of basic_blocks To ensure_that this simplification

 does_not reduce the effectiveness of the technique

 we insert a new

 basic_block between the source and the

 destination of an edge if the destination has more_than one_predecessor

 Doing_so obviously also eliminate all critical_edges in the program



 For each expression the algorithm first finds its earliest

 placement the positions where the computation can be placed to

 satisfy conditions (1) and (2)

 in Section seclcm-conditions This step consists of

 two data-flow_analyses a backward_pass that calculates which

 expressions are needed and a forward pass that computes which

 expressions are missing The intersection of the two will be seen

 to define

 the earliest_placement Then the algorithm finds the latest

 placement the positions where conditions (1) (2) and (3) of

 Section seclcm-conditions are

 satisfied It does so with a forward pass that determines if the

 placement of an expression can be postponed

 The frontier where the expression can no_longer be postponed is where the latest the expression can be placed

 Finally it

 eliminates unnecessary assignments to temporary_variables to

 satisfy condition (4) This task is done with another backward_data-flow

 analysis that determines if the expressions are used There

 are altogether four data-flow passes one for each of the

 conditions



 We abstract the semantics of each basic_block with two sets

 is the set of expressions computed in and is

 the set of expressions killed that is the set of expressions any of whose

 operands are defined in Example_expre will be used

 throughout the discussion of the four data-flow_analyses whose

 definitions are summarized in Fig figpre-df



 ex

 expre

 In the flow_graph in Fig_figpre(a) the expression_bc

 appears three times Because the basic_block is part of a loop

 the expression may be executed many_times The

 computation in block is not only loop_invariant it is also a

 redundant expression since the value has already_been used in basic

 block For this example we need to compute bc

 only twice

 once in basic_block and once along the path after and

 before The lazy_code motion algorithm will place the

 expression computations at the beginning of

 blocks and

 ex

 figurehtb

 figureuullmanalsuch9figspreeps

 Flow_graph of Example_expre

 figpre

 figure



 Needed Expressions



 The first of the four data-flow_analyses in lazy_code motion

 is a backward_pass that finds those basic_blocks

 that need the expression on entry We_say that an expression_bc is needed at a program point if all paths_leading

 from the point eventually compute the value of the expression_bc

 from the values of b and c that are available at that point



 In Fig_figpre(a) all the basic_blocks needing bc are

 shown as lightly_shaded boxes The expression_bc is needed in

 basic_blocks and It is not needed

 on entry to

 block because the value of c is recomputed within the block

 and therefore the value of bc that would be computed

 at the beginning of is not

 used along any path

 The expression_bc is not needed on entry to because

 it is unnecessary along the branch from to (although it

 would be used along the path )

 Similarly the expression is not needed at the beginning

 of because of the branch from to The need of an

 expression may oscillate along a path as illustrated by





 figure

 center

 tabularl_l l_l

 (a) Needed Expressions (b) Missing Expressions



 Domain_Sets of expressions Sets of expressions



 Direction Backwards_Forwards













 Transfer



 function





 Boundary







 Meet_()





 Initialization















 (c) Postponed Expressions (d) Used_Expressions



 Domain_Sets of expressions Sets of expressions



 Direction_Forwards Backwards













 Transfer



 function





 Boundary







 Meet_()





 Initialization



 tabular

 center



 align

 earliestB neededBin missingBin



 latestB_(earliestB postponedBin)



 (euseB (SsuccB(earliestS postponedSin)))

 align



 Four data-flow passes in partial-redundancy_elimination

 figpre-df

 figure



 The data-flow_equations for the needed expressions problem are shown

 in Fig figpre-df(a) The analysis is a backward

 pass A needed expression at the exit of a basic_block is a

 needed expression on entry only if it is not in the set

 Also a basic_block generates as new uses the set of

 expressions At the exit of the program none of the expressions are

 needed Since we are_interested in finding expressions that are

 needed along every subsequent path the meet_operator is set

 intersection Consequently the interior_points must_be

 initialized to the universal_set as was discussed for the

 available-expressions problem in Section_ae-subsect



 Completing the Square

 Needed expressions (also called very busy expressions elsewhere) is

 a type of data-flow_analysis we have not seen previously

 While we have_seen backwards-flowing frameworks such_as live-variable

 analysis (Sect live-var-subsect) and we have_seen frameworks

 where the meet is intersection such_as available_expressions

 (Sect ae-subsect) this is the first example of a useful

 framework that has both properties

 Almost all frameworks we use can be placed in one of four groups

 depending_on whether they flow forwards or backwards and depending_on

 whether they use union or intersection for the meet

 Notice also that the union frameworks always involve asking about

 whether there_exists a path along which something is true while the

 intersection frameworks ask whether_something is true along all paths



 Missing Expressions



 At first blush the positions for earliest placements are simply

 the frontier where control_flows from blocks that do_not need the

 expression

 to those

 that do However as illustrated by block in

 Fig_figpre(a) even_though does_not need the expression

 while does

 the expression has already_been computed when control_reaches

 As a result we do_not have to place bc at

 We therefore need another data-flow_analysis to detect missing

 expressions - those that would have to be computed at a point if

 they_were needed

 As we_shall see the expressions at a point that are both

 missing and needed are the expressions

 that require earliest_placement at that

 point

 Formally an expression xy is missing at a program

 point if there_exists at_least one path leading_from the entry to

 point along which either



 enumerate



 xy was never needed or



 At least_one

 of x and y was recomputed (xy was killed) since

 the last time xy was needed



 enumerate



 ex

 Shown with dark_shadows in Fig_figpre(a) are the basic_blocks

 for which expression_bc is missing on entry they are

 and The early-placement positions are represented_by

 the lightly_shaded boxes with dark_shadows and are

 thus blocks and

 Note for instance that bc is not missing on entry to

 because there is a path along

 which bc is needed at_least once - at in this case -

 and since the beginning of neither b nor c was

 recomputed

 ex



 The data-flow_equations for the missing expressions problem are shown

 in Fig_figpre-df(b)

 The analysis must_be done in a forward pass with union as the meet

 operator

 The_reason is that in the absence of expressions being computed or

 killed an expression is missing on entry to a block if and only

 if it is missing at the exit of one or_more of its_predecessors

 We initialize to the set of all expressions and

 other 's to the empty_set because all expressions are missing at

 the entry and we don't want to declare an expression missing elsewhere

 unless we actually find a path that shows it to be missing



 The transfer_function is explained as_follows

 A missing expression such_as bc on entry to a basic_block

 remains missing only if does_not need bc Let_us

 denote the result for the entry of obtained from the needed-expression

 analysis by Then if is the

 set of expressions that are missing

 on entry to surely will be missing on exit from



 In_addition all the expressions in

 the set have new values on exit from

 and therefore are surely missing

 These observations justify the formula in Fig_figpre-df(b) that

 the transfer_function



 Postponed Expressions



 Now we must consider how to satisfy condition (3) that expressions be

 computed at the latest possible moment during any execution

 We begin_with expressions computed at all their earliest points

 which as mentioned above are defined by



 earliestBneededBinmissingBin



 Notice_that we again use our convention of distinguishing among the

 solutions to various data-flow_frameworks by appending

 to the name of the framework



 Formally an expression xy is postponed at a program

 point if an early placement of xy is encountered along

 every_path from the entry_node to and there is no_subsequent use

 of xy after last such placement



 ex

 Let_us again consider expression_bc in Fig figpre

 The two earliest points for bc are and note_that

 these are the two blocks that are both lightly and darkly shaded in

 Fig_figpre(a) indicating that bc is in the intersection

 of and for these blocks and only these blocks

 We cannot_postpone bc from to because bc is

 used in

 We can postpone it from to however



 But we cannot_postpone bc from to

 The_reason is that although bc is not used in placing its

 computation at instead would lead to a redundant computation of

 bc along the path

 As we_shall see is one of the latest places we can compute bc

 ex



 The data-flow_equations for the postponed expressions problem are

 shown in Fig figpre-df(c) The analysis is a forward pass

 Intuitively a

 basic_block has the responsibility either to execute or to pass to

 its successors (for later execution)

 all the postponed expressions at the entry of as_well as

 all the expressions in If uses the

 expression then cannot_postpone its computation Thus the

 transfer_function of first finds the union of the input and

 then subtracts the set



 At the entry of the program

 no expressions are in the set so

 An expression is

 not postponed to the entry of a basic_block unless all its_predecessors

 include the expression in the set at their exit Thus the meet

 operator is set_intersection and

 the interior_points

 must_be initialized by the top_element of the semilattice the universal

 set



 Having computed the values for all blocks it is easy to

 find the latest placement as the desired positions are simply the frontier

 where an expression cannot be postponed any more That is

 an expression should be placed_at the beginning of a basic_block only

 if the expression is



 enumerate



 Either in its or set and



 Used in or cannot be postponed to all its successor nodes ie at

 least_one successor of does_not have the expression in its

 or sets



 enumerate



 ex

 Fig figpre(b) shows the result of the analysis The

 light-shaded boxes represent the blocks whose set includes

 bc The dark_shadows indicate those that include bc in

 their set The latest_placements of the expressions are

 thus the entries of blocks and since bc is in the

 set of but not and 's set

 includes bc and it uses bc The expressions are stored

 into the temporary_variable t in blocks and and t is

 used in place of bc everywhere else as shown in the figure

 ex



 Used_Expressions

 Finally a backward_pass is used to determine if the temporary

 variables introduced are used beyond the block they are in We_say

 that an expression is used at point if there_exists a path

 leading_from that uses the expression before the value is

 re-evaluated This analysis is similar to liveness_analysis



 The data-flow_equations for the used expressions problem are shown

 in Fig figpre-df(d) The analysis is a backward

 pass A used expression at the exit of a basic_block is a

 used expression on entry only if it is not in the set

 Also a basic_block generates as new uses the set of

 expressions At the exit of the program none of the expressions are

 used Since we are_interested in finding expressions that are

 used by any subsequent path the meet_operator is set

 union Thus the interior_points must_be

 initialized_with the top_element of the semilattice the empty_set



 Putting it All_Together



 All the steps of the algorithm are summarized in

 Algorithm alglazy-code-motion



 alg

 alglazy-code-motion

 Lazy Code_Motion



 A flow_graph for which and have_been computed

 for each block



 A modified flow_graph satisfying the four lazy_code motion conditions

 in Section seclcm-conditions



 enumerate



 Insert an empty basic_block along all edges entering a block with more

 than one_predecessor



 Find for all basic_blocks as defined in

 Fig figpre-df(a)



 Find for all basic_blocks as defined in

 Fig_figpre-df(b)



 Compute the earliest placements for all basic_blocks



 earliestB neededBin missingBin





 Find for all basic_blocks as defined in

 Fig figpre-df(c)



 Compute the latest_placements for all basic_blocks



 align

 latestB_(earliestB postponedBin)



 (euseB (S succB(earliestS postponedSin)))

 align

 Note_that denotes complementation with_respect to the set of all

 expressions computed by the program



 Find for all basic_blocks as defined in

 Fig figpre-df(d)



 For each expression say xy computed by the program do the

 following

 enumerate

 Create a new_temporary say t for xy

 For all basic_blocks such that

 xy is in

 add t xy at the beginning of

 For all basic_blocks such that xy is in



 euseB (latestB usedoutB)



 replace every original xy by t

 enumerate



 enumerate

 alg

 Partial-Redundancy Elimination

 secpre



 In this_section we consider in detail how to minimize the

 number of expression evaluations

 That is we_want to consider all possible_execution sequences in a flow

 graph and look_at the number of times an expression such_as is

 evaluated By moving around the places_where is evaluated and

 keeping the result in a temporary_variable when necessary we often

 can reduce the number of evaluations of this expression along many of

 the execution_paths while not increasing that number along any path

 Note_that the number of different places in the flow_graph where

 is evaluated may increase but that is relatively unimportant as_long

 as the number of executions of the addition is reduced



 Applying the code transformation developed here improves the

 performance of the resulting code since as we_shall see

 an operation is never applied

 unless it absolutely has to be

 Every optimizing_compiler must implement something_like the

 transformation described_here even if it uses a less aggressive

 algorithm than the one of this_section

 However there is another motivation for discussing the problem

 Finding the right place or

 places in the flow_graph at which to evaluate each

 expression requires four different_kinds of data-flow_analysis

 Thus the study of partial-redundancy_elimination as minimizing

 the number of

 expression evaluations is called will enhance our understanding of the

 role data-flow_analysis plays in a compiler



 Redundancy in programs exists in several forms As_discussed in

 Section seccse it may exist in the form of common

 subexpressions where several

 evaluations of the expression produce the same value

 It may also exist in the

 form of a loop-invariant expression that evaluates to the same value

 in every iteration of the loop Redundancy may also be partial

 if it is found along some of the paths but not_necessarily along all paths

 Common subexpressions and loop-invariant_expressions

 can be_viewed as special_cases

 of partial_redundancy thus a single partial-redundancy-elimination

 algorithm can be devised to eliminate all the various forms of

 redundancy



 In the following we first discuss the different forms of redundancy

 in order to build_up your intuition about the problem

 We then

 describe the generalized redundancy-elimination problem and finally we

 present the algorithm This algorithm is particularly interesting

 because it involves solving multiple data-flow_problems in both

 the forward and backward directions



 The Sources of Redundancy



 Figure figpre-intro illustrates the

 three forms of redundancy common_subexpressions

 loop-invariant_expressions and partially_redundant expressions

 The figure shows the code both before and after each optimization

 figurehtb

 figureuullmanalsuch9figspre-introeps

 Examples of (a) global_common subexpression (b) loop

 invariant code_motion (c) partial-redundancy_elimination

 figpre-intro

 figure



 Global_Common Subexpressions

 In Fig figpre-intro(a) the expression

 computed in block is redundant it

 has already

 been evaluated by the time the flow of control_reaches

 regardless of the path taken to get there As we

 observe in this example the value of the expression may be different

 on different paths We can optimize the code by storing the result of

 the computations of in blocks and

 in the same temporary_variable

 say and then assigning the value of to the variable

 in block

 instead of re-evaluating the expression Had there been an

 assignment to either or after the last computation of

 but before block the expression in block would not

 be redundant



 Formally we say that an expression is (fully) redundant

 at point

 if it is an available expression in the sense of

 Section_ae-subsect at that point

 That is the expression has_been computed along all paths

 reaching and the variables and were not

 redefined after the last expression was evaluated

 The latter condition is necessary because even_though the expression

 is textually executed before reaching the point the

 value of computed at point

 would have_been different because the operands might have

 changed



 Finding Deep Common_Subexpressions

 Using available-expressions analysis to identify

 redundant expressions only works for expressions that are

 textually_identical For_example an application of common_subexpression

 elimination will recognize that t1

 in the code_fragment



 center

 t1 b_c a t1 d

 center

 has the same value as does t2 in



 center

 t2 b_c e t2 d

 center

 as_long as the variables and have not been_redefined

 in between It does_not however recognize that and are also

 the same It is possible to find such deep common_subexpressions by

 re-applying common_subexpression elimination until_no new common

 subexpressions are found on one round

 It is also possible to use the framework of Exercise all-eq-exer

 to catch deep common_subexpressions



 Loop-Invariant Expressions



 Fig figpre-intro(b) shows an example of a loop-invariant

 expression The expression is loop-invariant because

 neither the variable nor is redefined within the loop

 We can optimize the program by_replacing all the re-executions in a

 loop by a single calculation outside the loop We assign the

 computation to a temporary_variable say and then replace

 the expression in the loop by There is one more point we

 need to consider when performing code_motion optimizations such_as

 this We should not execute any

 instruction that would not have executed without the optimization

 For_example if it is possible to exit the loop without executing the

 loop-invariant

 instruction at all then we should not move the instruction out of the

 loop There_are two_reasons



 enumerate



 If the instruction raises an exception then executing it may throw an

 exception that would not have happened in the original_program



 When the loop exits early the optimized program takes more time

 that the original_program



 enumerate



 To ensure_that loop-invariant_expressions in while-loops can be

 optimized compilers typically represent the statement



 verbatim

 while c

 S



 verbatim

 in the same way as the statement

 verbatim

 if c

 repeat

 S

 until not c



 verbatim

 In this way loop-invariant_expressions can be placed just prior to the

 repeat-until construct



 Unlike common-subexpression_elimination where a redundant expression

 computation is simply dropped loop-invariant-expression elimination_requires

 an expression from inside the loop to move outside the loop Thus

 this optimization is generally known_as loop-invariant code

 motion Loop-invariant code_motion may need to be repeated because

 once a variable is determined to to have a loop-invariant value

 expressions using that variable may also become loop-invariant



 Partially Redundant Expressions



 An_example of a partially_redundant expression is shown in

 Fig figpre-intro(c) The expression in block

 is redundant on the path but not on the

 path We can eliminate the redundancy

 on the former path by placing a computation of in block

 All the results of are written into a temporary

 variable and the calculation in block is replaced with

 Thus like loop-invariant code_motion partial-redundancy_elimination

 requires the placement of new expression computations



 Can All Redundancy Be Eliminated

 secpre-limit



 Is it possible to eliminate all redundant_computations along every

 path The answer is no unless we are allowed to change the flow

 graph by creating new blocks



 ex

 excritical-edge

 In the example shown in Fig figcritical-edge(a) the expression

 of is computed redundantly in block if the

 program follows the execution_path

 However we cannot simply move the computation of to block

 because doing_so would create an extra computation of

 when the path is taken



 What we would like to do is to insert the computation of

 only along the edge from block to block We can do so by

 placing the instruction in a new block say and making the

 flow of control from go_through before it reaches

 The transformation is shown in Fig figcritical-edge(b)

 ex

 figurehtb

 figureuullmanalsuch9figscriticaleps

 A critical_edge

 figcritical-edge

 figure



 We define a critical_edge of a flow_graph to be any edge leading

 from a node with more_than one successor to a node with more_than one

 predecessor By introducing new

 blocks along critical_edges we can always find a block to

 accommodate the desired expression placement

 For_instance the edge from to in

 Fig figcritical-edge(a) is critical because has two

 successors and has two predecessors



 Adding blocks may not be sufficient to allow the elimination of

 all redundant_computations As shown in Example expre-dup we

 may need to duplicate code so as to isolate the path where redundancy

 is found



 ex

 expre-dup

 In the example shown in Figure figpre-dup(a) the expression of

 is computed redundantly along the path

 We would like to remove the redundant

 computation of from block in this path and

 compute the expression only along the path

 However there is no single program point or edge

 in the source_program that corresponds uniquely to the latter path

 To create such a program point we can duplicate the pair of

 blocks and with one pair reached through and the

 other reached through as shown in

 Figure figpre-dup(b) The result of is saved in

 variable in block and moved to variable in the

 copy of reached from

 ex

 figurehtb

 figureuullmanalsuch9figspre-dupeps

 Code duplication to eliminate redundancies

 figpre-dup

 figure



 Since the number of paths is exponential in the number of

 conditional branches in the program eliminating all redundant

 expressions can greatly increase the size of the optimized code We

 therefore restrict our_discussion of redundancy-elimination

 techniques to those

 that may introduce additional blocks but that do_not duplicate

 portions of the control_flow graph



 The Lazy-Code-Motion Problem

 seclcm-conditions



 It is desirable for programs optimized with a

 partial-redundancy-elimination algorithm to have the following

 properties

 enumerate

 All redundant_computations of expressions that can be_eliminated

 without code duplication are eliminated



 The optimized program does_not perform any computation that is not in

 the original_program execution



 Expressions are computed at the latest possible time

 enumerate

 The last property is important because the values of expressions found to be

 redundant are usually held in registers until they are used

 Computing a value as late as possible minimizes its lifetime -

 the duration between the time the value is defined and

 the time it is last used which in turn minimizes its usage of a

 register

 We refer to the optimization of eliminating

 partial_redundancy with the goal of delaying the computations as much

 as possible as lazy_code motion



 To build_up your intuition of the problem we first discuss_how to

 reason_about partial_redundancy of a single expression along a single

 path For_convenience we assume for the rest of the discussion that

 every statement is a basic_block of its_own



 Full Redundancy



 An expression in block is redundant if along

 all paths_reaching has_been evaluated and the operands of

 have not been_redefined subsequently Let be the set of

 blocks each containing expression that renders in

 redundant The set of edges leaving the blocks in must

 necessarily form a cutset which if removed disconnects block

 from the entry of the program Moreover no operands of are

 redefined along the paths that lead from the blocks in to



 Partial Redundancy



 If an expression in block is only partially_redundant the

 lazy-code-motion algorithm attempts to render fully_redundant in

 by placing additional copies of the expressions in the flow_graph

 If the attempt is successful the optimized flow_graph will also have

 a set of basic_blocks each containing expression and whose

 outgoing_edges are a cutset between the entry and Like the

 fully_redundant case no operands of are redefined along the paths

 that lead from the blocks in to



 Anticipation of Expressions



 There is an additional

 constraint imposed on inserted expressions to ensure_that no extra

 operations are executed Copies of an expression must_be placed

 only at program points where the expression is anticipated We

 say that an expression is anticipated_at point if

 all paths_leading from the point eventually compute the value of

 the expression from the values of and that

 are available at that point



 Let_us now examine what it takes to eliminate partial_redundancy along

 an acyclic_path

 Suppose expression

 is evaluated only in blocks and

 and that the operands of are not redefined in blocks along the

 path There_are incoming_edges that join the path and there are

 outgoing_edges that exit the path We see that is not anticipated

 at the entry of block if and only if there_exists an outgoing

 edge leaving block

 that leads to an execution_path that does_not use the

 value of Thus anticipation limits how early an expression can

 be inserted



 We can create a cutset that includes the edge and

 that renders redundant in

 if

 is either

 available or anticipated_at the entry of If is anticipated

 but not

 available at the entry of we must place a copy of the expression



 along the incoming edge



 We have a choice of where to place the copies of the expression since

 there are usually several cutsets in the flow_graph that satisfy all

 the requirements In the above computation is introduced along the

 incoming_edges to the path of interest and so the expression is

 computed as close to the use as possible without_introducing

 redundancy Note_that these introduced operations may themselves be

 partially_redundant with other instances of the same expression in the

 program Such partial_redundancy may be_eliminated by moving

 these computations further up



 In summary anticipation of expressions limits how early an expression

 can be placed you cannot place an expression so early that it is not

 anticipated where you place it

 The earlier an expression is placed the more

 redundancy can be removed and among all solutions that

 eliminate the same redundancies the one that computes the expressions

 the latest minimizes the lifetimes of the registers holding the

 values of the expressions involved



 The Lazy-Code-Motion Algorithm



 This discussion thus motivates a

 four-step algorithm The first step computes anticipation to

 determine where expressions can be placed the second step finds the

 earliest cutset among those

 that eliminate as many redundant operations as

 possible without duplicating code and without_introducing any unwanted

 computations This step places the computations at program points

 where the values of their results are first anticipated The third

 step then pushes the cutset down to the point where any further delay

 would alter the semantics of the program or introduce redundancy The

 fourth and final_step is a simple pass to clean up the code by

 removing assignments to temporary_variables that are used only

 once Each step is accomplished with a data-flow pass the

 first and fourth are backward-flow problems the second and third are

 forward-flow problems



 Algorithm Overview

 enumerate

 Find all the expressions anticipated_at each program point using a

 backward_data-flow pass



 The second step places the computation where the values of the

 expressions are first anticipated_along some path After we have

 placed copies of an expression where the expression is first

 anticipated the expression would be available at program point

 if it has_been anticipated_along all paths_reaching

 Availability can be_solved using a forward_data-flow pass If we_wish

 to place the expressions at the earliest possible positions we can

 simply find those program points where the expressions are anticipated

 but are not available



 Executing an expression as_soon as it is anticipated may produce a

 value long before it is used An expression is postponable at

 a program point if the expression has_been anticipated and has yet to

 be used along any path reaching the program point Postponable

 expressions are found using a forward_data-flow pass We place

 expressions at those program points where they can no_longer be

 postponed



 A simple final backward_data-flow pass is used to eliminate

 assignments to temporary_variables that are used only once in the

 program

 enumerate



 Preprocessing Steps



 We_now present the full lazy-code-motion algorithm

 To keep the algorithm simple we only introduce new computations of

 expressions at the beginnings of blocks To ensure_that this

 simplification does_not reduce the effectiveness of the technique we

 insert a new block between the source and the destination of an

 edge if the destination has more_than one_predecessor Doing_so

 obviously also takes care of all critical_edges in the program



 We abstract the semantics of each block with two sets

 is the set of expressions computed in and is

 the set of expressions killed that is the set of expressions any of whose

 operands are defined in Example_expre will be used

 throughout the discussion of the four data-flow_analyses whose

 definitions are summarized in Fig figpre-df



 ex

 expre

 In the flow_graph in Fig_figpre(a) the expression

 appears three times Because the block is part of a loop

 the expression may be computed many_times The

 computation in block is not only loop-invariant it is also a

 redundant expression since its value already has_been used in

 block For this example we need to compute

 only twice

 once in block and once along the path after and

 before The lazy_code motion algorithm will place the

 expression computations at the beginning of

 blocks and

 ex

 figurehtb

 figureuullmanalsuch9figspreeps

 Flow_graph of Example_expre

 figpre

 figure



 Anticipated Expressions



 Recall that an expression is anticipated_at

 a program point if all paths_leading from point eventually

 compute the value of the expression from the values of

 and that are available at that point



 In Fig_figpre(a) all the blocks anticipating on entry are

 shown as lightly_shaded boxes The expression is anticipated in

 blocks and It is not anticipated

 on entry to

 block because the value of is recomputed within the block

 and therefore the value of that would be computed

 at the beginning of is not

 used along any path

 The expression is not anticipated on entry to because

 it is unnecessary along the branch from to (although it

 would be used along the path )

 Similarly the expression is not anticipated_at the beginning

 of because of the branch from to The anticipation of an

 expression may oscillate along a path as illustrated by





 figure

 center

 tabularlll

 (a) Anticipated Expressions (b) Available_Expressions



 Domain_Sets of expressions Sets of expressions



 Direction Backwards_Forwards



 Transfer



 function

 (



 Boundary





 Meet_()



 Equations









 Initialization















 (c) Postponable Expressions (d) Used_Expressions



 Domain_Sets of expressions Sets of expressions



 Direction_Forwards Backwards



 Transfer



 function





 Boundary





 Meet_()



 Equations









 Initialization



 tabular

 center



 align

 earliestB anticipatedBin -_availableBin



 latestB_(earliestB postponableBin)



 (euseB (SsuccB(earliestS postponableSin)))

 align



 Four data-flow passes in partial-redundancy_elimination

 figpre-df

 figure



 The data-flow_equations for the anticipated-expressions problem are shown

 in Fig figpre-df(a) The analysis is a backward

 pass An anticipated expression at the exit of a block is an

 anticipated expression on entry only if it is not in the set

 Also a block generates as new uses the set of

 expressions At the exit of the program none of the expressions are

 anticipated Since we are_interested in finding expressions that are

 anticipated_along every subsequent path the meet_operator is set

 intersection Consequently the interior_points must_be

 initialized to the universal_set as was discussed for the

 available-expressions problem in Section_ae-subsect



 Completing the Square

 Anticipated expressions (also called very busy expressions elsewhere) is

 a type of data-flow_analysis we have not seen previously

 While we have_seen backwards-flowing frameworks such_as live-variable

 analysis (Sect live-var-subsect) and we have_seen frameworks

 where the meet is intersection such_as available_expressions

 (Sect ae-subsect) this is the first example of a useful

 analysis that has both properties

 Almost all analyses we use can be placed in one of four groups

 depending_on whether they flow forwards or backwards and depending_on

 whether they use union or intersection for the meet

 Notice also that the union analyses always involve asking about

 whether there_exists a path along which something is true while the

 intersection analyses ask whether_something is true along all paths



 Available_Expressions



 At the end of this second step copies of an expression will be placed

 at program points where the expression is first anticipated If that

 is the case an expression will be available at program point

 if it is anticipated_along all paths_reaching This

 problem is similar to available-expressions described in

 Section_ae-subsect The transfer_function used here is slightly

 different though An expression is available on exit from a block

 if it is either



 enumerate



 Either

 enumerate

 Available or

 In the set of anticipated expressions

 upon_entry (ie it could be made available if we chose to

 compute it here)

 enumerate

 and



 Not killed in the block



 enumerate



 The data-flow

 equations for available_expressions are shown in

 Fig_figpre-df(b) To_avoid confusing the meaning of we

 refer to the result of an earlier analysis by appending

 to the name of the earlier analysis



 With the earliest_placement strategy the set of expressions placed_at

 block ie is defined as the set of anticipated

 expressions that are not_yet available That is



 earliestBanticipatedBin -_availableBin





 ex

 expre-redundant

 The expression in the flow_graph in

 Figure figpre-redundant is not anticipated_at the entry of

 block but is anticipated_at the entry of block It is

 however not necessary to compute the expression in block

 because the expression is already available due to block

 ex

 figurehtb

 figureuullmanalsuch9figspre-redundanteps

 Flow_graph for Example expre-redundant illustrating

 the use of availability

 figpre-redundant

 figure



 ex

 Shown with dark_shadows in Fig_figpre(a) are the blocks

 for which expression is not available they are

 and The early-placement positions are represented_by

 the lightly_shaded boxes with dark_shadows and are

 thus blocks and

 Note for instance that is considered available on entry to

 because there is a path along

 which is anticipated_at least once - at in this case -

 and since the beginning of neither_nor was

 recomputed

 ex



 Postponable Expressions



 The third_step postpones the computation of expressions as much as

 possible while preserving the original_program semantics and

 minimizing redundancy Example exearly illustrates the

 importance of this step



 ex

 exearly

 In the flow_graph shown in Figure figpre-early the expression

 is computed twice along the path

 The expression is anticipated even at the

 beginning of block If we compute the expression as_soon as it

 is anticipated we would have computed the expression in

 The result would have to be saved from the beginning through

 the execution of the loop comprising blocks and until it

 is used in block Instead we can delay the computation of

 expression until the beginning of and until the flow

 of control is about to transition from to

 ex

 figurehtb

 figureuullmanalsuch9figspre-earlyeps

 Flow_graph for Example exearly to illustrate the need

 for postponing an expression

 figpre-early

 figure





 Formally an expression is postponable to a

 program point if an early placement of is encountered

 along every_path from the entry_node to and there is no

 subsequent use of after the last such placement



 ex

 Let_us again consider expression in Fig figpre

 The two earliest points for are and note_that

 these are the two blocks that are both lightly and darkly shaded in

 Fig_figpre(a) indicating that is both anticipated

 and not available for these blocks and only these blocks

 We cannot_postpone from to because is

 used in

 We can postpone it from to however



 But we cannot_postpone from to

 The_reason is that although is not used in placing its

 computation at instead would lead to a redundant computation of

 along the path

 As we_shall see is one of the latest places we can compute



 ex



 The data-flow_equations for the postponable-expressions problem are

 shown in Fig figpre-df(c) The analysis is a forward pass

 We cannot_postpone an expression to the entry of the program so

 An expression is postponable to the exit

 of block if it is not used in the block and it is either

 postponable to the entry of or it is in

 An expression is not postponable to the

 entry of a block unless all its_predecessors include the

 expression in their sets at their exits Thus the meet

 operator is set_intersection and the interior_points must_be

 initialized to the top_element of the semilattice - the universal_set



 Roughly speaking an expression is placed_at the frontier where

 an expression transitions from being postponable to not being postponable

 More_specifically an expression may be placed_at the beginning of

 a block only if the expression is in 's earliest or

 postponable set upon_entry In_addition is in the

 postponement frontier of if one of the following holds

 enumerate



 is not in In other_words

 is in



 cannot be postponed to one of its successors In other_words

 there_exists a successor of such that is not in the earliest or

 postponable set upon_entry to that successor

 enumerate

 Expression can be placed_at the front of block in either of

 the above scenarios because of the new blocks introduced by the

 preprocessing step in the algorithm



 ex

 Fig figpre(b) shows the result of the analysis The

 light-shaded boxes represent the blocks whose set includes

 The dark_shadows indicate those that include in

 their set The latest_placements of the expressions are

 thus the entries of blocks and since



 enumerate



 is in the

 set of but not and



 's set

 includes and it uses



 enumerate

 The expression is stored

 into the temporary_variable in blocks and and

 is used in place of everywhere else as shown in the figure

 ex



 Used_Expressions

 Finally a backward_pass is used to determine if the temporary

 variables introduced are used beyond the block they are in We_say

 that an expression is used at point if there_exists a path

 leading_from that uses the expression before the value is

 re-evaluated This analysis is essentially liveness_analysis (for

 expressions rather_than for variables)



 The data-flow_equations for the used expressions problem are shown

 in Fig figpre-df(d) The analysis is a backward

 pass A used expression at the exit of a block is a

 used expression on entry only if it is not in the set

 A block generates as new uses the set of expressions in

 At the exit of the program none of the expressions are

 used Since we are_interested in finding expressions that are

 used by any subsequent path the meet_operator is set

 union Thus the interior_points must_be

 initialized_with the top_element of the semilattice - the empty_set



 Putting it All_Together



 All the steps of the algorithm are summarized in

 Algorithm alglazy-code-motion



 alg

 alglazy-code-motion

 Lazy Code_Motion



 A flow_graph for which and have_been computed

 for each block



 A modified flow_graph satisfying the four lazy_code motion conditions

 in Section seclcm-conditions



 enumerate



 Insert an empty block along all edges entering a block with more

 than one_predecessor



 Find for all blocks as defined in

 Fig figpre-df(a)



 Find for all blocks as defined in

 Fig_figpre-df(b)



 Compute the earliest placements for all blocks



 earliestB anticipatedBin -_availableBin





 Find for all blocks as defined in

 Fig figpre-df(c)



 Compute the latest_placements for all blocks



 align

 latestB_(earliestB postponableBin)



 (euseB (S in succB(earliestS postponableSin)))

 align

 Note_that denotes complementation with_respect to the set of all

 expressions computed by the program



 Find for all blocks as defined in

 Fig figpre-df(d)



 For each expression say computed by the program do the

 following

 enumerate

 Create a new_temporary say for

 For all blocks such that

 is in

 add t xy at the beginning of

 For all blocks such that is in



 euseB (latestB usedoutB)



 replace every original by

 enumerate



 enumerate

 alg



 Summary



 Partial-redundancy elimination finds many different forms of redundant

 operations in one unified algorithm This algorithm illustrates_how

 multiple data-flow_problems can be used to find optimal expression placement



 enumerate



 The placement constraints are provided by the anticipated-expressions

 analysis which is a backwards flow with an meet

 operator as it determines if expressions are used subsequent to

 each program point on all paths



 The earliest_placement of an expression is given by program points

 where the expression is anticipated but is not available Available

 expressions are found with a forwards flow analysis with an

 meet_operator that computes if an expression has_been anticipated

 before each program point along all paths



 The latest placement of an expression is given by program points where

 an expression can no_longer be postponed Expressions are

 postponable at a program point if for all paths_reaching the

 program point no use of the expression has_been encountered

 Postponable expressions are found

 with a forwards flow analysis with an meet

 operator



 Temporary assignments are eliminated unless they are used by some path subsequently We find used expressions with a

 backwards data flow pass and a operator



 enumerate



 sexer

 pre-exer

 For the flow_graph in Fig pre-exer-fig



 itemize



 a)

 Compute anticipated for the beginning and end of each block





 b)

 Compute available for the beginning and end of each block





 c)

 Compute earliest for each block





 d)

 Compute postponable for the beginning and end of each block





 e)

 Compute used for the beginning and end of each block





 f)

 Compute latest for each block





 g)

 Introduce temporary_variable show where it is computed and where it

 is used





 itemize

 sexer



 figurehtfb



 fileuullmanalsuch9figspre-exereps

 Flow_graph for Exercise pre-exer

 pre-exer-fig

 figure



 exer

 Repeat_Exercise pre-exer for the flow_graph of

 Fig_fg1-fig (see the exercises to Section_secopt-sources)

 You_may limit your analysis to the expressions and



 exer



 vhexer

 The concepts discussed in this_section can also be applied to

 eliminate partially_dead code A definition of a variable is

 partially_dead if the variable is live_on some paths and not others

 We can optimize the program execution by only performing the

 definition along paths where the variable is live Unlike

 partial-redundancy_elimination where expressions are moved before the

 original the new definitions are placed after the original

 Develop an algorithm to move partially_dead code so expressions are

 evaluated only where they will_eventually be used

 vhexer

 Region-Based_Analysis



 The iterative_data-flow analysis algorithm we have discussed so_far is

 just one approach to solving_data-flow problems Here we discuss

 another approach called region-based_analysis Recall

 that in the iterative-analysis

 approach we create transfer_functions for basic_blocks then

 find the fixedpoint solution by repeated_passes over the blocks Instead of

 creating transfer_functions just for individual blocks a

 region-based_analysis finds transfer_functions that summarize the

 execution of progressively_larger regions of the program

 Ultimately transfer_functions for

 entire procedures are constructed and then applied to get the desired

 data-flow_values directly



 While a data-flow_framework using an iterative_algorithm is

 specified_by a semilattice of data-flow_values and a family of

 transfer_functions closed_under composition region-based_analysis

 requires more

 A region-based framework

 includes both a semilattice of data-flow_values and a

 semilattice of transfer_functions that must possess

 a meet_operator a composition

 operator and a closure_operator

 We_shall see what all these elements entail in

 Section



 A region-based_analysis is particularly useful for data-flow_problems

 where paths that have cycles may change the data-flow_values

 The closure_operator allows the effect of a loop to be summarized more

 effectively than does iterative analysis The technique is also useful for

 interprocedural_analysis where transfer_functions associated_with a

 procedure call may be treated_like the transfer_functions associated_with

 basic_blocks



 For_simplicity we_shall consider only forward_data-flow problems in

 this_section We first illustrate_how region-based_analysis works by

 using the familiar example of reaching_definitions

 In Section we show a

 more compelling use of this technique when we

 study the analysis of induction_variables



 Regions



 In region-based_analysis a program is viewed_as a hierarchy of regions which are (roughly) portions of a flow_graph that have

 only one point of entry You_should find this concept of viewing

 code as a hierarchy of regions intuitive because a block-structured

 procedure is naturally organized as a hierarchy of regions Each

 statement in a block-structured program is a region as control_flow

 can only enter at the beginning of a statement Each level of

 statement nesting corresponds to a level in the region_hierarchy



 Formally a region of a flow_graph is a

 collection of nodes and edges such that







 There is a header in that dominates all the nodes in



 If some node can reach a node in without_going

 through then is also in



 is the set of all the control_flow edges between nodes and

 in except (possibly) for some that enter







 Clearly a natural_loop is a region but a region does_not necessarily

 have a back_edge and need not contain any cycles For

 example in Fig nodes and together_with

 the edge

 form a region so do nodes and with

 edges and



 However the subgraph with

 nodes and with edge does_not form a region

 because control

 may enter the subgraph at both nodes and

 More_precisely neither_nor dominates the other so

 condition (1) for a region is violated

 Even_if we picked say to be the header we

 would violate condition (2) since we can reach from without

 going_through and is not in the region





 figureuullmanalsuch9figsregionseps

 Examples of regions



 Region Hierarchies for Reducible Flow_Graphs



 In what_follows we_shall assume the flow_graph is reducible

 If occasionally we must deal_with nonreducible_flow graphs then we can

 use a technique called node_splitting discussed in

 Section



 To construct a hierarchy of regions we identify the natural_loops

 Recall from Section that in a reducible_flow

 graph any two natural_loops are either_disjoint or one is nested

 within the other

 The process of parsing a reducible_flow graph into its hierarchy of

 loops begins_with every block as a region by itself

 We call these regions leaf_regions

 Then we order the natural_loops from the inside out ie_starting

 with the innermost_loops

 To process a loop we replace the entire loop by a node in two steps







 First the body of the loop (all nodes and edges except the back

 edges to the header) is replaced_by a node representing a region

 Edges to the header of now enter the node for

 An edge from any exit of loop is replaced_by an edge from to the

 same destination

 However if the edge is a back_edge then it becomes a loop on

 We call a body_region



 Next we construct a region that represents the entire_natural

 loop

 We call a loop region

 The only_difference between and is that the latter includes

 the back_edges to the header of loop

 Put_another way when replaces in the flow_graph all we have

 to do is remove the edge from to itself



 We proceed this way reducing larger and larger loops to single nodes

 first with a looping edge and then without

 Since loops of a reducible_flow graph are nested or disjoint the loop

 region's node can represent all the nodes of the natural_loop

 in the series of flow_graphs

 that are constructed by this reduction process



 Eventually all natural_loops are reduced to single nodes

 At that point the flow_graph may be reduced to a single_node or there

 may be several nodes remaining with no loops ie the reduced flow

 graph is an acyclic_graph of more_than one node

 In the former_case we are done constructing the region_hierarchy while

 in the latter case we construct one more body_region for the entire

 flow_graph





 figureuullmanalsuch9figsrd-regioneps

 (a) An_example flow_graph for the reaching_definitions problem

 and (b) Its region_hierarchy





 Consider the control_flow graph in Fig (a) There is one

 back_edge in this flow_graph which leads from to The

 hierarchy of regions is shown in Fig (b) the

 edges shown are the edges in the region flow_graphs There_are

 altogether 8 regions







 Regions are leaf_regions representing

 blocks through

 respectively Every block is also an exit block in its

 region



 Body region

 represents the body of the only loop in the flow_graph it consists

 of regions and and three_interregion edges

 and It has two exit blocks and

 since they both have outgoing_edges not contained in the region

 Figure (a) shows the flow_graph with reduced to

 a single_node

 Notice_that although the edges and

 have both been replaced_by edge it is important to

 remember that the latter edge represents the two former edges since we

 shall have to propagate transfer_functions across this edge eventually

 and we need to know that what comes out of both blocks and

 will reach the header of



 Loop region

 represents the entire_natural loop It includes one

 subregion and one back_edge

 It has also two exit

 nodes again and

 Figure (b) shows the flow_graph after the entire

 natural_loop is reduced to



 Finally body_region is the top_region

 It includes three regions

 and three_interregion edges

 and

 When we reduce the flow_graph to it becomes a single_node

 Since there are no back_edges to its header there is no need

 for a final_step reducing this body_region to a loop region







 fileuullmanalsuch9figsreducingeps

 Steps in the reduction of the flow_graph of Fig to a single region



 To summarize the process of decomposing reducible_flow

 graphs hierarchically we offer the following algorithm





 Constructing a bottom-up order of regions of a reducible_flow graph



 A reducible_flow graph



 A list of regions of that can be used in region-based data-flow

 problems







 Begin the list with all the leaf_regions consisting of single blocks of

 in any order



 Repeatedly choose a natural_loop such that if there are any natural

 loops contained_within then these loops have had their body and

 loop regions added to the list already

 Add first the region consisting of the body of (ie without

 the back_edges to the header of ) and then the loop region of



 If the entire_flow graph is not itself a natural_loop add at the end

 of the list the region consisting of the entire_flow graph





 Where Reducible Comes_From

 We_now see_why reducible_flow graphs were given that name

 While we_shall not prove this fact the definition of

 reducible_flow graph used in this_book

 involving the back_edges of the graph is

 equivalent to several definitions in which we mechanically reduce the

 flow_graph to a single_node

 The process of collapsing natural_loops described in

 Section is one of them

 Another interesting definition is that the reducible_flow graphs are

 all and only those that can be reduced to a single_node by the

 following two transformations









 Remove an edge from a node to itself





 If node has a single predecessor and is not the entry of

 the flow_graph combine and







 Overview of a Region-Based_Analysis



 For each region and for each subregion within we

 compute a transfer_function that summarizes the effect

 of executing all possible paths

 leading_from the entry of

 to the entry of while staying within

 We_say that a block within is an exit block

 of region

 if it has an outgoing_edge to some block outside We also compute

 a transfer_function for each exit block of

 denoted that summarizes the effect

 of executing all possible paths within

 leading_from the entry of

 to the exit of



 We then proceed up the region_hierarchy computing transfer_functions

 for progressively_larger regions We begin_with regions that are

 single blocks where is just the identity_function and

 is the transfer_function for the block itself

 As we move up the hierarchy







 If is a body_region then the edges belonging

 to form an acyclic_graph on the subregions of

 We may proceed to compute the transfer_functions in a

 topological_order of the subregions



 If is a loop region then we only need to account for the effect of

 the back_edges to the header of





 Eventually we reach the top of

 the hierarchy and compute the

 transfer_functions for region

 that is the entire_flow graph

 How we perform each of these computations will be seen in

 Algorithm



 The next step is to compute the data-flow_values at the entry and exit

 of each block We process the regions in the reverse order

 starting_with region and working our way down the hierarchy

 For each region we compute the data flow values at the entry For

 region we apply

 to get the data flow

 values at the entry of the subregions in We repeat until

 we reach the basic_blocks at the leaves of the region_hierarchy



 Necessary Assumptions About Transfer_Functions



 In order for region-based_analysis to work we need to make certain

 assumptions about properties of the set of transfer_functions in the

 framework

 Specifically we need three

 primitive operations on transfer_functions composition meet and

 closure only the first is required for data-flow_frameworks that use

 the iterative_algorithm



 Composition



 The transfer_function of a sequence of

 nodes can be derived by_composing the functions representing the

 individual nodes Let and be transfer_functions of nodes

 and The effect of executing followed_by is

 represented_by

 Function composition has_been discussed in

 Section and an example using reaching_definitions

 was shown in Section To review let and

 be the

 and sets for

 Then





 f2 f1 (x) gen2 ((gen1 (x_- kill1))

 - kill2)



 (gen2 (gen1 -_kill2)) (x_- (kill1_kill2))

 Thus the and sets for

 are and

 respectively

 The same idea works for any transfer_function of the gen-kill form

 Other transfer_functions may also be closed but we have to consider

 each case separately



 Meet



 Here the transfer_functions themselves are values of a semilattice

 with a meet_operator

 The meet of two transfer_functions and

 is defined by

 where is the

 meet_operator for data-flow_values

 The meet_operator on transfer_functions

 is used to combine the effect of alternative paths of

 execution with the same end points Where it is not ambiguous from

 now on we_shall refer to the meet_operator of transfer_functions

 also as

 For the reaching-definitions_framework we have





 (f1 f2)(x) f1(x)_f2(x)



 (gen1 (x_- kill1))

 (gen2 (x_- kill2))



 (gen1 gen2) (x_- (kill1_kill2))

 That is the and sets for are

 and

 respectively

 Again the same argument applies to any set of gen-kill transfer

 functions



 Closure



 If represents the transfer_function of a

 cycle then represents the effect of going_around the cycle

 times In the case_where the number of iterations is not known we

 have to assume that the loop may be executed 0 or_more times We

 represent the transfer_function of such a loop by the closure of which is

 defined by









 Note_that must_be the identity transfer

 function since it represents the

 effect of going zero times_around the loop ie_starting at the entry

 and not moving

 If we let represent the identity transfer_function then we can

 write











 Suppose the transfer_function in a reaching_definitions framework

 has a set and a set Then





 f2(x) f(f(x))



 (gen ((gen (x_- kill)) -_kill)



 gen_(x -_kill)



 f3(x) f(f2(x))



 gen_(x -_kill)



 and so on any is

 That is going_around a loop doesn't affect the transfer_function if it

 is of the - form

 Thus





 f(x) I f1(x)_f2(x)



 x (gen (x_- kill))



 gen x

 That is the and sets for are and

 respectively Intuitively since we might not go_around a loop at all

 anything in will reach the entry to the loop

 In all subsequent_iterations the

 reaching_definitions include those in the set



 An Algorithm for Region-Based_Analysis



 The following algorithm solves a forward data-flow-analysis problem on

 a reducible_flow graph according to some framework that satisfies the

 assumptions of Section

 Recall that and

 refer to

 transfer_functions that

 transform data-flow_values at the entry to region into the correct

 value at the entry of subregion and the exit of the exit block

 respectively





 Region-Based_Analysis



 A data-flow_framework with the properties outlined in

 Section and a reducible_flow graph



 Data-flow values for each block of







 Use_Algorithm to construct the bottom-up sequence

 of regions of say where is the

 topmost region



 Perform the bottom-up analysis to

 compute the transfer_functions summarizing the effect of executing a

 region

 For each region in the bottom-up order do the following





 If is a leaf region corresponding to block let

 and

 the transfer_function associated_with block

 If is a body_region perform the computation of

 Fig (a)

 If is a loop region perform the computation of Fig (b)



 Perform the top-down_pass to find the data-flow_values at the

 beginning of each region







 For each region in the top-down order







 where is the immediate enclosing region of





 Let_us first look_at the details of how the bottom-up analysis works

 In line_(1) of Fig (a) we

 visit the subregions of a body_region in some topological_order

 Line (2) computes the transfer_function representing all the possible

 paths from the header of to the header of

 then in lines_(3) and (4) we compute

 the transfer_functions representing all the possible

 paths from the header of to the exists of - that is

 to the exits of all

 blocks that have successors outside

 Notice_that all the predecessors in

 must_be in regions that precede

 in the topological_order constructed at line_(1)

 Thus will have_been computed already

 in line_(4) of a previous iteration through the outer_loop



 For loop regions we perform the steps of lines_(1) through (4) in

 Fig (b) Line (2) computes the effect of going

 around the loop body_region zero_or more times Lines (3) and (4)

 compute the effect at the exits of the loop after one or_more iterations



 In the top-down_pass of the algorithm

 step 3(a) first assigns the boundary_condition

 to the input of the top-most region

 Then if is immediately contained

 in we can simply_apply the transfer_function to

 the data-flow value to compute













 Details of region-based data-flow computations





 Let_us apply_Algorithm to find reaching_definitions

 in the flow_graph in

 Fig (a)

 Step 1 constructs the bottom-up order in which the regions are visited this

 order will be the numerical order of their subscripts





 The values of the and sets for the five blocks

 are summarized

 below





 Remember the simplified rules for - transfer_functions

 from Section





 To take the meet of transfer_functions take the union of the 's

 and the intersection of the 's



 To compose transfer_functions take the union of both the 's and

 the 's

 However as an exception an expression that is generated_by the first

 function not generated_by the second but killed

 by the second is not in the of the result



 To take the closure of a transfer_function retain its and replace

 the by



 The first five regions are blocks

 respectively For

 is

 the identity_function and is

 the transfer_function for block



















 Computing transfer_functions for the flow_graph in

 Fig (a) using region-based_analysis



 The rest of the transfer_functions constructed in Step_2

 of Algorithm are

 summarized in Fig

 Region consisting of regions and

 represents the loop body and thus does_not include the back_edge

 The order of processing these regions will be the

 only topological_order First has no

 predecessors within remember that the edge

 goes outside Thus is the identity

 function(Strictly speaking we mean but

 when a region like is a single block it is often clearer if we

 use the block name rather_than the region name in this context)

 and is the transfer_function for

 block itself



 The header of region has one_predecessor within_namely

 The transfer_function to its entry is simply the transfer_function to the

 exit of which has already_been computed

 We compose this function with the transfer_function

 of within its_own region to compute the transfer_function to the

 exit of



 Last for the transfer_function to the entry of

 we must compute



 because both and are predecessors of the header of



 This transfer_function is composed with the transfer_function

 to get the desired function

 Notice for example that is not_killed in this transfer_function

 because the path does_not redefine variable



 Now_consider loop region It contains only one subregion

 which represents its loop body

 Since there is only one back_edge to the header of

 the transfer_function representing the execution of the loop

 body

 0 or_more times is just

 the set is and the

 set is

 There_are two exits_out of region blocks and



 Thus

 this transfer_function is composed with each of the transfer

 functions of to get the corresponding transfer_functions of

 Notice for instance how is in the set for

 because of paths like or even





 Finally consider the entire_flow graph

 Its subregions are and which we_shall consider in

 that topological_order

 As before the transfer_function is simply

 the identity_function and the

 transfer_function is just which in turn is





 The header of which is has only one_predecessor

 so the transfer_function to its entry is simply the transfer_function

 out of in region

 We compose with the transfer_functions to

 the exits of

 and within to obtain their corresponding transfer_functions within



 Lastly we consider

 Its header has two predecessors within_namely and



 Therefore we compute

 to get Since the transfer_function of block

 is the identity_function



 Step_3 computes the actual reaching_definitions from the transfer

 functions In step 3(a) since there are no

 reaching_definitions at the beginning of the program

 Figure shows_how step 3(b) computes the rest of

 the data-flow

 values The step starts with the subregions of Since the

 transfer_function from the start of to the start of each of its

 subregion has_been computed a single application of the transfer

 function finds the data-flow value at the start each subregion We

 repeat the steps until we get the data-flow_values of the leaf

 regions which are simply the individual basic_blocks

 Note_that the data-flow_values shown in Figure are exactly what we would get had we applied

 iterative_data-flow analysis to the same flow_graph as must_be the

 case of course



































































 Final steps of region-based flow analysis







 NOT SURE IF WE WANT TO DWELL ON THIS



 Efficiency of Region-Based_Analysis



 It is quite tricky to compare the iterative_algorithm with the

 hierarchical_algorithm

 To_begin consider the iterative_approach

 We observed in Section that the

 depth of a reducible_flow graph (as_defined in Section ) is

 typically 3 and therefore or about

 5 passes_through the flow_graph suffice

 Thus if there are nodes there will be or about

 computations of data-flow

 values - half for the 's and half for the 's

 Some of these computations may be more_complex than_others especially

 as a node may have a large_number of predecessors and therefore take

 extra time to compute its value

 However as we_wish to compare the running_time of two algorithms on the

 same flow_graph it is reasonable to count only the values computed and

 not worry_about the time needed to compute each one



 For the sake of comparison assume we are applying a data-flow_framework

 whose transfer_functions are of the - type

 Thus the data-flow_values can be represented_by bit_vectors as

 discussed in Section

 We may therefore estimate the cost of the iterative_algorithm as the

 computation of bit_vectors



 If we look_at the hierarchical_algorithm applied to a flow_graph of

 nodes and depth we observe several things







 Since we compute only 's in step 2 of Algorithm

 the work at a node that is in regions (other_than the region that is

 the node itself) involves computing values



 However each of those values is a transfer_function not a data-flow

 value and we require two bit_vectors to represent a transfer_function -

 one for and one for

 Thus bit-vectors are computed



 If the flow graph's depth is then there are at most regions

 in which any node finds itself body regions loop regions

 surrounding them and perhaps a final body_region representing the

 entire_flow graph

 Thus in step 2 the number of bit_vectors computed for all nodes is no more

 than and it could be less



 The third_step involves no computation since the 's are just the

 components of the corresponding transfer_function



 The last step of Algorithm computes data-flow

 values consisting of one bit_vector each





 Therefore the total_number of bit_vectors computed by the hierarchical

 algorithm is at most

 That figure appears to be

 almost twice the bit_vectors computed by the

 iterative_algorithm

 Further the

 hierarchical_algorithm is conceptually more_complex especially if

 there are nonreducible_flow graphs with which to contend

 However there are two important reasons_why the region-based approach

 turns_out to be more rather_than less efficient than the iterative

 approach







 a)

 The estimates of bit_vectors computed assume each node is at the maximum

 depth for the flow_graph In reality many nodes of a flow_graph will

 appear inside only one loop or no loops and the innermost_loops tend

 to be small

 Thus the average_number of regions in which a node appears

 may be considerably less_than our

 assumed regions



 b)

 One can avoid computing if is not the entire_flow graph

 and is not an exit of

 region ie there is no node out of that goes to a node not

 in

 The_reason is that if is a node which is an exit of then one

 can skip_over nodes like and compute for some region

 containing if we use the transfer_function for from the header

 of to

 For_example if of Fig were some interior

 region and not the entire_flow graph then we would have no need for

 in the computation of transfer_functions for

 We could compute by_composing (because

 is the only predecessor of the header of ) with







 Handling Nonreducible Flow_Graphs



 If nonreducible_flow graphs are expected to be common for the programs

 to be processed by a compiler or other program-processing software then

 we recommend using an iterative rather_than a hierarchy-based approach

 to data-flow_analysis

 However if we need only to be prepared for the occasional nonreducible

 flow_graph then the following node-splitting technique is

 adequate



 Construct regions from natural_loops to the extent possible

 If the flow_graph is nonreducible we_shall find that the resulting

 graph of regions has cycles but no back_edges so we cannot parse the

 graph any further

 A_typical situation is suggested in Fig (a)

 which has the same structure as the nonreducible_flow graph of

 Fig but the nodes in

 Fig may actually be complex regions as

 suggested by the smaller nodes within







 fileuullmanalsuch9figsnode-splittingeps

 Duplicating a region to make a nonreducible_flow graph become

 reducible



 We pick some region that has more_than one_predecessor and is not the

 header of the entire_flow graph

 If has predecessors make copies of the entire_flow graph

 and connect each predecessor of 's header to a different copy of



 Remember that only the header of a region could possibly have a

 predecessor outside that region

 It_turns out although we_shall not prove it that such node_splitting

 results in a reduction by at_least one in the number of regions after

 new back_edges are identified and their regions constructed

 The resulting graph may still not be reducible but by alternating a

 splitting phase with a phase where new natural_loops are identified and

 collapsed to regions we eventually are left with a single region ie

 the flow_graph has_been reduced





 The splitting shown in Fig (b) has turned the

 edge into a back_edge since now dominates



 These two regions may thus be_combined into one

 The resulting three regions - and the new region form

 an acyclic_graph and therefore may be_combined into a single body

 region

 We thus have reduced the entire_flow graph to a single

 region

 In_general additional splits may be necessary and in the worst_case

 the total_number of basic_blocks could become exponential in the number

 of blocks in the original flow_graph



 We must also think about how the result of the data-flow_analysis on the

 split flow_graph relates to the answer we desire for the original flow

 graph

 There_are two approaches we might consider







 Splitting regions may be beneficial for the optimization process and we

 can simply revise the flow_graph to have copies of certain blocks

 Since each duplicated block is entered along only a subset of the paths

 that reached the original the data-flow_values at these duplicated

 blocks will tend to contain more specific information than was available

 at the original

 For_instance fewer definitions may reach each of the duplicated_blocks

 that reach the original block



 If we_wish to retain the original flow_graph with no splitting then

 after analyzing the split flow_graph we look_at each split block

 and its corresponding set of blocks

 We may compute

 and

 similarly for the 's







 For the flow_graph of Fig (see the exercises for

 Section )







 Find all the possible regions You_may however omit from

 the list the regions consisting of a single_node and no edges



 Give the set of nested regions constructed by

 Algorithm



 Give a - reduction of the flow_graph as

 described in the box on Where 'Reducible' Comes_From in

 Section







 Repeat_Exercise on the following flow_graphs







 a)

 Fig



 b)

 Fig



 c)

 Your_flow graph from Exercise



 d)

 Your_flow graph from Exercise







 Prove that every natural_loop is a region





 Show that a flow_graph is reducible if and only it can be transformed to

 a single_node using







 a)

 The operations and described in the box

 in Section



 b)

 The region definition introduced in Section







 Show that when you apply node_splitting to a nonreducible_flow graph

 and then perform - reduction on the resulting split

 graph you wind_up with strictly fewer nodes than you started with





 What happens if you apply node-splitting and - reduction

 alternately to reduce a complete directed

 graph of nodes

 Symbolic Analysis



 We_shall use symbolic analysis in this_section to illustrate the use

 of region-based_analysis In this analysis we track the values of

 variables in programs symbolically as expressions of input variables

 and other variables which we call reference_variables

 Expressing variables in terms of the same set of reference

 variables draws out their relationships

 Symbolic analysis can be used for

 a range of purposes such_as optimization parallelization and analyses

 for program understanding













 An_example motivating symbolic analysis







 Consider the simple example in Fig

 Here we use as the sole reference variable

 Symbolic analysis will find that has the value and has

 the value after their_respective assignment statements in lines

 (2) and (3) This information is useful for example in determining

 that the two assignments in lines_(4) and (5) write to different

 memory_locations and can thus be executed in parallel Furthermore

 we can tell that the condition is never true

 thus allowing the optimizer to remove the conditional_statement in

 lines (6) and (7) all together



 Affine Expressions of Reference Variables



 Since we cannot create succinct and closed-form symbolic expressions

 for all values computed we choose an abstract domain and approximate

 the computations with the most_precise expressions within the domain

 We have_already seen an example of this strategy before constant_propagation

 In constant_propagation our abstract domain consists of the

 constants an symbol if we have not determined if the value

 is a constant and a special symbol that is used whenever a variable

 is found not to be a constant



 The symbolic analysis we present here expresses values as affine

 expressions of reference_variables whenever possible An expression

 is affine with_respect to variables if it can

 be_expressed as where

 are constants Such expressions are informally known_as

 linear expressions Strictly speaking an affine_expression is linear

 only if is zero We are_interested in affine_expressions

 because they are often used to index arrays in loops-such information

 is useful for optimizations and parallelization Much more will be said

 about this topic in Chapter



 Induction_Variables



 Instead of using program variables as reference

 variables an affine_expression can also be

 written in terms of the count of iterations through the loop

 Variables whose values can be_expressed

 as where is the count of iterations through the

 closest enclosing_loop are

 known_as induction_variables





 Consider the code





 Suppose we introduce for the loop a variable say

 representing the number of iterations executed The value is 0

 in the first iteration of the loop 1 in the second and so

 on We can express variable as an affine

 expression of namely Variable which is

 takes on values during successive iterations of the

 loop

 Thus has the affine_expression

 We_conclude that both and are induction_variables of this loop



 Expressing variables as affine_expressions of loop_indexes makes the

 series of values being computed explicit and enables several

 transformations The series of values taken on by an induction_variable

 can be computed with additions rather_than

 multiplications This transformation is known_as strength

 reduction and was_introduced in Section For

 instance we can eliminate the multiplication xm3 from the loop of

 Example by

 rewriting the loop as







 In_addition notice

 that the locations assigned 0 in that loop



 are also affine_expressions of the loop_index In_fact this series of

 integers

 is the only one that needs to be computed we need neither_nor

 The code above can be

 replaced simply by







 Besides speeding up the computation symbolic analysis

 is also useful for parallelization When the array indexes in a loop

 are affine_expressions of loop_indexes we can reason_about relations of

 data_accessed across the iterations For_example we can tell that

 the locations written are different in each iteration and therefore

 all the iterations in the loop can be executed in parallel on

 different_processors Such information is used

 in Chapters and to extract parallelism from sequential programs



 Other Reference Variables



 If a variable is not a linear function of the reference

 variables already chosen

 we have the option of treating its

 value as reference for future operations

 For_example in the code below





 while the value held by after the function call cannot itself be

 expressed_as a linear function of any reference_variables it can be used

 as reference for subsequent statements For_example using as a

 reference variable we can discover that

 is one larger_than at the end of the program













 Source code for Example







 Our running_example for this_section is based_on the source code

 shown in Fig

 The inner and outer loops are easy to understand since and are

 not modified except as required by the for-loops It is thus possible

 to replace and by reference

 variables and that count the number of

 iterations of the outer and inner_loops respectively That is we can

 let and and substitute for and throughout

 When translating to intermediate_code we can take_advantage of the fact

 that each loop iterates at_least once and so postpone the test for

 and to the ends of the loops

 Figure shows the flow_graph for the code of

 Fig after introducing and and treating the

 for-loops as if they_were repeat-loops







 fileuullmanalsuch9figsiv-regioneps



 The region graph of Example





 It_turns out that and are all induction_variables

 The sequences of values

 assigned to the variables in each line of the code are shown in

 Figure

 As we_shall see it is possible to discover the affine_expressions for

 these variables in terms of the reference_variables and That

 is at line_(4) at line_(7) and at line_(8)





































 Sequence of values seen in program points in Example



 Data-Flow Problem Formulation



 This analysis finds affine_expressions of reference_variables

 introduced (1) to count the number of iterations executed in each

 loop and (2) to hold values at the entry of regions where necessary

 This analysis also finds

 induction_variables loop invariants as_well as constants

 as degenerate affine_expressions Note_that

 this analysis cannot find all constants because it only tracks affine

 expressions of reference_variables



 Data-Flow Values Symbolic Maps



 The domain of data-flow_values for this analysis is symbolic_maps

 which are functions that map each variable in the program to a value

 The value is either an affine function of reference values or the

 special symbol to represent a non-affine expression If there

 is only one variable the bottom value of the semilattice is a map

 that sends the variable to The semilattice for variables

 is simply the product of the individual semilattices We use

 to denote the bottom of the semilattice which maps all variables to

 We can define the symbolic_map that sends all variables to an

 unknown value to be the top data-flow value as we did for constant

 propagation However we do_not need top values in region-based

 analysis







































 Symbolic maps of the program in Example





 The symbolic_maps associated_with each block for the code in

 Example are shown in Figure

 We_shall see later how these maps are discovered they are the result of

 doing region-based data-flow_analysis on the flow_graph of

 Fig



 The symbolic_map associated_with the entry of the program is At

 the exit of the value of is set to 0 Upon entry to

 block has value 0 in the first iteration and increments by

 one in each subsequent iteration of the outer

 loop Thus has value at the

 entry of the th_iteration and value at the end The symbolic

 map at the entry of maps variables to because the

 variables have unknown values on entry to the inner_loop (their values

 depend_on the number of iterations of the outer_loop so far)

 iteration The symbolic_map on exit from reflects the assignment

 statements to and in that block The rest of the symbolic

 maps can be deduced in a similar_manner

 Once we have established the validity of the maps in

 Fig we can replace each of the assignments to

 and in Fig by the appropriate affine

 expressions That is we can replace Fig by the code in

 Fig













 The code of Fig with assignments replaced_by

 affine_expressions of the reference_variables and





 Transfer Function of a Statement



 The transfer_functions in this data-flow_problem send symbolic_maps to

 symbolic_maps To_compute the transfer_function of an assignment

 statement we interpret the semantics of the statement and determine

 if the assigned variable can be_expressed as an affine_expression

 of the values on the right of the

 assignment The values of all other variables remain

 unchanged



 Cautions Regarding Transfer_Functions on Value Maps A

 subtlety in the way we define the transfer_functions on symbolic_maps

 is that we have options regarding how the effects of a computation are

 expressed When is the map for the input of a transfer_function

 is really just whatever value variable happens to have on

 entry We try very_hard to express the result of the transfer

 function as an affine_expression of values that are described by the

 input map



 You_should observe the proper interpretation of expressions like

 where is a transfer_function a map and a

 variable As is conventional in mathematics we apply functions from

 the left meaning that we first compute which is a map Since

 a map is a function we may then apply it to variable to produce a

 value







 The transfer_function of statement denoted is defined as

 follows





 If is not an assignment_statement then is the

 identity_function



 If is an assignment_statement to variable x then



































 The expression is intended to

 represent all the possible forms of expressions_involving arbitrary

 variables

 and that may appear on the

 right_side of an assignment to and that give a value

 that is an affine transformation on prior values of variables

 These expressions are





 and



 Note_that in many_cases one or_more of and are 0





 If the assignment is xyz then and

 If the assignment is xy5 then and



 Composition of Transfer_Functions

 To_compute where and are defined in terms

 of input map we substitute the value of in the

 definition of with the definition of We replace

 all operations on values with That is



 If then



 If then

































 The transfer_functions of the blocks in Example can

 be computed by_composing the transfer_functions of their constituent

 statements The transfer_functions are defined in Fig































 Transfer_Functions of Example



 Solution to Data-Flow Problem

 We use the

 notation and

 to refer to the input and output

 data-flow_values of block in iteration of the inner_loop and

 iteration of the outer_loop

 For the other blocks we use and to refer

 to these values in the th_iteration of the outer_loop

 Also

 We can see that the symbolic_maps shown in Fig satisfy the constraints_imposed by the transfer_functions listed in

 Fig





























 Constraints satisfied on each iteration of the nested_loops





 The first constraint says_that the output map of a basic_block is

 obtained_by applying the block's transfer_function to the input map

 The rest of the constraints say that the output map of a basic_block

 must_be greater_than or equal to the input map of a successor

 block in the execution



 Note_that our iterative_data-flow algorithm cannot produce the above

 solution because it lacks the concept of expressing data-flow_values

 in terms of the number of iterations executed Region-based_analysis

 can be used to find such solutions



 Region-Based Symbolic Analysis



 We can extend the region-based_analysis described in

 Section to find expressions of variables in the

 th_iteration of a loop A region-based symbolic analysis has a

 bottom-up_pass and a top-down_pass like other region-based

 algorithms The bottom-up_pass summarizes the effect of a region with

 a transfer_function that sends a symbolic_map at the entry to an output

 symbolic_map at the exit In the top-down_pass values of symbolic

 maps are propagated down to the inner regions



 The difference lies in how we handle loops In

 Section the effect of a loop is summarized with a

 closure_operator Given a loop with body its closure is

 defined as an_infinite meet of all possible numbers of applications of

 However to find induction_variables we need to determine if a

 value of a variable is an affine function of the number of iterations

 executed so_far The symbolic_map must_be parameterized by the number of

 the iteration being executed Furthermore whenever we know

 the total_number of

 iterations executed in a loop we can use that number to find

 the values of induction_variables after the loop For_instance in

 Example we claimed that has the value of after

 executing the th_iteration Since the loop has 100 iterations the

 value of must_be 100 at the end of the loop



 In what_follows we first define the primitive operators meet and

 composition of transfer_functions for symbolic analysis

 Then show_how we

 use them to perform region-based_analysis of induction_variables



 Meet of Transfer_Functions



 When computing the meet of

 two functions the value of a variable is unless

 the two functions map the variable to the same

 value and the value is not

 Thus





















 Parameterized Function Compositions



 To express a variable as an affine function of a loop_index we need

 to compute the effect of composing a function some given number of

 times If the effect of one iteration is summarized by transfer

 function then the effect of executing iterations for some

 is

 denoted Note_that when the

 identify function



 Variables in the program are divided_into three categories





 If where is a constant then

 for every value of We_say that is a basic

 induction_variable of the loop whose body is represented_by the transfer

 function



 If then for

 all The variable is not modified and it remains

 unchanged at the end of any number of iterations through the loop with

 transfer_function We_say that is a symbolic_constant in the

 loop



 If

 where each

 is either a basic_induction variable or a symbolic_constant then

 for







 We_say that is also an induction_variable though not a basic one

 Note_that the formula above does_not apply if



 In all other cases





 To_find the effect of executing a fixed_number of iterations we

 simply replace above by that number In the case_where the number

 of iterations is unknown the value at the start of the last iteration

 is given by In this case the only variables whose

 values can still be_expressed in the affine form are the

 loop-invariant variables























 For the innermost_loop in Example

 the effect of executing iterations is summarized by



 From the definition of we see that and are symbolic

 constants is a basic_induction variable as it is incremented by

 one every iteration is an induction_variable because it is an

 affine function the symbolic_constant and basic_induction

 variable

 Thus

























 If we could not tell how many_times the loop of block iterated

 then we could not use and would have to use to express the

 conditions at the end of the loop In this case we would have

























 A Region-Based Algorithm





 Region-Based Symbolic Analysis



 A reducible_flow graph



 Symbolic maps for each block of



 We make the following modifications to Algorithm



 We change how we construct the transfer_function for a loop region

 In the original algorithm we use the transfer_function

 to map the symbolic_map at the entry of loop region to a symbolic

 map at the entry of loop body after executing an unknown number of

 iterations It is defined to be the closure of the transfer_function

 representing all paths_leading back to the entry of the loop as shown

 in Fig (b) Here we define to

 represent the effect of execution from the start of the loop region to

 the entry of the th_iteration Thus











 If the number of iterations of a region is known the summary of the

 region is computed by_replacing with the actual count



 In the top-down_pass we compute to find the

 symbolic_map associated_with the entry of the th_iteration of a

 loop



 In the case_where the input value of a variable is used on the

 right-hand-side of a symbolic_map in region and

 upon_entry to the region we introduce a new reference variable

 add assignment t v to the beginning of region and all

 references of are replaced_by

 If we

 did_not introduce a reference variable at this point the

 value held by would penetrate into inner_loops







 Transfer_function relations in the bottom-up_pass for

 Example





 For Example we show_how the transfer_functions for the

 program are

 computed in the bottom-up_pass in Fig

 Region is the inner_loop with body

 The transfer

 function representing the path from the entry of region to the

 beginning of the th_iteration is The

 transfer_function representing the path to the end of the th

 iteration is



 Region consists of blocks and with loop region

 in the middle The

 transfer_functions from the entry of and can be computed

 in the same way as in the original algorithm Transfer_function

 represents the composition of block and

 the entire execution of the inner_loop since is the identity

 function Since the inner-loop is known

 to iterate 10 times we can replace by 10 to summarize the

 effect of the inner_loop precisely The rest of the transfer

 functions can be computed in a similar_manner The actual transfer

 functions computed are shown in Fig





















































 Transfer_functions computed in the bottom-up_pass for

 Example



 The symbolic_map at the entry of the program is simply We

 use the top-down_pass to compute the symbolic_map to the entry to

 successively nested regions until we find all the symbolic_maps for

 every basic_block We start by computing the data-flow_values for

 block in region



 Descending down to regions and



 Finally in regions



 Not surprisingly these equations produce the results we showed in

 Fig



 Example shows a simple program where every variable used

 in the symbolic_map has an affine_expression We use

 Example to illustrate why and how we introduce

 reference_variables in Algorithm













 (a) A loop where fluctuates









 (b) A reference variable makes an induction_variable



 The need to introduce reference_variables







 Consider the simple example in Fig (a)

 Let be the transfer_function summarizing the effect of executing

 iterations of the inner_loop Even_though the value of may

 fluctuate during the execution of the loop we see that is an

 induction_variable based_on the value of on entry of the loop that

 is Because is assigned an input value the

 symbolic_map upon_entry to the inner_loop maps to We

 introduce a new reference variable to save the value of upon

 entry and perform the substitutions as in Fig (b)





 For the flow_graph of Fig (see the exercises for

 Section ) give the transfer_functions for







 a) Block

 b) Block

 c) Block







 Consider the inner_loop of Fig consisting of blocks

 and If represents the number of times_around the loop

 and is the transfer_function for the loop body (ie excluding the edge

 from to ) from the entry of the loop (ie

 the beginning of ) to the exit from then what is

 Remember that takes as argument a map and assigns a value

 to each of variables and We denote these values

 and so on although we do_not know their values





 Now_consider the outer_loop of Fig consisting of blocks

 and Let be the transfer_function for

 the loop body from

 the entry of the loop at to its exit at Let measure

 the number of iterations of the inner_loop of and (which count

 of iterations we

 cannot know) and let measure the number of iterations of the outer

 loop (which we also cannot know) What is

 Pipelining

 pipeline-subsect



 In pipelining a task is decomposed into a number of stages to be

 performed on different_processors For_example a task computed using

 a loop of iterations can be structured as a pipeline of

 stages Each stage is assigned to a different processor when one

 processor is finished with its stage the results are passed as input

 to the next processor in the pipeline



 Introduction to Pipelining

 ex

 pipeline-loop-ex

 Consider the loop



 verbatim

 for_(i 1_i m_i)

 for_(j 1_j n_j)

 Xi Xi Yij

 verbatim

 This code sums up the th_row of and adds it to the element

 of The inner_loop corresponding to the summation must_be

 performed sequentially because of the data_dependence however the

 different summation tasks are independent We can parallelize this

 code by having each processor perform a separate summation Processor

 accesses row of and updates the element th_element of





 Alternatively we can structure the processors to execute the

 summation in a pipeline and derive parallelism by overlapping the

 execution of the summations as shown in Figure figpipeline-ex More

 specifically each iteration of the inner_loop can be treated_as a

 stage of a pipeline stage takes an element of generated in

 the previous stage adds to it an element of and passes the

 result to the next stage Notice_that in this case each processor

 accesses a column instead of a row of



 We can initiate a new task as_soon as the first processor is

 done with the first stage of the previous task

 At the beginning the pipeline is empty and only the first processor

 is executing the first stage After it completes the results are

 passed to the second processor while the first processor starts on the second

 task and so on In this way the pipeline gradually fills until all

 the processors are busy When the first

 processor_finishes with the last_task the pipeline starts to drain

 with more and more processors becoming idle until the last processor

 finishes the last_task In the steady_state tasks can be

 executed concurrently in a pipeline of processors

 ex



 figure

 verbatim

 Time Processors

 1_2 3

 1 X1Y11

 2 X2Y21 X1Y12

 3 X3Y31 X2Y22 X1Y13

 4 X4Y41 X3Y32 X2Y23

 5 X4Y42 X3Y33

 6 X4Y43

 verbatim

 Pipelined execution of Example_pipeline-loop-ex with

 and



 figpipeline-ex

 figure



 It is interesting to contrast pipelining with simple parallelism

 where different_processors execute different tasks

 itemize



 Pipelining

 can only be applied to nests of depth at_least two We

 can treat each iteration of the outer_loop as a task and the

 iterations in the inner_loop as stages of that task



 Tasks

 executed on a pipeline may share a dependence Information pertaining

 to the same stage of each task is held on the same processor thus

 results generated_by the th_stage of a task can be used by the

 th_stage of subsequent tasks without any interprocessor

 communication Similarly if there is a lot of code or data used by

 a stage it can be held locally by the processor for that stage



 If the tasks are independent then simple parallelization has better

 processor utilization because processors can execute all at once

 without_having to pay for the overhead of filling and draining the

 pipeline However as shown in Example_pipeline-loop-ex the

 data_accessed in a pipelined scheme is different from that of simple

 parallelization pipelining may be preferable if it leads to a

 significant reduction in communication traffic

 itemize



 ex

 expipeline

 The code of Fig_pipeline-nest-fig

 represents a data-access pattern found in many

 programs Here the new value of an element in the array depends_on the

 values of elements in its neighborhood In some_cases such an

 operation is performed repeatedly_until some convergence criterion is

 met



 figurehtfb



 verbatim

 for_(i 1_i n_i)

 for_(j 1_j n_j)

 Xij 05 (Xi-1j Xij-1)

 verbatim



 A loop_nest that can profit from pipelining

 pipeline-nest-fig



 figure



 Shown in Fig figpipeline is a picture of the data

 dependences Since dependences exist across both the rows and the

 columns in the iteration_space we cannot parallelize this code unless

 synchronization is introduced From the figure we can see that

 iterations along the diagonal share no dependences These sets of

 independent iterations are known_as wavefronts and can be

 executed in parallel as_long as synchronizations occur before each

 wavefront The amount of communication incurred between these

 wavefronts depends_on how the instances of the statement(s) are

 assigned to the different_processors



 figurehtfb

 fileuullmanalsuch11figspipelineeps

 The dependences of the pipelining example (Example expipeline)

 figpipeline

 figure



 We can execute Fig_pipeline-nest-fig as a pipeline There

 are two ways to pipeline the code First we can create a pipeline by

 making the th processor be responsible_for the th_row of that

 is those elements of the form

 The execution starts by having processor 1 execute

 iteration (11) It then signals processor 2 to execute iteration

 (21) ie to compute while processor 1

 goes on to execute iteration (21) and so on The

 SPMD code that implements this pipelining scheme is shown in

 Fig_signal-fig(a)



 figurehtfb



 verbatim

 1 p n

 for_(j 1_j n_j)

 if_(p 1)_wait (p-1)

 Xpj 05 (Xp-1j Xpj-1)

 if_(p n-1) signal_(p1)



 verbatim



 center

 (a) Processors_assigned to rows

 center



 verbatim

 1 p n

 for_(i 1_i n_i)

 if_(p 1)_wait (p-1)

 Xip 05 (Xi-1p Xip-1)

 if_(p n-1) signal_(p1)



 verbatim



 center

 (b) Processors_assigned to columns

 center



 Two implementations of pipelining for the code

 of Fig_pipeline-nest-fig

 signal-fig



 figure



 Similarly we can create a pipeline by having processor be

 responsible_for the th column - the elements

 This code is shown in

 Fig signal-fig(b)

 Notice_that

 instead of a barrier to synchronize along each wavefront either scheme

 requires that a processor synchronize with and communicate with only two

 other processors The iterations being performed at any one time form

 a relaxed wavefront through the computation that is the rows or

 columns may progress through the pipeline at slightly_different rates

 ex



 It is not a coincidence that we find more_than one way to pipeline the

 code in Example_expipeline

 In_fact pipelining along the rows

 and along the columns are just two special_cases of the many_ways one

 can pipeline this code Any partition-mapping function of the form

 where and are positive constants

 will create a pipeline that can execute

 the iterations in time

 The important general principle to note is that whenever there are alternate

 sequential_execution orders that satisfy the data_dependences in a

 nested loop there is pipelined_parallelism and vice_versa



 The strategy to find all the parallelism_requiring as little

 synchronization as possible goes as_follows We use

 Algorithm_algnosync to find all degrees of synchronization-free

 parallelism If more parallelism is desired we use

 Algorithm_alg1sync to find all parallelism_requiring at most a

 constant number of synchronizations If we need more parallelism we

 try to find more parallelism in nontrivial strongly_connected

 components As noted above nodes belonging to a nontrivial strongly

 connected_component must have a common outer_loop nest To_find

 pipeline parallelism we ask if there is affine_transform that changes

 the composition of instructions in each iteration of the outer_loop

 while satisfying the data_dependences If there is we are guaranteed

 the presence of pipelining parallelism Otherwise we have no choice

 on the composition of operations in each iteration of the outer_loop

 We can then repeat the process on the inner_loops to find more

 parallelism



 ex

 exnsync1

 This example_illustrates why parallelization may need to be reapplied

 to inner_loops

 Figure synch-pipe-fig is a more_complex version of the problem we

 saw in Example_exscc2



 figurehtfb

 center

 (a)

 center

 verbatim

 for_(i 0 i_100 i)

 for_(j 0 j_100 j)

 Xj Xj Yij (s1)

 Zi XAi (s2)



 verbatim

 center

 (b)

 center

 verbatim

 s1 - s1 s1- s2 s2-s1

 verbatim

 A sequential outer_loop (a) and its program dependence graph

 synch-pipe-fig

 figure



 As shown in the program dependence graph in

 Figure synch-pipe-fig(b) statements and belong to the

 same strongly_connected component Because we do_not know the

 contents of matrix we must assume that the access in statement

 may read from any of the elements of There is a true

 dependence from statement to statement and a

 anti-dependence from statement to statement There is no

 opportunity for pipelining either because all operations belonging to

 iteration in the outer_loop must_precede those in iteration

 To_find more parallelism we repeat the parallelization process to the

 inner_loop The iterations in the second loop can be_parallelized

 without_synchronization Thus 200 barriers are needed with one

 executed before and one after the execution of the inner_loop

 ex



 Time-Partition_Constraints

 time-part-subsect



 We_now focus_on the problem of finding pipelined_parallelism in a loop

 nest Our_goal is to take the computation in the loop_nest and turn

 them into a set of pipelinable tasks To_find pipelined_parallelism

 we do_not solve directly what is to be executed on each processor

 like we did with parallelization Instead we ask the following

 fundamental question What are all the possible_execution sequences

 that honor the original data_dependences in the loop Obviously the

 original execution sequence satisfies all the data_dependences The

 question is if there are affine transforms that can create an

 alternate schedule where iterations of the outermost_loop execute a

 different set of operations from the original such that all the

 dependences are satisfied If we can find such transforms we can

 pipeline the loop The key is that if there is freedom in the

 scheduling of operations there is parallelism details of how we

 derive pipelined_parallelism from such transforms will be explained

 later



 In this case we_wish to find a one-dimensional affine_transform one

 for each statement that maps the original loop_index values to an

 iteration number in the outermost_loop The transform is legal if the

 assignment can satisfy all the data_dependences in the program The

 time-partition_constraints shown below simply say that if one

 operation is dependent upon the other then the first must_be assigned

 an iteration in the outermost_loop no earlier than that of the second

 If they are assigned in the same iteration then it is understood that

 the first will be executed after than the second within the iteration



 An affine-partition mapping of a program is a legal time partition if and

 only if for every two (not_necessarily distinct)_accesses sharing a dependence

 say



 in statement_nested in loops

 and





 in statement_nested in loops

 the one-dimensional partition_mappings and

 for statements and

 respectively satisfy the time-partition_constraints







 1in

 in and in such

 that



 arrayc

 i1s1s2i2



 B1 i1 b1 0



 B2 i2 b2 0



 F1_i1 f1_F2 i2_f2

 array



 we have





 C1i1_c1 C2i2

 c2





 This constraint illustrated in Fig figtime looks

 remarkably similar to the space-partition_constraints It is a

 relaxation of the space-partition_constraints in that if two iterations

 refer to the same_location they do_not necessarily have to be mapped

 to the same partition we only require that the original relative

 execution order between the two iterations is preserved

 That is the conclusion here has where the space-partition

 constraints have





 figurehtfb

 THE PICTURE NEEDS TO BE MODIFIED THE CURRENT PICTURE DOES NOT SHOW

 OFF THE DIFFERENCE BETWEEN TIME-PARTITIONS VS SPACE-PARTITIONS WE

 SHOULD HAVE TWO i's i1_i2 AND THEY MAPPED TO DIFFERENT TIME STEPS t1

 t2

 fileuullmanalsuch11figstimeeps

 Time-Partition_Constraints

 figtime

 figure



 We know that there_exists at_least one solution to the time-partition

 constraints We can simply map operations in each of the outermost

 loop back to the same iteration and all the data_dependences will be

 satisfied This is the only solution to the time-partition

 constraints for codes that cannot be pipelined On the other_hand if

 we can find multiple independent_solutions to time-partition

 constraints of a program the program can be pipelined The details

 of how we pipeline a loop will be discussed in Section ch11fpn

 We saw an example of each in Examples exnsync1

 and expipeline and we discuss their time constraints in

 Examples exnsync1p2 and expipelinesoln respectively



 ex

 exnsync1p2

 Let_us consider Example_exnsync1 and in particular the data

 dependences of references to array in statements and

 Because the access is not affine in statement we approximate

 the access by modeling matrix simply as a scalar_variable in

 dependence analysis involving statement Let

 be the index value of a dynamic_instance of and let be

 the index value of a dynamic_instance of Let the computation

 mappings of statements and be

 and respectively



 Let_us first consider the time-partition_constraints imposed_by

 dependences from statement to Thus

 the transformed_th

 iteration of must_be no later than the transformed

 th_iteration of that is





 arrayrr

 C11_C12



 array





 arrayr

 i



 j



 array





 c1

 C21_i' c2



 Expanding



 C11 i C12j c1_C21 i'_c2



 Since can be arbitrarily_large independent of and it must

 be that

 Thus one possible solution to the constraints is



 C11_C21 1 and C12 c1_c2 0





 Similar arguments about the data_dependence from to and

 back to itself

 will yield a similar answer In this particular solution the th

 iteration of the outer_loop which consists of the instance of

 and all instances of are all assigned to timestep

 Other legal choices of and yield

 similar assignments although there might be timesteps at which nothing

 happens

 That is all ways to schedule the outer_loop

 require the iterations to execute in the same order as in the original code

 This statement holds whether all 100 iterations are executed on the same

 processor on 100 different_processors or anything in-between

 ex



 ex

 expipelinesoln

 In Example_expipeline the write_reference Xij

 shares a dependence with itself and the two read references in the

 code

 Let_us first consider the constraint due to the dependence from

 Xij to Xi-1j Let and be two

 data-dependent instances of Xij and Xi-1j respectively

 Instance can depend_on only if

 that is either or and But for the

 instances to access the same array_element it must_be that and



 Therefore the time-partition_constraint reduces

 to





 arrayrr

 C11_C12

 array





 arrayr

 i



 j



 array







 arrayr

 c1



 array





 arrayrr

 C11_C12

 array





 arrayr

 i'



 j'



 array







 arrayr

 c1



 array







 If we substitute for and for the above inequality

 simplifies to

 Thus there are two independent_solutions in the two-dimensional index

 space and these form the basis_vectors for the solution space





 arrayr

 1



 0



 array





 arrayr

 0



 1



 array





 If we consider the dependence from Xij to Xij-1 we draw

 the same conclusion Thus one solution based_on the first vector

 is to assign the th_iteration of the outer_loop to the th

 timestep Of_course we still need to schedule the iterations of the

 inner_loop on within these coarse timesteps



 Another solution is based_on the vector Here we assign all

 iterations of the inner_loop that have index to the th timestep

 regardless of That is we break up each iteration of the outer

 loop or equivalently we swap the order of the two loops in the nest



 A third solution among the infinite set of possibilities is to use the

 sum of the two basis_vectors for the solution space

 That is we assign the iteration with index values and to the

 timestep In this solution at each timestep we execute all the

 iterations along a diagonal wavefront as in Example_expipeline

 This choice corresponds to the pipelined solution outlined in that

 example

 ex



 Solving Time-Partition_Constraints by Farkas'Lemma



 Since the time-partition_constraints are similar to the

 space-partition_constraints can we use a similar algorithm to solve

 them

 Unfortunately the slight difference_between the two problems

 translates_into a big technical difference_between the two solution

 methods

 Algorithm_algnosync simply solves for

 and such that for all in

 and in if



 F1_i1 f1_F2 i2_f2

 then



 C1i1_c1 C2i2_c2



 The linear_inequalities due to the loop_bounds are only used in

 determining if two references share a data_dependence and not used

 otherwise



 To_find solutions to the time-partition_constraints we cannot ignore

 the linear_inequalities ignoring them

 often would allow only the trivial_solution of placing all iterations in the

 same partition Thus the algorithm to find solutions to the

 time-partition_constraints must handle both equalities and

 inequalities



 The general problem we_wish to solve is given a matrix A

 find a vector c such that for all vectors x such that

 it is the case that



 In other_words we are seeking c such that the inner product of

 c and any coordinates in the polyhedron defined by the

 inequalities always yields a

 non-negative answer



 This problem is addressed by Farkas'_Lemma

 Let A be an matrix of reals

 and let c be a real nonzero

 -vector Farkas'_lemma says_that either the primal_system of

 inequalities



 Ax_0 cT x 0



 has a real-valued_solution x or the dual_system



 AT_y c y 0



 has a real-valued_solution y but never both



 The dual_system can be handled by using Fourier-Motzkin_elimination to

 project_away the variables of y For each c that has a

 solution in the dual_system the lemma guarantees that there are no

 solutions to the primal_system Put_another way we can prove the

 negation of the primal_system ie we can prove that



 for all x such that by finding a

 solution y to the dual_system

 and



 About Farkas'_Lemma

 The proof of

 the lemma can be found in many standard texts on linear_programming

 Farkas'_Lemma originally proved in 1901

 is one of the theorems of the alternative

 These

 theorems are all equivalent but despite attempts over the years a

 simple intuitive proof for this lemma or any of its equivalents has

 not been_found



 alg

 algsync

 Finding a set of legal maximally independent affine_time-partition mappings

 for an outer sequential loop



 A loop_nest with array_accesses



 A maximal_set of linearly_independent time-partition_mappings



 The following steps constitute the algorithm



 enumerate



 Find all data-dependent pairs of accesses in a program



 For each pair of data_dependent accesses



 in statement_nested in loops

 and



 in statement_nested in loops

 let and be the

 (unknown) time-partition

 mappings of statements and respectively

 The time-partition_constraint states that for all

 in and in if



 arrayc

 i1s1s2i2



 B1 i1 b1 0



 B2 i2 b2 0



 F1_i1 f1_F2 i2_f2

 array



 we have





 C1i1_c1 C2i2

 c2



 Since

 is a disjunctive union of a

 number of clauses We create a system of constraints for each clause

 and solve each of them separately as_follows

 enumerate

 Similarly to step (trick-item) in Algorithm_algnosync apply

 Gaussian_elimination to

 the equations



 F1_i1 f1_F2 i2_f2



 to reduce the vector





 arrayc

 i1



 i2



 1



 array





 to some vector of unknowns x

 Let c be all the unknowns in the partition_mappings

 Express the linear inequality constraints_due to the partition

 mappings as



 cTDx 0



 for some matrix D

 Express the precedence constraints on the loop_index variables and

 the loop_bounds as



 Ax_0



 for some matrix A

 Apply Farkas'_Lemma finding x to satisfy the two constraints

 above is equivalent to finding y such that



 AT_y DTc and y 0



 Note_that here is in the

 statement of Farkas'_Lemma and we are using the negated form of the

 lemma

 In this form apply Fourier-Motzkin_elimination to project_away

 the variables and express the constraints on the

 coefficients c as

 Let

 be the system without the constant terms

 enumerate



 Find a maximal_set of linearly_independent solutions to

 using Algorithm figtime-math in

 Appendix time-math-ch

 The approach of that complex algorithm

 is to keep_track of the current set of solutions for each

 of the statements then incrementally look for more independent

 solutions by inserting constraints that force the solution to be

 linearly_independent for at_least one statement



 From each solution of found derive one affine

 time-partition mapping The constant terms are

 derived using

 enumerate

 alg



 ex

 The constraints for Example_exnsync1p2 can be written as





 arrayrrrr

 -C11_-C12 C21 (c2-c1)



 array





 arrayc

 i



 j



 i'



 1



 array



 0









 arrayrrrr

 -1_0 1_0

 array





 arrayc

 i



 j



 i'



 1



 array



 0





 Farkas'_lemma says_that these constraints are equivalent to





 arrayr

 -1



 0



 1



 0



 array





 arrayr

 z



 array







 arrayr

 -C11



 -C12



 C21



 c2-c1



 array

 and

 z 0



 Solving this system we get



 C11_C21 0 and

 C12 c2-c1 0



 Notice_that these constraints are satisfied by the particular

 solution we obtained

 in Example_exnsync1p2

 ex



 Fully_Permutable Loop Nests

 ch11fpn



 As_discussed above if we can find more_than one independent solution

 to the time-partition_constraints for a loop the loop has pipelined

 parallelism If there exist independent_solutions then it is

 possible to transform the loop to have outermost_loops such that

 these loops can be arbitrarily permuted without_changing the semantics

 of the original_program We_say that the loop has fully

 permutable_loops A number of transformations are applicable to fully

 permutable nests We can permute the loops to get generate different

 sequential_execution schedules we can pipeline the loops for

 parallelism we can apply_blocking to improve data_locality for

 uniprocessors and reduce synchronization and communication for

 multiprocessors



 Permuted Sequential Loops



 We start by_looking at the code in Example pi



 ex

 perm-loops-ex

 Recall the loop_nest of Example_expipeline which involves the

 nest



 verbatim

 for_(i 1_i n_i)

 for_(j 1_j n_j)

 Xij 05 (Xi-1j Xij-1)

 verbatim

 We saw in Example expipelinesoln that there were two independent

 solutions to the time-dependence constraints which we represented_by

 the basis_vectors and These vectors correspond to

 variation in the direction and the direction respectively

 That is to say the time constraints tell_us to order the instances of

 the assignment_statement having the same value of according to their

 values (lowest first) and order instances having the same value of

 according to their values However those are the only

 constraints we could for example order the instances and

 arbitrarily



 In this case the fully_permutable loops are the two loops that

 correspond to the indexes and themselves The fact that they

 are permutable says_that we could just as_well execute the loop_nest as



 verbatim

 for_(j 1_i n_j)

 for_(i 1_i n_i)

 Xij 05 (Xi-1j Xij-1)

 verbatim



 Observe also that we chose the basis_vectors and

 arbitrarily although they are in some sense the obvious choice We

 could just as_well have chosen another pair of independent vectors like

 and That choice says_that one direction in the time

 space is and the other is There_are still two fully

 permutable_loops but now one has index and the other has index

 In this case the loops look_like



 verbatim

 for_(i 1_i n_i)

 for (p i1 p in p)

 Xip-i 05 (Xi-1p-i Xip-i-1)

 verbatim

 These loops also can be reordered they become



 verbatim

 for (p 2 p 2n p)

 for_(i max(1p-n) i min(p-1n) i)

 Xip-i 05 (Xi-1p-i Xip-i-1)

 verbatim

 Finally observe that in this example the dimension of the time space

 2 equals the total depth of the loop_nest It is however quite

 possible that there would be more nested_loops than the dimension of the

 time space That would be the case for instance if the assignment

 statement in our_running example were replaced_by one or_more loops that

 had the same effect on the dependences Xij had to be preceded by

 Xi-1j and Xij-1

 ex



 We can use

 Algorithm_algenumerate to create a sequential program that

 serially iterates through the -dimensions of partitions

 lexicographically and executes the body of each partition in the

 original_sequential order It should be clear that the program generated

 thusly is

 legal The dependences spanning iterations of the outermost_loops

 are honored since the time-partition_constraints are satisfied

 The ordering within the

 body of the th loop is legal because it is simply the original

 sequential_execution order The first loops of the code generated

 are fully_permutable because the independent affine_partition mappings

 can be arbitrarily ordered



 Pipelining



 A -deep_fully permutable_loop nest has degrees of pipeline

 parallelism The iteration with index values

 can be executed

 without violating data_dependence constraints provided iterations



 (p1-1 p2 pk)

 (p1 p2-1 p3 pk)

 (p1 pk-1 pk-1)



 have_been

 executed

 We can assign the partitions to processors with each

 processor executing iterations with fixed values of

 the first indexes

 Each_processor executes its portion of the iterations in sequential

 order It can execute iteration (ie its task with the th loop

 index equal to ) as_long as it receives a signal

 from processors



 (p1-1 p2 pk-1) (p1

 pk-2 pk-1-1)



 that they have executed their iterations with the th loop_index equal

 to A relaxed

 version of a -dimensional wavefront is being executed at any one

 time



 Blocking



 A -deep_fully permutable_loop nest can be blocked in

 -dimensions Instead of manipulating the individual partitions we

 can aggregate blocks of iterations into one_unit Blocking is useful

 for enhancing data_locality as_well as for minimizing the overhead of

 pipelining



 figure



 verbatim

 for_(i0 in i)

 for (j1 jn_j)

 S



 verbatim



 center

 (a) A simple loop_nest

 center



 verbatim

 for (ii 0 iin ib)

 for (jj 0 jjn jjb)

 for_(i iib i min(iib-1 n) i)

 for_(j iib j min(jjb-1 n) j)

 S



 verbatim



 center

 (b) A blocked_version of this loop_nest

 center



 A 2-dimensional loop_nest and its blocked_version

 blocked-code-fig



 fileuullmanalsuch11figstileeps

 Execution order before and after blocking a 2-deep_loop nest

 figtile



 figure



 Suppose we have a two-dimensional fully_permutable loop_nest as in

 Fig blocked-code-fig(a) and we

 wish to break the computation into blocks The

 execution order of the blocked code is shown in

 Fig figtile and the equivalent code is in

 Fig blocked-code-fig(b)



 We can coarsen the granularity of pipelining by assigning a column

 of blocks to one processor Notice_that

 each processor synchronizes with its

 predecessors and successors only at block boundaries Thus another

 advantage of blocking is that programs

 only need to communicate data

 accessed at the boundaries of the block with their neighbor blocks

 Values that are interior to a block are managed by only one processor



 figure



 verbatim

 for_(i 1_i N_i)

 for_(j 1_j i-1 j)

 for (k 1 k j-1 k)

 Xij_Xij - Xik Xjk

 Xij_Xij Xjj



 for (m 1 m i-1 m)

 Xii Xii - Xim Xim

 Xii sqrt(Xii)



 verbatim



 Cholesky_decomposition

 cholesky-fig



 verbatim

 for (i2 1 i2 N i2)

 for (j2 1 j2 i2 j2)

 beginning of code for processor_(i2j2)

 for (k2 1 k2 i2 k2)



 Mapping_i2 i_j2 j_k2 k

 if (j2i2 k2j2)

 Xi2j2_Xi2j2 -_Xi2k2 Xj2k2



 Mapping_i2 i_j2 j_k2 j

 if (j2k2 j2i2)

 Xi2j2_Xi2j2 Xj2j2



 Mapping_i2 i_j2 i_k2 m

 if_(i2j2 k2i2)

 Xi2i2 Xi2i2 -_Xi2k2 Xi2k2



 Mapping_i2 i_j2 i_k2 i

 if_(i2j2 j2k2)

 Xk2k2 sqrt(Xk2k2)



 ending of code for processor_(i2j2)



 verbatim



 Figure cholesky-fig written as a fully_permutable loop

 nest

 cholesky-nest-fig



 figure



 ex

 We_now use a real numerical algorithm -

 Cholesky_decomposition - to illustrate_how Algorithm_algsync handles

 single loop_nests with only pipelining parallelism

 The code shown in Fig cholesky-fig implements

 an algorithm operating on a 2-dimensional data array The

 executed iteration_space is a triangular pyramid since only

 iterates up to the value of the outer_loop index and

 only_iterates to the value of The loop has four statements all

 nested in different loops



 Applying Algorithm_algsync to this program finds three

 legitimate time dimensions It nests all the operations some of

 which were_originally nested in 1- and 2-deep_loop nests into a

 3-dimensional fully_permutable loop_nest The code together_with the

 mappings is shown in Fig cholesky-nest-fig



 The code_generation routine guards the execution of the operations

 with the original loop_bounds to ensure_that the new programs execute

 only operations that are in the original code We can pipeline this

 code by mapping the 3 dimensional structure to a 2-dimensional

 processor space Iterations are assigned to the processor

 with ID Each_processor executes the innermost_loop the loop

 with the index

 Before it

 executes the th_iteration the processor_waits for signals from

 the processors with ID's and After it executes its

 iteration it signals processors and

 ex



 Putting it All_Together

 ch11par



 We have described two powerful parallelization algorithms in the last

 two sections We can combine these algorithms recursively to find

 more degrees of parallelism at the cost of higher synchronization

 Thus in Example_exnsync1 we found that

 applying_Algorithm algsync determines that there is

 only one legal outer_loop which is the one in the original code of

 Fig synch-pipe-fig

 Then we can apply_Algorithm alg1sync to the time partition to

 parallelize the inner_loop We can treat the code within a partition

 like a whole program the only_difference being that the partition

 number is treated_like a symbolic_constant



 alg

 algpar

 Find all the degrees of parallelism in a program with all the

 parallelism being as coarse-grained as possible



 A program to be_parallelized



 A parallelized version of the same program



 Do the following



 enumerate



 Find the maximum_degree of parallelism_requiring no synchronization

 Apply_Algorithm algnosync to the program



 Find the maximum_degree of parallelism that requires

 synchronizations_Apply Algorithm_alg1sync to each of the space

 partitions_found in step 1



 Find the maximum_degree of parallelism that requires

 synchronizations_Apply Algorithm_algsync to each of the

 partitions_found in step 2 Then apply_Algorithm alg1sync to

 each of the partitions assigned to each processor



 Find the maximum_degree of parallelism with successively greater

 degrees of synchronizations Recursively apply step 3 to computation

 belonging to each of the space partitions generated_by the previous

 step until all parallelism is found



 enumerate

 alg



 The above algorithm does_not address the full

 parallelization problem Remember that while synchronization is important

 communication can also impose a significant overhead If we simply

 minimize synchronization we can increase communication unnecessarily

 Here is a concrete example



 ex

 As_discussed in Example_exadi optimizing the first and second

 loop_nests independently finds parallelism in each of the nests but

 requires that the matrix be transposed between the

 loops incurring data_traffic

 If we use Algorithm_algsync to find_pipelined parallelism we

 find that we can turn the entire program into a fully_permutable loop

 nest as in Fig another-nest-fig

 We then can apply_blocking to reduce the communication overhead This

 scheme would incur synchronizations but would_require much less

 communication

 ex



 figurehtfb



 verbatim

 for_(j 0 j_n j)

 for_(i 1_i n1 i)

 if (i n) Xij f(Xij Xi-1j)

 if (j 0) Xi-1j g(Xi-1jXi-1j-1)



 verbatim



 A fully_permutable loop_nest for the code of

 Example_exadi

 another-nest-fig



 figure



 Additionally it sometimes helps to

 distinguish_between global and neighboring communication The cost of

 transposing a matrix is significantly higher_than having neighboring

 processors share boundary data When trying to find parallelism

 across strongly_connected components we can minimize_communication by

 requiring that accesses to the same data be mapped to neighboring

 processors Constraints on the partition_mapping in the

 space-partition_constraints are changed from



 Cs1i1 cs1 - Cs2i2

 cs2 0



 to



 Cs1i1 cs1 - Cs2i2

 cs2

 where is a small constant

 Elimination of Global_Common Subexpressions

 The available_expressions data-flow_problem discussed in the

 last section allows_us to determine if an expression at point

 in a flow_graph is a common_subexpression

 The following algorithm formalizes the intuitive ideas

 presented in Section 102 for eliminating common_subexpressions



 alg

 alggcse

 Global common_subexpression elimination



 A flow_graph with available expression information



 A revised flow_graph



 For every statement of the form xyz

 (Recall we continue to use



 as a generic operator)

 such that

 yz

 is available at the beginning of

 's block and neither

 y

 nor

 z

 is defined prior to

 statement in that block do the following

 enumerate

 To discover the evaluations of

 yz

 that reach 's block we follow flow

 graph edges searching backward from 's block

 However we do_not go_through any block that

 evaluates

 yz

 The last evaluation of

 yz

 in each block encountered is an evaluation of

 yz

 that reaches

 Create a new variable

 u

 Replace each statement

 wyz

 found in (1) by

 verbatim

 u yz

 w u

 verbatim

 Replace statement by

 xu

 enumerate

 alg



 Some remarks about this algorithm are in order

 enumerate

 The search in step (1) of the algorithm for the evaluations of

 yz

 that reach statement can also be formulated as a data-flow

 analysis problem

 However it does_not make sense to solve it for all expressions

 yz

 and all statements or blocks because too_much irrelevant information

 is gathered

 Rather we should perform a graph search on the flow_graph for

 each relevant statement and expression

 Not all changes made by Algorithm_105 are improvements

 We might wish to limit the number of different

 evaluations reaching found in

 step (1) probably to one

 However copy_propagation to be discussed next often allows benefit

 to be obtained even when several evaluations of

 yz

 reach

 Algorithm_105 will miss the fact that

 az

 and

 cz

 must have the same value in

 verbatim

 a xy c xy

 vs

 b az d cz

 verbatim

 because this simple approach to common_subexpressions considers

 only the literal expressions themselves rather_than the values computed by

 expressions

 Kildall 1973 presents a method for catching such equivalences

 on one pass

 we_shall discuss the ideas in Section 1011

 However they can be caught with multiple passes of Algorithm_105

 and one might consider repeating it until_no further changes occur

 If

 a

 and

 c

 are temporary_variables that are not used outside the block

 in which they appear

 then the common_subexpression

 (xy)z

 can be caught by treating the temporary_variables specially as

 in the next example

 enumerate



 ex

 exgcse

 Suppose that there are no assignments to the array

 a

 in the flow_graph of Fig 1034(a) so we can safely

 say that at2 and at6

 are common_subexpressions

 The problem is to eliminate this common_subexpression

 figure

 verbatim

 FIGURE

 verbatim

 Eliminating the common_subexpression 4i

 figure



 The common_subexpression

 4i

 in Fig 1034(a) has_been eliminated in Fig 1034(b)

 One_way to determine that at2

 and at6 are also common_subexpressions

 is to replace t2 and t6 by u

 using copy_propagation (to be discussed next) both expressions then become

 au

 which can be_eliminated by reapplying Algorithm_105

 Note_that the same new variable

 u

 is inserted in both blocks in Fig 1034(b)

 so local copy_propagation is enough to convert both at2

 and at6 to

 au



 There is another_way which takes into_account the fact

 that temporary_variables are inserted by the compiler and

 are used only within blocks they appear in

 We_shall look more closely at the way expressions are

 represented during the computation of available_expressions

 to get around the fact that different temporary_variables

 may represent the same expression

 The recommended technique for representing sets of expressions

 is to assign a number to each expression and use bit_vectors with bit

 representing the expression numbered

 The value-numbering techniques of Section 52

 can be applied during the numbering of expressions

 to treat temporary_variables in a special way



 In more_detail suppose

 4i

 has value number 15

 The expressions at2 and at6

 will get the same value number if we use the value number 15

 rather_than the names of

 the temporary_variables t2 and t6

 Suppose the resulting value number is 18

 Then bit 18 will represent both

 at2 and at6

 during data-flow_analysis and we can determine that at6

 is available and can be_eliminated

 The resulting code is indicated in Fig 1034(c)

 We use (15) and (18) to represent temporaries corresponding to

 expressions with those value numbers

 Actually t6 is useless and would be_eliminated during local live

 variable analysis

 Also t7 being a temporary would not be computed itself

 rather uses of t7 would be replaced_by uses of (18)

 ex



 Copy Propagation



 Algorithm_105 just presented and various other algorithms

 such_as induction-variable_elimination discussed

 later in this_section introduce

 copy statements

 of the form

 xy

 Copies may also be generated directly by the intermediate

 code_generator although most of these involve temporaries local to

 one block and can be removed by the dag construction

 discussed in Section 98

 It is sometimes possible to eliminate copy_statement if we determine

 all places_where this definition of

 x

 is used

 We may then substitute

 y

 for

 x

 in all these places provided

 the following conditions are met by every such use of

 x

 enumerate

 Statement must_be the only definition of

 x

 reaching

 (that is the ud-chain for use consists only of )

 On every_path from to including paths that go_through

 several_times (but do_not go_through a second time)

 there are no assignments to

 y

 enumerate



 Condition (1) can be checked using ud-chaining information but what of

 condition (2)

 We_shall set up a new data-flow_analysis problem in which

 is the set of copies

 such that every

 path from the initial_node to the beginning of contains the

 statement

 and subsequent to the last occurrence of

 there are no assignments to

 y

 The set

 can be defined correspondingly but with_respect

 to the end of

 We_say copy_statement

 is

 generated

 in block if occurs in and there is no_subsequent assignment

 to

 y

 within

 We_say

 is

 killed

 in if

 x

 or

 y

 is assigned there

 and is not in

 The notion that assignments to

 x

 kill

 xy

 is

 familiar from reaching_definitions but the idea that assignments to

 y

 do so is special to this problem

 Note the important consequence of the fact that different assignments

 xy

 kill each other

 can contain only one copy_statement with

 x

 on the left



 Let be the universal_set of all copy statements in the program

 It is important to note_that different statements

 xy

 are different in

 Define to be the set of all copies generated in block

 and to be the set of copies in that are

 killed in

 Then the following equations relate the quantities defined



 outB cgenB ( inB - ckillB)



 (1012)



 inB P a predecessor of B outP





 Equations 1012 are identical to Equations 1010 if

 is replaced_by and

 by

 Thus 1012 can be_solved by Algorithm 103 and we_shall not

 discuss the matter further

 We_shall however give an example that exposes some of the

 nuances of copy optimization



 ex

 excopy-prop

 Consider the flow_graph of Fig 1035

 Here

 xy and

 xz

 Also

 xy

 since y is assigned in

 Finally

 xz since

 x

 is assigned in and xy

 for the same reason

 figure

 verbatim

 FIGURE

 verbatim

 Example flow_graph

 figure



 The other 's and 's are



 Also

 by Equations 1012

 Algorithm 103 in one pass determines that



 in B2 in B3 out B1 xy





 Likewise

 and



 out B3 in B4 out B4 xz





 Finally



 We observe that neither copy xy nor xz reaches the

 use of

 x

 in in the sense of Algorithm_105

 It is true but irrelevant that both these definitions of

 x

 reach in the

 sense of reaching_definitions

 Thus neither copy may be propagated as it is not possible

 to substitute

 y

 (respectively

 z

 for

 x

 in all uses of

 x

 that

 definition xy (respectively xz) reaches

 We could substitute

 z

 for

 x

 in but that would not improve the code

 ex



 We_now specify the details of the algorithm to remove copy statements



 alg

 algcopy-prop

 Copy propagation



 A flow_graph with ud-chains

 giving the definitions_reaching block and

 with representing the solution to Equations 1012

 that is the set of copies

 xy

 that reach block along every

 path with no assignment to

 x

 or

 LaTeX Warning Reference 'figdf-alg' on page 32 undefined

 y

 following the last occurrence

 of

 xy

 on the path

 We also need du-chains giving the uses

 of each definition



 A revised flow_graph



 For each copy

 do the following

 enumerate

 Determine those uses of

 x

 that are reached by this definition

 of

 x

 namely



 Determine whether for every use of

 x

 found in (1) is in

 where is the block of this particular use

 and moreover no definitions of

 x

 or

 y

 occur prior to this use of

 x

 within

 Recall that if is in then

 is the only definition of

 x

 that reaches

 If meets the conditions of (2) then remove and replace

 all uses of

 x

 found in (1) by

 y

 enumerate

 alg





 The Principal Sources of Optimization

 secopt-sources



 A compiler optimization must preserve the semantics of the

 original_program Except in very special circumstances

 once a programmer chooses and implements a particular

 algorithm the compiler cannot understand enough about the program to

 replace it with a substantially different and more_efficient

 algorithm A compiler knows only how to apply relatively low-level

 semantic transformations using general facts such_as algebraic

 identities like or program semantics such_as the fact that

 performing

 the same operation on the same values yields the same result



 Causes of Redundancy



 There_are many redundant operations in a typical program Sometimes

 the redundancy is available at the source_level

 For_instance a programmer may find

 it more direct and convenient to recalculate some result leaving it to

 the compiler to recognize that only one such calculation is necessary

 But more

 often the redundancy is a side_effect of having written the program

 in a high-level_language In most languages (other_than C or C where

 pointer arithmetic is allowed) programmers have no

 choice but to refer to elements of an array or fields in a structure

 through accesses like or



 As a program is compiled each of these

 high-level data-structure accesses expands into a number of

 low-level_arithmetic operations such_as the computation of the location

 of the th_element of a matrix Accesses to the same data_structure

 often share_many common low-level_operations Programmers are not

 aware of these low-level_operations and cannot eliminate the

 redundancies themselves It is in fact preferable from a

 software-engineering perspective that programmers only access data elements

 by their

 high-level names the programs are easier to write and more

 importantly easier to understand and evolve By having a compiler

 eliminate the

 redundancies we get the best of both worlds the programs are both

 efficient and easy to maintain



 A Running Example Quicksort

 secquicksort



 In the following we_shall use a fragment of a

 sorting program called quicksort to

 illustrate several important code-improving

 transformations The C program in Fig_figqs is derived_from

 SedgewickfootnoteR Sedgewick Implementing Quicksort

 Programs Comm_ACM 21 1978 pp 847-857footnote

 who discussed the hand-optimization of such a program

 We_shall not discuss all the subtle algorithmic aspects of this

 program here for example the fact that must contain the

 smallest of the sorted elements and the largest



 figurehtb



 verbatim

 void quicksort(int m int n)

 recursively sorts am through an



 int i_j

 int_v x

 if (n m) return

 fragment begins here

 i_m-1 j_n v an

 while (1)

 do i_i1 while (ai v)

 do j_j-1 while (aj v)

 if (i j) break

 x_ai ai_aj aj_x swap ai_aj



 x_ai ai an an x swap ai an

 fragment ends here

 quicksort(mj) quicksort(i1n)



 verbatim



 C code for quicksort

 figqs



 figure



 Before we can optimize away the redundancies in address_calculations

 the address operations in a program first must_be broken_down

 into low-level_arithmetic operations to expose the redundancies

 In the rest of this_chapter we assume that the intermediate_representation

 consists of three-address_statements where temporary_variables are

 used to hold all the results of intermediate expressions

 Intermediate_code

 for the marked fragment of the program in Fig_figqs is shown

 in Fig_figqs-code



 figurehtb



 verbatim

 (1) i_m-1 (16) t7 4i

 (2) j_n (17) t8 4j

 (3) t1 4n (18) t9 at8

 (4) v at1 (19) at7 t9

 (5) i_i1 (20) t10_4j

 (6) t2 4i (21) at10_x

 (7) t3 at2 (22) goto (5)

 (8) if t3v goto (5) (23) t11 4i

 (9) j_j-1 (24) x at11

 (10) t4 4j (25) t12 4i

 (11) t5_at4 (26) t13 4n

 (12) if t5v goto (9) (27) t14 at13

 (13) if ij goto (23) (28) at12 t14

 (14) t6_4i (29) t15 4n

 (15) x at6 (30) at15 x

 verbatim



 Three-address_code for fragment in Fig_figqs

 figqs-code



 figure



 In this example we assume that integers occupy four_bytes

 The assignment

 x_ai is translated as in Section array-3code-subsect

 into the two three-address_statements



 verbatim

 t6_4i

 x at6

 verbatim

 as shown in steps (14) and (15) of

 Fig_figqs-code

 Similarly aj_x becomes



 verbatim

 t10_4j

 at10_x

 verbatim

 in steps (20) and (21)

 Notice_that every array

 access in the original_program translates_into a pair of steps

 consisting of a

 multiplication and an array-subscripting operation As a result

 this short program_fragment translates_into a rather long sequence of

 three-address operations



 figure

 figureuullmanalsuch9figsqs-fgeps

 Flow_graph for the quicksort fragment

 figqs-fg

 figure



 Figure figqs-fg is the flow_graph for the program in

 Fig_figqs-code

 Block is the entry_node

 All conditional and unconditional_jumps to statements in

 Fig_figqs-code have_been replaced in Fig_figqs-fg by jumps

 to the block of which the statements are leaders

 as in Section bb-sect

 In Fig_figqs-fg there are three loops

 Blocks and are loops by themselves

 Blocks and together

 form a loop with the only entry point



 Semantics-Preserving Transformations



 There_are a number of ways in which a compiler can

 improve a program without_changing the function it computes

 Common-subexpression elimination

 copy_propagation

 dead-code_elimination

 and constant_folding

 are common examples of such function-preserving (or semantics-preserving) transformations

 we_shall consider each in turn



 Frequently a program will include several calculations of

 the same value such_as an offset in an array

 As_mentioned in Section secquicksort

 some of these duplicate calculations cannot be

 avoided by the programmer because they lie below the level of detail

 accessible within the source_language

 For_example block shown in Fig figlocal-cse(a)

 recalculates and

 although none of these calculations were requested

 explicitly by the programmer



 figurehtb

 figureuullmanalsuch9figslocal-cseeps

 Local common-subexpression_elimination

 figlocal-cse

 figure



 Global_Common Subexpressions

 seccse



 An occurrence of an expression is called a

 common_subexpression

 if was previously_computed and the values of the

 variables in have not changed since the previous computation

 We avoid recomputing if we can use its

 previously_computed

 value

 that is the variable to which the previous computation of was

 assigned has not changed in the interimfootnoteIf

 has changed it may still be possible to reuse the

 computation of if we assign its value to a new variable as_well as

 to and use the value of in place of a recomputation of

 footnote



 ex

 The assignments to t7 and t10

 in Fig figlocal-cse(a)

 compute the common_subexpressions and

 respectively

 These steps

 have_been eliminated in Fig_figlocal-cse(b) which uses

 t6 instead of t7 and t8 instead of t10



 This change is what would result if we reconstructed

 the intermediate_code from the dag (directed acyclic graph) for

 the basic_block as in Section old98



 ex



 ex

 exlocal-cse

 Figure figlocal-cse-after

 shows the result of eliminating both

 global and local common_subexpressions

 from blocks and in the flow_graph of Fig_figqs-fg

 We first discuss the transformation of and

 then mention some subtleties involving arrays



 figurehtb

 figureuullmanalsuch9figslocal-cse-aftereps

 and after common-subexpression_elimination

 figlocal-cse-after

 figure



 After local common_subexpressions are eliminated

 still evaluates and

 as shown in Fig_figlocal-cse(b)

 Both are

 common_subexpressions in particular the three statements



 verbatim

 t8 4j

 t9 at8

 at8 x

 verbatim

 in can be replaced_by



 verbatim

 t9 at4

 at4 x

 verbatim

 using computed in block

 In Fig_figlocal-cse-after observe that

 as control passes from the evaluation of

 in

 to there is no change to

 and no change to

 so can be used if

 is needed



 Another common_subexpression comes to light in

 after replaces

 The new expression corresponds to the value of

 at the source_level

 Not_only does

 retain its value as control leaves and then

 enters but

 a value computed into a temporary

 does too because there are

 no assignments to elements of the array

 in the interim

 The statements



 verbatim

 t9 at4

 at6 t9

 verbatim

 in therefore can be replaced_by



 verbatim

 at6 t5

 verbatim



 Analogously the value assigned to

 in block of Fig_figlocal-cse(b)

 is seen to be the same as the value assigned to

 in block

 Block in Fig_figlocal-cse-after is the result of eliminating

 common_subexpressions corresponding to the values of the source_level

 expressions and

 from in Fig_figlocal-cse(b)

 A similar series of transformations has_been done to

 in Fig_figlocal-cse-after



 The expression in

 blocks and of Fig_figlocal-cse-after is not

 considered a common_subexpression

 although can be used in both places

 After control leaves and before

 it reaches it can go

 through where there are assignments to

 Hence

 may not have the same value on reaching as it did on

 leaving and it is not safe to treat as

 a common_subexpression

 ex



 Copy Propagation



 Block in Fig_figlocal-cse-after

 can be further improved

 by eliminating

 using two new transformations

 One concerns assignments of the form

 u_v

 called

 copy statements

 or

 copies

 for short

 Had we gone into more_detail in Example exlocal-cse copies would

 have arisen much sooner because the normal algorithm for

 eliminating common_subexpressions introduces them

 as do several other algorithms



 figurehtb

 figureuullmanalsuch9figscopyeps

 Copies introduced during common_subexpression elimination

 figcopies

 figure



 ex

 In order to eliminate the common_subexpression from the statement

 c_de

 in Fig figcopies(a) we must use a new

 variable

 to hold the value of

 The value of variable instead of that of the expression

 is assigned to in Fig figcopies(b)

 Since control may reach

 c_de

 either after the assignment to

 or after the assignment to

 it would be incorrect to replace

 c_de

 by either

 c a

 or by

 c b

 ex



 The idea_behind the

 copy-propagation

 transformation is to use for

 wherever possible after the copy_statement

 u_v

 For_example the assignment x t3 in block of

 Fig_figlocal-cse-after is a copy

 Copy propagation applied to

 yields the code in Fig figcopy

 This change may not appear to be an improvement

 but as we_shall see in Section dead-code-subsect

 it gives_us the opportunity

 to eliminate the assignment to



 figurehtfb



 center

 tabularl

 x t3



 at2 t5



 at4 t3



 goto



 tabular

 center



 Basic block after copy_propagation

 figcopy

 figure



 Dead-Code Elimination

 dead-code-subsect



 A variable is live

 at a point in a program if

 its value can be used subsequently

 otherwise it is

 dead

 at that point

 A related idea is

 dead

 (or

 useless)

 code -

 statements that compute values that never get used

 While the programmer is unlikely to introduce any dead_code

 intentionally it may appear as the result of previous transformations



 ex

 dead-code-ex

 Suppose

 debug

 is set to

 TRUE

 or

 FALSE

 at various points in the program and used in statements like



 verbatim

 if (debug) print

 verbatim

 It may be possible for the compiler to deduce that each time

 the program reaches this statement the value of

 debug

 is

 FALSE

 Usually it is because there is one particular statement



 verbatim

 debug FALSE

 verbatim

 that must_be the last assignment to

 debug

 prior to any tests of the value of debug

 no_matter what sequence of branches the

 program actually takes

 If copy_propagation replaces

 debug

 by FALSE then the print statement is dead because it

 cannot be reached

 We can eliminate both the test and the print operation from the

 object code

 More_generally deducing at_compile time that the value of

 an expression is a constant and using the constant instead

 is known_as

 constant_folding

 ex



 One advantage of copy_propagation is that it often turns

 the copy_statement into dead_code

 For_example

 copy_propagation followed_by dead-code_elimination

 removes the assignment to

 and transforms the code in Fig figcopy

 into



 center

 tabularl

 at2 t5



 at4 t3



 goto



 tabular

 center

 This code is a further improvement of block

 in Fig_figlocal-cse-after



 Code_Motion



 Loops are a very important place

 for optimizations especially the inner_loops

 where programs tend to spend the bulk of their time

 The running_time of a program may be improved if we decrease the

 number of instructions in an inner_loop

 even if we increase the amount of code outside that loop



 An_important modification

 that decreases the amount of code in a loop is

 code_motion

 This transformation takes an expression

 that yields the same result independent of the number

 of times a loop is executed (a

 loop-invariant computation)

 and evaluates the expression before the loop

 Note_that the notion before the loop assumes the

 existence of an entry for the loop that is one basic_block to

 which all jumps from outside the loop go (see

 Section loops-subsect)



 ex

 code-motion-ex

 Evaluation of

 is a loop-invariant computation in the following while-statement



 verbatim

 while (i limit-2) statement does_not change limit

 verbatim

 Code motion will result in the equivalent code

 verbatim

 t limit-2

 while (i t) statement does_not change limit or t

 verbatim

 Now the computation of is performed once before we enter

 the loop

 Previously there would be calculations of

 if we iterated

 the body of the loop times

 ex



 Induction_Variables and Reduction in Strength



 Another important optimization is to find induction_variables in loops

 and optimize their computation A variable is said to be an

 induction_variable if there is a positive or negative constant

 such that each time is assigned its value increases by For

 instance and are induction_variables in the loop containing

 of Fig_figlocal-cse-after

 Induction variables can be computed with a single increment (addition or

 subtraction) per loop

 iteration The transformation of replacing an expensive operation such

 as multiplication by

 a cheaper one such_as addition

 is known_as strength_reduction

 But induction_variables not only allow_us sometimes to perform a

 strength_reduction often it is possible to eliminate all but one of a

 group of induction_variables whose values remain in lock step as we go

 around the loop



 figurehtb

 figureuullmanalsuch9figsstrengtheps

 Strength reduction applied to in block

 figstrength

 figure



 When processing loops it is useful to work inside-out that is we

 shall start with the inner_loops and proceed to progressively_larger

 surrounding loops

 Thus we_shall see_how this optimization applies to our quicksort

 example by

 beginning with one of the innermost_loops by itself

 Note_that the values of

 and remain in lock step every time the value of

 decreases by 1 the value of decreases by 4 because

 is assigned to

 These variables and

 thus form a good example of a pair of induction

 variables



 When there are two or_more induction_variables in a loop

 it may be possible to get rid of all but one

 For the inner_loop of in Fig_figlocal-cse-after

 we cannot get rid of either

 or completely is used in and

 is used

 in

 However we can illustrate reduction in strength and

 a part of the process of induction-variable_elimination

 Eventually will be_eliminated when the outer_loop

 consisting of blocks and is considered



 ex

 exinduction

 As the relationship surely

 holds after assignment to in Fig_figlocal-cse-after

 and is not changed elsewhere in the inner_loop around

 it follows that just after the statement

 j_j-1

 the relationship must hold

 We may therefore replace the assignment t4 4j

 by t4 t4-4

 The only problem is that does_not have a value

 when we enter block for the first time



 Since we must maintain the relationship

 on entry to the block we place an initialization

 of at the end of the block where

 itself is initialized shown by the dashed addition to

 block in Fig figstrength

 Although we have added one more instruction which is executed once in

 block the replacement of a multiplication by a subtraction will

 speed_up the object code if multiplication takes more time

 than addition or subtraction

 as is the case on many machines

 ex



 figurehtb

 figureuullmanalsuch9figsinductioneps

 Flow_graph after induction-variable_elimination

 figinduction

 figure



 We_conclude this_section with one more instance

 of induction-variable_elimination

 This example treats and

 in the context of

 the outer_loop containing

 and



 ex

 exstrength

 After reduction in strength is applied to the inner_loops around

 and the only use of and

 is to determine the outcome of the test in block

 We know that the values of

 and satisfy the relationship

 while those of

 and satisfy the relationship

 Thus the test can substitute for

 Once this replacement is made

 in block and

 in block become dead variables and the assignments

 to them in these blocks become dead_code that can be_eliminated

 The resulting flow_graph is shown in Fig figinduction

 ex



 figurehtfb

 fileuullmanalsuch9figsfg1eps

 Flow_graph for Exercise fg1-exer

 fg1-fig

 figure



 The code-improving_transformations we have discussed have_been effective

 In Fig figinduction the numbers of instructions in

 blocks and have_been reduced from 4 to 3 compared with

 the original flow_graph in Fig_figqs-fg

 In the number has_been reduced from

 9 to 3

 and in

 from 8 to 3

 True has grown from four instructions to six

 but is executed only once in the fragment

 so the total running_time is barely affected by the

 size of



 exer

 fg1-exer

 For the flow_graph in Fig_fg1-fig



 itemize



 a)

 Identify the loops of the flow_graph



 b)

 Statements (1) and (2) in are both copy statements in which

 and are given constant values For which uses of and can we

 perform copy_propagation and replace these uses of variables by uses of

 a constant Do so wherever possible



 c)

 Identify any global_common subexpressions for each loop



 d)

 Identify any induction_variables for each loop Be sure to take into

 account any constants introduced in (b)



 e)

 Identify any loop-invariant computations for each loop



 itemize

 exer



 exer

 Apply the transformations of this_section to the flow_graph of

 Fig_identity-fg-fig

 exer



 exer

 Apply the transformations of this_section to your flow_graphs from

 (a) Exercise mm-code-exer

 (b) Exercise primes-code-exer

 exer



 exer

 In Fig dot-prod-fig is intermediate_code to compute the dot

 product of two vectors and

 Optimize this code by eliminating common_subexpressions performing

 reduction in strength on induction_variables and eliminating all the

 induction_variables you can

 exer



 figurehtfb



 verbatim

 dp 0

 i 0

 L t1 i8

 t2 At1

 t3 i8

 t4 Bt3

 t5 t2t4

 dp dpt5

 i_i1

 if in goto_L

 verbatim



 Intermediate_code to compute the dot product

 dot-prod-fig



 figure

 Optimizations



 The most challenging task in designing compiler optimizations is the

 problem of formulating the right program analysis We need to decide

 what we_wish to compute and also the precision with which the analysis

 is performed The goal is to find an analysis that produces most of

 the benefits while keeping the analysis as simple as possible to

 minimize the compilation time and the analysis development time In

 the following we give a high-level overview of some of the major

 compiler optimizations developed and the precision of the analyses

 used



 All optimizing compilers include techniques to eliminate redundancy

 in a program especially those introduced because of high-level

 control_flow and data_structure accesses These techniques are known

 as data-flow_analyses They are usually applied to a procedure

 at a time in a flow-sensitive manner and only on simple scalar

 variables allocated on the stack Register_allocation probably the

 most_important optimization in a compiler also takes advantage of

 data-flow_analysis



 Iteration-sensitive techniques and array data_dependence analyses are

 useful in optimizing applications in the scientific domain These

 applications typically consist of large regular data sets and their

 processing can be sped up through parallel_execution and clever uses

 of the memory hierarchy These techniques are useful for high-level

 loop transformations and instruction scheduling Many of these

 scientific applications are written in Fortran and thus require no

 pointer_alias analysis Interprocedural context-sensitive and

 flow-sensitive analyses have_been shown to be useful for translating

 sequential programs into parallel code for multiprocessors



 Finally software productivity tools require the most demanding

 analyses The analyses must_be able to understand pointer aliases

 because of their prevalence in programs They must also be

 interprocedural because many of the programming problems exist across

 procedural boundaries The analyses must also be path-sensitive

 because otherwise the results are too imprecise to be

 helpful





 By differentiating between the contexts in which a function is called

 a context-sensitive_analysis produces much more_precise results but

 is also much more_expensive A calling_context of a function is

 defined by the invocation_sites on the call stack at the time the

 function is executed Since there can be an_unbounded number of

 possible calling_contexts in the presence of recursive_cycles an

 abstraction on the calling_contexts to reduce them to a finite number



 Shivers proposed the concept of k-limiting where calling_contexts are

 distinguished by only the last call_sites pushed on the

 stackshivers91 Emami et_al proposed a more_precise model

 based_on invocation graphsMEmami94 In the absence of

 recursive_calls the analysis computes the points-to_relations for

 each function under all possible calling_contexts where the calling

 context of an function instance is defined as the sequence of call

 sites pushed_onto its call stack Recursive cycles are handled by

 collapsing them to a single_node and modeling it by its fixed point

 solution



 More recently introduced context-sensitive_pointer alias_analyses are

 mostly elimination-based

 algorithmsfahndrich00scalableOOPSLA99WhaleyRWilson95 The

 main idea is to first compute for each function a summary of its

 effects on points-to_relations This is performed in a bottom-up

 manner callees' summaries are applied to compute the callers'

 summaries and fixed-point computation is used to compute summaries of

 functions in each strongly_connected component of a call_graph After

 the summaries are computed the points-to result of given contexts can

 be computed on demand This_approach avoids the need of storing the

 result of all the exponentially_many contexts all at once however

 storing just the summaries has demonstrated still to be too_expensive

 to work on large_programs









 Domain

 description

 Context is all the call_paths in a program with paths

 through methods participating in recursive_cycles collapsed

 description



 Program Relations

 description

 Strongly connected_component a pair

 if can eventually call and can eventually call

 description



 Computed Relations







 Inference_Rules





 equation

 (main i n) mI (i m) IE

 (c i (i m) m)

 main

 equation



 equation

 (m1 i n) mI (i m2) IE (m1 m2) SCC

 (c i c(i m) m)

 notrecursive

 equation



 equation

 (m1 i n) mI) (i m2) IE (m1 m2) SCC

 (c i c m)

 recursive

 equation











 such that

 are non-factory method and are factory

 methods and is an allocation_site in



 is the context representing an acyclic call path in FG



 enumerate

 Advanced Topics in Garbage_Collection

 other-gc-sect



 We close our investigation of garbage_collection with brief treatments

 of four additional topics



 enumerate



 Garbage_collection in parallel environments



 Partial relocations of objects



 Garbage_collection for languages that are not type-safe



 The interaction between programmer-controlled and automatic_garbage

 collection



 enumerate



 Parallel and Concurrent Garbage_Collection

 secgcparallel



 Garbage_collection becomes even more challenging when applied to

 applications running in parallel on a multiprocessor machine It is

 not uncommon for server applications to have thousands of threads

 running at the same time

 each of these threads is a mutator

 Typically the heap will consist of gigabytes of memory



 Scalable garbage-collection algorithms must take_advantage of the

 presence of multiple processors We_say a garbage

 collector is parallel if it uses multiple threads it is concurrent if it runs simultaneously with the mutator



 We_shall describe a parallel and mostly concurrent collector that uses

 a concurrent and parallel phase that does most of the tracing work

 and then a stop-the-world phase that guarantees all the reachable

 objects are found and reclaims the storage This algorithm introduces

 no new basic concepts in garbage_collection per se it shows_how we

 can combine the ideas described so_far to create a full solution to the

 parallel-and-concurrent collection problem However there are some

 new implementation issues that arise due to the nature of parallel

 execution We_shall discuss_how this algorithm coordinates multiple

 threads in a parallel computation using a rather common work-queue

 model



 To understand the design of the algorithm we must keep in mind the

 scale of the problem Even the root_set of a parallel application is

 much larger consisting of every thread's stack register set and

 globally accessible variables The amount of heap_storage can be very

 large and so is the amount of reachable data The rate at which mutations

 take place is also much greater



 To reduce the pause time we can adapt the basic ideas developed

 for incremental analysis to overlap garbage_collection with

 mutation Recall that an incremental analysis as discussed in

 Section short-pause-sect performs the following three

 steps



 enumerate



 Find the root_set This step is normally performed atomically that is

 with the mutator(s) stopped



 Interleave the tracing of the reachable_objects with the

 execution of the mutator(s) In this period every time a mutator

 writes a reference that points from a Scanned object to an

 Unreached object we remember that reference As_discussed

 in Section incr-reach-subsect we have options regarding the

 granularity with which these references are remembered In this

 section we_shall assume the card-based scheme where we divide

 the heap into sections called cards and maintain a bit map

 indicating which cards are dirty (have had one or_more

 references within them rewritten)



 Stop the mutator(s) again to rescan all the cards that may

 hold references to unreached objects



 enumerate



 For a large multithreaded application the set of objects reached by

 the root_set can be very_large It is infeasible to take the time and

 space to visit all such objects while all mutations cease Also due

 to the large heap and the large_number of mutation threads many cards

 may need to be rescanned after all objects have_been scanned once

 It is thus advisable to scan some of these cards in parallel while

 the mutators are allowed to continue to execute concurrently



 To implement the tracing of step_(2) above in parallel

 we_shall use multiple garbage-collecting threads concurrently with

 the mutator threads to trace most of the reachable_objects

 Then to implement step (3) we stop

 the mutators and use parallel threads to ensure

 that all reachable_objects are found



 The tracing of step_(2) is carried_out by having each mutator

 thread perform part of the garbage_collection along with its_own work

 In_addition we use threads that

 are dedicated purely to collecting garbage Once garbage_collection has_been

 initiated whenever a mutator thread performs some memory-allocation

 operation it also performs some tracing computation The pure

 garbage-collecting threads are put to use only when a machine has idle

 cycles As in incremental analysis whenever a mutator writes a

 reference that points from a Scanned object to an Unreached

 object the card that holds this reference is marked dirty and needs

 to be rescanned



 Here is an outline of the parallel concurrent garbage-collection

 algorithm



 enumerate



 Scan the root_set for each mutator thread and put all objects directly

 reachable from that thread into the Unscanned_state The

 simplest incremental approach to this step

 is to wait_until a mutator thread calls the memory

 manager and have it scan its_own root_set if that has not already

 been done If some mutator thread has not called a memory allocation

 function but all the rest of tracing is done then this thread must_be

 interrupted to have its root_set scanned



 Scan objects that are in the Unscanned_state To support parallel

 computation we use a work queue of fixed-size work packets

 each of which holds a number of Unscanned objects Unscanned objects are placed in work packets as

 they are discovered Threads looking for work will dequeue these work

 packets and trace the Unscanned objects therein This strategy allows

 the work to be spread evenly among workers in the tracing_process If

 the system runs out of space and we cannot find the space to create

 these work packets we simply mark the cards holding the objects to

 force them to be scanned The latter is always possible because the

 bit array holding the marks for the cards has already_been allocated



 Scan the objects in dirty cards When there are no more Unscanned objects left in the work queue and all threads' root sets

 have_been scanned the cards are rescanned for reachable_objects As

 long_as the mutators continue to execute dirty cards continue to be

 produced Thus we need to stop the tracing_process using some

 criterion such_as allowing cards to be rescanned only once or a

 fixed_number of times or when the number of outstanding cards is

 reduced to some threshold As a result this parallel and concurrent

 step normally terminates_before completing the trace which is finished

 by the final_step below



 The final_step guarantees that all reachable_objects are marked as reached

 With all the mutators stopped the root sets for all the threads can

 now be found quickly using all the processors in the system Because

 the reachability of most objects has_been traced only a small number

 of objects are expected to be

 placed in the Unscanned_state All the threads then participate

 in tracing the rest of the reachable_objects and rescanning all the

 cards



 enumerate



 It is important that we control the rate at which

 tracing takes place The tracing phase is like a race The mutators

 create new objects and new references that must_be scanned and the

 tracing tries to scan all the reachable_objects and rescan the dirty

 cards generated in the meanwhile It is not desirable to start the

 tracing too_much before a garbage_collection is needed

 because that will increase the amount of floating garbage On

 the other_hand we cannot wait_until the memory is exhausted before the

 tracing starts because then mutators will not be_able to make forward

 progress and the situation degenerates to that of a stop-the-world collector

 Thus

 the algorithm must choose the time to commence the collection and the

 rate of tracing appropriately An estimate of the mutation rate from

 previous cycles of collection can be used to help in the decision

 The tracing rate is dynamically adjusted to account for the work

 performed by the pure garbage-collecting threads



 Partial Object Relocation



 As_discussed starting in Section secgcrelocate copying or compacting

 collectors are advantageous because they eliminate fragmentation

 However these collectors have nontrivial overheads A compacting

 collector requires moving all objects and updating all the references

 at the end of garbage_collection A copying collector figures out

 where the reachable_objects go as tracing proceeds if tracing is

 performed incrementally we need either to translate a mutator's every

 reference or to move all the objects and update their references at the

 end Both options are very expensive especially for a large heap



 We can instead use a copying generational_garbage collector It is

 effective in collecting immature objects and reducing

 fragmentation but can be expensive when collecting mature

 objects We can use the train_algorithm to limit the amount of

 mature data analyzed each time However the overhead of the train

 algorithm is sensitive to the size of the remembered_set for each

 partition



 There is a hybrid collection scheme that uses

 concurrent tracing to reclaim all the unreachable_objects and at the

 same time moves only a part of the objects

 This method reduces fragmentation

 without incurring the full cost of relocation in each collection

 cycle



 enumerate



 Before tracing begins choose a part of the heap that will be evacuated



 As the reachable_objects are marked also remember all the

 references pointing to objects in the designated area



 When tracing

 is complete sweep the storage in parallel to reclaim the space

 occupied by unreachable_objects



 Finally evacuate the reachable

 objects occupying the designated area and fix up the references to the

 evacuated objects



 enumerate



 Conservative Collection for Unsafe Languages

 secconservativegc



 As_discussed in Section secgc-overview it is impossible to

 build a garbage_collector that is guaranteed to work for all C and

 C programs Since we can always compute an address with

 arithmetic_operations no memory_locations in C and C can ever

 be shown to be unreachable However many C or C programs never

 fabricate addresses in this way It has_been demonstrated that a

 conservative_garbage collector - one that does_not necessarily

 discard all garbage - can be built to work well in practice for

 this class of programs



 A conservative_garbage collector assumes that we cannot fabricate an

 address or derive the address of an allocated chunk of memory without

 an address pointing somewhere in

 the same chunk We can find all the garbage in

 programs satisfying such an assumption by treating as a valid address

 any bit pattern

 found anywhere in reachable memory as_long as that bit pattern

 may be construed as a memory_location

 This scheme may classify some data erroneously as addresses

 It is correct however since it only causes the

 collector to be conservative and keep more data than necessary



 Object relocation requiring all references to the old locations be

 updated to point to the new locations is incompatible with

 conservative_garbage collection Since a conservative_garbage

 collector does_not know if a particular bit pattern refers to an actual

 address it cannot change these patterns to point to new addresses



 Here is how a conservative_garbage collector works First the memory

 manager is modified to keep a data map of all the allocated chunks

 of memory This map allows_us to find easily the starting and ending

 boundary of the chunk of memory that spans a certain address The

 tracing starts by scanning the program's root_set to find any bit

 pattern that looks like a memory_location without worrying about its

 type By looking up these potential addresses in the data map we

 can find the starting addresses of those chunks of memory that

 might be reached and place

 them in the Unscanned_state We then scan all the unscanned

 chunks find more (presumably)

 reachable chunks of memory and place them on the work

 list until the work list becomes empty After tracing is done we

 sweep through the heap_storage using the data map to locate and free

 all the unreachable chunks of memory



 Weak References



 Sometimes programmers use a language with garbage_collection but

 also wish to manage memory or parts of memory themselves That is

 a programmer may know that certain objects are never going to be

 accessed again even_though references to the objects remain

 An_example from compiling will suggest the problem



 ex

 gc-identifier-ex

 We have_seen that the lexical_analyzer often manages a symbol_table by

 creating an object for each identifier it_sees These objects may appear as

 lexical values attached to leaves of the parse_tree representing those

 identifiers for instance

 However it is also useful to create a hash_table keyed by the

 identifier's string to locate these objects That table makes it

 easier for the lexical_analyzer to find the object when it encounters a

 lexeme that is an_identifier



 When the compiler passes the scope of an_identifier its symbol-table

 object no

 longer has any references from the parse_tree or probably any other

 intermediate structure used by the compiler

 However a reference to the object is still sitting in the hash_table

 Since the hash_table is part of the root_set of the compiler

 the object cannot be garbage collected If another identifier with the

 same lexeme as is encountered then it will be discovered that

 is out of scope and the reference to its object will be deleted

 However if no other identifier with this lexeme is encountered then

 's object may remain as uncollectable yet useless throughout

 compilation

 ex



 If the problem suggested by Example gc-identifier-ex is important

 then the compiler_writer could arrange to delete from the hash_table all

 references to objects as_soon as their scope ends However a technique

 known_as weak references allows the programmer to rely_on

 automatic_garbage collection and yet not have the heap burdened with

 reachable yet truly unused objects Such a system allows certain

 references to be declared weak An_example would be all the

 references in the hash_table we have_been discussing When the garbage

 collector scans an object it does_not follow weak references within

 that object and does_not make the objects they point to reachable Of

 course such an object may still be reachable if there is another

 reference to it that is not weak



 hexer

 In Section secconservativegc we suggested that it was possible

 to garbage collect for C programs that do_not fabricate expressions

 that point to a place within a chunk unless there is an address that

 points somewhere within that same chunk Thus we rule out code like



 verbatim

 p 12345

 x p

 verbatim

 because while might point to some chunk accidentally there could be no

 other pointer to that chunk On the other_hand with the code above it

 is more likely that points nowhere and executing that code will

 result in a segmentation fault However in C it is possible to write

 code such that a variable like is guaranteed to point to some chunk

 and yet there is no pointer to that chunk Write such a program

 hexer

 Parallelism With MinimumSynchronization

 Parallelism With Minimum Synchronization



 ch11par



 We have described three powerful parallelization algorithms in the

 last three sections Algorithm_algnosync finds all parallelism

 requiring no synchronizations Algorithm_alg1sync finds all

 parallelism_requiring only a constant number of synchronizations and

 Algorithm_algsync finds all the pipelinable parallelism

 requiring synchronizations where is the number of

 iterations in the outermost_loop As a first approximation our goal

 is to parallelize as much of the computation as possible while

 introducing as little synchronization as necessary



 Algorithm_algpar below finds all the degrees of parallelism in a

 program starting_with the coarsest granularity of parallelism In

 practice to parallelize a code for a multiprocessor we do_not need

 to exploit all the levels of parallelism just the outermost possible

 ones until all the computation is parallelized and all the processors

 are fully utilized



 alg

 algpar

 Find all the degrees of parallelism in a program with all the

 parallelism being as coarse-grained as possible



 A program to be_parallelized



 A parallelized version of the same program



 Do the following



 enumerate



 Find the maximum_degree of parallelism_requiring no synchronization

 Apply_Algorithm algnosync to the program



 Find the maximum_degree of parallelism that requires

 synchronizations_Apply Algorithm_alg1sync to each of the space

 partitions_found in step 1 (If no_synchronization-free parallelism

 is found the whole computation is left in one partition)



 Find the maximum_degree of parallelism that requires

 synchronizations_Apply Algorithm_algsync to each of the

 partitions_found in step 2 to find_pipelined parallelism Then apply

 Algorithm_alg1sync to each of the partitions assigned to each

 processor or the body of the sequential loop if no pipelining is

 found



 Find the maximum_degree of parallelism with successively greater

 degrees of synchronizations Recursively apply Step_3 to computation

 belonging to each of the space partitions generated_by the previous

 step

 enumerate

 alg



 ex

 Let_us now return to Example_exnsync1 No parallelism is found

 by Steps 1 and 2 of Algorithm_alg1sync that is we need more

 than a constant number of synchronizations to parallelize this code

 In Step_3 applying_Algorithm algsync determines that there is

 only one legal outer_loop which is the one in the original code of

 Fig synch-pipe-fig So the loop has no pipelined_parallelism

 In the second part of Step_3 we apply_Algorithm alg1sync to

 parallelize the inner_loop We treat the code within a partition like

 a whole program the only_difference being that the partition number

 is treated_like a symbolic_constant In this case the inner_loop is

 found to be parallelizable and therefore the code can be_parallelized

 with synchronization_barriers

 ex



 Algorithm_algpar finds all the parallelism in a program at each

 level of synchronization The algorithm prefers parallelization

 schemes that have less synchronization but less synchronization does

 not mean that the communication is minimized Here we discuss two

 extensions to the algorithm to address its weaknesses



 enumerate

 Step_2 of the algorithm parallelizes each strongly_connected

 component independently if no_synchronization-free parallelism is

 found However it may be possible to parallelize a number of the

 components without_synchronization and communication One_solution is

 to greedily find synchronization-free_parallelism among subsets of

 the program dependence graph that share the most amount of data



 If communication is necessary between strongly_connected components

 we note_that some communication is more_expensive than_others For

 example the cost of transposing a matrix is significantly higher_than

 just having to communicate between neighboring_processors Suppose

 and are statements in two separate strongly_connected

 components accessing the same data in iterations

 and respectively If we cannot find

 partition_mappings

 and

 for statements

 and respectively

 such that



 C1i1_c1 - C2i2

 c2 0



 we instead try to satisfy the constraint



 C1i1_c1 - C2i2

 c2



 where is a small constant



 Sometimes we would rather perform more synchronizations to minimize

 communication Example expipeline-adi discusses one such

 example Thus if we cannot parallelize a code with just neighborhood

 communication among strongly_connected components we should attempt

 to pipeline the computation instead of parallelizing each component

 independently As shown in Example expipeline-adi pipelining

 can be applied to a sequence of loops

 enumerate



 ex

 expipeline-adi

 For the ADI integration algorithm in Example_exadi we have

 shown that optimizing the first and second loop_nests independently

 finds parallelism in each of the nests However such a scheme would

 require that the matrix be transposed between the

 loops incurring data_traffic

 If we use Algorithm_algsync to find_pipelined parallelism we

 find that we can turn the entire program into a fully_permutable loop

 nest as in Fig another-nest-fig

 We then can apply_blocking to reduce the communication overhead This

 scheme would incur synchronizations but would_require much less

 communication

 ex



 figurehtfb



 verbatim

 for_(j 0 j_n j)

 for_(i 1_i n1 i)

 if (i n) Xij f(Xij Xi-1j)

 if (j 0) Xi-1j g(Xi-1jXi-1j-1)



 verbatim



 A fully_permutable loop_nest for the code of

 Example_exadi

 another-nest-fig



 figure

 Introduction

 overview-sect



 The analysis phase of a compiler breaks up a source_program into

 constituent pieces and produces an internal representation for it

 called intermediate_code The synthesis phase translates the

 intermediate_code into the target program



 Analysis is organized around the syntax of the language to be

 compiled The syntax of a programming_language describes the

 proper form of its programs while the semantics of the

 language defines what its programs mean that is what each

 program does when it executes For specifying syntax we present a

 widely used notation called context-free_grammars or BNF (for

 Backus-Naur Form) in Section syntax-sect With the notations

 currently available the semantics of a language is much more

 difficult to describe than the syntax For specifying semantics

 we_shall therefore use informal descriptions and suggestive

 examples



 Besides specifying the syntax of a language a context-free

 grammar can be used to help guide the translation of programs In

 Section trans-sect we introduce a grammar-oriented

 compiling technique known_as syntax-directed_translation

 Parsing or syntax analysis is introduced in

 Section parse-sect



 The rest of this_chapter is a quick tour through the model of a

 compiler_front end in Fig front-fig We begin_with the

 parser For_simplicity we consider the syntax-directed

 translation of infix expressions to postfix form a notation in

 which operators appear after their operands For_example the

 postfix form of the expression is Translation

 into_postfix form is rich enough to illustrate syntax analysis

 yet simple enough that the translator is shown in full in

 Section_postfix-sect The simple translator handles

 expressions like consisting of digits separated_by plus

 and minus_signs One reason for starting_with such simple

 expressions is that the syntax analyzer can work directly with the

 individual characters for operators and operands



 figurehtfb

 A model

 of a compiler_front end front-fig

 figure



 A lexical_analyzer allows a translator to handle multicharacter

 constructs like identifiers which are written as sequences of

 characters but are treated_as units called tokens during

 syntax analysis for example in the expression count1

 the identifier count is treated_as a unit The lexical

 analyzer in Section lexan-sect allows numbers identifiers

 and white_space (blanks tabs and newlines) to appear within

 expressions



 Next we consider intermediate-code_generation Two forms of

 intermediate_code are illustrated in Fig do-trans-fig One

 form called abstract_syntax trees or simply syntax

 trees represents the hierarchical syntactic_structure of the

 source_program In the model in Fig front-fig the parser

 produces a syntax_tree that is further translated_into

 three-address_code Some compilers combine parsing and

 intermediate-code_generation into one component



 figurehtfb



 Intermediate_code for do ii1

 while(aiv) do-trans-fig

 figure



 The root of the abstract_syntax tree in Fig do-trans-fig(a)

 represents an entire do-while loop The left_child of the root

 represents the body of the loop which consists of only the

 assignment ii1 The right_child of the root

 represents the condition aiv An implementation of

 syntax_trees appears in Section syntree-sect



 The other common intermediate_representation shown in

 Fig do-trans-fig(b) is a sequence of three-address

 instructions a more complete example appears in

 Fig quick-quad-fig This form of intermediate_code takes

 its name from instructions of the form where

 op is a binary_operator and are the addresses for

 the operands and is the address for the result of the

 operation A three-address_instruction carries out at most one

 operation typically a computation a comparison or a branch



 In Appendix together-sect we put the techniques in this

 chapter together to build a compiler_front end in Java The front

 end translates statements into assembly-level instructions

 BDD-Based Approach



 Context-Insensitive Points-to_Analysis



 A Complete BDD Based Approach

 Represent programs as relations This is how we do call-graph

 discovery



 Context-Sensitive Points-to_Analysis



 There_are many kinds of pointer_alias analysis Points to is one of

 them and probably one of the easiest ones



 Background

 Different approaches

 - elimination-based algorithms - solve a much harder problem

 - partial transfer_functions

 - invocation graph based



 Invocation-graph based

 - may be large in contexts but answers are relatively_small



 Approach is to create an exploded_call graph and use our

 context-insensitive algorithm



 Definition of context-sensitive paths

 How to number the contexs

 What are the properties

 What is it a good idea



























 exampleExample











 f

 f

 j

 j

 st

 st





 project

 join

 rename





 propertyProperty

 ie

 eg

 et_al

 a la



 IR

 OR

 Rules

 vP

 vPfilter

 hP

 assignc

 vPc

 vPt

 vP0t

 hPt

 vP0c

 hPc

 vT

 hT

 aT

 mI

 mV

 IE

 IEf

 cha

 assign

 store

 load

 actual

 formal

 mVc

 hPc

 Ht

 IEc

 IEt

 IEf

 IEa

 Id

 ctxt





 vTc

 fT

 vTfilter



 naive

 bddbddb



 theoremTheorem

 algorithmtheoremAlgorithm



 Input

 Output

 Method

 Domain

 Input Relations

 Output Relations

 Inference_Rules





 PLDI'04 June 9-11 2004 Washington DC USA



 2004



 1-58113-807-5040006





 Cloning-Based Context-Sensitive_Pointer Alias Analysis



 Using Binary Decision Diagrams





 John Whaley Monica S_Lam



 Computer_Science Department



 Stanford University



 Stanford CA 94305



 jwhaley lamstanfordedu

























 This_paper presents the first scalable context-sensitive

 inclusion-based_pointer alias_analysis for Java programs Our

 approach to context_sensitivity is to create a clone of a method for

 every context of interest and run a context-insensitive

 algorithm over the expanded_call graph to get context-sensitive

 results For precision we generate a clone for every acyclic_path

 through a program's call_graph treating methods in a strongly

 connected_component as a single_node Normally this formulation is

 hopelessly intractable as a call_graph often has acyclic

 paths or_more We show that these exponential relations can be

 computed efficiently using binary_decision diagrams (BDDs) Key to

 the scalability of the technique is a context_numbering scheme that

 exposes the commonalities_across contexts We applied our algorithm

 to the most_popular applications available on Sourceforge and found

 that the largest programs with hundreds of thousands of Java

 bytecodes can be analyzed in under 20 minutes



 This_paper shows that pointer_analysis and many other queries and

 algorithms can be described succinctly and declaratively using

 Datalog a logic_programming language We have developed a system

 called that automatically_translates Datalog_programs into

 highly_efficient BDD implementations We used this approach to

 develop a variety of context-sensitive algorithms including side

 effect analysis type analysis and escape_analysis







 D34Programming LanguagesProcessorsCompilers



 E2DataData Storage Representations



 Algorithms Performance Design Experimentation Languages



 context-sensitive_inclusion-based pointer_analysis Java

 scalable cloning binary_decision diagrams program analysis Datalog

 logic_programming



 Introduction



 A compiler optimization must preserve the semantics of the

 orignal program Except in very special circumstances

 once a programmer chooses and implements a particular

 algorithm the compiler cannot understand enough about the program to

 replace it with a completely different and more_efficient

 algorithm A compiler only knows how to apply relatively low-level

 semantic transformations using general facts such_as algebraic

 identities like or program semantics such_as the fact that

 performing

 the same operation on the same values yields the same results



 Sources of Redundancy



 There_are many redundant operations in a typical program Sometimes

 the redundancy is available at the source_level

 For_instance a programmer may find

 it more direct and convenient to recalculate some result leaving it to

 the compiler to recognize that only one such calculation is necessary

 But more

 often the redundancy is a side-effect of having written the program

 in a high-level_language In most languages (other_than C or C where

 pointer arithmetic is allowed) programmers have no

 choice but to refer to elements of an array or fields in a structure

 through accesses like Aij or X-f1 Each of these

 high-level data-structure accesses expands into a number of

 low-level_arithmetic operations such_as the computation of the location

 of the th_element of a matrix Accesses to similar data_structures

 often share_many common low-level_operations Programmers are not

 aware of these low-level_operations and cannot eliminate the

 redundancies themselves It is in fact preferable from a

 software-engineering perspective that programmers only access data elements

 by their

 high-level names the programs are easier to write and more

 importantly easier to understand By having a compiler eliminate the

 redundancies we get the best of both worlds the programs are both

 efficient and easy to understand



 A Running Example Quicksort



 In the following we will use a sorting program called quicksort to

 illustrate_how programs are represented and how they can be

 optimized The C program in Fig is derived_from

 SedgewickR Sedgewick Implementing Quicksort

 Programs Comm_ACM bf21 pp 847-857who discussed the hand-optimization of such a program

 We_shall not discuss the algorithmic aspects of this

 program here for example the fact that a0 must contain the

 smallest of the sorted elements and amax the largest











 C code for quicksort





 Before we can optimize away the redundancy in address_calculations

 the address operations in a program must first be broken_down

 into low-level_arithmetic operations to expose the redundancy

 In the rest of the chapter we assume that the intermediate_code

 consists of three-address_statements where temporary_variables are

 used to hold all the results of intermediate expressions

 Intermediate_code

 for a portion of the program in Fig is shown

 in Fig











 Three-address_code for fragment in Fig





 Assuming in this example that integers occupy four_bytes an

 assignment such_as

 x_ai is translated_into statements





 as shown in steps (14) and (15) of

 Fig

 Similarly aj_x becomes





 in steps (20) and (21)

 Notice_that every array

 access in the original_program translates_into a pair of steps

 consisting of a

 multiplication and an array subscripting operation As a result

 this short program_fragment translates_into a rather long sequence of

 three-address operations



 Computing Offsets in Intermediate_Code

 The intermediate_code can be (relatively) independent of

 the target_machine so the optimizer does_not have to change

 much if the code_generator is replaced_by one for a different machine

 The intermediate_code in Fig assumes that each

 element of the integer array

 a

 takes four_bytes

 Some intermediate codes eg P-code for Pascal leave

 it to the code_generator to fill in the size of array_elements

 so the intermediate_code is independent of the size of a machine word

 We could have done the same in our intermediate_code

 if we replaced 4 by a symbolic_constant



 In the code optimizer a procedure is typically represented_by a

 control_flow graph where nodes represent basic_blocks and edges

 indicate the flow of control as discussed in Section





 figureuullmanalsuch9figsqs-fgeps

 Flow_graph



 Figure contains the flow_graph for the program in

 Fig

 is the initial_node

 All conditional and unconditional_jumps to statements in

 Fig have_been replaced in Fig by jumps

 to the block of which the statements are leaders (first statements

 in their_respective blocks)

 Note_that for a conditional goto like statement (8) of Fig

 there are arcs in the flow_graph to both the block whose leader is

 mentioned in the goto - in this case - and the block whose

 leader follows the conditional goto - here



 In Fig there are three loops

 and are loops by themselves

 Blocks and together

 form a loop with the only entry point



 From Datalog to BDDs



 In this_paper we represent the program and analysis results as relations in a deductive database Analyses are specified in

 Datalog a declarative language similar to Prolog that operates on

 relations This formulation is powerful and succinct Furthermore

 these Datalog_programs can be_implemented directly using_BDDs leading

 to a highly_efficient implementation In_fact the algorithm

 descriptions we present in this_paper are copied verbatim from the

 actual implementation



 This_section is split into three_parts

 Section gives a brief overview of relations

 Datalog and the notation we will use in the rest of the paper

 Section presents the context-insensitive_points-to

 analysis of Berndl in Datalog and our notation

 Section gives an overview of BDDs and outlines how

 to efficiently execute Datalog_programs using BDD_operations





 In this_section we start with a brief introduction to Datalog We

 then show_how Datalog can be used to describe the context-insensitive

 points-to_analysis due to Berndl at a high_level We then

 describe_how our bddbddb system translates a Datalog_program

 into an efficient_implementation using_BDDs



 Datalog



 Some relations exist a priori for example

 relations that are extracted directly from the program code We call

 such relations input relations Likewise the relations that

 consist of the final results of the Datalog_program are called output

 relations



 A Datalog_program consists of a set of domains input relations

 extracted_from the program to be analyzed output relations and rules

 describing how the output relations can be computed from the input





 We represent a program and all its analysis results as relations

 Conceptually a relation is a two-dimensional table The

 columns are the attributes each of which has a domain

 defining the set of possible attribute values The rows are the

 tuples of attributes that share the relation If tuple is

 in relation we say that predicate is true



 A Datalog_program consists of a set of rules written in a

 Prolog-style notation where a predicate is defined as a conjunction

 of other predicates For_example the Datalog rule



 says_that is true if and are

 all true Variables in the predicates can be replaced with

 constants which are surrounded_by double-quotes or don't-cares

 which are signified by underscores Predicates on the right_side of

 the rules can be inverted



 Datalog is more_powerful than SQL which is based_on relational

 calculus because Datalog predicates can be recursively

 defined If none of the predicates in a Datalog

 program is inverted then there is a guaranteed minimal_solution

 consisting of relations with the least number of tuples Conversely

 programs with inverted predicates may not have a unique minimal

 solution Our_system accepts a subclass of Datalog_programs

 known_as stratified programs for which minimal

 solutions always exist Informally rules in such programs can be

 grouped into strata each with a unique minimal_solution that can be

 solved in sequence



 Context-Insensitive Points-to_Analysis



 We_now review Berndl et al's context-insensitive_points-to

 analysis while also introducing the Datalog notation

 This algorithm assumes that a call_graph computed using simple class

 hierarchy analysis is available a priori Heap objects are

 named by their allocation_sites The algorithm finds the objects

 possibly pointed to by each variable and field of heap_objects in

 the program Shown in Algorithm is the exact Datalog

 program as fed to that implements Berndl's algorithm To keep

 the first example simple we defer the discussion of using types to

 improve precision until Section





 Context-insensitive_points-to analysis with a precomputed

 call_graph



 Domains



 123456123456712345678









 Relations





















 Rules

 lcl

 (vh)_- 0(vh)



 (v1h) -_(v1v2) (v2h)



 (h1fh2) -_(v1fv2)



 (v1h1) (v2h2)



 (v2h2) -_(v1fv2)



 (v1h1) (h1fh2)

 A Datalog_program has three sections domains relations and rules

 A domain declaration has a name a size and an_optional

 file name that provides a name for each element in the domain

 internally represented as an ordinal number from 0 to The

 latter allows to communicate with the users with meaningful

 names A relation declaration has an_optional keyword

 specifying whether it is an input or output relation the name of the

 relation and the name and domain of every attribute A relation

 declared as neither input nor output is a temporary relation generated

 in the analysis but not written out Finally the rules follow the

 standard Datalog syntax The rule numbers introduced here

 for the sake of exposition are not in the actual program



 We can express all information found in the intermediate

 representation of a program as relations To_avoid inundating readers

 with too_many definitions all at once we define the relations as they

 are used

 The domains and relations used in Algorithm are





 V is the domain of variables It represents all the allocation

 sites formal_parameters return values thrown exceptions cast

 operations and dereferences in the program There is also

 a special global variable for use in accessing static variables





 H is the domain of heap_objects Heap objects are named by the

 invocation_sites of object_creation methods To increase precision

 we also statically identify factory_methods and treat_them as object

 creation methods











 F is the domain of field descriptors in the program Field

 descriptors are used when loading from a field (v vf) or

 storing to a field (vf v) There is a special field

 descriptor to denote an array_access





 V_H is the initial variable points-to_relation

 extracted_from object allocation statements in the source_program

 means there is an invocation_site that assigns a

 newly_allocated object to variable





 V F V represents store statements

 says_that there is a statement vf v

 in the program





 V F V represents load statements



 says_that there is a statement v vf in the program





 V V is the assignments relation due to

 passing of arguments and return values means that

 variable includes the points-to set of variable Although

 we do_not cover exceptions here they work in an

 analogous manner





 V_H is the output variable points-to_relation

 means that variable can point to heap_object





 H_F H is the output heap points-to

 relation_means that field of heap_object

 can point to heap_object



 Note_that local_variables and their assignments are factored away

 using a flow-sensitive analysis The assign

 relation is derived by using a precomputed call_graph



 The sizes of the domains are determined by the number of variables

 heap_objects and field descriptors in the input program



 Rule_() incorporates the initial variable points-to_relations into

 Rule_() finds the transitive_closure over inclusion edges

 If includes and variable can point to object

 then can also point to Rule_() models the effect

 of store_instructions on heap_objects Given a statement vf v

 if v can point to and v can point to

 then f

 can point to Rule_() resolves load instructions

 Given a statement v vf if v can point to and

 f can point to then v can point to

 Applying these rules until the results converge finds all the

 possible context-insensitive_points-to relations in the program



 Improving Points-to_Analysis with Types



 Context-insensitive_points-to analysis with type_filtering

 Domains



 Domains_from Algorithm_plus



 12345612345678123456789

 Relations



 Relations from Algorithm_plus













 Rules

 lcl

 (vh)_- (vtv)_(hth) (tvth)



 (vh)_- 0(vh)



 (v1h) -_(v1v2) (v2h)



 (v1h)



 (h1fh2) -_(v1fv2)



 (v1h1) (v2h2)



 (v2h2) -_(v1fv2) (v1h1)



 (h1fh2) (v2h2)

 Because Java is type-safe variables can only point to objects of assignable types Assignability is similar to the subtype

 relation with allowances for interfaces null values and

 arrays By dropping targets of unassignable types in

 assignments and load statements we can eliminate many impossible

 points-to_relations that result from the imprecision of the analysis

 (We could similarly perform type_filtering on stores into

 heap_objects However because all stores must go_through variables

 such a type_filter would only catch one extra case - when the base

 object is a null constant)



 Adding type_filtering to Algorithm is simple in Datalog

 We add a new domain to represent types and new relations to represent

 assignability as_well as type declarations of

 variables and heap_objects We compute the type_filter and modify the

 rules in Algorithm to filter out unsafe assignments and

 load operations









 T is the domain of type descriptors (ie classes) in the

 program





 V T represents the declared types of variables

 means that variable is declared with type





 H T represents the types of objects_created at a

 particular creation_site In Java the type created by a new

 instruction is usually known statically(The type of a

 created object may not be known precisely if for example the object

 is returned by a native method or reflection is used Such types

 are modeled conservatively as all possible types)

 means that the object_created at has type





 T_T is the relation of assignable types

 means that type is assignable to type





 V_H is the type_filter relation

 means that it is type-safe to assign heap_object

 to variable



 Rule_() in Algorithm defines the

 relation It is type-safe to assign heap_object of type

 to variable of type if is assignable from

 Rules () and () are the same as Rules

 () and () in Algorithm Rules

 () and () are analogous to Rules

 () and () with the additional_constraint that

 only points-to_relations that match the type_filter are inserted



 Translating Datalog into Efficient BDD Implementations



 We first describe_how Datalog_rules can be translated_into

 operators from relational algebra such_as join and project

 then show_how to translate these operations into BDD_operations



 Query Resolution



 We can find the solution to an unstratified query or a stratum of a

 stratified query simply by_applying the inference_rules repeatedly

 until none of the output relations change We can apply a Datalog

 rule by performing a series of relational natural_join project and

 rename_operations A natural_join operation combines rows from two

 relations if the rows share the same value for a common attribute A

 project operation removes an attribute from a relation A rename

 operation changes the name of an attribute to another one



 For_example the application of Rule_() can be

 implemented as



 123123123123













 We first rename the attribute in relation from variable to

 source so that it can be joined with relation assign to

 create a new points-to_relation The attribute dest of the

 resulting relation is changed to variable so that the tuples can

 be added to the tuples accumulated thus far



 The system uses the three following optimizations to speed_up

 query resolution







 Attributes naming Since the names of the attributes must match when

 two relations are joined the choice of attribute names can affect the

 costs of rename_operations Since the renaming cost is highly

 sensitive to how the relations are implemented the system

 takes the representation into_account when minimizing the renaming

 cost





 Rule application order A rule needs to be applied only if the input

 relations have changed optimizes the ordering of the rules

 by analyzing the dependences_between the rules For_example Rule 1

 in Algorithm does_not depend_on any of the other rules

 and can be applied only once at the beginning of the query resolution





 Incrementalization We only need to re-apply a rule on those

 combinations of tuples that we have not seen before Such a technique

 is known_as incrementalization in the BDD literature and

 semi- fixpoint evaluation in the database

 literature Our_system also identifies

 loop-invariant relations to avoid unnecessary difference and rename

 operations Shown below is the result of incrementalizing the repeated

 application of Rule_()



 123456123123123



 repeat























 until









 Relational Algebra in BDD



 We_now explain how BDDs work and how they can be used to implement

 relations and relational operations BDDs (Binary Decision Diagrams)

 were_originally invented for hardware verification to efficiently

 store a large_number of states that share_many

 commonalities



 They are an efficient_representation of boolean_functions A BDD is a

 directed acyclic_graph (DAG) with a single root node and two terminal

 nodes representing the constants one and zero Each non-terminal

 node in the DAG represents an input variable and has exactly two

 outgoing_edges a high_edge and a low_edge The high_edge represents

 the case_where the input variable for the node is true and the low

 outgoing_edge represents the case_where the input variable is false

 On any path in the DAG from the root to a terminal node the value of

 the function on the truth values on the input variables in the path is

 given by the value of the terminal node To evaluate a BDD for a

 specific input one simply starts at the root node and for each node

 follows the high_edge if the input variable is true and the low_edge

 if the input variable is false The value of the terminal node that

 we reach is the value of the BDD for that input



 The variant of BDDs that we use are called ordered binary

 decision_diagrams or OBDDs Ordered refers to the

 constraint that on all paths through the graph the variables respect a

 given linear order In_addition OBDDs are maximally reduced

 meaning that nodes with the same variable name and low and high

 successors are collapsed as one and nodes with identical low and high

 successors are bypassed Thus the more commonalities there are in

 the paths_leading to the terminals the more compact the OBDDs are

 Accordingly the amount of the sharing and the size of the

 representation depends greatly on the ordering of the variables



 We can use BDDs to represent relations as_follows Each element

 in an -element domain is represented as an_integer between 0

 and using bits A relation

 is represented as a boolean function

 such that

 iff and iff





 A number of highly-optimized BDD packages are

 available the operations they provide can be used

 directly to implement relational operations efficiently For_example

 the replace operation in BDD has the same semantics as the

 rename operation the relprod operation in BDD finds the

 natural_join between two relations and projects away the common

 domains For_example if and

 the operation is defined as













 Let_us now use a concrete example to illustrate the significance of

 variable_ordering Suppose relation

 contains tuples and relation

 contains tuples If in the variable

 order the bits for the first attribute come before the bits for the

 second the BDD will need to represent the sequence

 separately for each relation However if instead the bits for the

 second attribute come first the BDD can

 share the representation for the sequence

 between and Unfortunately the problem of finding the

 best variable_ordering is NP-complete Our

 system automatically explores different alternatives empirically to

 find an effective ordering





 Incrementalization



 When re-applying a rule we only need to apply it to those new tuples

 to which the rule has not been applied before Such a technique is

 known_as incrementalization in the BDD literature and

 semi- fixpoint evaluation in the database

 literature



 To incrementalize a rule for each subgoal we combine the new elements

 in the relation with all elements of the other relations and union the

 results For_example if there are three subgoal relations

 and with new elements and the algorithm will

 calculate Obviously we can skip a

 calculation when a difference relation is empty



 The deductive_engine automatically transforms the rules to maximize

 opportunities for incrementalization It also identifies

 loop-invariant relations to avoid unnecessary difference operations







 We use a BDD-based deductive_engine that takes as input the Datalog

 programs exactly as they are presented in this_paper

 The deductive_engine transforms the rules into efficient_BDD

 operations automatically optimizing the evaluation order and

 incrementalizing the computation This_section gives a brief overview

 of BDDs and how the Datalog_programs map into BDD_operations







 A BDD can be thought of as a set of binary strings A particular

 binary_string is a member of the set if and only if the boolean

 function for that binary_string equals one A binary_string can

 partitioned_into substrings each of which corresponds to an attribute

 in a tuple A substring of bits can be used to represent

 elements in a domain



 By partitioning

 the bits into substrings each substring can be

 a binary_string can also represent strings of

 non-binary numbers For_example the bits are used for the

 first number bits are used for the second

 number etc These substrings are called physical_domains



 Each element in an -element domain is

 represented as an_integer between 0 and using

 bits A relation

 is represented as a boolean function

 such that

 if and

 if





 We represent relations in this manner Each element of a domain

 is given a unique number Each attribute of a relation is then assigned a

 physical_domain with a set of variables where is the number of

 bits necessary for the domain that corresponds to the attribute If

 and and each domain

 requires two bits the tuple is represented_by

 A relation is then a set of

 these binary strings If a relation does_not use a physical_domain

 those variables are don't-care effectively making those physical

 domains contain the universal_set





 Because ROBDDs exhibit shared substructure they can efficiently

 represent similar sets Furthermore as sets get larger the size of

 the BDD data_structure can become smaller because the number of

 don't-care bits increases (In BDDs the universal_set is

 represented_by a single_node the terminal node for 1) Luckily the

 results of program analyses especially context-sensitive program

 analyses often contain large sets and a lot of redundancy This

 makes BDDs a particularly apt data_structure to represent the results

 of many program analyses







 In ROBDDs for any given function

 there is exactly one node which represents

 that function Because this is true for all nodes in the ROBDD the

 ROBDD exhibits shared substructure where many paths can share

 the same nodes This leads to an efficient_representation of similar

 functions which is critical to our ability to handle the many

 contexts in context-sensitive_analysis





 The deductive_engine must also assign attributes to physical_domains

 such that there are no conflicting assignments within a relation or a

 rule We would also like to minimize the BDD rename_operations and

 avoid any rename_operations that change the relative order of physical

 domains in a BDD The deductive_engine models the assignment as a

 constraint problem taking into_account the variable_ordering and

 trying to avoid renamings within loops





 Call_Graph Discovery



 The call_graph generated using class_hierarchy analysis can have many

 spurious call_targets which can lead to many spurious points-to

 relations We can get more_precise results by

 creating the call_graph on the fly using points-to_relations As the

 algorithm generates points-to_results they are used to identify the

 receiver types of the methods invoked and to bind calls to target

 methods and as call_graph edges are discovered we use them to find

 more points-to_relations The algorithm_converges when no new call

 targets and no new pointer relations are found





 Context-insensitive_points-to analysis that computes

 call_graph on the fly

 Domains



 Domains_from Algorithm_plus



 123456123456712345678













 Relations



 Relations from Algorithm with the modification

 that is now a computed relation plus





















 Rules



 Rules from Algorithm_plus

 lcl

 (im) - 0(im)



 (im2) - (m1in) (i0v)



 (vh) (ht) (tnm2)



 (v1v2) - (im) (mzv1)



 (izv2)

 Modifying Algorithm to discover call graphs on the fly

 is simple Instead of an input relation computed from a

 given call_graph we derive it from method

 invocation statements and points-to_relations









 I is the domain of invocation_sites in the program An

 invocation_site is a method invocation of the form

 r pm(p p)

 Note_that





 N is the domain of method names used in invocations In an

 invocation

 r pn(p p)

 n is the method name





 M is the domain of implemented methods in the program It does

 not include abstract or interface methods





 Z is the domain used for numbering parameters





 T N M

 encodes virtual_method dispatch information from the class

 hierarchy means that is the target of

 dispatching the method name on type







 I Z V encodes the actual_parameters

 for invocation_sites

 means that is passed as parameter

 number at invocation_site





 M Z V encodes formal_parameters for methods

 means that formal_parameter of

 method is represented_by variable





 I M are the initial invocation_edges They

 record the invocation_edges whose targets are statically

 bound In Java some calls are static or non-virtual Additionally

 local type analysis combined with analysis of the class_hierarchy

 allows_us to determine that some calls have a single target

 means that invocation_site can be analyzed

 statically to call method





 M I N represents invocation_sites

 means that method contains an

 invocation_site with virtual_method name

 Non-virtual invocation_sites are given a special null method

 name which does_not appear in the relation













 I M is an output relation encoding all

 invocation_edges means that invocation_site calls

 method



 The rules in Algorithm compute the relation

 used in Algorithm

 Rules () and () find the invocation_edges

 with the former handling statically bound targets and the latter

 handling virtual calls Rule_() matches invocation_sites

 with the type of the this pointer and the class_hierarchy

 information to find the possible target methods If an invocation

 site with method name is invoked on variable and can

 point to and has type and invoking on type leads

 to method then is a possible target of invocation



 Rule_() handles parameter passing(We

 also match thread objects to their corresponding run() methods

 even_though the edges do_not explicitly appear in the call graph) If

 invocation_site has a target method variable is passed

 as argument number and the formal_parameter of method is

 then the points-to set of includes the points-to set of

 Return values are handled in a likewise

 manner only the inclusion relation is in the opposite direction

 We see that as the discovery of more variable points-to ()

 can create

 more invocation_edges () which in turn can create more

 assignments () and more points-to_relations

 The algorithm_converges when all the relations stabilize













 Context Sensitive Points-To



 A context-insensitive or monomorphic analysis produces just one

 set of results for each method regardless how many_ways a method may

 be invoked This leads to imprecision because information from

 different calling_contexts must_be merged so information

 along one calling_context can propagate to other calling_contexts A

 context-sensitive or polymorphic analysis avoids this

 imprecision by allowing different_contexts to have different

 results



 We can make a context-sensitive_version of a context-insensitive

 analysis as_follows We make

 a clone of a method for each path through the call_graph linking each

 call_site to its_own unique clone We then run the original

 context-insensitive_analysis over the exploded_call graph However this

 technique can require an exponential (and in the presence of cycles

 potentially unbounded) number of clones to be created



 It has_been observed that different_contexts of the same method often

 have many similarities For_example parameters to the same method

 often have the same types or similar aliases This observation led to

 the concept of partial transfer_functions (PTF) where summaries for

 each input pattern are created on the fly as they are

 discovered

 However PTFs are notoriously difficult to implement and get correct

 as the programmer must explicitly calculate the input patterns and

 manage the summaries Furthermore the technique has not been shown

 to scale to very_large programs



 Our_approach is to allow the exponential explosion to occur and rely

 on the underlying BDD representation to find and exploit the

 commonalities_across contexts BDDs can express large sets of

 redundant data in an efficient manner Contexts with identical

 information will automatically be shared at the data_structure level

 Furthermore because BDDs operate down at the bit level it can even

 exploit commonalities between contexts with different information

 BDD_operations operate_on entire relations at a time rather_than one

 tuple at a time Thus the cost of BDD_operations depends_on the size

 and shape of the BDD relations which depends greatly on the variable

 ordering rather_than the number of tuples in a relation Also due

 to caching in BDD packages identical subproblems only have to be

 computed once Thus with the right variable_ordering the results

 for all contexts can be computed very efficiently





 Numbering Call Paths









 Example of path numbering The graph on the left is the

 original graph Nodes and are in a cycle and therefore

 are placed in one equivalence_class Each edge is marked with path

 numbers at the source and target of the edge The graph on the right

 is the graph with all of the paths expanded







 The six contexts of function in

 Example



 We implement cloning by giving each call path to a method a

 unique context number Variables and invocation_sites of a clone of a

 method invocation are given the same context number An invocation

 edge in a cloned_call graph links a call_site in one context with a

 context of the target method



 A call path is a sequence of invocation_edges

 such that is an invocation_site in an

 entry method typically main(Other entry methods in

 typical_programs are static class initializers object finalizers and

 thread run methods) and is an invocation_site in

 method for all



 For programs without recursion every call path to a method defines a

 context for that method To handle recursive programs which have an

 unbounded_number of call_paths we first find the strongly_connected

 components (SCCs) in a call_graph By eliminating all method

 invocations whose caller and callee belong to the same SCC from the

 call_paths we get a finite set of reduced_call paths Each

 reduced_call path to an SCC defines a context for the methods in the

 SCC Thus information from different paths_leading to the SCCs are

 kept separate but the methods within the SCC invoked with the same

 incoming call path are analyzed context-insensitively





 Figure (a) shows a small call_graph with just six

 methods and a set of invocation_edges Each invocation_edge has a

 name being one of through its source is labeled by the

 context number of the caller and its sink by the context number of the

 callee The numbers will be explained in Example

 Methods and belong to a strongly_connected component so

 invocations along edges and are eliminated in the computation

 of reduced_call graphs While there are infinitely many call_paths

 reaching method there are only six reduced_call paths_reaching

 as shown in Figure Thus has six clones

 one for each reduced_call path





 Under this definition of context_sensitivity large_programs can have

 many_contexts For_example pmd from our test programs has 1971

 methods and contexts In the BDD representation we give

 each reduced_call path reaching a method a distinct context

 number It is important to find a context_numbering scheme that

 allows the BDDs to share commonalities_across contexts

 Algorithm shows one such scheme



 Generating context-sensitive_invocation edges from a

 call_graph

 A call_multigraph



 Context-sensitive invocation_edges

 C_I C_M where

 C is the domain of context_numbers

 means that

 invocation_site in context calls method in context



 A method with clones will be given numbers

 Nodes with no predecessors are given a singleton context numbered 1



 Find strongly_connected components in the input call_graph

 The th clone of a method always calls the th clone of another

 method belonging to the same component



 Collapse all methods in a strongly_connected component to a

 single_node to get an acyclic reduced graph



 For each node in the reduced graph in topological_order



 1212341234 Set the counts of contexts created to 0



 For each incoming edge



 If the predecessor of the edge has contexts



 create clones of node



 Add tuple to for















 We_now show the results of applying_Algorithm to

 Example the root node is given context

 number 1 We_shall visit the invocation

 edges from left to right Nodes and being members of a

 strongly_connected component are represented as one node The

 strongly_connected component is reached by two edges from Since

 has only one context we create two clones one reached by each

 edge For method the predecessor on each of the two incoming

 edges has two contexts thus has four clones Method has

 two clones one for each clone that invokes Finally method

 has six clones Clones 1-4 of method invoke clones 1-4

 and clones 1-2 of method call clones 5-6 respectively The

 cloned graph is shown in Figure (b)



 The numbering_scheme used in Algorithm plays up the

 strengths of BDDs Each method is assigned a contiguous range of

 contexts which can be represented efficiently in BDDs The contexts

 of callees can be computed simply by_adding a constant to the contexts

 of the callers this operation is also cheap in BDDs Because the

 information for contexts that share common tail sequences are likely

 to be similar this numbering allows the BDD data_structure to share

 effectively across common contexts For_example the

 sequentially-numbered clones 1 and 2 of both have a common tail

 sequence Because of this the contexts are likely to be similar

 and therefore the BDD can take_advantage of the redundancies



 To optimize the creation of the cloned invocation graph we have

 defined a new primitive that creates a BDD representation of

 contiguous ranges of numbers in O() operations where is the

 number of bits in the domain In essence the algorithm creates one

 BDD to represent numbers below the upper_bound and one to represent

 numbers above the lower_bound and computes the conjunction of these

 two BDDs





 It is simple to construct a BDD to represent contiguous numbers

 for example a boolean

 function that accepts 4-bit numbers in the range from 0 to 13 is

 simply









 However no primitive for constructing contiguous ranges exists so we

 have added an operator for this purpose called range The

 definition of the operator is simple though tedious and is omitted

 here due to space constraints It suffices to say that range

 runs in O() BDD_operations where is the number of bits in the

 domain Once such a BDD is created we can use an efficient_BDD add

 operation to generate the corresponding numbers offset by a constant



 Context-Sensitive_Pointer Analysis with a Pre-computed Call_Graph



 We are now_ready to present our context-sensitive_pointer analysis

 We assume the presence of a pre-computed_call graph created for

 example by using a context-insensitive_points-to analysis

 (Algorithm ) We apply_Algorithm to

 the call_graph to generate the context-sensitive_invocation edges

 Once that is created we can simply_apply

 a context-insensitive_points-to analysis on the exploded_call graph to

 get context-sensitive_results

 We keep the results separate for each clone by

 adding a context number to methods variables invocation_sites

 points-to_relations etc





 In this_section we present the context-sensitive_pointer analysis for

 pre-computed_call graphs A clone of method is identified by a

 context Each cloned method has its_own cloned

 variables cloned invocation_sites each of which is

 connected by its_own invocation_edge to the cloned

 target







 Context-sensitive points-to_analysis with a pre-computed

 call_graph

 Domains



 Domains_from Algorithm_plus



 12123456712345678C 9223372036854775808

 Relations



 Relations from Algorithm_plus

 3em











 Rules

 0em

 lcl

 (vh)_- (vtv)_(hth) (tvth)



 (cvh) -_0(vh) (ch)



 (c1v1h) - (c1v1c2v2)



 (c2v2h) (v1h)



 (h1fh2) -_(v1fv2)



 (cv1h1) (cv2h2)



 (cv2h2) -_(v1fv2) (cv1h1)



 (h1fh2)

 (v2h2)



 2l(c1v1c2v2) -55ex

 lcl

 - (c2ic1m) (mzv1)



 (izv2)



 )

 C is the domain of context_numbers Our BDD_library uses signed 64-bit

 integers to represent domains so the size is limited to







 C_I C M) is the set of

 context-sensitive_invocation edges

 means that invocation_site in context calls method

 in context This relation is computed using

 Algorithm







 C_V C V) is the

 context-sensitive_version of the relation

 means variable in context includes the

 points-to set of variable in context due to parameter

 passing Again return values are handled analogously







 C_V H) is the context-sensitive

 version of the variable points-to_relation ()

 means variable

 in context can point to heap_object



 Rule_() interprets the context-sensitive_invocation edges

 to find the bindings between actual and formal_parameters The rest

 of the rules are the context-sensitive counterparts to those found in

 Algorithm





 CHECK THE PARAGRAPH

 Algorithm refers to allocated_objects simply by its

 allocation_site We can improve the precision of the algorithm by

 naming the heap allocation_sites by their call_paths For programs

 that use the same method to allocate all objects of the same class

 such is the case of the factory design pattern this naming scheme

 degenerates to a simple type analysis To gain better precision we

 consider methods that return freshly allocated_objects as a factory method and name heap_objects by the call_paths through

 factory_methods For optimization we only do this for objects that

 have fields that can hold more_than one type of object as determined

 by our context-insensitive_analysis (see

 Section )

 )



 Algorithm takes advantage of a pre-computed_call graph to

 create an efficient context_numbering scheme for the contexts We can

 compute the call_graph on the fly while enjoying the benefit of the

 numbering_scheme by numbering all the possible contexts) with a

 conservative call_graph and delaying the generation of the invocation

 edges only if warranted by the points-to_results We can reduce the

 iterations necessary by exploiting the fact that many of the

 invocation_sites of a call_graph created by a context-insensitive

 analysis have single targets Such an algorithm has an execution time

 similar to Algorithm but is of primarily academic interest

 as the call_graph rarely improves due to the extra precision from

 context-sensitive_points-to information





 Besides cloning methods we can also clone the heap_objects to

 increase the precision of the analysis For each invocation_site

 in context given an invocation_edge the returned

 object is given context

 )



 Context-Sensitive_Pointer Analysis with Call_Graph Discovery



 In a similar_manner as before we can further improve the precision of

 the analysis by using the context-sensitive_points-to results to

 determine the context-sensitive call_graph on-the-fly Since it is

 critical that the context_numbering allows commonalities of the

 results be_exploited we use Algorithm to first

 compute the context_numbers for a conservative call_graph

 (Algorithm) determines which of the edges are actually

 present on the fly and creates only the necessary parameter bindings





 ) Context-sensitive points-to_analysis with call_graph discovery

 Domains)



 )Domains from Algorithm



 Relations)



 )Relations from Algorithm_plus







 Rules)



 Rules from Algorithm except Rule_() plus

 0em

 lcl

 (ciicmm) - 0(im) (ciicmm)



 (ciicmm2) - (m1in) (i0v)



 (civh) (ht)



 (tnm2) (ciicmm2)



 2l(civ1cmv2)

 -55ex

 lcl

 - (ciicmm) (mzv1)



 (izv2)



 Numbering of invocation_edges C_I

 C_M iff

 in the invocation_edge from to context matches

 context according to the context-sensitive_points-to information



 Rule_() adds context_numbers to call_graph edges that

 are known statically Rule_() matches invocations to

 target methods in a context-sensitive manner Notice_that it uses the

 context-sensitive_points-to information to discover call_targets on a

 per-context basis Thus the call_graph edges themselves are

 context-sensitive Rule_() creates the effects of

 parameter_passing due to the context-sensitive call_graph edges



 Generating context-sensitive_invocation edges from a

 call_graph

 A multigraph_where is a

 set of vertices is a set of invocation_sites

 is a set of edges such that iff



 A relation that maps a

 calling_context and a given invocation to the callee's context and vertex



 Let be a

 graph where entire strongly_connected components in are reduced to a

 single vertex That is let be the strongly_connected

 component representing vertex in









 All vertices within a strongly_connected component have the same

 number of contexts A strongly_connected component has as many

 contexts as the sum of all its predecessor vertices' contexts Let

 be the number of contexts given to a strongly_connected

 component





















 We define a total order

 for all edges to the same component















































 Queries and Other Analyses



 The algorithms in sections and generate vast amounts of results in the form of

 relations Using the same declarative programming interface we can

 conveniently query the results and extract exactly the information we

 are_interested in This_section shows a variety of queries and

 analyses that make use of pointer information and context_sensitivity



 We also give an example of using our cloning technique to get a

 polymorphic version of a monomorphic type analysis and an example of

 using a thread_context to compute thread_escape analysis





 Debugging a Memory Leak



 Memory leaks can occur in Java when a reference to an object remains

 even after it will no_longer be used One common approach of debugging

 memory_leaks is to use a

 dynamic tool that locates the allocation_sites of memory-consuming

 objects Suppose that upon reviewing the information the programmer

 thinks objects allocated in line 57 in file ajava should

 have_been freed He may wish to know which objects may be holding

 pointers to the leaked objects and which operations may have stored the

 pointers He can consult the static analysis results by supplying the

 queries



 lcl

 whoPointsTo57(hf) - (hfajava57)



 whoDunnit(cv1fv2) -_(v1fv2)



 (c v2 ajava57)

 The first query finds the objects and their fields that may point to

 objects allocated at ajava57 the second finds the

 store_instructions and the contexts under which they are executed that

 create the references





 Heap Connectivity



 When using a reference-counting garbage_collector it is useful to

 know whether or not an object can be involved in a

 cycle acyclic objects do_not need to be

 cycle-collected To_find cyclic objects we simply compute a

 transitive_closure on the relation and see if an object can

 transitively point to itself



 lcl

 reachable(h1h2) - (h1fh2)



 reachable(h1h3) - reachable(h1h2) (h2fh3)

 The heap connectivity relation () can also be used

 to find the set of objects that can be accessed by unknown code or to

 guide partitioning strategies for garbage

 collectors





 Finding a Security Vulnerability







 The Java Cryptography Extension (JCE) is a library of cryptographic

 algorithms Misuse of the JCE API can lead to security

 vulnerabilities and a false sense of security For_example many

 operations in the JCE use a secret key that must_be supplied_by the

 programmer It is important that secret keys be cleared after they

 are used so they cannot be recovered by attackers with access to

 memory Since String objects are immutable and cannot be

 cleared secret keys should not be stored in String objects but

 in an array of characters or bytes instead



 To guard against misuse the function that accepts the secret key

 only allows arrays of characters or

 bytes as input However a programmer not versed in security issues

 may have stored the key in a String object and then use a

 routine in the String class to convert it to an array of

 characters We can write a query to audit programs for the presence

 of such idioms Let be an input relation

 specifying that variable is the return value of method We

 define a relation which indicates if the

 object was directly derived_from a String Specifically it

 records the objects that are returned by a call to a method in the

 String class An invocation to method

 is a vulnerability if the first

 argument points to an object derived_from a String



 lcl

 fromString(h) - (Stringm) Mret(mv)



 (vh)



 vuln(ci) - (iPBEKeySpecinit())



 (i1v) (cvh)



 fromString(h)

 Notice_that this query does_not only find cases where the object

 derived_from a String is immediately supplied to

 This query will also identify cases where

 the object has passed through many variables and heap_objects





 Limiting Access of Untrusted Code



 Java includes some unsafe_objects that provide direct unchecked

 access to memory or the file system the sunmiscUnsafe class

 defines one such example Obviously access to unsafe_objects must_be

 confined to trusted code Such confinement unfortunately is not

 enforced by the normal Java protection mechanism but is left up to

 the programmers This is an error-prone task because a seemingly

 unrelated change could cause

 the object to inadvertently become accessible to untrusted_code





 However the objects are used

 in multiple packages and therefore their usage cannot be enforced with

 the normal Java protection mechanism Instead they rely_on the code

 not to leak such objects to untrusted_code Because this property is

 totally unchecked it is very error-prone and sensitive to

 modification -





 We can use points-to_information to check if these unsafe_objects may

 have_been leaked to untrusted_code First we check if any safe

 method may expose unsafe_objects A non-private method could

 potentially be called from untrusted_code so unsafe_objects should

 never be returned by a non-private method Likewise unsafe_objects

 should never be written into non-private static fields Second we

 check if any unsafe code has access to unsafe_objects Variables in

 unsafe code should never refer to unsafe_objects Moreover no

 non-private fields from objects reachable from untrusted_code should

 point to unsafe_objects



 In the following we assume the availability of relations



 which contain the sets of unsafe_objects trusted

 methods private methods and private fields respectively Also let

 be an input relation specifying that variable belongs

 to method



 lcl

 untrustedV(v) - (mv) trustedM(m)



 untrustedV(v) - Mret(mv) privateM(m)



 untrustedH(h)_- untrustedV(v) (vh)



 untrustedH(h)_- (globalfh) privateF(f)



 untrustedH(h)_- untrustedH(h1) (h1fh)



 privateF(f)



 exposed(h) - unsafe(h) untrustedH(h)

 keeps_track of the variables that are accessible

 by untrusted_code It consists of variables of untrusted

 methods and return values of non-private methods

 holds heap_objects that are accessible by

 untrusted_code It includes objects_pointed to by untrusted variables

 objects written into a non-private global variable and objects that

 are transitively reachable from other untrusted objects Any unsafe

 object that is in the relation is potentially

 exposed





 Aliased Parameters



 We can easily use the points-to_information to discover if parameters

 are aliased under certain contexts In many_cases code can be

 optimized if it can be proven that parameters are not aliased Because

 it is context-sensitive this can be used to automatically redirect call

 sites to call specialized versions of methods











 Type Refinement



 Libraries are written to handle the most general types of

 objects possible and their full generality is typically not used in

 many applications By analyzing the actual types of objects used in

 an application we can refine the types of the variables and

 object fields Type refinement can be used to reduce overheads in

 cast operations resolve virtual_method calls and gain better

 understanding of the program



 We_say that variable can be legally declared as written

 if is a supertype of the types of all the

 objects can point to The type of a variable is refinable if the

 variable can be declared to have a more_precise type To_compute the

 super types of we first find the

 types of objects_pointed to by We then intersect the supertypes

 of all the exact_types to get the desired solution we do so in

 Datalog by finding the complement of the union of the complement of

 the exact_types



 lcl

 varExactTypes(vt) - (vh) (ht)



 notVarType(vt) - varExactTypes(vtv) (ttv)



 varSuperTypes(vt) - notVarType(vt)



 refinable(vtc) - (vtd) varSuperTypes(vtc)



 (tdtc) td tc

 The above shows a context-insensitive type_refinement query We find

 for each variable the type to which it can be refined regardless of

 the context Even_if the end result is context-insensitive it is

 more_precise to take_advantage of the context-sensitive_points-to

 results available to determine the exact_types as shown in the first

 rule In Section we compare the accuracy of

 this context-insensitive query with a context-sensitive_version





 holds the types of objects_pointed to by

 We discover the supertypes of all such types by doing a double negation which

 effectively performs a universal quantification







 Querying Relations



 We can perform queries on the results of the analysis by selecting

 elements of the relations that match some criteria For_example to

 find the points-to set of a variable under all contexts we can

 select from the relation as such









 To_find the types of objects that can point to the object









 The combination of these last two queries allows_us to infer polymorphic

 types based_on pointer information for variables and heap_objects

 (with recursion treated by collapsing cycles as described in

 Section )



 The possibilities for queries are almost endless Furthermore these

 types of inference_rules translate directly into BDD_operations and

 are very efficient to compute





 Interprocedural Data Flow



 By combining the points-to_relations that we compute with the initial

 relations from the program statements we can construct relations that

 model the flow of data across method calls and through pointer

 indirections







 By taking the transitive_closure on the usedef or defuse

 relations we can obtain a program slice of the flow of data to

 or from a point The slice of the locations leading up to a variable

 is usedef while the slice of locations following a variable is

 defuse These slices are useful for taint

 analysis provenance analysis

 and flow analysis









 Context-Sensitive Mod-Ref Analysis



 Mod-ref analysis is used to determine what fields of what objects may

 be modified or referenced by a statement or call

 site



 We can use the context-sensitive_points-to results to solve a context-sensitive

 version of this query We define to mean that is a

 local variable in The relation specifies the set of

 variables and contexts of methods that are transitively reachable from

 a method means that calling method with

 context can transitively call a method with local variable

 under_context



 lcl

 (cmcv) - (mv)



 (c1m1c3v3) - (m1i)(c1ic2m2)



 (c2m2c3v3)

 The first rule simply says_that a method in context can reach

 its local variable The second rule_says that if method in

 context calls method in context then in

 context can also reach all variables reached by method in

 context



 We can now define the mod and ref set of a method as_follows



 lcl

 mod(cmhf) - (cmcvv)



 (vf) (cvvh)



 ref(cmhf) - (cmcvv)



 (vf) (cvvh)

 The first rule_says that if method in context can reach a

 variable in context and if there is a store through that

 variable to field of object then in context can

 modify field of object The second rule for defining the ref

 relations is analogous

































































 Context-Sensitive Type Analysis



 Our cloning technique can be applied to add context_sensitivity to

 other context-insensitive algorithms The example we show here is the

 type_inference of variables and fields By not distinguishing between

 instances of heap_objects this analysis does_not generate results as

 precise as those extracted_from running the complete context-sensitive

 pointer_analysis as discussed in Section but

 is much faster

 because there

 is less differentiation



 The basic type analysis is similar to -CFA Each variable

 and field in the program has a set of concrete types that it can refer

 to The sets are propagated through calls returns loads and

 stores By using the path numbering_scheme in

 Algorithm we can convert this basic analysis into

 one which is context-sensitive-in essence making the analysis

 into a -CFA analysis where is the depth of the call_graph and

 recursive_cycles are collapsed





 Context-sensitive type analysis

 Domains



 Domains_from Algorithm



 Relations



 Relations from Algorithm_plus











 Rules

 0em

 lcl

 (vt) -_(vtv) (tvt)



 (cvt) -_0(vh) (ch) (ht)



 (cv1v1t) - (cv1v1cv2v2)



 (cv2v2t) (v1t)



 (ft) - (fv2) (v2t)



 (vt) - (fv) (ft)



 (vt)



 2l(c1v1c2v2)

 -55ex

 lcl

 - (c2ic1m) (mzv1)



 (izv2)









 C_V T is the context-sensitive

 variable type relation

 means that variable in context can refer to an object

 of type This is the analogue of in the points-to_analysis





 F_T is the field type relation

 means that field can point to an object of type





 V T is the type_filter relation

 means that it is type-safe to assign an object of type to variable



 Rule_() initializes the relation based_on the

 initial local points-to_information contained in combining it

 with to get the type and to get the context_numbers

 Rule_() does transitive_closure on the relation

 filtering with to enforce type safety

 Rules () and () handle stores and loads

 respectively They differ from their counterparts in the pointer

 analysis in that they do_not use the base object only the field

 Rule_() models the effects of parameter_passing

 in a context-sensitive manner



 Thread Escape Analysis



 Our last example is a thread_escape analysis which determines if

 objects_created by one thread may be used by another The results of

 the analysis can be used for optimizations such_as synchronization

 elimination and allocating objects in thread-local heaps as_well as

 for understanding programs and checking for possible race conditions

 due to missing synchronizations

 This example_illustrates how we can vary context_sensitivity to

 fit the needs of the analysis



 We_say that an object allocated by a thread has escaped if it

 may be accessed by another thread This notion is stronger than

 most other formulations where an object is said to escape if it can be

 reached by another thread











 Java threads being subclasses of javalangThread are

 identified by their creation_sites In the special_case where a

 thread_creation can execute only once a thread can simply be named by

 the creation_site The thread that exists at virtual machine startup

 is an example of a thread that can only be created once A creation

 site reached via different call_paths or embedded in loops or

 recursive_cycles may generate multiple threads To distinguish

 between thread instances created at the same site we create two

 thread contexts to represent two separate thread instances If an

 object_created by one instance is not accessed by its clone then it

 is not accessed by any other instances created by the same call_site

 This scheme creates at most twice as many_contexts as there are thread

 creation_sites



 We clone the thread run()_method one for each thread_context

 and place these clones on the list of entry methods to be analyzed

 Methods (transitively) invoked by a context's run()_method all

 inherit the same context A clone of a method not only has its_own

 cloned variables but also its_own cloned object_creation sites

 In this way objects_created by separate threads are distinct from each

 other We run a points-to_analysis over this slightly expanded_call

 graph an object_created in a thread_context escapes if it is accessed

 by variables in another thread_context





 Thread-sensitive pointer_analysis

 Domains



 Domains_from Algorithm



 Relations



 Relations from Algorithm_plus













 Rules



 0em

 lcl

 (vh)_- (vtv)_(hth) (tvth)



 (c1vc2h) - (c1vc2h)



 (cvch) -_0(vh) (ch)



 (c2v1chh) -_(v1v2) (c2v2chh)



 (v1h)



 (c1h1fc2h2) -_(v1fv2) (cv1c1h1)



 (cv2c2h2)



 (cv2c2h2) -_(v1fv2) (cv1c1h1)



 (c1h1fc2h2)



 (v2h2)



 C_H encodes the non-thread objects_created by a

 thread means that a thread with context may execute

 non-thread allocation_site in other_words there is a call path

 from the run()_method in context to allocation_site





 C_V C_H is the set of

 initial inter-thread points-to_relations This includes the

 points-to_relations for thread_creation sites and for the global

 object means that thread has an thread

 allocation_site and points to the newly_created thread

 context (There are usually two contexts assigned to each allocation

 site) All global objects across all contexts are given the same

 context



 C_V C_H is the

 thread-sensitive version of the variable points-to_relation

 means variable in context can point

 to heap_object created under_context



 C_H F C_H is the

 thread-sensitive version of the heap points-to_relation

 means that field of heap_object

 created under_context can point to heap_object created

 under_context



 REDUNDANT

 These creation_sites are special in that they

 have different context_numbers for the variable and heap_object It

 also_incorporates the constraint that the global object is aliased

 across all contexts





 Rule_() incorporates the initial points-to_relations for

 thread_creation sites Rule_() incorporates the points-to

 information for non-thread creation_sites which have

 the context_numbers of threads that can reach the method The other

 rules are analogous to those of the context-sensitive_pointer analysis

 with an additional context attribute for the heap_objects



 From the analysis results we can easily determine which objects

 have escaped An object_created by thread_context has

 escaped written if it is accessed by a

 different context Complications involving unknown code such

 as native_methods could also be handled using this technique



 lcl

 escaped(ch) - (cvch) cv c



 Conversely an object_created by context is captured written

 if it has not escaped Any captured object

 can be allocated on a thread-local heap



 lcl

 captured(ch) - (cvch) escaped(ch)



 We can also use escape_analysis to eliminate unnecessary

 synchronizations We define a relation indicating

 if the program contains a synchronization operation performed on

 variable A synchronization for variable under_context is

 necessary written if

 and can point to an escaped_object



 lcl

 neededSyncs(cv) - syncs(v) (cvchh)



 escaped(chh)



 Notice_that is context-sensitive Thus we can

 distinguish when a synchronization is necessary only for certain

 threads and generate specialized versions of methods for those threads





 Finally we can compute all the objects reachable by a thread with the

 query







 Experimental Results



 htb





 Information about the benchmarks we used to test our analysesThe sizes

 are based_on reachable code as determined by Algorithm



 In this_section we present some experimental_results of using

 on the Datalog algorithms presented in this_paper We

 describe our testing methodology and benchmarks present the analysis times

 evaluate the results of the analyses

 and

 provide some insight on our experience

 of developing these analyses and the tool



 Methodology







 The input to is more or less the Datalog_programs exactly as

 they are presented in this_paper (We added a few rules to handle

 return values and threads and added annotations for the physical

 domain assignments of input relations) The input relations were

 generated with the Joeq compiler infrastructure The

 entire implementation is only 2500 lines of code

 uses the JavaBDD library an open-source library based

 on the BuDDy library The entire system is available as

 open-source and we hope that others will find it

 useful





 Due to a limitation in the

 implementation we used a maximum of 63 bits for the context domain

 any contexts numbered beyond were_merged into a single

 context



















 All experiments were performed on a 22GHz Pentium 4 with Sun JDK

 14204 running on Fedora Linux Core 1 For

 the context-insensitive and context-sensitive experiments

 respectively we used initial BDD table sizes of 4M and 12M the

 tables could grow by 1M and 3M after each garbage_collection the BDD

 operation cache sizes were 1M and 3M









 To test the scalability and applicability of the algorithm we applied

 our technique to 21 of the most_popular Java projects on Sourceforge

 as of November 2003 We simply walked down the list of 100 Java

 projects sorted by activity selecting the ones that would compile

 directly as standalone applications They are all real applications

 with tens of thousands of users each As far as we know these are

 the largest benchmarks ever reported for any context-sensitive Java

 pointer_analysis As a point of comparison the largest benchmark in

 the specjvm suite javac would rank only 13th in our list



 For each application we chose an applicable main() method as

 the entry point to the application We included all class

 initializers thread run methods and finalizers We ignored null

 constants in the analysis-every points-to set is automatically

 assumed to include null Exception objects of the same type were

 merged We treated reflection and native_methods as returning unknown

 objects Some native_methods and special fields were modeled

 explicitly



 A short description of each of the benchmarks is included in

 Figure along with their vital statistics The

 number of classes methods and bytecodes were those discovered by the

 context-insensitive on-the-fly call_graph construction algorithm so

 they include only the reachable parts of the program and the class

 library



 The number of context-sensitive (CS) paths is for the most part

 correlated to the number of methods in the program with the exception

 of pmd pmd has an astounding paths in the call

 graph which requires 79 bits to represent pmd has different

 characteristics because it contains code generated_by the parser

 generator JavaCC Many machine-generated methods call the same class

 library routines leading to a particularly egregious exponential

 blowup The JavaBDD library only supports physical_domains up to 63

 bits contexts numbered beyond were_merged into a single

 context The large_number of paths also caused the algorithm to

 require many more rule applications to reach a fixpoint solution



 Analysis Times



 htb





 Analysis times and peak memory_usages for each of the

 benchmarks and analyses Time is in seconds and memory is in

 megabytes



 We measured the analysis times and memory_usage for each of the

 algorithms presented in this_paper (Figure ) The

 algorithm with call_graph discovery in each iteration computes a

 call_graph based_on the points-to_relations from the previous

 iteration The number of iterations taken for that algorithm is also

 included here



 All timings reported are wall-clock times from a cold start and

 include the various overheads for Java garbage_collection BDD garbage

 collection growing the node table etc The memory numbers reported

 are the sizes of the peak number of live BDD nodes during the course

 of the algorithm We measured peak BDD memory_usage by setting the

 initial table size and maximum table size increase to 1MB and only

 allowed the table to grow if the node table was more_than 99 full

 after a garbage collection(To avoid garbage collections it

 is recommended to use more memory Our timing runs use the default

 setting of 80)



 The context-insensitive analyses

 (Algorithms and )

 are remarkably fast

 the type-filtering version was able to complete in under 45 seconds on

 all benchmarks

 It is interesting to notice that introducing type_filtering actually

 improved the analysis time and memory_usage Along with being more

 accurate the points-to sets are much_smaller in the type-filtered

 version leading to faster analysis times













 For Algorithm the call_graph discovery sometimes

 took over 40 iterations to complete but it was very effective in

 reducing the size of the call_graph as compared to CHA

 The complexity of the call_graph discovery algorithm seems to vary with the

 number of virtual call_sites that need resolving-jedit and megamek

 have many methods declared as final but jxplorer has none leading to

 more call_targets to resolve and longer analysis times



 The analysis times and memory_usages of our context-sensitive

 points-to_analysis (Algorithm ) were on the whole very

 reasonable It can analyze most of the small and medium size

 benchmarks in a few minutes and it successfully finishes analyzing

 even the largest benchmarks in under 19 minutes This is rather

 remarkable considering that the context-sensitive formulation is

 solving up to times as many relations as the

 context-insensitive version Our scheme of numbering the contexts

 consecutively allows the BDD to efficiently represent the similarities

 between calling_contexts The analysis times are most directly

 correlated to the number of paths in the call_graph From the

 experimental data presented here it appears that the analysis time of

 the context-sensitive algorithm scales approximately with

 where is the number of paths in the call_graph more

 experiments are necessary to determine if this trend persists across

 more programs



 -

 The context-sensitive type analysis_(Algorithm ) is as

 expected quite a bit faster and less memory-intensive than the

 context-sensitive_points-to analysis Even_though it uses the same

 number of contexts it is an order of magnitude faster_than

 the context-sensitive_points-to analysis This is because in the type

 analysis the number of objects that can be pointed to is much_smaller

 which greatly increases sharing in the BDD The thread-sensitive

 pointer_analysis (Algorithm ) has analysis times and

 memory_usages that are roughly comparable to those of the

 context-insensitive_pointer analysis even_though it includes thread

 context information This is because the number of thread_creation

 sites is relatively_small and we use at most two contexts per thread



 Evaluation of Results





 It is difficult to come_up with meaningful metrics with which to

 evaluate points-to_analysis in general and even more so with

 context-sensitive_analysis Taking full advantage of context

 sensitivity requires a context-sensitive query Turning a

 context-sensitive_analysis result back into a context-insensitive

 version by projecting away the context causes us to lose much of the

 benefit of context_sensitivity Most straightforward metrics such_as

 points-to set size are meaningless because they depend_on the

 precision used one could obtain the best result by simply

 representing everything by a single object

 Our formulation of context_sensitivity is also unique in that it can

 distinguish_between huge numbers of different_contexts - far far

 too_many to iterate over This makes it difficult to collect some

 kinds of statistics



 Instead we evaluate the analyses in terms of some of the actual uses

 of the analysis information described in Section

 First we evaluated the escape_analysis of

 Section with the thread-sensitive analysis

 (Algorithm )





 An in-depth analysis of the accuracy of the analyses with_respect to

 each of the queries in Section is beyond the scope

 of this_paper Instead we show the results of two specific queries thread

 escape_analysis (Section ) and type_refinement

 (Section )



 The results of the escape_analysis are shown in

 Figure The first two columns give the number of

 captured and escaped_object creation_sites respectively

 The next two columns give the number of unneeded and needed

 synchronization operations





 The single-threaded benchmarks have only one escaped_object the

 global object from which static variables are accessed In the

 multi-threaded benchmarks the analysis is effective in finding 30-50

 of the allocation_sites to be captured and 15-30 of the

 synchronization operations to be unnecessary These are static

 numbers to fully evaluate the results would_require dynamic execution

 counts which is outside of the scope of this_paper



 The results of the type_refinement query are shown in

 Figure We tested the query across six different

 analysis variations From left to right they are context-insensitive

 pointer_analysis without and with type_filtering context-sensitive

 pointer_analysis and context-sensitive type analysis with the context

 projected away and context-sensitive_pointer and type analysis on the

 fully cloned graph Projecting away the context in a

 context-sensitive_analysis makes the result context-insensitive

 however it can still be more_precise than context-insensitive

 analysis because of the extra precision at the intermediate steps of

 the analysis We measured the percentages of variables that can point

 to multiple types and variables whose types can be refined



 -

 Including the type_filtering makes the algorithm strictly more

 precise Likewise the context-sensitive_pointer analysis is strictly

 more_precise than both the context-insensitive_pointer analysis and

 the context-sensitive type analysis We can see this trend in the

 results As the precision increases the percentage of multi-typed

 variables drops and the percentage of refinable

 variables increases The context-insensitive_pointer analysis and the

 context-sensitive type analysis are not directly comparable in some

 cases the pointers are more_precise in other cases the context-sensitive

 types are more_precise



 When we do_not project_away the context the context-sensitive_results

 are remarkably precise-the percentage of multi-typed variables is

 never_greater than 1 for the pointer_analysis and 2 for the type

 analysis Projecting away the context loses much of the benefit of

 context_sensitivity but is still noticeably more_precise than using

 a context-insensitive_analysis









 Results of escape_analysis



 htb





 Results of the type_refinement query Numbers are percentages

 Columns labeled multi and refine refer to multi-type variables and refinable-type variables respectively



 Experience



 All the experimental_results reported here are generated using

 At the early stages of our research we hand-coded every

 points-to_analysis using BDD_operations directly and spent a

 considerable amount of time tuning their performance Our

 context-numbering scheme is the reason why the analysis would finish

 at all on even small programs Every one of the optimizations

 described in Section was first carried_out

 manually After considerable effort megamek still took over three

 hours to analyze and jxplorer did_not complete at all

 The incrementalization was very

 difficult to get correct and we found a subtle bug months after the

 implementation was completed We did_not incrementalize the outermost

 loops as it would have_been too tedious and error-prone It was also

 difficult to experiment with different rule application orders



 To get even better performance and more_importantly to make it

 easier to develop new queries and analyses we created We

 automated and extended the optimizations we have used in our manual

 implementation and implemented a few new ones to empirically choose

 the best BDD_library parameters The end result is that code

 generated_by outperforms our manually tuned context-sensitive

 pointer_analysis by as much as an order of magnitude Even better we

 could use to quickly and painlessly develop new analyses that

 are highly_efficient such_as the type analysis in

 Section and the thread_escape analysis in

 Section





 Through the development of

 we were_able to speed_up the algorithm by an order of magnitude and

 also quickly and easily develop new analyses and queries



 When developing instead of trying to solve the specific

 problems for each analysis we took it upon ourselves to solve

 problems in a general way so that other analyses could potentially

 benefit For_example rather_than requiring the user to

 incrementalize by hand will automatically incrementalize the

 input program transforming the rules into a form that is more

 suitable to incrementalization identifying loop-invariant relations

 and minimizing rename_operations Rather_than requiring the user

 to specify an iteration order analyzes the rules to come_up

 with a fast iteration order Rather_than relying on the user to

 guide the assignment of attributes to physical_domains tries

 to find the best assignment itself by_taking into_account the loop

 structure and variable_ordering Rather_than requiring the user to

 specify the variable_ordering will experimentally try all

 partial domain orders for each operation keeping_track of the time

 spent on each and coming up with the best overall ordering for the

 algorithm can also automatically sweep the BDD_library

 parameters to find the fastest node table size and cache size



 By confronting problems in this manner we were_able to develop a

 robust and efficient tool for specifying and solving analyses

 Because automatically incrementalized parts of the algorithm

 that we did_not automatically found a better variable_ordering than

 we did manually optimized the rule ordering and domain assignment

 and automatically found better BDD node table and cache sizes it was

 able to improve the execution time of the context-sensitive_pointer

 analysis by an order of magnitude Even better because we solved the

 problems in a general way we could use to quickly and

 painlessly develop highly_efficient versions of other analyses such

 as the type analysis in Section or the thread_escape

 analysis in Section































 Related Work















 This_paper describes a scalable cloning-based points-to_analysis that

 is context-sensitive field-sensitive inclusion-based and implemented

 using_BDDs Our program analyses expressed in Datalog are

 translated by into a BDD implementation automatically We

 also presented example queries using our system to check for

 vulnerabilities infer types and find objects that escape a thread

 Due to space constraints we can only describe work that is very closely

 related to ours



 There is a good deal of related work associated_with each of the above

 topics we_shall attempt to only describe the most relevant here

















 Cloning-based context-sensitive_analysis

 Cooper proposed a summary-based interprocedural data-flow

 analysis where a bottom-up analysis is used to create summaries for

 each method and actual data-flow_values are propagated in a top-down

 manner to all the different procedures Where

 data-flow_values differ for different call_paths they create separate

 clones Our_approach simply creates a clone for every context we_wish

 to evaluate and directly compute their results by_applying a

 context-insensitive algorithm on the cloned_call graph The approach

 is like inlining except that the actual code is not duplicated

 function boundaries are retained some of the calling_contexts can

 invoke the same clone and consequently we can handle recursion This

 approach is the closest to Emami's invocation-graph

 approach as discussed below





 Scalable pointer analyses Most of the scalable algorithms

 proposed are context-insensitive and flow-insensitive The first

 scalable pointer_analysis proposed was a unification-based algorithm

 due to Steensgaard Das extended

 the unification-based approach to include

 one-level-flow and one level of

 context_sensitivity Subsequently a number of

 inclusion-based algorithms have_been shown to scale to large

 programs



 A number of context-sensitive but flow-insensitive analyses have_been

 developed recently The

 C pointer_analysis due to Fahndrich et

 al has_been demonstrated to work on a

 200K-line gcc program Unlike ours their algorithm is

 unification-based and field-independent meaning that fields in a

 structure are modeled as having the same_location Their

 context-sensitive_analysis discovers targets of function pointers

 on-the-fly Our algorithm first computes the call_graph using a

 context-insensitive_pointer alias_analysis there are significantly

 more indirect calls in Java programs the target of our

 technique due to virtual_method invocations Their algorithm uses

 CFL-reachability queries to implement context_sensitivity

 Instead of computing context-sensitive solutions on demand we compute

 all the context-sensitive_results and represent them in a form

 convenient for further analysis



 Other context-sensitive_pointer analysis Some of the earlier

 attempts of context-sensitive_analysis are

 flow-sensitive

 Our analysis is similar to the work by Emami in that they also

 compute context-sensitive_points-to results directly for all the

 different_contexts Their analysis is flow-sensitive ours uses flow

 sensitivity only in summarizing each method intraprocedurally While

 our technique treats all members of a strongly_connected component in

 a call_graph as one_unit their technique only ignores subsequent

 invocations in recursive_cycles On the other_hand their technique

 has only been_demonstrated to work for programs under 3000 lines

 To be fair their work was done in 1994 when machines were

 much slower and had much less memory



 As_discussed in Section using summaries

 is another common approach to context_sensitivity It is difficult to

 compute a compact summary if a fully flow-sensitive result is desired

 One_solution is to use the concept of partial transfer_functions

 which create summaries for observed calling

 contexts The same summary can be

 reused by multiple contexts that share the same relevant alias

 patterns This technique has_been shown to handle C programs up to

 20000 lines



 One_solution is to allow only weak updates

 that is a write to a variable only adds a value to the contents of

 the variable without removing the previously held value This greatly

 reduces the power of a flow-sensitive analysis This_approach has

 been used to handle programs up to 70000 lines of code However on

 larger programs the representation still becomes too large to deal

 with Because the goal of the prior work was escape_analysis it was

 not necessary to maintain precise points-to_relations for locations

 that escape so the algorithm achieved scalability by collapsing

 escaped nodes



 BDD-based pointer_analysis BDDs have recently been used in a

 number of program analyses such_as predicate

 abstraction shape

 analysis and in particular

 points-to_analysis Zhu proposed a summary-based

 context-sensitive_points-to analysis for C programs and reported

 preliminary experimental_results on C programs with less_than 5000

 lines Berndl showed that BDDs

 can be used to compute context-insensitive inclusion-based_points-to

 results for large Java programs efficiently In the same conference

 this_paper is presented Zhu and Calman describe a cloning-based

 context-sensitive_analysis for C pointers assuming that only the safe

 C subset is used The largest program reported in their experiment

 has about 25000 lines and contexts



 High-level languages and tools for program analysis The

 use of Datalog and other logic_programming languages has previously

 been_proposed for describing program

 analyses Our

 system implements Datalog using_BDDs and has

 been used to compute context-sensitive_points-to results and other

 advanced analyses Other examples of systems that translate program

 analyses and queries written in logic_programming languages into

 implementations using_BDDs include Toupie and

 CrocoPat Jedd is a Java language extension that

 provides a relational algebra abstraction over

 BDDs



















 Many

 other systems have attempted to provide a uniform framework in which

 to implement program analysis including set- and

 type-constraints graph rewrite

 systems and



 -calculus

 which uses non-binary decision_diagrams



 Filtering with vPfilter is like type-based alias

 analysis

 Aggregate analysis to detect when a polymorphic type is used in a monomorphic way







 could be_solved efficiently by

 representing the constraints in BDD form and expressing the inference

 rules as BDD_operations We extend this work by_adding the ability to

 compute the call_graph on-the-fly and context-sensitivity





 Space limitations preclude us from reviewing the large volume of

 research on pointer_alias analysis in detail Since this_paper

 presents a context-sensitive_pointer alias_analysis that can handle

 large_programs we_shall first overview scalable algorithms proposed

 to date and then discuss the more_recent research in

 context-sensitive_pointer alias_analysis







 Conclusion























 This_paper shows that by using_BDDs it is possible to obtain

 efficient implementations of context-sensitive analyses using an

 extremely simple technique We clone all the methods in a call_graph

 one per context of interest and simply_apply a context-insensitive

 analysis over the cloned graph to get context-sensitive_results By

 numbering similar contexts contiguously the BDD is able to handle the

 exponential_blowup of contexts by exploiting their commonalities We

 showed that this approach can be applied to type_inference thread

 escape_analysis and even fully context-sensitive_points-to analysis on

 large_programs



 This_paper shows that we can create efficient BDD-based analyses

 easily By keeping data and analysis results as relations we can

 express queries and analyses in terms of Datalog The system we

 have developed automatically converts Datalog_programs into

 BDD implementations that are even more_efficient than those we have

 painstakingly hand-tuned



 Context-sensitive pointer_analysis is the cornerstone of deep program

 analysis for modern programming_languages By combining (1)

 context-sensitive_points-to results (2) a simple approach to context

 sensitivity and (3) a simple logic-programming based query framework

 we believe we have made it much_easier to create advanced program

 analyses





 In this_paper we presented a context-sensitive_inclusion-based

 pointer_alias analysis that scales to hundreds of thousands of Java

 bytecodes Our technique is based_on cloning the call_paths in the

 original_program and applying a simple context-insensitive algorithm

 to the expanded_call graph As a program gets large there is an

 exponential_blowup in the number of paths however we can represent

 the large_number of contexts efficiently using_BDDs and the set-based

 semantics of BDD_operations allow_us to compute the results for all

 contexts in parallel In essence the BDD data_structure automatically

 exploits the commonalities between different_contexts leading to

 an efficient_representation and an efficient algorithm



 We have implemented this technique on top of a BDD-based

 deductive database system that automatically_translates our

 declarative specifications into highly_efficient BDD code

 allows_us to abstract away the implementation details and reduce the

 analysis to just a few lines automatically handles the

 optimizations and transformations With we were_able to

 quickly and easily develop efficient implementations of other

 context-sensitive analyses and queries Our context-sensitive

 points-to_analysis can analyze even the largest of our benchmarks

 in under 19 minutes





 Our_approach to represent all analysis constraints and computed

 relations as BDDs and handle context_sensitivity by cloning has two

 important advantages (1) the results are readily accessible with BDD

 queries (2) it is easy to develop and implement new algorithms

 These advantages are significant in enabling researchers to create

 more and better techniques We hope to have demonstrated this point

 by developing two context-sensitive_pointer alias_analyses and four

 meaningful context-sensitive analyses that use the pointer_alias

 analysis results and applying them to twenty real large_programs





 Our implementation is available publicly at

 httpjoeqsourceforgenet We will post the information collected

 on the applications on their Sourceforce site when this_paper is

 published









 Acknowledgments





 This material is based upon work supported by the National Science

 Foundation under Grant No 0086160 and an NSF Graduate Student

 Fellowship We thank our anonymous referees for their helpful comments









 abbrv





 Parser

 parser-java-sect



 The parser reads a stream of tokens and builds a syntax_tree by

 calling the appropriate constructor functions from

 Sections expr-java-sect-stmt-java-sect The current

 symbol_table is maintained as in the translation_scheme in

 Fig symtab-scheme-fig in Section symtab-sect



 Package parser contains one class_Parser



 footnotesize

 flushleft

 1)_package parser File Parserjava



 2)_import javaio_import lexer_import symbols import inter



 3)_public class_Parser



 4) private Lexer lex lexical_analyzer for this parser



 5) private Token look lookahead token



 6) Env top null current or top symbol_table



 7) int used 0 storage used for declarations

 8)_public Parser(Lexer l) throws_IOException lex l move()



 9) void move() throws_IOException look lexscan()



 10) void error(String s) throw_new Error(near line lexline s)



 11) void match(int t) throws_IOException



 12) if(_looktag t ) move()



 13) else error(syntax error)



 14)



 flushleft

 footnotesize



 Like the simple expression translator in

 Section_postfix-sect class_Parser has a procedure for

 each nonterminal The procedures are based_on a grammar formed_by

 removing left_recursion from the source-language grammar in

 Section source-lang-sect



 Parsing begins_with a call to procedure program which calls

 block() (line 16) to parse the input stream and build the

 syntax_tree Lines 17-18 generate intermediate_code



 footnotesize

 flushleft

 15)_public void program() throws_IOException program - block



 16) Stmt s block()



 17) int begin snewlabel() int after snewlabel()



 18) semitlabel(begin) sgen(begin after) semitlabel(after)



 19)



 flushleft

 footnotesize



 Symbol-table handling is shown explicitly in procedure block(An attractive alternative is to add methods push and pop to class_Env with the current table

 accessible through a static variable Envtop) Variable top (declared on line 5) holds the top symbol_table variable

 savedEnv (line 21) is a link to the previous symbol_table



 footnotesize

 flushleft

 20) Stmt block() throws_IOException block - decls_stmts



 21) match('') Env savedEnv top top new Env(top)



 22) decls() Stmt s stmts()



 23) match('') top savedEnv



 24) return s



 25)



 flushleft

 footnotesize



 Declarations result in symbol-table entries for identifiers (see

 line 30) Although not shown here declarations can also result in

 instructions to reserve storage for the identifiers at_run time



 footnotesize

 flushleft

 26) void decls() throws_IOException



 27) while(_looktag TagBASIC ) D - type ID



 28) Type p type() Token_tok look match(TagID) match('')



 29) Id id new Id((Word)tok p used)



 30) topput( tok id )



 31) used used pwidth



 32)



 33)



 34) Type type() throws_IOException



 35) Type p (Type)look expect looktag TagBASIC



 36) match(TagBASIC)



 37) if(_looktag ''_) return p T - basic



 38) else_return dims(p) return array type



 39)



 40) Type dims(Type p) throws_IOException



 41) match('') Token_tok look match(TagNUM) match('')



 42) if(_looktag ''_)



 43) p dims(p)



 44) return_new Array(((Num)tok)value p)



 45)



 flushleft

 footnotesize



 Procedure stmt has a switch statement with cases

 corresponding to the productions for nonterminal Stmt Each

 case builds a node for a construct using the constructor

 functions discussed in Section stmt-java-sect The nodes for

 while and do statements are constructed when the parser sees the

 opening keyword The nodes are constructed before the statement

 is parsed to allow any enclosed break statement to point back to

 its enclosing_loop Nested loops are handled by using variable

 StmtEnclosing in class_Stmt and savedStmt

 (declared on line 52) to maintain the current enclosing_loop



 footnotesize

 flushleft

 46) Stmt stmts() throws_IOException



 47) if_( looktag_'' )_return StmtNull



 48) else_return new Seq(stmt() stmts())



 49)



 50) Stmt stmt() throws_IOException



 51) Expr_x Stmt s s1 s2



 52) Stmt savedStmt save enclosing_loop for breaks



 53) switch( looktag )



 54) case_''



 55) move()



 56) return StmtNull



 57) case TagIF



 58) match(TagIF) match('(')_x bool() match(')')



 59) s1_stmt()



 60) if(_looktag TagELSE )_return new If(x s1)



 61) match(TagELSE)



 62) s2 stmt()



 63) return_new Else(x s1 s2)



 64) case TagWHILE



 65) While whilenode new While()



 66) savedStmt StmtEnclosing StmtEnclosing whilenode



 67) match(TagWHILE) match('(')_x bool() match(')')



 68) s1_stmt()



 69) whilenodeinit(x s1)



 70) StmtEnclosing savedStmt reset StmtEnclosing



 71) return whilenode



 72) case TagDO



 73) Do donode new Do()



 74) savedStmt StmtEnclosing StmtEnclosing donode



 75) match(TagDO)



 76) s1_stmt()



 77) match(TagWHILE) match('(')_x bool() match(')') match('')



 78) donodeinit(s1 x)



 79) StmtEnclosing savedStmt reset StmtEnclosing



 80) return donode



 81) case TagBREAK



 82) match(TagBREAK) match('')



 83) return_new Break()



 84) case_''



 85) return block()



 86) default



 87) return assign()



 88)



 89)



 flushleft

 footnotesize



 For_convenience the code for assignments appears in an auxiliary

 procedure assign



 footnotesize

 flushleft

 90) Stmt assign() throws_IOException



 91) Stmt_stmt Token_t look



 92) match(TagID)



 93) Id id topget(t)



 94) if( id null_) error(ttoString() undeclared)



 95) if(_looktag ''_) S - id E



 96) move() stmt new Set(id bool())



 97)



 98) else S - L E



 99) Access x offset(id)



 100) match('') stmt new SetElem(x bool())



 101)



 102) match('')



 103) return stmt



 104)



 flushleft

 footnotesize



 The parsing of arithmetic and boolean_expressions is similar In

 each case an appropriate syntax-tree_node is created Code

 generation for the two is different as discussed in

 Sections expr-java-sect-jumping-java-sect



 footnotesize

 flushleft

 105) Expr bool() throws_IOException



 106) Expr_x join()



 107) while(_looktag TagOR )



 108) Token_tok look_move() x new Or(tok x join())



 109)



 110) return x



 111)



 112) Expr join() throws_IOException



 113) Expr_x equality()



 114) while(_looktag TagAND )



 115) Token_tok look_move() x new And(tok x equality())



 116)



 117) return x



 118)



 119) Expr equality() throws_IOException



 120) Expr_x rel()



 121) while(_looktag TagEQ looktag TagNE )



 122) Token_tok look_move() x new Rel(tok x rel())



 123)



 124) return x



 125)



 126) Expr rel() throws_IOException



 127) Expr_x expr()



 128) switch( looktag )



 129) case_'' case TagLE case TagGE case_''



 130) Token_tok look_move() return_new Rel(tok x expr())



 131) default



 132) return x



 133)



 134)



 135) Expr expr() throws_IOException



 136) Expr_x term()



 137) while(_looktag '' looktag '-' )



 138) Token_tok look_move() x new Arith(tok x term())



 139)



 140) return x



 141)



 142) Expr term() throws_IOException



 143) Expr_x unary()



 144) while(looktag '' looktag_'' )



 145) Token_tok look_move() x new Arith(tok x unary())



 146)



 147) return x



 148)



 149) Expr unary() throws_IOException



 150) if(_looktag '-' )



 151) move()_return new Unary(Wordminus unary())



 152)



 153) else_if( looktag_'' )



 154) Token_tok look_move() return_new Not(tok unary())



 155)



 156) else_return factor()



 157)



 flushleft

 footnotesize



 The rest of the code in the parser deals_with factors in

 expressions The auxiliary procedure offset generates code

 for array address_calculations as discussed in

 Section array-calc-subsect



 footnotesize

 flushleft

 158) Expr factor() throws_IOException



 159) Expr_x null



 160) switch( looktag )



 161) case '('



 162) move()_x bool() match(')')



 163) return x



 164) case TagNUM



 165) x new Constant(look TypeInt) move()_return x



 166) case TagREAL



 167) x new Constant(look TypeFloat) move()_return x



 168) case TagTRUE



 169) x ConstantTrue move()_return x



 170) case TagFALSE



 171) x ConstantFalse move()_return x



 172) default



 173) error(syntax error)



 174) return x



 175) case TagID



 176) String_s looktoString()



 177) Id id topget(look)



 178) if( id null_) error(looktoString() undeclared)



 179) move()



 180) if(_looktag ''_) return id



 181) else_return offset(id)



 182)



 183)



 184) Access offset(Id a) throws_IOException I - E_E I



 185) Expr i Expr w Expr t1_t2 Expr loc inherit id



 186) Type type atype



 187) match('') i bool() match('') first index I - E



 188) type ((Array)type)of



 189) w new Constant(typewidth)



 190) t1 new Arith(new Token('') i w)



 191) loc t1



 192) while(_looktag ''_) multi-dimensional I - E I



 193) match('') i bool() match('')



 194) type ((Array)type)of



 195) w new Constant(typewidth)



 196) t1 new Arith(new Token('') i w)



 197) t2 new Arith(new Token('') loc t1)



 198) loc t2



 199)



 200) return_new Access(a loc type)



 201)



 202)



 flushleft

 footnotesize

 Parsing

 parse-sect



 Parsing is the process of determining if a string of tokens can be

 generated_by a grammar In discussing this problem it is helpful

 to think of a parse_tree being constructed even_though a compiler

 may not construct one in practice However a parser must_be capable

 of constructing the tree in principle or else the translation cannot be

 guaranteed correct



 This_section introduces a parsing method recursive descent

 that can be used both to parse and to

 implement syntax-directed translators A complete Java program

 implementing the translation_scheme of Fig post-scheme-fig

 appears in the next section A viable alternative is to use a

 software tool to generate a translator directly from a translation

 scheme See Section yacc-sect for the description of such a

 tool - Yacc it can implement the translation_scheme of

 Fig post-scheme-fig without modification



 For any context-free

 grammar there is a parser that takes at most time to

 parse a string of tokens But cubic time is generally too_expensive

 Fortunately for real programming_languages we can generally design

 a grammar

 that can be_parsed quickly Linear algorithms suffice to parse

 essentially all languages that arise in practice Programming-language

 parsers almost_always make a single left-to-right_scan

 over the input looking ahead one token at a time and constructing

 pieces of the parse_tree as they go



 Most parsing methods fall into one of two classes called the top-down and bottom-up methods These terms refer to the

 order in which nodes in the parse_tree are constructed In top-down

 parsers

 construction starts at the root and proceeds towards the

 leaves while in bottom-up parsers construction starts at the leaves

 and proceeds towards the root The popularity of top-down parsers

 is due to the fact that efficient parsers can be constructed more

 easily by hand using top-down methods Bottom-up parsing however

 can handle a larger class of grammars and translation_schemes so

 software tools for generating parsers directly from grammars often

 use bottom-up methods



 Top-Down Parsing

 basic-topdown-subsect



 We introduce top-down_parsing by considering a grammar that is

 well-suited for this class of methods Later in this_section we

 consider the construction of top-down parsers in general The

 grammar in Fig_stmt-gram-fig generates a subset of the

 statements of C or Java We use the boldface tokens if and for

 for the keywords

 if and for respectively to emphasize

 that these character sequences are treated_as units ie as single

 terminal_symbols Further the

 token expr represents expressions a more complete grammar

 would use a nonterminal expr and have productions for

 nonterminal expr Similarly other is a token

 representing other statement constructs



 figurehtfb



 center

 tabularr_c l

 stmt expr



 if_( expr ) stmt



 for ( optexpr_optexpr optexpr ) stmt



 other







 optexpr



 expr

 tabular

 center



 A grammar for some statements in C and Java

 stmt-gram-fig

 figure



 The top-down construction of a parse_tree like the one in

 Fig stmt-tree-fig is done by starting_with the root

 labeled with the starting nonterminal stmt and repeatedly performing

 the following two steps

 enumerate



 At node_labeled with nonterminal select one of the

 productions for and construct children at for the symbols

 in the production_body



 Find the next node at which a subtree is to be constructed ie the

 leftmost unexpanded nonterminal of the tree

 enumerate



 figurehtfb



 A

 parse_tree according to the grammar in Fig_stmt-gram-fig

 stmt-tree-fig

 figure



 For some grammars the above steps can be_implemented during a

 single left-to-right_scan of the input_string The current token

 being scanned in the input is frequently referred to as the lookahead_symbol Initially the lookahead_symbol is the first

 ie leftmost token of the input_string

 Figure stmt-parse-fig illustrates the parsing of the string



 figurehtfb





 Top-down parsing while scanning the input from left to

 right stmt-parse-fig

 figure



 center

 for (_expr expr )

 other

 center

 Initially the token for is the lookahead_symbol and the

 known part of the parse_tree consists of the root labeled with

 the starting nonterminal stmt in

 Fig stmt-parse-fig(a) The objective is to construct the

 remainder of the parse_tree in such a way that the string

 generated_by the parse_tree matches the input_string



 For a match to occur nonterminal stmt in

 Fig stmt-parse-fig(a) must derive a string that starts with

 the lookahead_symbol for In the grammar of

 Fig_stmt-gram-fig there is just one production for stmt that can derive such a string so we select it and

 construct the children of the root labeled with the symbols in the

 production_body

 This expansion of the parse_tree is shown in

 Fig stmt-parse-fig(b)



 Each of the three snapshots in Fig stmt-parse-fig has

 arrows marking the lookahead_symbol in the input and the node in

 the parse_tree that is being considered Once children are

 constructed at a node we next consider the leftmost child In

 Fig stmt-parse-fig(b) children have just been constructed

 at the root and the leftmost child_labeled with for is

 being considered



 When the node being considered in the parse_tree is for a terminal

 and the terminal matches the lookahead_symbol then we advance in

 both the parse_tree and the input The next token in the input

 becomes the new lookahead_symbol and the next child in the parse

 tree is considered In Fig stmt-parse-fig(c) the arrow in

 the parse_tree has advanced to the next child of the root and the

 arrow in the input has advanced to the next token ( A

 further advance will take the arrow in the parse_tree to the child

 labeled with nonterminal optexpr and take the arrow in the

 input to the token



 At the nonterminal node_labeled optexpr we repeat the

 process of selecting a production for a nonterminal

 Productions with as the body (-productions)

 are used as a default when no

 other production can be used With nonterminal optexpr and

 lookahead the -production is used since does_not match the only other production for optexpr with

 token expr in the body



 In_general the selection of a production for a nonterminal may

 involve trial-and-error that is we may have to try a production

 and backtrack to try another production if the first is found to

 be unsuitable A production is unsuitable if after using the

 production we cannot complete the tree to match the input_string

 Backtracking is not needed however in an important special_case

 called predictive_parsing which we discuss next



 Predictive_Parsing

 predictive-sect



 Recursive-descent parsing is a top-down method of syntax

 analysis in which a set of recursive procedures is used to process

 the input

 One procedure is associated_with each nonterminal of a

 grammar Here we consider a simple form of recursive-descent

 parsing called predictive_parsing in which the lookahead

 symbol unambiguously determines the flow of control through the

 procedure body for each nonterminal The sequence of procedure

 calls during the analysis of an input_string implicitly defines a

 parse_tree for the input



 The predictive_parser in Fig pred-parser-fig consists of

 procedures for the nonterminals stmt and optexpr of

 the grammar in Fig_stmt-gram-fig and an additional

 procedure match used to simplify the code for stmt

 and optexpr Procedure match compares its argument

 with the lookahead_symbol and advances to the next_input

 token if they match Thus match changes the value of

 variable lookahead a global variable that

 holds the currently scanned input

 token



 figurehtfb

 center

 tabularl

 void stmt()



 switch ( lookahead )



 case expr



 match(expr) match('') break



 case if



 match(if) match('(') match(expr) match(')') stmt()



 break



 case for



 match(for) match('(')



 optexpr()_match('') optexpr()_match('') optexpr()



 match(')') stmt() break



 case other



 match(other) break



 default



 syntax error















 void optexpr()



 if_( lookahead expr ) match(expr)











 void match(token t)



 if_( lookahead t ) lookahead

 next token



 else syntax error





 tabular

 center

 Pseudocode for a predictive_parser

 pred-parser-fig

 figure



 Parsing begins_with a call of the procedure for the starting

 nonterminal stmt in our grammar With the same input as in

 Fig stmt-parse-fig lookahead is initially the first

 token for Procedure stmt executes code corresponding

 to the production



 center

 stmt for ( optexpr_optexpr optexpr ) stmt

 center

 In the code for the production_body - that is the for case of

 procedure stmt - each terminal is matched with

 the lookahead_symbol and each nonterminal leads to a call of its

 procedure in the following sequence of calls



 center

 tabularl

 match(for) match('(')



 optexpr()_match('') optexpr()_match('') optexpr()



 match(')') stmt()

 tabular

 center



 Predictive parsing relies on information_about what first symbols

 can be generated_by a production_body More_precisely let

 be a string of grammar_symbols (terminals andor

 nonterminals)

 We define

 to be the set of tokens that appear as the

 first symbols of one or_more strings of terminals

 generated from If

 is or can generate then

 is also in



 The details of how one computes are in

 Section first-follow-subsect Here we_shall just use ad-hoc

 reasoning to deduce the symbols in typically

 will either begin_with a terminal which is therefore the only symbol in

 or will begin_with a nonterminal whose

 production_bodies begin_with terminals in which case these terminals

 are the only members of



 For_example with_respect to the grammar of Fig_stmt-gram-fig

 the following are correct calculations of



 center

 tabularr_c l











 tabular

 center



 sets must_be considered if there are two productions

 and Ignoring

 -productions for the moment predictive_parsing requires

 and to be disjoint The

 lookahead_symbol can then be used to decide which production to

 use if the lookahead_symbol is in then

 is used Otherwise if the lookahead_symbol is in

 then is used



 When to Use -Productions



 Productions with as the body require special treatment

 The recursive-descent_parser uses an -production as a

 default when no other production can be used



 With the input of Fig stmt-parse-fig after the tokens for and ( are matched the lookahead_symbol is At

 this point procedure optexpr is called and the code



 center

 if_( lookahead expr ) match(expr)

 center

 in its body is executed Nonterminal optexpr has two

 productions with bodies expr and The lookahead

 symbol does_not match the token expr so the

 production with body expr cannot apply In_fact the

 procedure returns without_changing the

 lookahead_symbol or doing anything else

 Doing nothing corresponds to applying an

 -production



 More_generally consider a variant of the productions in

 Fig_stmt-gram-fig where optexpr generates an

 expression nonterminal instead of the token expr



 center

 tabularr_c l

 optexpr expr





 tabular

 center

 Thus optexpr either generates an expression using

 nonterminal or it generates While parsing optexpr if the lookahead_symbol is not in

 then the -production is used









































 Designing a Predictive Parser



 We can generalize the technique introduced informally in

 Section predictive-sect so it applies to any grammar that has

 disjoint_sets for the production_bodies belonging to any

 nonterminal

 We_shall also see that when we have a translation_scheme - that is a

 grammar with embedded actions - it is possible to execute those

 actions as part of the procedures designed for the parser



 Recall that a predictive_parser is a program consisting of a procedure

 for every nonterminal The procedure for nonterminal does two things



 enumerate



 It decides which -production to use by_examining the

 lookahead_symbol The production with body (where is

 not the empty string) is used if the

 lookahead_symbol is in If there is a conflict

 between two nonempty bodies for any lookahead_symbol then we cannot use

 this parsing method on this grammar In_addition

 the -production for if it exists is

 used if the lookahead_symbol is not in the set for any

 other production_body for



 The procedure then mimics the body of the chosen production

 That is the symbols of the body are executed in turn from the

 left

 A nonterminal is executed by a call to the procedure for that

 nonterminal and a terminal matching the lookahead_symbol results in

 the next_input symbol being read If at some point the terminal in the

 body does_not match the lookahead_symbol an error is

 declared



 enumerate

 Figure pred-parser-fig is the result of applying these rules

 to the grammar in Fig_stmt-gram-fig



 Just as a translation_scheme is formed_by extending a grammar a

 syntax-directed translator can be formed_by extending a predictive

 parser An algorithm for this purpose is given in

 Section sdt-sect The following limited construction

 suffices for the present



 enumerate



 Construct a predictive_parser ignoring the actions in

 productions



 Copy the actions from the translation_scheme into the

 parser If an action appears after grammar symbol in

 production then it is copied after the implementation of

 in the code for Otherwise if it appears at the beginning of

 the production then it is copied just_before the code for the

 production_body

 enumerate



 We_shall construct such a translator in

 Section_postfix-sect



 Left-Recursion

 left-rec-subsect



 It is possible for a recursive-descent_parser to loop forever A

 problem arises with left-recursive productions like



 center



 center

 where the leftmost symbol of the body is the same as the

 nonterminal at the head of the production Suppose the procedure

 for decides to apply this production The body begins_with

 so the procedure for is called recursively

 Since the lookahead_symbol changes only

 when a terminal in the body is matched no change to the input

 took place between recursive_calls of

 As a result the second call to does exactly what the first call

 did which means a third call to and so on in an_infinite

 loop



 A left-recursive production can be_eliminated by rewriting the

 offending production Consider a nonterminal with two

 productions



 center



 center

 where and are sequences of terminals and

 nonterminals that do_not start with For_example in



 center



 center

 nonterminal string and

 string



 The nonterminal and its production are said to be

 left-recursive because the production

 has itself as the leftmost symbol on the

 right_side Repeated application of this production builds up a

 sequence of 's to the right of as in

 Fig leftrec-fig(a) When is finally replaced_by

 we have a followed_by a sequence of zero_or more

 's

 figurehtfb

 Left-

 and right-recursive ways of generating a string

 leftrec-fig

 figure



 The same effect can be achieved as in Fig leftrec-fig(b)

 by rewriting the productions for in the following manner

 using a new nonterminal



 center

 tabularr_c l







 tabular

 center

 Nonterminal and its production are

 right-recursive

 because this production for has itself as the last symbol

 on the right_side Right-recursive productions lead to trees that

 grow down towards the right as in Fig leftrec-fig(b)

 Trees growing down to the right make it harder to translate

 expressions containing left-associative operators such_as minus

 In Section adapting-trans-subsect however we_shall see that the proper

 translation of expressions into_postfix notation can still be

 attained by a careful design of the translation_scheme based_on a

 right-recursive grammar



 In Section left-rec-elim-subsect we consider more general

 forms of left-recursion and show_how all left-recursion can be

 eliminated from a grammar

 Peephole Optimization

 peephole-sect



 While most production compilers produce good code through careful

 instruction_selection and register_allocation a few use an

 alternative strategy they generate naive code and then improve

 the quality of the target code by_applying optimizing

 transformations to the target program The term optimizing is

 somewhat misleading because there is no guarantee that the

 resulting code is optimal under any mathematical measure

 Nevertheless many simple transformations can significantly

 improve the running_time or space requirement of the target

 program



 A simple but effective technique for locally improving the target

 code is peephole optimization which is done by_examining a

 sliding window of target instructions (called the peephole)

 and replacing instruction sequences within the peephole by a

 shorter or faster sequence whenever possible Peephole

 optimization can also be applied directly after intermediate_code

 generation to improve the intermediate_representation



 The peephole is a small sliding window on a program The code in

 the peephole need not be contiguous although some implementations

 do require this It is characteristic of peephole optimization

 that each improvement may spawn opportunities for additional

 improvements In_general repeated_passes over the target code are

 necessary to get the maximum benefit In this_section we_shall

 give the following examples of program transformations that are

 characteristic of peephole optimizations



 itemize



 Redundant-instruction elimination



 Flow-of-control optimizations



 Algebraic simplifications



 Use of machine idioms



 itemize





 Eliminating Redundant Loads and Stores



 If we see the instruction sequence



 verbatim

 LD_R0 a

 ST a R0

 verbatim

 in a target program we can delete the store instruction because

 whenever it is executed the first instruction will ensure_that

 the value of a has already_been loaded into register R0 Note_that if the store instruction had a label we could not

 be_sure that the first instruction is always executed before the

 second so we could not remove the store instruction Put_another

 way the two instructions have to be in the same basic_block for

 this transformation to be safe



 Redundant loads and stores of this nature would not be generated

 by the simple code_generation algorithm of the previous section

 However a naive code_generation algorithm like the one in

 Section instr-select-subsect would generate redundant

 sequences such_as these



 Eliminating Unreachable Code



 Another opportunity for peephole optimization is the removal of

 unreachable instructions An unlabeled instruction immediately

 following an unconditional_jump may be removed This operation can

 be repeated to eliminate a sequence of instructions For_example

 for debugging purposes a large program may have within it certain

 code fragments that are executed only if a variable debug is

 equal to 1 In the intermediate_representation this code may look

 like



 flushleft

 tabularl

 ' if debug 1 goto L1'



 ' goto_L2'



 ' L1 'print debugging information



 ' L2'



 tabular

 flushleft



 One obvious peephole optimization is to eliminate jumps over

 jumps Thus no_matter what the value of debug the code

 sequence above can be replaced_by



 flushleft

 tabularl

 ' if debug 1 goto_L2'



 ' 'print debugging information



 ' L2'



 tabular

 flushleft



 If debug is set to 0 at the beginning of the program

 constant_propagation would transform this sequence into



 flushleft

 tabularl

 ' if 0_1 goto_L2'



 ' 'print debugging information



 ' L2'



 tabular

 flushleft



 Now the argument of the first statement always evaluates to true so the statement can be replaced_by goto_L2 Then all

 statements that print debugging information are unreachable and

 can be_eliminated one at a time



 Flow-of-Control Optimizations



 Simple intermediate code-generation algorithms

 frequently produce jumps to jumps jumps to conditional jumps

 or conditional jumps to jumps

 These unnecessary jumps can be_eliminated in either the intermediate

 code or the target code by the following types of peephole

 optimizations

 We can replace the sequence



 flushleft

 tabularl

 ' goto L1'



 '_'



 ' L1 goto_L2'



 tabular

 flushleft

 by the sequence



 flushleft

 tabularl

 ' goto_L2'



 '_'



 ' L1 goto_L2'

 tabular

 flushleft



 If there are now no jumps to L1 then it may be

 possible to eliminate the statement L1 goto_L2

 provided it is preceded by an unconditional_jump



 Similarly the sequence



 flushleft

 tabularl

 ' if a b goto L1'



 '_'



 ' L1 goto_L2'

 tabular

 flushleft

 can be replaced_by the sequence



 flushleft

 tabularl

 ' if a b goto_L2'



 '_'



 ' L1 goto_L2'

 tabular

 flushleft



 Finally suppose there is only one jump to L1

 and L1 is preceded by an unconditional goto

 Then the sequence



 verbatim

 goto L1



 L1 if a b goto_L2

 L3

 verbatim

 may be replaced_by the sequence



 verbatim

 if a b goto_L2

 goto L3



 L3

 verbatim

 While the number of instructions in the two sequences

 is the same we sometimes skip the unconditional_jump

 in the second sequence but never in the first

 Thus the second sequence is superior to the first in

 execution time



 Algebraic Simplification and Reduction in Strength



 In Section bb-opt-sect we discussed algebraic_identities

 that could be used to simplify 's These algebraic_identities

 can also be used by a peephole optimizer to eliminate

 three-address_statements such_as



 verbatim

 x x 0

 verbatim

 or



 verbatim

 x x 1

 verbatim

 in the peephole



 Similarly reduction-in-strength transformations can be applied

 in the peephole to replace expensive operations by

 equivalent cheaper ones on the target_machine

 Certain machine_instructions are considerably cheaper

 than_others and can often be used as special_cases of more

 expensive operators

 For_example is invariably cheaper to implement as

 than as a call to an exponentiation routine

 Fixed-point multiplication or division by a power of two

 is cheaper to implement as a shift

 Floating-point division by a constant can be approximated

 as multiplication by a constant which may be cheaper



 Use of Machine Idioms



 The target_machine may have hardware instructions to implement

 certain specific operations efficiently Detecting situations that

 permit the use of these instructions can reduce execution time

 significantly For_example some machines have auto-increment and

 auto-decrement addressing_modes These add or subtract one from an

 operand before or after using its value The use of the modes

 greatly improves the quality of code when pushing or popping a

 stack as in parameter_passing These modes can also be used in

 code for statements like xx1



 exer

 Construct an algorithm that will perform redundant-instruction

 elimination in a sliding peephole on target_machine code

 exer



 exer

 Construct an algorithm that will do flow-of-control optimizations

 in a sliding peephole on target_machine code

 exer

 exer

 Construct an algorithm that will do simple algebraic

 simplifications and reductions in strength in a sliding peephole

 on target_machine code

 exer

 Pipelining

 pipeline-subsect



 In pipelining a task is decomposed into a number of stages to be

 performed on different_processors For_example a task computed using

 a loop of iterations can be structured as a pipeline of

 stages Each stage is assigned to a different processor when one

 processor is finished with its stage the results are passed as input

 to the next processor in the pipeline



 In the following we start by explaining the concept of pipelining in

 more_detail We then show a real-life numerical algorithm known_as

 successive over-relaxation to illustrate the conditions under which

 pipelining can be applied in Section ch11sor We then formally

 define the constraints that need to be_solved in

 Section time-part-subsect and describe an algorithm for solving

 them in Section solve-time-part-subsect Codes that have

 multiple independent_solutions to the time-partition_constraints are

 known_as having outermost_fully permutable_loops such loops can

 be pipelined easily as discussed in Section ch11fpn



 What is Pipelining

 ex

 pipeline-loop-ex

 Consider the loop



 verbatim

 for_(i 1_i m_i)

 for_(j 1_j n_j)

 Xi Xi Yij

 verbatim

 This code sums up the th_row of and adds it to the th_element

 of The inner_loop corresponding to the summation must_be

 performed sequentially because of the data_dependence however the

 different summation tasks are independent We can parallelize this

 code by having each processor perform a separate summation Processor

 accesses row of and updates the th_element of





 Alternatively we can structure the processors to execute the

 summation in a pipeline and derive parallelism by overlapping the

 execution of the summations as shown in Figure figpipeline-ex More

 specifically each iteration of the inner_loop can be treated_as a

 stage of a pipeline stage takes an element of generated in

 the previous stage adds to it an element of and passes the

 result to the next stage Notice_that in this case each processor

 accesses a column instead of a row of This may be preferable

 depending_on the context in which this code is executed



 We can initiate a new task as_soon as the first processor is

 done with the first stage of the previous task

 At the beginning the pipeline is empty and only the first processor

 is executing the first stage After it completes the results are

 passed to the second processor while the first processor starts on the second

 task and so on In this way the pipeline gradually fills until all

 the processors are busy When the first

 processor_finishes with the last_task the pipeline starts to drain

 with more and more processors becoming idle until the last processor

 finishes the last_task In the steady_state tasks can be

 executed concurrently in a pipeline of processors

 ex



 figure

 center

 tabularcccc

 Time 3cProcessors



 2-4

 1_2 3



 1 111



 2 221 112



 3 331 222 113



 4 441 332 223



 5 442 333



 6 443



 tabular

 center

 Pipelined execution of Example_pipeline-loop-ex with

 and



 figpipeline-ex

 figure



 It is interesting to contrast pipelining with simple parallelism

 where different_processors execute different tasks

 itemize



 Pipelining

 can only be applied to nests of depth at_least two We

 can treat each iteration of the outer_loop as a task and the

 iterations in the inner_loop as stages of that task



 Tasks executed on a pipeline may share dependences Information

 pertaining to the same stage of each task is held on the same

 processor thus results generated_by the th_stage of a task can be

 used by the th_stage of subsequent tasks with no communication

 cost Similarly input data used by the same stage of different tasks

 need to reside only on one processor as illustrated by

 Example_pipeline-loop-ex



 If the tasks are independent then simple parallelization has better

 processor utilization because processors can execute all at once

 without_having to pay for the overhead of filling and draining the

 pipeline However as shown in Example_pipeline-loop-ex the

 data_accessed in a pipelined scheme is different from that of simple

 parallelization pipelining may be preferable if it reduces communication

 itemize



 Successive Over-Relaxation (SOR) An Example

 ch11sor



 Successive over-relaxation (SOR) is a technique for accelerating

 the convergence of relaxation methods for solving sets of simultaneous

 linear equations A relatively_simple template illustrating its

 data-access pattern is shown in Fig_pipeline-nest-fig(a)

 Here the new value of an element in the array depends_on the values

 of elements in its neighborhood Such an operation is performed

 repeatedly_until some convergence criterion is met



 figurehtfb

 (a) Original source

 verbatim

 for_(i 1_i m_i)

 for_(j 1_j n_j)

 Xij 05 (Xi-1j Xij-1)

 verbatim



 (b) Data_dependences in the code



 (LABEL THE VERTICAL AXIS AS i AND THE HORIZONTAL AXIS AS j

 REDUCE A COLUMN OF THE FIGURE

 THE RECTANTULAR SHAPE MAKES IT

 EASIER TO SHOW THE DIFFERENT PERMUTATIONS)



 fileuullmanalsuch11figspipelineeps



 (c) Permuted version of the original_program

 verbatim

 for_(j 1_j n_j)

 for_(i 1_i m_i)

 Xij 05 (Xi-1j Xij-1)

 verbatim

 An_example of successive over-relaxation (SOR)

 pipeline-nest-fig

 figure



 Shown in Fig pipeline-nest-fig(b) is a picture of the data

 dependences Since dependences exist across both the rows and the

 columns in the iteration_space we cannot parallelize this code unless

 synchronization is introduced Since the longest chain of dependences

 consists of edges by introducing synchronization we should be

 able to find one degree of parallelism and execute the operations

 in unit time For_example as all the iterations along the

 diagonals in Fig pipeline-nest-fig(b) can be

 executed in parallel we can execute the diagonals sequentially

 starting_with the diagonal at the origin and proceeding outwards We

 refer to such an execution scheme as wavefronting



 Fully_Permutable Loops



 We first observe that the two loops in the SOR_code are fully

 permutable meaning that they can be permuted without_changing the

 semantics of the original_program By permuting the inner_loop with

 the outer as shown in Fig pipeline-nest-fig(b) the iterations

 shown in Fig pipeline-nest-fig(c) are executed column by column

 instead of row by row From the figure we can easily see that this

 ordering preserves the relative ordering between every data_dependent

 pair of accesses



 As we permute the loops we change the set of operations executed in

 each iteration of the outermost_loop drastically This freedom in

 scheduling means that operations in the program are not strictly

 ordered and that plenty of parallelism exists We can shown that if

 a loop has outermost_fully permutable_loops by performing just

 synchronizations we can get degrees of parallelism

 We can such parallelism in many different_ways as discussed below



 Pipelining Fully_Permutable Loops



 A loop with outermost_fully permutable_loops can be structured as

 a pipeline with dimensions In the SOR_example so

 we can structure the processors as a linear pipeline



 We can pipeline the SOR_code in two different_ways corresponding to

 the two possible permutations shown in Fig_pipeline-nest-fig(a)

 and (c) respectively First we can assign each row of the

 computation to the same processor Each_processor executes the

 computation represented_by the inner_loop of the code in

 Fig_pipeline-nest-fig(a) Except for the first processor every

 processor must_wait for the result of iteration

 executed by processor before it can execute iteration

 The code to implement such a pipeline is shown in

 Fig_signal-fig(a) Similarly we can create a pipeline by

 having processor execute the inner_loop of the code in

 Fig pipeline-nest-fig(c) The code to implement such a

 pipeline is shown in Fig signal-fig(b)



 figurehtfb

 verbatim

 1 p m

 for_(j 1_j n_j)

 if_(p 1)_wait (p-1)

 Xpj 05 (Xp-1j Xpj-1)

 if_(p m-1) signal_(p1)



 verbatim



 center

 (a) Processors_assigned to rows

 center



 verbatim

 1 p n

 for_(i 1_i m_i)

 if_(p 1)_wait (p-1)

 Xip 05 (Xi-1p Xip-1)

 if_(p n-1) signal_(p1)



 verbatim



 center

 (b) Processors_assigned to columns

 center



 Two implementations of pipelining for the code

 of Fig_pipeline-nest-fig

 signal-fig

 figure



 With pipelining each processor only synchronizes and communicates

 with only at most two other processors The iterations being

 performed at any one time form a relaxed wavefront through the

 computation that is the rows or columns may progress through the

 pipeline at slightly_different rates This is superior to a

 parallelization scheme that requires all the processors to synchronize

 using a barrier



 Parallelizable Inner Loops



 A loop_nest with fully_permutable loops can also be transformed to

 give an outer_loop that contains inner parallelizable loops

 (Note that the converse is not true a loop_nest may have inner

 parallel loops but it may not be fully_permutable and pipelinable)



 For_example we can turn the SOR_code into an outer_loop with a

 parallel inner_loop separated_by barriers Specifically we can

 generate an inner_loop composing of iterations along the

 diagonals in Fig_pipeline-nest-fig and is thus parallelizable

 One_way to do so is to map the old indexes to new indexes

 with the affine_transform





 arrayl

 g



 h



 array







 arrayl_l

 1 1



 0_1



 array





 arrayl

 i



 j



 array







 arrayl

 -1



 0



 array







 The transformed code shown in Fig figwavefront(a) can be

 generated_by changing the axes of the iterations space and applying

 Algorithm_algenumerate to generate the bounds of the new loops

 The dependences of the new iteration_space are shown in

 Fig_pipeline-nest-fig



 figure

 (a) A parallelizable inner_loop

 verbatim

 for (g 1 g mn-1 g)

 for (h max(1g-n1) h min(gn) h)

 Xg-j1h 05 (Xg-jh Xg-j1h-1)

 verbatim

 (b) Data_dependences of transformed code



 DRAW A SKEWED VERSION OF FIG pipeline-nest-fig(b)

 verbatim

 63

 5253

 414243

 313233

 2122

 11

 verbatim

 Wavefronting the computation in

 Fig_pipeline-nest-fig

 figwavefront

 figure



 All the iterations in the inner_loop can be executed in parallel and

 can be assigned to any processors provided that a barrier is inserted

 before each execution How we assign the iterations change how much

 communication is needed One_way to minimize_communication is to

 assign each column of the iteration_space to the same processor In

 fact with this assignment there is no need for all processors to

 participate in a barrier since it only needs to synchronize with the

 processor before and after We observe that this assignment ends up

 giving each processor the same iterations as the pipelined code in in

 Fig signal-fig(b)



 Had we used the affine_transform





 arrayl

 g



 h



 array







 arrayl_l

 1 1



 1_0



 array





 arrayl

 i



 j



 array







 arrayl

 -1



 0



 array





 we could create a wavefronting code where assigning the columns to the

 same processor yields the same assignment as Fig_signal-fig(a)

 The corresponding code and dependences are shown in

 Fig figwavefront2



 figure

 (a) A parallelizable inner_loop

 verbatim

 for (g 1 g mn-1 i1)

 for (h max(1 g-m1) h min(gm) h)

 Xhg-h1 05 (Xhg-h Xh-1g-h1)

 verbatim

 (b) Data_dependences of transformed code

 DRAW A SKEWED VERSION OF FIG pipeline-nest-fig(b)

 verbatim

 64

 53 54

 4243 44

 313233

 2122

 11

 verbatim

 Another way of wavefronting the computation in

 Fig_pipeline-nest-fig

 figwavefront2

 figure



 Many More Choices



 We have shown two ways to pipeline the SOR_code and their wavefront

 analogues





 We have shown that we can pipeline a fully_permutable loop_nest in two

 different_ways or we can execute diagonals of the iteration_space in

 a wavefront These are but a few of the many_ways in which the

 computation can be_parallelized Instead of a diagonal

 we can choose a wavefront at an 150 diagonal In_fact any

 diagonal between and is a legal wavefront

 Similarly we do_not have to pipeline the computation along just the

 rows and or just the columns We can further map the computation to

 processors via any partition-mapping function of the form

 where and are positive constants to create a

 pipeline that can execute the iterations in time



 General Principle



 This example_illustrates the following general principle that is key

 to the theory of pipelining If we can come_up with different

 outermost_loops for a loop_nest that satisfy all the dependences then

 we can pipeline the computation A loop with outermost_fully

 permutable_loops has degrees of pipelinable or wavefrontable

 parallelism



 Loops that cannot be pipelined do_not have alternate

 outermost_loops Example_exnsync1 shows one such an example

 To honor all the dependences each iteration in the outermost_loop

 must execute precisely the computation found in the original code

 However such code may still contain parallelism in the inner_loops

 which can be_exploited by introducing at_least synchronizations

 where is the number of iterations in the outermost_loop



 ex

 exnsync1

 Figure synch-pipe-fig is a more_complex version of the problem we

 saw in Example_exscc2



 figurehtfb

 center

 (a)

 center

 verbatim

 for_(i 0 i_100 i)

 for_(j 0 j_100 j)

 Xj Xj Yij (s1)

 Zi XAi (s2)



 verbatim

 center

 (b)

 center

 verbatim

 s1 - s1 s1- s2 s2-s1

 verbatim

 A sequential outer_loop (a) and its program dependence graph

 synch-pipe-fig

 figure



 As shown in the program dependence graph in

 Figure synch-pipe-fig(b) statements and belong to the

 same strongly_connected component Because we do_not know the

 contents of matrix we must assume that the access in statement

 may read from any of the elements of There is a true

 dependence from statement to statement and a

 anti-dependence from statement to statement There is no

 opportunity for pipelining either because all operations belonging to

 iteration in the outer_loop must_precede those in iteration

 To_find more parallelism we repeat the parallelization process to the

 inner_loop The iterations in the second loop can be_parallelized

 without_synchronization Thus 200 barriers are needed with one

 executed before and one after the execution of the inner_loop

 ex



 Time-Partition_Constraints

 time-part-subsect



 We_now focus_on the problem of finding pipelined_parallelism Our

 goal is to turn a computation into a set of pipelinable tasks To

 find_pipelined parallelism we do_not solve directly what is to be

 executed on each processor like we did with parallelization

 Instead we ask the following fundamental question What are all the

 possible_execution sequences that honor the original data_dependences

 in the loop Obviously the original execution sequence satisfies all

 the data_dependences The question is if there are affine transforms

 that can create an alternate schedule where iterations of the

 outermost_loop execute a different set of operations from the original

 such that all the dependences are satisfied If we can find such

 transforms we can pipeline the loop The key is that if there is

 freedom in the scheduling of operations there is parallelism details

 of how we derive pipelined_parallelism from such transforms will be

 explained later



 In this case we_wish to find a one-dimensional affine_transform one

 for each statement that maps the original loop_index values to an

 iteration number in the outermost_loop The transform is legal if the

 assignment can satisfy all the data_dependences in the program The

 time-partition_constraints shown below simply say that if one

 operation is dependent upon the other then the first must_be assigned

 an iteration in the outermost_loop no earlier than that of the second

 If they are assigned in the same iteration then it is understood that

 the first will be executed after than the second within the iteration



 An affine-partition mapping of a program is a legal time

 partition if and only if for every two (not_necessarily distinct)

 accesses_sharing a dependence say



 in statement_nested in loops

 and





 in statement_nested in loops

 the one-dimensional partition_mappings

 and

 for statements and

 respectively satisfy the time-partition_constraints







 1in

 in and in such

 that



 arrayc

 i1s1s2i2



 B1 i1 b1 0



 B2 i2 b2 0



 F1_i1 f1_F2 i2_f2

 array



 we have





 C1i1_c1 C2i2

 c2





 This constraint illustrated in Fig figtime looks

 remarkably similar to the space-partition_constraints It is a

 relaxation of the space-partition_constraints in that if two iterations

 refer to the same_location they do_not necessarily have to be mapped

 to the same partition we only require that the original relative

 execution order between the two iterations is preserved

 That is the constraints here have where the space-partition

 constraints have





 figurehtfb

 THE PICTURE NEEDS TO BE MODIFIED THE CURRENT PICTURE DOES NOT SHOW

 OFF THE DIFFERENCE BETWEEN TIME-PARTITIONS VS SPACE-PARTITIONS WE

 SHOULD HAVE TWO i's AND THEY MAPPED TO DIFFERENT TIME

 STEPS



 fileuullmanalsuch11figstimeeps

 Time-Partition_Constraints

 figtime

 figure



 We know that there_exists at_least one solution to the time-partition

 constraints We can simply map operations in each of the outermost

 loop back to the same iteration and all the data_dependences will be

 satisfied This is the only solution to the time-partition

 constraints for codes that cannot be pipelined On the other_hand if

 we can find multiple independent_solutions to time-partition

 constraints of a program the program can be pipelined Each

 independent solution corresponds to a loop in the outermost_fully

 permutable nest As you can expect there is only solution

 independent solution to the timing_constraints extracted_from the

 program in Example_exnsync1 where there is no pipelined

 parallelism and that there are two independent_solutions to the

 SOR_code example



 ex

 exnsync1p2

 Let_us consider Example_exnsync1 and in particular the data

 dependences of references to array in statements and

 Because the access is not affine in statement we approximate

 the access by modeling matrix simply as a scalar_variable in

 dependence analysis involving statement Let

 be the index value of a dynamic_instance of and let be

 the index value of a dynamic_instance of Let the computation

 mappings of statements and be

 and respectively



 Let_us first consider the time-partition_constraints imposed_by

 dependences from statement to Thus

 the transformed_th

 iteration of must_be no later than the transformed

 th_iteration of that is





 arrayrr

 C11_C12



 array





 arrayr

 i



 j



 array





 c1

 C21_i' c2



 Expanding



 C11 i C12j c1_C21 i'_c2



 Since can be arbitrarily_large independent of and it must

 be that

 Thus one possible solution to the constraints is



 C11_C21 1 and C12 c1_c2 0





 Similar arguments about the data_dependence from to and

 back to itself

 will yield a similar answer In this particular solution the th

 iteration of the outer_loop which consists of the instance of

 and all instances of are all assigned to timestep

 Other legal choices of and yield

 similar assignments although there might be timesteps at which nothing

 happens

 That is all ways to schedule the outer_loop

 require the iterations to execute in the same order as in the original code

 This statement holds whether all 100 iterations are executed on the same

 processor on 100 different_processors or anything in-between

 ex



 ex

 expipelinesoln

 In the SOR_code shown in

 Fig_pipeline-nest-fig(a)

 the write_reference Xij

 shares a dependence with itself and the two read references in the

 code

 Let_us first consider the constraint due to the dependence from

 Xij to Xi-1j Let and be two

 data-dependent instances of Xij and Xi-1j respectively

 Instance can depend_on only if

 that is either or and But for the

 instances to access the same array_element it must_be that and



 Therefore the time-partition_constraint reduces

 to





 arrayrr

 C11_C12

 array





 arrayr

 i



 j



 array







 arrayr

 c1



 array





 arrayrr

 C11_C12

 array





 arrayr

 i'



 j'



 array







 arrayr

 c1



 array







 If we substitute for and for the above inequality

 simplifies to

 Thus there are two independent_solutions in the two-dimensional index

 space and these form the basis_vectors for the solution space





 arrayr

 1



 0



 array





 arrayr

 0



 1



 array





 The first of these solutions preserve the execution order of the

 iterations in the outermost_loop whereas the second maps each of the

 iterations in Fig pipeline-nest-fig(b) to a different outer

 loop iteration The first solution corresponds to the original code

 shown in

 Fig_pipeline-nest-fig(a) the second corresponds to that in

 Fig pipeline-nest-fig(c)

 ex



 Solving Time-Partition_Constraints by Farkas'Lemma

 solve-time-part-subsect



 Since time-partition_constraints are similar to

 space-partition_constraints can we use a similar algorithm to solve

 them

 Unfortunately the slight difference_between the two problems

 translates_into a big technical difference_between the two solution

 methods

 Algorithm_algnosync simply solves for

 and such that for all in

 and in if



 F1_i1 f1_F2 i2_f2

 then



 C1i1_c1 C2i2_c2



 The linear_inequalities due to the loop_bounds are only used in

 determining if two references share a data_dependence and not used

 otherwise



 To_find solutions to the time-partition_constraints we cannot ignore

 the linear_inequalities ignoring them

 often would allow only the trivial_solution of placing all iterations in the

 same partition Thus the algorithm to find solutions to the

 time-partition_constraints must handle both equalities and

 inequalities



 The general problem we_wish to solve is given a matrix A

 find a vector c such that for all vectors x such that

 it is the case that



 In other_words we are seeking c such that the inner product of

 c and any coordinates in the polyhedron defined by the

 inequalities always yields a

 non-negative answer



 This problem is addressed by Farkas'_Lemma

 Let A be an matrix of reals

 and let c be a real nonzero

 -vector Farkas'_lemma says_that either the primal_system of

 inequalities



 Ax_0 cT x 0



 has a real-valued_solution x or the dual_system



 AT_y c y 0



 has a real-valued_solution y but never both



 The dual_system can be handled by using Fourier-Motzkin_elimination to

 project_away the variables of y For each c that has a

 solution in the dual_system the lemma guarantees that there are no

 solutions to the primal_system Put_another way we can prove the

 negation of the primal_system ie we can prove that



 for all x such that by finding a

 solution y to the dual_system

 and



 About Farkas'_Lemma

 The proof of

 the lemma can be found in many standard texts on linear_programming

 Farkas'_Lemma originally proved in 1901

 is one of the theorems of the alternative

 These

 theorems are all equivalent but despite attempts over the years a

 simple intuitive proof for this lemma or any of its equivalents has

 not been_found



 alg

 algsync

 Finding a set of legal maximally independent affine_time-partition mappings

 for an outer sequential loop



 A loop_nest with array_accesses



 A maximal_set of linearly_independent time-partition_mappings



 The following steps constitute the algorithm



 enumerate



 Find all data-dependent pairs of accesses in a program



 For each pair of data_dependent accesses



 in statement_nested in loops

 and



 in statement_nested in loops

 let and be the

 (unknown) time-partition

 mappings of statements and respectively

 The time-partition_constraint states that for all

 in and in if



 arrayc

 i1s1s2i2



 B1 i1 b1 0



 B2 i2 b2 0



 F1_i1 f1_F2 i2_f2

 array



 we have





 C1i1_c1 C2i2

 c2



 Since

 is a disjunctive union of a

 number of clauses We create a system of constraints for each clause

 and solve each of them separately as_follows

 enumerate

 Similarly to step (trick-item) in Algorithm_algnosync apply

 Gaussian_elimination to

 the equations



 F1_i1 f1_F2 i2_f2



 to reduce the vector





 arrayc

 i1



 i2



 1



 array





 to some vector of unknowns x

 Let c be all the unknowns in the partition_mappings

 Express the linear inequality constraints_due to the partition

 mappings as



 cTDx 0



 for some matrix D

 Express the precedence constraints on the loop_index variables and

 the loop_bounds as



 Ax_0



 for some matrix A

 Apply Farkas'_Lemma finding x to satisfy the two constraints

 above is equivalent to finding y such that



 AT_y DTc and y 0



 Note_that here is in the

 statement of Farkas'_Lemma and we are using the negated form of the

 lemma

 In this form apply Fourier-Motzkin_elimination to project_away

 the variables and express the constraints on the

 coefficients c as

 Let

 be the system without the constant terms

 enumerate



 Find a maximal_set of linearly_independent solutions to

 using Algorithm figtime-math in

 Appendix time-math-ch

 The approach of that complex algorithm

 is to keep_track of the current set of solutions for each

 of the statements then incrementally look for more independent

 solutions by inserting constraints that force the solution to be

 linearly_independent for at_least one statement



 From each solution of found derive one affine

 time-partition mapping The constant terms are

 derived using

 enumerate

 alg



 ex

 The constraints for Example_exnsync1p2 can be written as





 arrayrrrr

 -C11_-C12 C21 (c2-c1)



 array





 arrayc

 i



 j



 i'



 1



 array



 0









 arrayrrrr

 -1_0 1_0

 array





 arrayc

 i



 j



 i'



 1



 array



 0





 Farkas'_lemma says_that these constraints are equivalent to





 arrayr

 -1



 0



 1



 0



 array





 arrayr

 z



 array







 arrayr

 -C11



 -C12



 C21



 c2-c1



 array

 and

 z 0



 Solving this system we get



 C11_C21 0 and

 C12 c2-c1 0



 Notice_that these constraints are satisfied by the particular

 solution we obtained

 in Example_exnsync1p2

 ex



 Code Transformations

 ch11fpn



 If there exist independent_solutions to the time-partition

 constraints of a loop_nest then it is possible to transform the loop

 nest to have outermost_fully permutable_loops which can be

 transformed to create degrees of pipelining or to create

 inner parallelizable loops Furthermore we can apply_blocking to

 fully_permutable loops to improve data_locality of uniprocessors as

 well_as reducing synchronization among processors in a parallel

 execution



 Fully_Permutable Loops

 We_now address how we create a loop_nest with outermost_fully

 permutable_loop nest given independent_solutions to the

 time-partition_constraints of a loop



 We associate a loop_index variable with the th

 independent solution Thus every statement has has a partition

 mapping such that



 ij Csj is csj



 where is a loop_index for statement

 We create a sequential program that serially executes the iteration

 space spanned by in a lexicographic

 order Each of the partitions executes the dynamic instances mapped

 to that partition in the original_sequential order

 Algorithm algsimpleSPMD can be used to create such a program



 ex

 The solutions found in Example expipelinesoln for our SOR

 example were





 arrayr

 1



 0



 array





 arrayr

 0



 1



 array







 Associating the first and second loop_index with the first and second

 solution respectively we get the code in Fig

 Fig_pipeline-nest-fig(a) if we reverse the order of the

 solutions we get the code in Fig pipeline-nest-fig(c)

 ex



 The solutions to time-partition_constraints are not unique We can

 construct an fully_permutable loop_nests out of any linearly

 independent_solutions For_example we could have picked the

 solutions





 arrayr

 1



 0



 array





 arrayr

 1



 1



 array





 and generate a corresponding fully_permutable loop_nest



 Pipelining



 We can easily transform a loop with outermost_fully permutable

 loop_nest into a code with degrees of pipeline parallelism



 ex

 Let_us return to our SOR_example Because the two loops in the SOR

 example are fully_permutable we know that iteration can

 be executed provided iterations and have

 been executed We can guarantee this order in a pipeline as_follows

 We assign iteration to processor Each_processor executes

 iterations in the inner_loop in the original_sequential order thus

 guaranteeing that iteration executes after

 In_addition we require that processor_waits for the signal from

 processor that it has executed iteration before it

 executes iteration This technique generates the pipelined

 codes Fig_signal-fig(a) and (b) from the fully_permutable loops

 Fig_pipeline-nest-fig(a) and (c) respectively

 ex



 In_general given outermost_fully permutable_loop

 iteration with index values

 can be executed

 without violating data_dependence constraints provided iterations



 i1-1 i2 ik

 i1 i2-1 i3 ik

 i1 ik-1 ik-1



 have_been

 executed

 We can thus assign the partitions of the first dimensions of the

 iteration_space to processors and have each processor

 executes the iterations in the th loop sequentially

 It can execute iteration in the th loop

 as_long as it receives a signal

 from processors



 p1-1 p2 pk-1 p1

 pk-2 pk-1-1



 that they have executed their th_iteration in the th loop



 Wavefronting

 It is also easy to generate inner parallelizable loops from a

 loop with outermost_fully permutable_loops Although pipelining

 is preferable we include this information here for completeness



 We partition the computation of a loop with outermost_fully

 permutable_loops using a new index variable where is

 defined to be some linear_combination of all the indices in the

 permutable_loop nest For_example is one

 such combination



 We create an outermost sequential loop that iterates through the

 partitions in increasing order the computation nested_within each

 partition is ordered as before The first loops within each

 partition are guaranteed to be parallelizable Applying this

 algorithm to the SOR_code in Fig_pipeline-nest-fig(a)

 produces the wavefronting code in Fig figwavefront



 Blocking



 A -deep_fully permutable_loop nest can be blocked in

 -dimensions Instead of manipulating the individual partitions we

 can aggregate blocks of iterations into one_unit Blocking is useful

 for enhancing data_locality as_well as for minimizing the overhead of

 pipelining



 figure



 verbatim

 for_(i0 in i)

 for (j1 jn_j)

 S



 verbatim



 center

 (a) A simple loop_nest

 center



 verbatim

 for (ii 0 iin ib)

 for (jj 0 jjn jjb)

 for_(i iib i min(iib-1 n) i)

 for_(j iib j min(jjb-1 n) j)

 S



 verbatim



 center

 (b) A blocked_version of this loop_nest

 center



 A 2-dimensional loop_nest and its blocked_version

 blocked-code-fig



 fileuullmanalsuch11figstileeps

 Execution order before and after blocking a 2-deep_loop nest

 figtile



 figure



 Suppose we have a two-dimensional fully_permutable loop_nest as in

 Fig blocked-code-fig(a) and we

 wish to break the computation into blocks The

 execution order of the blocked code is shown in

 Fig figtile and the equivalent code is in

 Fig blocked-code-fig(b)



 We can coarsen the granularity of pipelining by assigning a column

 of blocks to one processor Notice_that

 each processor synchronizes with its

 predecessors and successors only at block boundaries Thus another

 advantage of blocking is that programs

 only need to communicate data

 accessed at the boundaries of the block with their neighbor blocks

 Values that are interior to a block are managed by only one processor





 ex

 We_now use a real numerical algorithm -

 Cholesky_decomposition - to illustrate_how Algorithm_algsync handles

 single loop_nests with only pipelining parallelism

 The code shown in Fig cholesky-fig implements

 an algorithm operating on a 2-dimensional data array The

 executed iteration_space is a triangular pyramid since only

 iterates up to the value of the outer_loop index and

 only_iterates to the value of The loop has four statements all

 nested in different loops



 figure



 verbatim

 for_(i 1_i N_i)

 for_(j 1_j i-1 j)

 for (k 1 k j-1 k)

 Xij_Xij - Xik Xjk

 Xij_Xij Xjj



 for (m 1 m i-1 m)

 Xii Xii - Xim Xim

 Xii sqrt(Xii)



 verbatim



 Cholesky_decomposition

 cholesky-fig



 verbatim

 for (i2 1 i2 N i2)

 for (j2 1 j2 i2 j2)

 beginning of code for processor_(i2j2)

 for (k2 1 k2 i2 k2)



 Mapping_i2 i_j2 j_k2 k

 if (j2i2 k2j2)

 Xi2j2_Xi2j2 -_Xi2k2 Xj2k2



 Mapping_i2 i_j2 j_k2 j

 if (j2k2 j2i2)

 Xi2j2_Xi2j2 Xj2j2



 Mapping_i2 i_j2 i_k2 m

 if_(i2j2 k2i2)

 Xi2i2 Xi2i2 -_Xi2k2 Xi2k2



 Mapping_i2 i_j2 i_k2 i

 if_(i2j2 j2k2)

 Xk2k2 sqrt(Xk2k2)



 ending of code for processor_(i2j2)



 verbatim



 Figure cholesky-fig written as a fully_permutable loop

 nest

 cholesky-nest-fig



 figure



 Applying Algorithm_algsync to this program finds three

 legitimate time dimensions It nests all the operations some of

 which were_originally nested in 1- and 2-deep_loop nests into a

 3-dimensional fully_permutable loop_nest The code together_with the

 mappings is shown in Fig cholesky-nest-fig



 The code_generation routine guards the execution of the operations

 with the original loop_bounds to ensure_that the new programs execute

 only operations that are in the original code We can pipeline this

 code by mapping the 3 dimensional structure to a 2-dimensional

 processor space Iterations are assigned to the processor

 with ID Each_processor executes the innermost_loop the loop

 with the index

 Before it

 executes the th_iteration the processor_waits for signals from

 the processors with ID's and After it executes its

 iteration it signals processors and

 ex



 Pipelining

 pipeline-subsect



 In pipelining a task is decomposed into a number of stages to be

 performed on different_processors For_example a task computed using

 a loop of iterations can be structured as a pipeline of

 stages Each stage is assigned to a different processor when one

 processor is finished with its stage the results are passed as input

 to the next processor in the pipeline



 In the following we start by explaining the concept of pipelining in

 more_detail We then show a real-life numerical algorithm known_as

 successive over-relaxation to illustrate the conditions under which

 pipelining can be applied in Section ch11sor We then formally

 define the constraints that need to be_solved in

 Section time-part-subsect and describe an algorithm for solving

 them in Section solve-time-part-subsect Programs that have

 multiple independent_solutions to the time-partition_constraints are

 known_as having outermost_fully permutable_loops such loops can

 be pipelined easily as discussed in Section ch11fpn



 What is Pipelining



 Our initial attempts to parallelize loops

 partitioned the iterations of a loop_nest so that two iterations that

 shared data were assigned to the same processor Pipelining allows

 processors to share data but generally does so only in a local

 way with data passed from one processor to another that is adjacent in

 the processor space

 Here is a simple example



 ex

 pipeline-loop-ex

 Consider the loop



 verbatim

 for_(i 1_i m_i)

 for_(j 1_j n_j)

 Xi Xi Yij

 verbatim

 This code sums up the th_row of and adds it to the th_element

 of The inner_loop corresponding to the summation must_be

 performed sequentially because of the data dependence(Remember

 that we do_not take_advantage of the assumed commutativity and

 associativity of addition)

 however the

 different summation tasks are independent We can parallelize this

 code by having each processor perform a separate summation Processor

 accesses row of and updates the th_element of





 Alternatively we can structure the processors to execute the

 summation in a pipeline and derive parallelism by overlapping the

 execution of the summations as shown in Fig figpipeline-ex More

 specifically each iteration of the inner_loop can be treated_as a

 stage of a pipeline stage takes an element of generated in

 the previous stage adds to it an element of and passes the

 result to the next stage Notice_that in this case each processor

 accesses a column instead of a row of If is stored in

 column-major form there is a gain in locality by partitioning according

 to columns rather_than by rows



 We can initiate a new task as_soon as the first processor is

 done with the first stage of the previous task

 At the beginning the pipeline is empty and only the first processor

 is executing the first stage After it completes the results are

 passed to the second processor while the first processor starts on the second

 task and so on In this way the pipeline gradually fills until all

 the processors are busy When the first

 processor_finishes with the last_task the pipeline starts to drain

 with more and more processors becoming idle until the last processor

 finishes the last_task In the steady_state tasks can be

 executed concurrently in a pipeline of processors

 ex



 figure

 center

 tabularcccc

 Time 3cProcessors



 2-4

 1_2 3



 1 111



 2 221 112



 3 331 222 113



 4 441 332 223



 5 442 333



 6 443



 tabular

 center

 Pipelined execution of Example_pipeline-loop-ex with

 and



 figpipeline-ex

 figure



 It is interesting to contrast pipelining with simple parallelism

 where different_processors execute different tasks

 itemize



 Pipelining

 can only be applied to nests of depth at_least two We

 can treat each iteration of the outer_loop as a task and the

 iterations in the inner_loop as stages of that task



 Tasks executed on a pipeline may share dependences Information

 pertaining to the same stage of each task is held on the same

 processor thus results generated_by the th_stage of a task can be

 used by the th_stage of subsequent tasks with no communication

 cost Similarly each input data element

 used by a single stage of different tasks

 needs to reside only on one processor as illustrated by

 Example_pipeline-loop-ex



 If the tasks are independent then simple parallelization has better

 processor utilization because processors can execute all at once

 without_having to pay for the overhead of filling and draining the

 pipeline However as shown in Example_pipeline-loop-ex the

 pattern of data accesses

 in a pipelined scheme is different from that of simple

 parallelization

 Pipelining may be preferable if it reduces communication

 itemize



 Successive Over-Relaxation (SOR) An Example

 ch11sor



 Successive over-relaxation (SOR) is a technique for accelerating

 the convergence of relaxation methods for solving sets of simultaneous

 linear equations A relatively_simple template illustrating its

 data-access pattern is shown in Fig figsor(a)

 Here the new value of an element in the array depends_on the values

 of elements in its neighborhood Such an operation is performed

 repeatedly_until some convergence criterion is met



 figurehtfb

 verbatim

 for_(i 0 i_m i)

 for_(j 0 j_n j)

 Xj1 13 (Xj Xj1 Xj2)

 verbatim



 (a) Original source



 fileuullmanalsuch11figssoreps



 (b) Data_dependences in the code



 An_example of successive over-relaxation (SOR)

 figsor

 figure



 Shown in Fig figsor(b) is a picture of the key data

 dependences We do_not show dependences that can be inferred by the

 dependences already included in the figure For_example iteration

 depends_on iterations and so on It is

 clear from the dependences that there is no_synchronization-free

 parallelism Since the longest chain of dependences consists of

 edges by introducing synchronization we should be_able to

 find one degree of parallelism and execute the operations in

 unit time



 In_particular we observe that iterations that lie along the

 diagonals(Ie the sequences of points formed_by

 repeatedly moving down 1 and right 2)

 in Fig figsor(b) do_not share any

 dependences They only depend_on the iterations that lie along

 diagonals closer to the origin Therefore we can parallelize this

 code by executing iterations on each diagonal in order starting_at

 the origin and proceeding outwards We refer to the iterations along

 each diagonal as a wavefront and such a parallelization scheme

 as wavefronting



 Fully_Permutable Loops



 We first introduce the notion of full permutability a concept

 useful for pipelining and other optimizations Loops are fully

 permutable if they can be permuted arbitrarily without_changing the

 semantics of the original_program Once loops are put in a fully

 permutable form we can easily pipeline the code and apply

 transformations such_as blocking to improve data_locality



 The SOR_code as it written in Fig figsor(a) is not fully

 permutable As shown in Section ch11transforms permuting two

 loops means that iterations in the original iteration_space are

 executed column by column instead of row by row For_instance

 the original computation in iteration 23 would execute

 before that of 14 violating the dependences shown in

 Fig figsor(b)



 We can however transform the code to make it fully_permutable

 Applying the affine_transform





 arrayl_l

 1_0



 1 1



 array





 to the code yields the code shown in Fig figsorfp(a)

 This transformed code is fully_permutable and its permuted version is

 shown in Fig figsorfp(c) We also show the iteration_space

 and data_dependences of these two programs in Fig figsorfp(b) and

 (d) respectively From the figure we can easily see that this

 ordering preserves the relative ordering between every data-dependent

 pair of accesses



 figurehtpb

 verbatim

 for_(i 0 i_m i)

 for_(j i_j in j)

 Xj-i1 13 (Xj-i Xj-i1 Xj-i2)

 verbatim



 (a) The code in Fig figsor transformed by



















 fileuullmanalsuch11figssorfp1eps



 (b) Data_dependences of the code in (a)



 verbatim

 for_(j 0 j mn j)

 for_(i max(0j) i min(mj) i)

 Xj-i1 13 (Xj-i Xj-i1 Xj-i2)

 verbatim



 (c) A permutation of the loops in (a)



 fileuullmanalsuch11figssorfp2eps



 (d) Data_dependences of the code in (c)



 Fully permutable version of the code Fig figsor

 figsorfp

 figure



 When we permute loops we change the set of operations executed in

 each iteration of the outermost_loop drastically The fact that we

 have this degree of freedom in scheduling means that there is a lot of

 slack in the ordering of operations in the program Slack in

 scheduling means opportunities for parallelization We show later in

 this_section that if a nest has outermost_fully permutable_loops

 by introducing just synchronizations we can get

 degrees of parallelism ( is the number of iterations in a loop)



 Pipelining Fully_Permutable Loops

 pipe-loop-subsect



 A loop with outermost_fully permutable_loops can be structured as

 a pipeline with dimensions In the SOR_example so

 we can structure the processors as a linear pipeline



 We can pipeline the SOR_code in two different_ways shown in

 Fig_signal-fig(a) and Fig signal-fig(b) corresponding

 to the two possible permutations shown in Fig figsorfp(a) and

 (c) respectively In each case every column of the iteration_space

 constitutes a task and every row constitutes a stage We assign

 stage to processor thus each processor_executes the inner

 loop of the code Ignoring boundary conditions a processor can

 execute iteration only after processor has executed

 iteration



 figurehtfb



 verbatim

 0 p m

 for_(j p j pn j)

 if_(p 0) wait_(p-1)

 Xj-p1 13 (Xj-p Xj-p1 Xj-p2)

 if_(p min (mj)) signal_(p1)



 verbatim



 center

 (a) Processors_assigned to rows

 center



 verbatim

 0 p mn

 for_(i max(0p) i min(mp) i)

 if_(p max(0i)) wait_(p-1)

 Xp-i1 13 (Xp-i Xp-i1 Xp-i2)

 if_(p mn) (p i) signal_(p1)



 verbatim



 center

 (b) Processors_assigned to columns

 center



 Two pipelining implementations of the code

 from Fig figsorfp

 signal-fig

 figure



 Suppose every processor takes exactly the same amount of time to

 execute an iteration and synchronization happens instantaneously Both

 these pipelined schemes would execute the same iterations in parallel

 the only_difference is that they have different processor assignments

 All the iterations executed in parallel lie along the

 diagonals in the iteration_space in Fig figsorfp(b) which

 corresponds to the diagonals in the iteration_space of the

 original code see Fig figsor(b)



 However in practice processors with caches do_not always execute the

 same code in the same amount of time and the time for synchronization

 also varies Unlike the use of synchronization_barriers which forces

 all processors to operate in lockstep pipelining requires processors

 to synchronize and communicate with at most two other processors

 Thus pipelining has relaxed wavefronts allowing some processors to

 surge ahead while others lag momentarily This flexibility reduces

 the time processors spend waiting for other processors and improves

 parallel performance



 The two pipelining schemes shown above are but two of the many_ways in

 which the computation can be pipelined As we said once a loop is

 fully_permutable we have a lot of freedom in how we_wish to

 parallelize the code The first pipeline scheme maps iteration

 to processor the second maps iteration to

 processor We can create alternative pipelines by mapping

 iteration to processor provided and

 are positive constants Such a scheme would create pipelines with

 relaxed wavefronts between and both exclusive



 General Theory



 The example just completed

 illustrates the following general theory underlying

 pipelining if we can come_up with at_least two

 different outermost_loops for a

 loop_nest and satisfy all the dependences then we can pipeline the

 computation A loop with outermost_fully permutable_loops has

 degrees of pipelined_parallelism



 Loops that cannot be pipelined do_not have alternative

 outermost_loops Example_exnsync1 shows one such instance

 To honor all the dependences each iteration in the outermost_loop

 must execute precisely the computation found in the original code

 However such code may still contain parallelism in the inner_loops

 which can be_exploited by introducing at_least synchronizations

 where is the number of iterations in the outermost_loop



 figurehtfb



 verbatim

 for_(i 0 i_100 i)

 for_(j 0 j_100 j)

 Xj Xj Yij (s1)

 Zi XAi (s2)



 verbatim



 center

 (a)

 center



 fileuullmanalsuch11figssynch-pipeeps



 center

 (b)

 center



 A sequential outer_loop (a) and its PDG

 (b)

 synch-pipe-fig

 figure



 ex

 exnsync1

 Figure synch-pipe-fig is a more_complex version of the problem we

 saw in Example_exscc2

 As shown in the program dependence graph in

 Fig synch-pipe-fig(b) statements and belong to the

 same strongly_connected component Because we do_not know the

 contents of matrix we must assume that the access in statement

 may read from any of the elements of There is a true

 dependence from statement to statement and an

 antidependence from statement to statement There is no

 opportunity for pipelining either because all operations belonging to

 iteration in the outer_loop must_precede those in iteration

 To_find more parallelism we repeat the parallelization process on the

 inner_loop The iterations in the second loop can be_parallelized

 without_synchronization Thus 200 barriers are needed with one

 before and one after each execution of the inner_loop

 ex



 Time-Partition_Constraints

 time-part-subsect



 We_now focus_on the problem of finding pipelined_parallelism Our

 goal is to turn a computation into a set of pipelinable tasks To

 find_pipelined parallelism we do_not solve directly for what is to be

 executed on each processor like we did with loop parallelization

 Instead we ask the following fundamental question What are all the

 possible_execution sequences that honor the original data_dependences

 in the loop Obviously the original execution sequence satisfies all

 the data_dependences The question is if there are affine transformations

 that can create an alternative schedule where iterations of the

 outermost_loop execute a different set of operations from the original

 and yet all the dependences are satisfied If we can find such

 transforms we can pipeline the loop The key point is that if there is

 freedom in scheduling operations there is parallelism details

 of how we derive pipelined_parallelism from such transforms will be

 explained later



 To_find acceptable reorderings of the outer

 loop we_wish to find one-dimensional affine transforms one

 for each statement that map the original loop_index values to an

 iteration number in the outermost_loop The transforms are legal if the

 assignment can satisfy all the data_dependences in the program The

 time-partition_constraints shown below simply say that if one

 operation is dependent upon the other then the first must_be assigned

 an iteration in the outermost_loop no earlier than that of the second

 If they are assigned in the same iteration then it is understood that

 the first will be executed after than the second within the iteration



 An affine-partition mapping of a program is a legal-time

 partition if and only if for every two (not_necessarily distinct)

 accesses_sharing a dependence say



 F1 F1 f1 B1 b1

 in statement which is nested in loops

 and



 F2 F2 f2 B2 b2



 in statement_nested in loops

 the one-dimensional partition_mappings

 and

 for statements and

 respectively satisfy the time-partition_constraints



 itemize

 For all in and in such

 that

 itemize

 a)



 b)



 c)

 and

 d)



 itemize

 it is the case that





 itemize



 This constraint illustrated in Fig figtime looks

 remarkably similar to the space-partition_constraints It is a

 relaxation of the space-partition_constraints in that if two iterations

 refer to the same_location they do_not necessarily have to be mapped

 to the same partition we only require that the original relative

 execution order between the two iterations is preserved

 That is the constraints here have where the space-partition

 constraints have





 figurehtfb



 fileuullmanalsuch11figstimeeps

 Time-Partition_Constraints

 figtime



 figure



 We know that there_exists at_least one solution to the time-partition

 constraints We can map operations in each iteration of the outermost

 loop back to the same iteration and all the data_dependences will be

 satisfied This solution is the only solution to the time-partition

 constraints for programs that cannot be pipelined On the other_hand if

 we can find several independent_solutions to time-partition

 constraints the program can be pipelined Each

 independent solution corresponds to a loop in the outermost_fully

 permutable nest For_instance there is only one

 independent solution to the timing_constraints extracted_from the

 program in Example_exnsync1 where there is no pipelined

 parallelism As_another instance there are two independent_solutions to the

 SOR_code example of Section ch11sor



 ex

 exnsync1p2

 Let_us consider Example_exnsync1 and in particular the data

 dependences of references to array in statements and

 Because the access is not affine in statement we approximate

 the access by modeling matrix simply as a scalar_variable in

 dependence analysis involving statement Let

 be the index value of a dynamic_instance of and let be

 the index value of a dynamic_instance of Let the computation

 mappings of statements and be

 and respectively



 Let_us first consider the time-partition_constraints imposed_by

 dependences from statement to Thus

 the transformed_th

 iteration of must_be no later than the transformed

 th_iteration of that is





 arrayrr

 C11_C12



 array





 arrayr

 i



 j



 array





 c1

 C21_i' c2



 Expanding we get



 C11 i C12j c1_C21 i'_c2



 Since can be arbitrarily_large independent of and it must

 be that

 Thus one possible solution to the constraints is



 C11_C21 1 and C12 c1_c2 0





 Similar arguments about the data_dependence from to and

 back to itself

 will yield a similar answer In this particular solution the th

 iteration of the outer_loop which consists of the instance of

 and all instances of are all assigned to timestep

 Other legal choices of and yield

 similar assignments although there might be timesteps at which nothing

 happens

 That is all ways to schedule the outer_loop

 require the iterations to execute in the same order as in the original code

 This statement holds whether all 100 iterations are executed on the same

 processor on 100 different_processors or anything in between

 ex



 ex

 expipelinesoln

 In the SOR_code shown in Fig figsor(a) the write_reference

 shares a dependence with itself and with the three read

 references in the code We are

 seeking computation mapping

 for the assignment_statement such that





 arrayrr

 C1 C2

 array





 arrayr

 i



 j



 array







 arrayr

 c



 array





 arrayrr

 C1 C2

 array





 arrayr

 i'



 j'



 array







 arrayr

 c



 array





 if there is a dependence from to By definition

 that is either or





 Let_us consider three of the pairs of data_dependences

 enumerate

 True dependence from write access to read access

 Since the instances must access the same_location

 or Substituting into the timing_constraints

 we get



 C1 (i'-i) - C2_0



 Since the precedence constraints reduce to

 Therefore



 C1 - C2_0





 Antidependence from read access to write access

 Here or

 Substituting into the timing_constraints

 we get



 C1 (i'-i) C2_0



 When we get



 C2_0



 When since we get



 C1 0





 Output dependence from write access back to itself

 Here The timing_constraints reduce to



 C1 (i'-i) 0



 Since only is relevant we again get



 C1 0



 enumerate



 The rest of the

 dependences do_not yield any new constraints

 In total there are three constraints



 arrayc

 C1 0



 C2_0



 C1 - C2_0

 array



 Here are two independent_solutions to these constraints





 arrayr

 1



 0



 array





 arrayr

 1



 1



 array





 The first solution preserves the execution order of the iterations in

 the outermost_loop Both the original SOR_code in

 Fig figsor(a) and the transformed code shown in

 Fig figsorfp(a) are examples of such an arrangement The

 second solution places iterations lying along the

 diagonals in the same outer_loop The code shown in

 Fig figsorfp(b) is an example of a code with that outermost

 loop composition



 Notice_that there are many other possible pairs of

 independent_solutions For_example





 arrayr

 1



 1



 array





 arrayr

 2



 1



 array





 would also be independent_solutions to the same constraints

 We choose the simplest vectors to simplify code transformation

 ex



 Solving Time-Partition_Constraints by Farkas'Lemma

 solve-time-part-subsect



 Since time-partition_constraints are similar to

 space-partition_constraints can we use a similar algorithm to solve

 them

 Unfortunately the slight difference_between the two problems

 translates_into a big technical difference_between the two solution

 methods

 Algorithm_algnosync simply solves for

 and such that for all in

 and in if



 F1_i1 f1_F2 i2_f2

 then



 C1i1_c1 C2i2_c2



 The linear_inequalities due to the loop_bounds are only used in

 determining if two references share a data_dependence and are not used

 otherwise



 To_find solutions to the time-partition_constraints we cannot ignore

 the linear_inequalities ignoring them

 often would allow only the trivial_solution of placing all iterations in the

 same partition Thus the algorithm to find solutions to the

 time-partition_constraints must handle both equalities and

 inequalities



 The general problem we_wish to solve is given a matrix A

 find a vector c such that for all vectors x such that

 it is the case that



 In other_words we are seeking c such that the inner product of

 c and any coordinates in the polyhedron defined by the

 inequalities always yields a

 nonnegative answer



 This problem is addressed by Farkas'_Lemma

 Let A be an matrix of reals

 and let c be a real nonzero

 -vector Farkas'_lemma says_that either the primal_system of

 inequalities



 Ax_0 cT x 0



 has a real-valued_solution x or the dual_system



 AT_y c y 0



 has a real-valued_solution y but never both



 The dual_system can be handled by using Fourier-Motzkin_elimination to

 project_away the variables of y For each c that has a

 solution in the dual_system the lemma guarantees that there are no

 solutions to the primal_system Put_another way we can prove the

 negation of the primal_system ie we can prove that



 for all x such that by finding a

 solution y to the dual_system

 and



 About Farkas'_Lemma

 The proof of

 the lemma can be found in many standard texts on linear_programming

 Farkas'_Lemma originally proved in 1901

 is one of the theorems of the alternative

 These

 theorems are all equivalent but despite attempts over the years a

 simple intuitive proof for this lemma or any of its equivalents has

 not been_found



 alg

 algsync

 Finding a set of legal maximally independent affine_time-partition mappings

 for an outer sequential loop



 A loop_nest with array_accesses



 A maximal_set of linearly_independent time-partition_mappings



 The following steps constitute the algorithm



 enumerate



 Find all data-dependent pairs of accesses in a program



 For each pair of data-dependent accesses



 in statement_nested in loops

 and



 in statement_nested in loops

 let and be the

 (unknown) time-partition

 mappings of statements and respectively

 Recall the time-partition_constraints state that



 itemize

 For all in and in such

 that

 itemize

 a)



 b)



 c)

 and

 d)



 itemize

 it is the case that





 itemize

 Since

 is a disjunctive union of a

 number of clauses we can create a system of constraints for each clause

 and solve each of them separately as_follows

 enumerate

 Similarly to step (trick-item) in Algorithm_algnosync apply

 Gaussian_elimination to

 the equations



 F1_i1 f1_F2 i2_f2



 to reduce the vector





 arrayc

 i1



 i2



 1



 array





 to some vector of unknowns x

 Let c be all the unknowns in the partition_mappings

 Express the linear inequality constraints_due to the partition

 mappings as



 cTDx 0



 for some matrix D

 Express the precedence constraints on the loop_index variables and

 the loop_bounds as



 Ax_0



 for some matrix A

 Apply Farkas'_Lemma Finding x to satisfy the two constraints

 above is equivalent to finding y such that



 AT_y DTc and y 0



 Note_that here is in the

 statement of Farkas'_Lemma and we are using the negated form of the

 lemma

 In this form apply Fourier-Motzkin_elimination to project_away

 the variables and express the constraints on the

 coefficients c as

 Let

 be the system without the constant terms

 enumerate



 Find a maximal_set of linearly_independent solutions to

 using Algorithm figtime-math in

 Appendix time-math-ch

 The approach of that complex algorithm

 is to keep_track of the current set of solutions for each

 of the statements then incrementally look for more independent

 solutions by inserting constraints that force the solution to be

 linearly_independent for at_least one statement



 From each solution of found derive one affine

 time-partition mapping The constant terms are

 derived using

 enumerate

 alg



 ex

 The constraints for Example_exnsync1p2 can be written as





 arrayrrrr

 -C11_-C12 C21 (c2-c1)



 array





 arrayc

 i



 j



 i'



 1



 array



 0









 arrayrrrr

 -1_0 1_0

 array





 arrayc

 i



 j



 i'



 1



 array



 0





 Farkas'_lemma says_that these constraints are equivalent to





 arrayr

 -1



 0



 1



 0



 array





 arrayr

 z



 array







 arrayr

 -C11



 -C12



 C21



 c2-c1



 array

 and

 z 0



 Solving this system we get



 C11_C21 0 and

 C12 c2-c1 0



 Notice_that these constraints are satisfied by the particular

 solution we obtained

 in Example_exnsync1p2

 ex



 Code Transformations

 ch11fpn



 If there exist independent_solutions to the time-partition

 constraints of a loop_nest then it is possible to transform the loop

 nest to have outermost_fully permutable_loops which can be

 transformed to create degrees of pipelining or to create

 inner parallelizable loops Furthermore we can apply_blocking to

 fully_permutable loops to improve data_locality of uniprocessors as

 well_as reducing synchronization among processors in a parallel

 execution



 Exploiting Fully_Permutable Loops

 We can create a loop_nest with outermost_fully permutable_loops

 easily from independent_solutions to the time-partition

 constraints We can do so by simply making the th solution the

 th_row of the new transform Once the affine_transform is created

 Algorithm algsimpleSPMD can be used to generate the code



 ex

 The solutions found in Example expipelinesoln for our SOR

 example were





 arrayr

 1



 0



 array





 arrayr

 1



 1



 array





 Making the first solution the first row and the second solution the

 second row we get the transform





 arrayc c

 1_0



 1 1



 array





 which yields the code in

 Fig figsorfp(a)



 Making the second solution the first row instead we get the transform





 arrayc c

 1 1



 1_0



 array





 which yields the code in Fig figsorfp(c)

 ex



 It is easy to see that such transforms produce a legal sequential

 program The first row partitions the entire iteration_space

 according to the first solution The timing_constraints guarantee

 that such a decomposition does_not violate any data_dependences

 Then we partition the iterations in each of the outermost_loop

 according to the second solution Again this must_be legal because

 we are dealing with just subsets of the original iteration_space The

 same goes for the rest of the rows in the matrix Since we can order

 the solutions arbitrarily the loops are fully_permutable



 Exploiting Pipelining



 We can easily transform a loop with outermost_fully permutable

 loops into a code with degrees of pipeline parallelism



 ex

 Let_us return to our SOR_example After the loops are transformed to

 be fully_permutable we know that iteration can be

 executed provided iterations and have

 been executed We can guarantee this order in a pipeline as_follows

 We assign iteration to processor Each_processor executes

 iterations in the inner_loop in the original_sequential order thus

 guaranteeing that iteration executes after

 In_addition we require that processor_waits for the signal from

 processor that it has executed iteration before it

 executes iteration This technique generates the pipelined

 code Fig_signal-fig(a) and (b) from the fully_permutable loops

 Fig figsorfp(a) and (c) respectively

 ex



 In_general given outermost_fully permutable_loops

 the iteration with index values

 can be executed

 without violating data-dependence constraints provided iterations



 i1-1 i2 ik

 i1 i2-1 i3 ik

 i1 ik-1 ik-1



 have_been

 executed

 We can thus assign the partitions of the first dimensions of the

 iteration_space to processors as_follows Each_processor

 is responsible_for one set of iterations whose indexes agree in the

 first dimensions and vary over all values of the th index

 Each_processor

 executes the iterations in the th loop sequentially

 The processor corresponding to values for the

 first loop_indexes can execute iteration in the th loop

 as_long as it receives a signal

 from processors



 p1-1 p2 pk-1 p1

 pk-2 pk-1-1



 that they have executed their th_iteration in the th loop



 Wavefronting

 It is also easy to generate inner parallelizable loops from a

 loop with outermost_fully permutable_loops Although pipelining

 is preferable we include this information here for completeness



 We partition the computation of a loop with outermost_fully

 permutable_loops using a new index variable where is

 defined to be some combination of all the indices in the

 permutable_loop nest For_example is one

 such combination



 We create an outermost sequential loop that iterates through the

 partitions in increasing order the computation nested_within each

 partition is ordered as before The first loops within each

 partition are guaranteed to be parallelizable Intuitively if given

 a two-dimensional iteration_space this transform groups iterations

 along diagonals as an execution of the outermost_loop

 This strategy

 guarantees that iterations within each iteration of the outermost

 loop have no data_dependence



 Blocking



 A -deep_fully permutable_loop nest can be blocked in

 -dimensions Instead of assigning the iterations to processors based

 on the value of the outer or inner_loop indexes we

 can aggregate blocks of iterations into one_unit Blocking is useful

 for enhancing data_locality as_well as for minimizing the overhead of

 pipelining



 figure



 verbatim

 for_(i0 in i)

 for (j1 jn_j)

 S



 verbatim



 center

 (a) A simple loop_nest

 center



 verbatim

 for (ii 0 iin ib)

 for (jj 0 jjn jjb)

 for_(i iib i min(iib-1 n) i)

 for_(j iib j min(jjb-1 n) j)

 S



 verbatim



 center

 (b) A blocked_version of this loop_nest

 center



 A 2-dimensional loop_nest and its blocked_version

 blocked-code-fig



 fileuullmanalsuch11figstileeps

 Execution order before and after blocking a 2-deep_loop nest

 figtile



 figure



 Suppose we have a two-dimensional fully_permutable loop_nest as in

 Fig blocked-code-fig(a) and we

 wish to break the computation into blocks The

 execution order of the blocked code is shown in

 Fig figtile and the equivalent code is in

 Fig blocked-code-fig(b)



 If we assign each block to one processor then all the passing of data

 from one iteration to another that is within a block requires no

 interprocessor communication

 Alternatively

 we can coarsen the granularity of pipelining by assigning a column

 of blocks to one processor Notice_that

 each processor synchronizes with its

 predecessors and successors only at block boundaries Thus another

 advantage of blocking is that programs

 only need to communicate data

 accessed at the boundaries of the block with their neighbor blocks

 Values that are interior to a block are managed by only one processor



 ex

 We_now use a real numerical algorithm -

 Cholesky_decomposition - to illustrate_how Algorithm_algsync handles

 single loop_nests with only pipelining parallelism

 The code shown in Fig cholesky-fig implements

 an algorithm operating on a 2-dimensional data array The

 executed iteration_space is a triangular pyramid since only

 iterates up to the value of the outer_loop index and

 only_iterates to the value of The loop has four statements all

 nested in different loops



 figure



 verbatim

 for_(i 1_i N_i)

 for_(j 1_j i-1 j)

 for (k 1 k j-1 k)

 Xij_Xij - Xik Xjk

 Xij_Xij Xjj



 for (m 1 m i-1 m)

 Xii Xii - Xim Xim

 Xii sqrt(Xii)



 verbatim



 Cholesky_decomposition

 cholesky-fig



 verbatim

 for (i2 1 i2 N i2)

 for (j2 1 j2 i2 j2)

 beginning of code for processor_(i2j2)

 for (k2 1 k2 i2 k2)



 Mapping_i2 i_j2 j_k2 k

 if (j2i2 k2j2)

 Xi2j2_Xi2j2 -_Xi2k2 Xj2k2



 Mapping_i2 i_j2 j_k2 j

 if (j2k2 j2i2)

 Xi2j2_Xi2j2 Xj2j2



 Mapping_i2 i_j2 i_k2 m

 if_(i2j2 k2i2)

 Xi2i2 Xi2i2 -_Xi2k2 Xi2k2



 Mapping_i2 i_j2 i_k2 i

 if_(i2j2 j2k2)

 Xk2k2 sqrt(Xk2k2)



 ending of code for processor_(i2j2)



 verbatim



 Figure cholesky-fig written as a fully_permutable loop

 nest

 cholesky-nest-fig



 figure



 Applying Algorithm_algsync to this program finds three

 legitimate time dimensions It nests all the operations some of

 which were_originally nested in 1- and 2-deep_loop nests into a

 3-dimensional fully_permutable loop_nest The code together_with the

 mappings is shown in Fig cholesky-nest-fig



 The code-generation routine guards the execution of the operations

 with the original loop_bounds to ensure_that the new programs execute

 only operations that are in the original code We can pipeline this

 code by mapping the 3-dimensional structure to a 2-dimensional

 processor space Iterations are assigned to the processor

 with ID Each_processor executes the innermost_loop the loop

 with the index

 Before it

 executes the th_iteration the processor_waits for signals from

 the processors with ID's and After it executes its

 iteration it signals processors and

 ex



 Parallelism With Minimum Synchronization

 ch11par



 We have described three powerful parallelization algorithms in the

 last three sections Algorithm_algnosync finds all parallelism

 requiring no synchronizations Algorithm_alg1sync finds all

 parallelism_requiring only a constant number of synchronizations and

 Algorithm_algsync finds all the pipelinable parallelism

 requiring synchronizations where is the number of

 iterations in the outermost_loop As a first approximation our goal

 is to parallelize as much of the computation as possible while

 introducing as little synchronization as necessary



 Algorithm_algpar below finds all the degrees of parallelism in a

 program starting_with the coarsest granularity of parallelism In

 practice to parallelize a code for a multiprocessor we do_not need

 to exploit all the levels of parallelism just the outermost possible

 ones until all the computation is parallelized and all the processors

 are fully utilized



 alg

 algpar

 Find all the degrees of parallelism in a program with all the

 parallelism being as coarse-grained as possible



 A program to be_parallelized



 A parallelized version of the same program



 Do the following



 enumerate



 Find the maximum_degree of parallelism_requiring no synchronization

 Apply_Algorithm algnosync to the program



 Find the maximum_degree of parallelism that requires

 synchronizations_Apply Algorithm_alg1sync to each of the space

 partitions_found in step 1 (If no_synchronization-free parallelism

 is found the whole computation is left in one partition)



 Find the maximum_degree of parallelism that requires

 synchronizations_Apply Algorithm_algsync to each of the

 partitions_found in step 2 to find_pipelined parallelism Then apply

 Algorithm_alg1sync to each of the partitions assigned to each

 processor or the body of the sequential loop if no pipelining is

 found



 Find the maximum_degree of parallelism with successively greater

 degrees of synchronizations Recursively apply Step_3 to computation

 belonging to each of the space partitions generated_by the previous

 step

 enumerate

 alg



 ex

 Let_us now return to Example_exnsync1 No parallelism is found

 by Steps 1 and 2 of Algorithm_algpar that is we need more

 than a constant number of synchronizations to parallelize this code

 In Step_3 applying_Algorithm algsync determines that there is

 only one legal outer_loop which is the one in the original code of

 Fig synch-pipe-fig So the loop has no pipelined_parallelism

 In the second part of Step_3 we apply_Algorithm alg1sync to

 parallelize the inner_loop We treat the code within a partition like

 a whole program the only_difference being that the partition number

 is treated_like a symbolic_constant In this case the inner_loop is

 found to be parallelizable and therefore the code can be_parallelized

 with synchronization_barriers

 ex



 Algorithm_algpar finds all the parallelism in a program at each

 level of synchronization The algorithm prefers parallelization

 schemes that have less synchronization but less synchronization does

 not mean that the communication is minimized Here we discuss two

 extensions to the algorithm to address its weaknesses



 Considering Communication Cost



 Step_2 of Algorithm_algpar parallelizes each strongly_connected

 component independently if no_synchronization-free parallelism is

 found However it may be possible to parallelize a number of the

 components without_synchronization and communication One_solution is

 to greedily find synchronization-free_parallelism among subsets of

 the program dependence graph that share the most data



 If communication is necessary between strongly_connected components

 we note_that some communication is more_expensive than_others For

 example the cost of transposing a matrix is significantly higher_than

 just having to communicate between neighboring_processors Suppose

 and are statements in two separate strongly_connected

 components accessing the same data in iterations

 and respectively If we cannot find

 partition_mappings

 and

 for statements

 and respectively

 such that



 C1i1_c1 - C2i2 -

 c2 0



 we instead try to satisfy the constraint



 C1i1_c1 - C2i2 -

 c2



 where is a small constant



 Trading Communication for Synchronization



 Sometimes we would rather perform more synchronizations to minimize

 communication Example expipeline-adi discusses one such

 example Thus if we cannot parallelize a code with just neighborhood

 communication among strongly_connected components we should attempt

 to pipeline the computation instead of parallelizing each component

 independently As shown in Example expipeline-adi pipelining

 can be applied to a sequence of loops



 ex

 expipeline-adi

 For the ADI integration algorithm in Example_exadi we have

 shown that optimizing the first and second loop_nests independently

 finds parallelism in each of the nests However such a scheme would

 require that the matrix be transposed between the

 loops incurring data_traffic

 If we use Algorithm_algsync to find_pipelined parallelism we

 find that we can turn the entire program into a fully_permutable loop

 nest as in Fig another-nest-fig

 We then can apply_blocking to reduce the communication overhead This

 scheme would incur synchronizations but would_require much less

 communication

 ex



 figurehtfb



 verbatim

 for_(j 0 j_n j)

 for_(i 1_i n1 i)

 if (i n) Xij f(Xij Xi-1j)

 if (j 0) Xi-1j g(Xi-1jXi-1j-1)



 verbatim



 A fully_permutable loop_nest for the code of

 Example_exadi

 another-nest-fig



 figure



 exer

 In Section pipe-loop-subsect we discussed the possibility of

 using diagonals other_than the horizontal and vertical axes to pipeline

 the code of Fig figsorfp Write code analogous to the loops of

 Fig signal-fig for the diagonals (a)

 (b)

 exer



 exer

 pascal-exer

 Figure blocked-code-fig(b) can be simplified if divides

 evenly Rewrite the code under that assumption

 exer



 figurehtfb



 verbatim

 for_(i0 i100_i)

 Pi0 1 s1

 Pii 1 s2



 for (i2 i100_i)

 for (j1 ji j)

 Pij Pi-1j-1 Pi-1j s3

 verbatim



 Computing Pascal's triangle

 pascal-code-fig



 figure



 sexer

 In Fig pascal-code-fig is a program to compute the first 100 rows

 of Pascal's triangle That is will become the number of ways

 to choose things out of

 for



 itemize

 a) Rewrite the code as a single fully_permutable loop_nest

 b) Use 100 processors in a pipeline to implement this code

 Write the code for each processor in terms of and indicate the

 synchronization necessary

 c) Rewrite the code using square blocks of 10 iterations on a

 side Since the iterations form a triangle there will be only

 blocks Show the code for a processor

 assigned to the th block in the direction and the th block

 in the direction in terms of and

 itemize



 sexer



 figurehtfb



 verbatim

 for_(i0 i100 1)

 Ai 00 B1i s1

 Ai990 B2i s2



 for (j1 j99 j)

 A 0j0 B3j s3

 A99j0 B4j s4



 for_(i0 i99 i)

 for_(j0 j99 j)

 for (k1 k100 k)

 Aijk (Aijk-1 Ai-1jk-1

 Ai1jk-1 Aij-1k-1

 Aij1k-1)5 s5

 verbatim



 Code for Exercise x1-exer

 x1-fig



 figure



 hexer

 x1-exer

 Repeat_Exercise pascal-exer for the code of Fig x1-fig

 However note_that the iterations for this problem form a 3-dimensional

 cube of side 100 Thus the blocks for part (c) should be

 and there are 1000 of them

 hexer



 hexer

 Let_us apply_Algorithm algsync to a simple example of the

 time-partition_constraints In what_follows assume that the vector

 is and vector is

 technically both these vectors are transposed The condition

 consists of the following

 disjunction



 itemize



 or

 and



 itemize

 The other equalities and inequalities are



 center

 tabularr_c l

 0



 0











 tabular

 center

 Finally the time-partition inequality with unknowns

 and is



 center



 center



 itemize



 a)

 Solve the time-partition_constraints for case - that is where

 In_particular eliminate as many of and

 as you can and set up the matrices and as in

 Algorithm_algsync Then apply Farkas'_Lemma to the resulting

 matrix inequalities



 b)

 Repeat part (a) for the case_where and



 itemize

 hexer

 Programming_Language Basics

 plbasics-sect



 In this_section we_shall cover the most_important terminology and

 distinctions that appear in the study of programming_languages It is not

 our purpose to cover all concepts or all the popular programming

 languages We assume that the reader is familiar with at_least

 one of C C C or Java and may have encountered other languages

 as_well



 The StaticDynamic Distinction

 stat-dyn-subsect



 Among the most_important issues that we face when designing a compiler

 for a language is what decisions can the compiler make about a program

 If a language uses a policy that allows the compiler to decide an issue

 then we say that the language uses a static policy or that the

 issue can be decided at_compile time On the other_hand a

 policy that only allows a decision to be made when we execute the

 program is said to be a dynamic policy or to require a decision

 at_run time



 One issue on which we_shall concentrate is the scope of declarations

 The scope of a declaration of is the region of the program in which

 uses of refer to this declaration A language uses static

 scope or lexical scope if it is possible to determine the

 scope of a declaration by_looking only at the program Otherwise

 the language uses dynamic_scope With dynamic_scope as the

 program_runs the same use of could refer to any of several

 different declarations of



 Most_languages such_as C and Java use static_scope

 We_shall discuss static_scoping in Section static-scope-subsect



 ex

 java-static-ex

 As_another example of the staticdynamic

 distinction consider the use of the term

 static as it applies to data in a Java class declaration

 In Java a variable is a name for a location in memory

 used to hold a data value

 Here static refers not to the scope of the variable but rather

 to the ability of the compiler to determine the location in memory where

 the declared variable can be found

 A declaration like



 verbatim

 public_static int x

 verbatim

 makes a class variable and says_that there is only one copy of

 no_matter how many objects of this class are created

 Moreover the compiler can determine a location in memory where

 this integer will be held In_contrast had static been

 omitted from this declaration then each object of the class would have

 its_own location where would be held

 and the compiler could not determine

 all these places in advance of running the program

 ex



 Environments and States

 two-stage-subsect



 Another important distinction we must make when discussing

 programming_languages is whether changes occurring as the program

 runs affect the values of data elements or affect the

 interpretation of names for that data For_example the execution

 of an assignment such_as xy1 changes the value

 denoted_by the name More_specifically the assignment

 changes the value in whatever location is denoted_by



 It may be less

 clear that the location denoted_by can change at_run time

 For_instance as we discussed in Example java-static-ex

 if is not a static (or class) variable then every object of the

 class has its_own location for an instance of variable

 In that case the assignment to can change any of those instance

 variables depending_on the object to which a method containing that

 assignment is applied



 figurehtfb



 Two-stage mapping from names to values

 two-stage-fig

 figure



 The association of names with locations in memory (the store) and then with

 values can be described by two mappings that change as the program

 runs (see Fig two-stage-fig)



 enumerate



 The environment is a mapping from names to locations

 in the store Since variables refer to locations (l-values in the

 terminology of C)

 we could alternatively define an environment as a mapping from

 names to variables



 The state is a mapping from locations in store to

 their values That is the state maps l-values to their

 corresponding r-values in the terminology of C



 enumerate



 Environments change according to the scope rules of a language



 figurehtfb

 center

 tabularl_l l





 'int i' '' global ''







 'void f('') '



 'int i' '' local ''







 'i 3' '' use of local ''







 ''







 'x i 1' '' use of global ''



 tabular

 center

 Two declarations of the name

 env-change-fig

 figure



 ex

 Consider the C program_fragment in Fig env-change-fig

 Integer is declared a global variable and also declared as a

 variable local to function When is executing the

 environment adjusts so that name refers to the location

 reserved for the that is local to and any use of

 such_as the assignment i 3 shown explicitly refers to

 that location Typically the local is given a place on the

 run-time_stack



 Names Identifiers and Variables

 Although the

 terms name and variable often refer to the same_thing we

 use them carefully to distinguish_between compile-time names and

 the run-time locations denoted_by names



 An identifier is a

 string of characters typically letters or digits that refers to

 (identifies) an entity such_as a data object a procedure a

 class or a type

 All identifiers are names but not all names are identifiers Names can

 also be expressions For

 example the name might denote the field of a structure

 denoted_by Here and

 are identifiers while is a name but not an

 identifier Composite names like are called qualified names



 A variable refers to a particular location of the store It

 is common for the same identifier to be declared more_than once

 each such declaration introduces a new variable Even_if each

 identifier is declared just once an_identifier local to a

 recursive_procedure will refer to different locations of the store

 at different times



 Whenever a function other_than is executing uses of

 cannot refer to the that is local to Uses of name in

 must_be within the scope of some other declaration of An

 example is the explicitly shown statement x i1 which is

 inside some procedure whose definition is not shown The in

 presumably refers to the global

 As in most languages declarations in C must_precede their

 use so a function that

 comes before the global cannot refer to it

 ex



 Procedures Functions and Methods To_avoid

 saying procedures functions or methods each time we_want to

 talk_about a subprogram that may be called we_shall usually refer

 to all of them as procedures The exception is that when

 talking explicitly of programs in languages_like C that have only

 functions we_shall refer to them as functions

 Or if we are discussing a language like Java that has only methods we

 shall use that term instead



 A function generally returns a value of some type (the return

 type) while a procedure does_not return any value C and

 similar languages which have only functions treat procedures as

 functions that have a special return type void to signify no

 return value Object-oriented languages_like Java and C

 use the term methods These can behave like either

 functions or procedures but are associated_with a

 particular class



 The environment and state mappings in Fig two-stage-fig

 are dynamic but there are a few exceptions



 enumerate



 Static versus dynamic binding of names to locations

 Most binding of names to locations is dynamic and we discuss

 several approaches to this binding throughout the section Some

 declarations such_as the global in Fig env-change-fig

 can be given a location in the store once and for all as the

 compiler generates object codefootnoteTechnically the C

 compiler will assign a location in virtual_memory for the global

 leaving it to the loader and the operating_system to

 determine where in the physical_memory of the machine will be

 located However we_shall not worry_about relocation issues

 such_as these which have no impact on compiling Instead we

 treat the address space that the compiler uses for its output code

 as if it gave physical_memory locationsfootnote



 Static versus dynamic binding of locations to values

 The binding of locations to values (the second stage in

 Fig two-stage-fig) is generally dynamic as_well since we

 cannot_tell the value in a location until we run the program

 Declared constants are an exception For_instance the C

 definition

 verbatim

 define ARRAYSIZE 1000

 verbatim

 binds the name ARRAYSIZE to the value 1000 statically We

 can determine this binding by_looking at the statement and we

 know that it is impossible for this binding to change when the

 program executes



 enumerate



 Static Scope and Block Structure

 static-scope-subsect



 Most_languages including C and its family use static_scope The

 scope rules for C are based_on program structure the scope of a

 declaration is determined implicitly by where the declaration

 appears in the program Later languages such_as C Java and

 C also

 provide explicit control over scopes through the use of keywords

 like public private and protected



 In this_section we consider static-scope rules for a language with

 blocks where a block is a grouping of declarations and

 statements C uses braces '' and '' to delimit a

 block the alternative use of begin and end for the

 same purpose dates_back to Algol



 ex

 To a first approximation the C static-scope policy is as_follows



 enumerate



 A C program consists of a sequence of top-level declarations

 of variables and functions



 Functions may have variable declarations within them where

 variables include local_variables and parameters The scope of

 each such declaration is restricted to the function in which it

 appears



 The scope of a top-level declaration of a name consists

 of the entire program that follows with the exception of those

 statements that lie within a function that also has a declaration of



 enumerate



 The additional detail regarding the C static-scope policy deals

 with variable declarations within statements We examine such

 declarations next and in Example block-structure-ex

 ex



 In C the syntax of blocks is given by



 enumerate



 One type of statement is a block Blocks can appear anywhere that other

 types of statements such_as assignment statements can appear



 A block is a sequence of declarations followed_by a sequence of statements

 all surrounded_by braces



 enumerate



 Note_that this syntax allows blocks to be nested inside each

 other This nesting property is referred to as block

 structure The C family of languages has block structure except

 that a function may not be defined inside another function



 We_say that a declaration belongs to a block if is

 the most_closely nested block containing that is is

 located within but not within any block that is nested_within





 The static-scope rule for variable declarations in

 block-structured languages is as_follows If declaration of

 name belongs to block then the scope of is all of

 except for any blocks nested to any depth within

 in which is redeclared Here is redeclared in if

 some other declaration of the same name belongs to



 An_equivalent way to express this rule is to focus_on a use of a

 name Let be all the blocks that surround

 this use of with the smallest nested_within

 which is nested_within and so on Search for the

 largest such that there is a declaration of belonging to

 This use of refers to the declaration in

 Alternatively this use of is within the scope of the declaration in





 figurehtfb

 Blocks

 in a C program block-structure-fig

 figure



 ex

 block-structure-ex The C program in

 Fig block-structure-fig has four blocks with several

 definitions of variables and As a memory aid each

 declaration initializes its variable to the number of the block to

 which it belongs



 For_instance consider the declaration int a 1 in block

 Its scope is all of except for those blocks nested

 (perhaps deeply) within that have their own declaration of

 nested immediately_within does_not have a

 declaration of but does does_not have a

 declaration of so block is the only place in the entire

 program that is outside the scope of the declaration of the name

 that belongs to That is this scope includes and

 all of except for the part of that is within

 The scopes of all five declarations are summarized in

 Fig bs-scopes-fig



 figurehtfb



 center

 tabularll

 Declaration Scope



 int a 1



 int b 1



 int b 2



 int a 3



 int b 4



 tabular

 center



 Scopes of declarations in

 Example block-structure-ex bs-scopes-fig



 figure



 From another point of view let_us consider the output statement

 in block and bind the variables and used there to

 the proper declarations The list of surrounding blocks in order of

 increasing size

 is Note_that does_not surround the

 point in question has a declaration of so it is to

 this declaration that this use of refers and the value of

 printed is 4 However does_not have a declaration of

 so we next look_at That block does_not have a declaration

 of either so we proceed to Fortunately there is a

 declaration int a 1 belonging to that block so the value

 of printed is 1 Had there been no such declaration the

 program would have_been erroneous

 ex



 Explicit Access Control



 Classes and structures introduce a new scope for their members If

 p is an object of a class with a field (member) then the use

 of in refers to field in the class

 definition

 In analogy with block structure the scope of a member

 declaration in a class extends to any subclass

 except if has a local declaration of the same name



 Through the use of keywords_like public private and

 protected object-oriented languages such_as C or Java provide

 explicit control over access to member names in a superclass

 These keywords support encapsulation by restricting access

 Thus private names are purposely given a scope that includes only

 the method declarations and definitions associated_with that class

 and any friend classes (the C term) Protected names are accessible to

 subclasses Public names are accessible from outside the class



 In C a class definition may be separated from the definitions

 of some or all of its methods Therefore a name associated

 with the class may have a region of the code that is outside

 its scope followed_by another region (a method definition) that

 is within its scope In_fact regions inside and outside the scope

 may alternate until all the methods have_been defined



 Declarations and Definitions The apparently

 similar terms declaration and definition for

 programming-language concepts are actually quite different

 Declarations tell_us about the types of things while definitions

 tell_us about their values Thus int i is a declaration of

 while i 1 is a definition of



 The difference is more significant when we deal_with methods or

 other procedures In C a method is declared in a class

 definition by giving the types of the arguments and result of the

 method (often called the signature for the method) The

 method is then defined ie the code for executing the method is

 given in another place Similarly it is common to define a

 C function in one file and declare it in other files where the

 function is used



 Dynamic Scope

 dyn-subsect



 Technically any scoping policy is dynamic if it is based_on

 factor(s) that can be known only when the program executes The

 term dynamic_scope however usually refers to the following

 policy a use of a name refers to the declaration of in

 the most_recently called not-yet-terminated procedure with such a declaration

 Dynamic scoping of this type appears only in special situations

 We_shall consider two examples of dynamic policies macro expansion

 in the C preprocessor and method resolution in object-oriented

 programming



 ex

 In the C program of Fig dyn-scope-fig identifier

 is a macro that stands_for expression But what is

 We cannot resolve statically that is in terms of the

 program text



 figurehtfb

 center

 tabularl

 define a (x1)



 int x 2



 void b() int x 1 printf(

 void c() printf(

 void_main() b() c()

 tabular

 center

 A macro whose names must_be scoped dynamically

 dyn-scope-fig

 figure





 In_fact in order to interpret we must use the usual

 dynamic-scope rule We examine all the function calls that are

 currently active and we take the most_recently called function

 that has a declaration of It is to this declaration that the

 use of refers



 In the example of Fig dyn-scope-fig the function main first calls function As executes it prints the

 value of the macro Since must_be substituted for

 we resolve this use of to the declaration int x1 in

 function The_reason is that has a declaration of so

 the in the printf in refers to this Thus

 the value printed is 2



 After finishes and is called we again need to print the

 value of macro However the only accessible to is the

 global The printf statement in thus refers to this

 declaration of and value 3 is printed

 ex



 Analogy Between Static and Dynamic Scoping While

 there could be any number of static or dynamic policies for

 scoping there is an interesting relationship_between the normal

 (block-structured) static_scoping rule and the normal dynamic

 policy In a sense the dynamic rule is to time as the static rule

 is to space While the static rule asks us to find the declaration

 whose unit (block) most_closely surrounds the physical location of

 the use the dynamic rule asks us to find the declaration whose

 unit (procedure invocation) most_closely surrounds the time of the

 use



 Dynamic scope resolution is also essential for polymorphic

 procedures those that have two or_more definitions for the same

 name depending only on the types of the arguments In some

 languages such_as ML (see_Section ml-intro-subsect) it is

 possible to determine statically types for all uses of names in

 which case the compiler can replace each use of a procedure name

 by a reference to the code for the proper procedure However

 in other languages such_as Java and C there are times when the

 compiler cannot make that determination



 ex

 A distinguishing feature of object-oriented_programming is the

 ability of each object to invoke the appropriate method in

 response to a message In other_words the procedure called when

 is executed depends_on the class of the object denoted_by

 at that time A_typical example is as_follows



 enumerate



 There is a class with a method_named



 is a subclass of and has its_own method_named





 There is a use of of the form where is an

 object of class



 enumerate

 Normally it is impossible to tell at_compile time whether

 will be of class or of the subclass If the method

 application occurs several_times it is highly likely that some

 will be on objects denoted_by that are in class but not

 while others will be in class It is not until run-time

 that it can be decided which definition of is the right one

 Thus the code generated_by the compiler must determine the class

 of the object and call one or the other method_named

 ex



 Parameter Passing Mechanisms

 param-subsect



 All programming_languages have a notion of a procedure but they can

 differ in how these procedures get their arguments

 In this_section we_shall consider how the

 actual_parameters (the parameters used in the call of a procedure) are

 associated_with the formal_parameters (those used in the

 procedure definition) Which mechanism is used determines how the

 calling-sequence code treats parameters The great majority of

 languages use either call-by-value or call-by-reference

 or both We_shall explain these terms and another method known_as

 call-by-name that is primarily of historical interest



 Call-by-Value



 In call-by-value the actual_parameter is evaluated (if it

 is an expression) or copied (if it is a variable)

 The value is placed in the location belonging to the corresponding formal

 parameter of the called procedure

 This method is used

 in C and Java and is a common option in C as_well as in most other

 languages Call-by-value has the effect that all computation

 involving the formal_parameters done by the called procedure is

 local to that procedure and the actual_parameters themselves

 cannot be changed



 Note_however that in C we can pass a pointer

 to a variable to allow that variable to be changed by the callee

 Likewise array names passed as parameters in C C or Java give the

 called procedure what is in effect a pointer or reference to the array

 itself Thus if is the name of an array of the calling_procedure and

 it is passed by value to corresponding formal_parameter then an

 assignment such_as xi 2 really changes the array_element

 to 2

 The_reason is that although gets a copy of the value of that value

 is really a pointer to the beginning of the area of the store where the array

 named is located



 Similarly in Java many variables are really references or pointers to

 the things they stand_for This observation applies to arrays strings and

 objects of all classes Even_though Java uses call-by-value exclusively

 whenever we pass the name of an object to a called procedure the value

 received by that procedure is in effect a pointer to the object

 Thus the called procedure is able to affect the value of the object itself



 Call-by-Reference



 In call-by-reference the address of the actual_parameter is

 passed to the callee as the value of the corresponding formal_parameter

 Uses of the formal

 parameter in the code of the callee are implemented_by following

 this pointer to the location indicated by the caller Changes to

 the formal_parameter thus appear as changes to the actual

 parameter



 If the actual_parameter is an expression however then the

 expression is evaluated before the call and its value stored in a

 location of its_own

 Changes to the formal_parameter change the value in this location but can have

 no effect on the data of the caller



 Call-by-reference is used for ref parameters in C and is an

 option in many other languages It is almost essential when the

 formal_parameter is a large object array or structure The

 reason is that strict call-by-value requires that the caller

 copy the entire actual_parameter into the space belonging to the

 corresponding formal_parameter

 This copying gets expensive when

 the parameter is large

 As we noted when discussing call-by-value languages such_as Java solve the

 problem of passing arrays strings or other objects by copying only a

 reference to those objects The effect is that Java behaves as if it used

 call-by-reference for anything other_than a basic type such_as an_integer or

 real



 Call-by-Name



 A third mechanism - call-by-name - was used in the early

 programming_language Algol_60 It requires that the callee execute

 as if the actual_parameter were substituted literally for the

 formal_parameter in the code of the callee as if the formal

 parameter were a macro standing for the actual_parameter (with

 renaming of local names in the called procedure to keep them

 distinct) When the actual_parameter is an expression rather_than

 a variable some unintuitive behaviors occur which is one reason

 this mechanism is not favored today



 Aliasing



 There is an interesting consequence of call-by-reference parameter_passing

 or its simulation as in Java where references to objects are passed by

 value It is possible that two formal_parameters can refer to the same

 location such variables are said to be aliases of one another

 As a result any two variables which may appear to take their

 values from two distinct formal_parameters can become aliases of each

 other as_well



 ex

 alias-ex

 Suppose is an array belonging to a procedure and calls another

 procedure with a call Suppose also that parameters are

 passed by value but that array names are really references to the location

 where the array is stored as in C or similar languages

 Now and have become aliases of each other The important point is

 that if within there is an assignment x10 2 then the value of

 also becomes 2

 ex



 It_turns out that understanding aliasing and the mechanisms that create it

 is essential if a compiler is to optimize a program

 As we_shall see starting in Chapter_code-op-ch there are many

 situations_where we can only optimize code if we can be_sure certain

 variables are not aliased For_instance we might determine that x 2 is the only place that variable is ever assigned

 If so then we

 can replace a use of by a use of 2 for example replace a x3

 by the simpler a 5 But suppose there were another variable

 that was aliased to Then an assignment y 4 might have the

 unexpected effect of changing It might also mean that replacing a x3 by a 5 was a mistake the proper value of could be 7

 there



 figurehtfb



 Block-structured code bs-exer-fig

 figure



 exer

 bs1-exer For the block-structured C code of

 Fig bs-exer-fig(a) indicate the values assigned to

 and

 exer



 exer

 bs2-exer

 Repeat_Exercise bs1-exer for the code of

 Fig bs-exer-fig(b)

 exer



 exer

 scope-exer For the block-structured code of

 Fig scope-exer-fig assuming the usual static_scoping of

 declarations give the scope for each of the twelve declarations

 exer



 figurehtfb

 center

 tabularl

 '_int w x_y z_Block B1 '



 '_int x z_Block B2 '



 '_int w x Block B3 '



 '_'



 '_int w x Block B4 '



 '_int y_z Block B5 '



 '_'



 ''



 tabular

 center

 Block structured code for Exercise scope-exer

 scope-exer-fig



 figure



 exer

 What is printed by the following C code



 verbatim

 define a (x1)

 int x 2

 void b() x a printf(

 void c() int x 1 printf(

 void_main() b() c()

 verbatim

 exer

 A Translator for Simple Expressions

 postfix-sect



 Using the techniques of the last three sections we now construct

 a syntax-directed translator in the form of a working Java

 program that translates arithmetic_expressions into_postfix form

 To keep the initial program manageably small we start with

 expressions consisting of digits separated_by plus and minus

 signs We extend the program in Section lexan-sect to translate

 expressions that include numbers and other operators Since

 expressions appear as a construct in so many languages it is

 worth studying their translation in detail



 A syntax-directed_translation scheme often serves as the

 specification for a translator The scheme in

 Fig_post-scheme2-fig (repeated from

 Fig post-scheme-fig) defines the translation to be

 performed here



 figurehtfb

 center

 tabularr_c l_l

 print



 print











 print



 print







 print



 tabular

 center

 Actions for translating into_postfix notation

 post-scheme2-fig

 figure



 Often the underlying_grammar of a given scheme has to be modified

 before it can be_parsed with a predictive_parser In_particular

 the grammar underlying the scheme in Fig_post-scheme2-fig

 is left-recursive and as we saw in the last section a predictive

 parser cannot handle a left-recursive grammar



 It appears there is a conflict on the one hand we need a grammar

 that facilitates translation on the other_hand we need a

 significantly different grammar that facilitates parsing The

 solution is to begin_with the grammar for easy translation and

 carefully transform it to facilitate parsing By eliminating the

 left-recursion in Fig_post-scheme2-fig we can obtain a

 grammar suitable for use in a predictive recursive-descent

 translator



 Abstract and Concrete Syntax

 absyn-subsect



 A useful starting point for designing a translator is a data

 structure called an abstract_syntax tree In an abstract

 syntax_tree for an expression each interior_node represents an

 operator the children of the node represent the operands of the

 operator More_generally any programming construct can be handled by making

 up an operator for the construct and treating as operands the

 semantically meaningful_components of that construct



 In the abstract_syntax tree for 9-52 in

 Fig syntree-fig the root represents the operator

 The subtrees of the root represent the subexpressions 9-5

 and 2 The grouping of 9-5 as an operand reflects the

 left-to-right evaluation of operators at the same precedence

 level Since - and have the same precedence 9-52 is equivalent to (9-5)2



 figurehtfb

 Syntax

 tree for 9-52 syntree-fig

 figure



 Abstract syntax_trees or simply syntax_trees resemble parse

 trees to an extent

 However in the syntax_tree

 interior_nodes represent programming_constructs while in the parse_tree

 the interior_nodes represent nonterminals

 Many nonterminals of a grammar represent programming_constructs but

 others are helpers of one sort of another such_as those

 representing terms factors or other variations of expressions

 In the syntax_tree these helpers typically become invisible

 To emphasize the contrast a parse_tree

 is sometimes_called a concrete syntax_tree and the underlying_grammar

 is called a concrete syntax for the language



 Comparing the

 syntax_tree in Fig syntree-fig with a corresponding parse

 tree like Fig list-tree-fig we note_that each interior

 node in the syntax_tree is associated_with an operator with no

 helper nodes for single productions (a production whose body

 consists of a single nonterminal and nothing else) like

 or for -productions like





 It is desirable for a translation_scheme to be based_on a grammar

 whose parse_trees are as close to syntax_trees as possible The

 grouping of subexpressions by the grammar in

 Fig_post-scheme2-fig is similar to their grouping in syntax

 trees

 For_example subexpressions of the addition operator are

 given by expr and term in the production_body





 Adapting the Translation Scheme

 adapting-trans-subsect



 The left-recursion-elimination technique sketched in

 Fig leftrec-fig can also be applied to productions

 containing semantic_actions

 First we can use the same idea no_matter how many productions for

 there are

 In our example will be and there are two left-recursive

 productions for and one that is not left-recursive

 The technique transforms the

 productions into



 center

 tabularr_c l







 tabular

 center



 Second we need to transform productions that have embedded actions not

 just terminals and nonterminals

 Semantic actions embedded in the productions are simply carried

 along in the transformation as if they_were terminals



 ex

 Consider the translation_scheme of Fig_post-scheme2-fig

 Let

 print

 print and

 Then the left-recursion eliminating

 transformation produces the translation_scheme in

 Fig post-scheme3-fig The productions in

 Fig_post-scheme2-fig have_been transformed into the

 productions for and a new nonterminal rest plays the role

 of

 The productions for are

 repeated from Fig_post-scheme2-fig

 Figure 952-trans-fig shows_how 9-52 is translated

 using the grammar in Fig post-scheme3-fig

 ex



 figurehtfb

 center

 tabularr_c l

 expr_term rest







 rest term print rest



 - term print rest











 term 0 print



 1 print







 9 print

 tabular

 center

 Translation scheme after left-recursion elimination

 post-scheme3-fig

 figure



 figurehtfb

 fileuullmanalsuch2figs952-transeps

 Translation of 9-52 to 95-2

 952-trans-fig

 figure



 Left-recursion elimination must_be done carefully to ensure_that

 we preserve the ordering of semantic_actions For_example the

 transformed scheme in Fig post-scheme3-fig has the actions

 print and print

 in the middle of the second and third productions before

 nonterminal rest If the actions were to be moved to the

 end after rest then the translations would be incorrect

 We leave it to the reader to show that 9-52 would then be

 translated incorrectly into 952- the postfix_notation for

 9-(52) instead of the desired 95-2 the postfix

 notation for (9-5)2



 Procedures for the Nonterminals

 post-pseudo-subsect



 Functions expr rest and term in

 Fig_post-code-fig are the essence of an implementation of

 the syntax-directed_translation scheme in

 Fig post-scheme3-fig These functions mimic the production

 bodies of the corresponding nonterminals



 figurehtfb

 center

 tabularl

 void expr()



 term() rest()











 void rest()



 if_( lookahead ')



 match term() print rest()







 else if_( lookahead ')



 match term() print rest()







 else do_nothing with the input











 void term()



 if_( lookahead is a digit )



 t lookahead match(lookahead) print







 else syntax error





 tabular

 center

 Pseudo code for nonterminals expr rest and

 term post-code-fig

 figure



 Function expr implements the production

 by the calls term() and rest()



 Function rest implements the three productions for

 nonterminal rest in Fig post-scheme3-fig It applies

 the first production if the lookahead_symbol is a plus sign the

 second production if the lookahead_symbol is a minus_sign and the

 production in all other cases The

 first two productions for rest are implemented_by the first two

 branches of

 the if-statement in procedure rest If the lookahead

 symbol is the plus sign is matched by the call match After the call term() the semantic

 action is implemented_by writing a plus character The second

 production is similar with instead of Since the

 third production for rest has as its right_side

 the last else-clause in function rest does nothing



 The ten productions for generate the ten digits Since

 each of these productions generates a digit and prints it the

 same code in Fig_post-code-fig implements them all If the

 test succeeds variable t saves the digit represented_by

 lookahead so it can be written after the call to match Note_that match changes the lookahead_symbol so the

 digit needs to be saved for later printing(As a minor

 optimization we could print before calling match to avoid

 the need to save the digit In_general changing the order of

 actions and grammar_symbols is risky since it could change

 what the translation does)



 Optimizing the Translator

 leftrec-elim-subsect



 Before showing a complete program we_shall make one

 speed-improving transformation to the code in

 Fig_post-code-fig

 Certain recursive_calls can be replaced_by iterations When the

 last statement executed in a procedure body is a recursive call to

 the same procedure the call is said to be tail-recursive

 For_example in function rest the calls of rest()

 with lookahead and - are tail-recursive because in each

 of these branches

 the recursive call to rest is the last thing that the given

 call of rest does



 We can speed_up a program by_replacing tail-recursion by

 iteration For a procedure without parameters a tail-recursive

 call can be replaced simply by a jump to the beginning of the

 procedure The code for rest can be_rewritten as the

 pseudocode of Fig tail-rec-elim-fig



 figurehtfb



 center

 tabularp4in l

 2lvoid rest()



 L if( lookahead ')



 match term() print goto_L







 else if_( lookahead ')



 match term() print goto_L



 else









 tabular

 center



 Eliminating tail-recursion in the procedure rest of

 Fig_post-code-fig

 tail-rec-elim-fig



 figure



 As_long as the lookahead_symbol is a plus or a minus_sign

 procedure rest matches the sign calls term to match a

 digit and repeats the process by jumping to label at the

 beginning of the procedure rest This pseudocode can be

 rewritten using a while statement in programming_languages like

 Java that do_not permit jumps



 The complete Java program will include one more change Once the

 tail-recursive calls to rest in Fig_post-code-fig are

 replaced_by jumps the only remaining call to rest is from

 the body of procedure expr The two procedures can therefore

 be integrated into one by_replacing the call rest() by the

 body of procedure rest



 The Complete Program



 The complete Java program for our translator appears in

 Fig full-code-fig The first line of

 Fig full-code-fig beginning with import provides

 access to the package javaio for system input and output

 The rest of the code consists of the two classes Parser and

 Postfix Class Parser contains variable lookahead and functions Parser expr_term and

 match



 figuretbp

 verbatim

 import_javaio

 class_Parser

 static_int lookahead



 public Parser() throws_IOException

 lookahead Systeminread()





 void expr() throws_IOException

 term()

 while(true)

 if( lookahead ''_)

 match('') term() Systemoutwrite('')



 else_if( lookahead '-' )

 match('-') term() Systemoutwrite('-')



 else_return







 void term() throws_IOException

 if( CharacterisDigit((char)lookahead) )

 Systemoutwrite((char)lookahead) match(lookahead)



 else throw_new Error(syntax error)





 void match(int t) throws_IOException

 if( lookahead t ) lookahead Systeminread()

 else throw_new Error(syntax error)







 public_class Postfix

 public_static void main(String args) throws_IOException

 Parser parse new Parser()

 parseexpr() Systemoutwrite('



 verbatim

 Java program to translate infix expressions into_postfix

 form full-code-fig

 figure



 A Few Salient Features of Java

 If you are unfamiliar with Java the following are some

 details of Java that will help you follow the code in

 Fig full-code-fig



 itemize



 A class in Java consists of a sequence of variable and function

 definitions



 Parentheses enclosing function parameter lists are

 needed even if there are no parameters hence we write expr() term() and rest() These functions are

 actually procedures because they do_not return values as signified

 by the keyword void before the function name



 Functions

 communicate either by passing parameters by value or by

 accessing shared data For_example the functions term() and

 rest() examine the lookahead_symbol using the variable lookahead that they can all access since they all belong to the

 same class_Parser



 Like C Java uses for assignment for an equality test

 and for an inequality test



 The clause throws_IOException in the definition of term()

 declares that an exception called IOException

 can occur Such an exception occurs if there is no input to be

 read when the function match uses the routine read

 Any function that calls match must also declare that an IOException can occur during its_own execution



 itemize





 Execution begins_with function main which is defined

 in class Postfix

 Function main creates an instance parse of class_Parser and calls its function expr to parse an expression



 The function Parser with the same name as its class is a

 constructor it is called automatically when an object of the

 class is created Notice from its definition at the beginning of class

 Parser that the constructor Parser initializes

 variable lookahead by reading a token Tokens consisting of

 single characters are supplied_by the system input routine read which reads the next_character from the input file Note

 that lookahead is declared to be an_integer rather_than a

 character to anticipate the fact that

 additional tokens other_than single characters will be

 introduced in later sections



 Function expr is the result of the optimization discussed in

 Section leftrec-elim-subsect It is formed from functions

 expr and rest in Fig_post-code-fig by

 eliminating tail recursion from rest and then replacing the

 only remaining call rest() in expr by the body of

 function rest The code for expr in

 Fig full-code-fig calls term and then has a

 while-loop that forever tests whether lookahead matches

 either '' or '-' Control exits from this while-loop

 when it reaches the return statement Within the loop the

 inputoutput facilities of the System class are used to

 write a character



 Function term uses the routine isDigit from the Java

 class Character to test if the lookahead_symbol is a digit

 The routine isDigit expects to be applied to a character

 however lookahead is declared to be an_integer

 anticipating future extensions The construction (char)lookahead casts or forces lookahead to be a

 character In a small change from Fig_post-code-fig the

 semantic_action of writing the lookahead character occurs before

 the call to match



 The function match checks tokens it reads the next_input

 token if the lookahead_symbol is matched and signals an error

 otherwise by executing



 center

 throw_new Error(syntax error)

 center

 This code creates a new exception of class Error and

 supplies it the string syntax error as an error

 message Java does_not require Error exceptions to be

 declared in a throws clause since they are meant to be used

 only for abnormal events that should never occur(Error

 handling can be streamlined using the exception-handling

 facilities of Java One approach is to define a new exception say

 SyntaxError that extends the system class Exception

 Throw SyntaxError instead of Error when an error is

 detected in either term or match Further handle the

 exception in main by enclosing the call parseexpr()

 within a try statement that catches exception SyntaxError writes a message and terminates We would need to

 add a class SyntaxError to the program in Fig

 full-code-fig To complete the extension in addition to IOException functions match and term must now

 declare that they can throw SyntaxError Function expr which calls them must also declare that it can throw SyntaxError)

 Partial-Redundancy Elimination

 secpre



 In this_section we consider in detail how to minimize the

 number of expression evaluations

 That is we_want to consider all possible_execution sequences in a flow

 graph and look_at the number of times an expression such_as is

 evaluated By moving around the places_where is evaluated and

 keeping the result in a temporary_variable when necessary we often

 can reduce the number of evaluations of this expression along many of

 the execution_paths while not increasing that number along any path

 Note_that the number of different places in the flow_graph where

 is evaluated may increase but that is relatively unimportant as_long

 as the number of evaluations of the expression is reduced



 Applying the code transformation developed here improves the

 performance of the resulting code since as we_shall see

 an operation is never applied

 unless it absolutely has to be

 Every optimizing_compiler implements something_like the

 transformation described_here even if it uses a less aggressive

 algorithm than the one of this_section

 However there is another motivation for discussing the problem

 Finding the right place or

 places in the flow_graph at which to evaluate each

 expression requires four different_kinds of data-flow_analyses

 Thus the study of partial-redundancy_elimination as minimizing

 the number of

 expression evaluations is called will enhance our understanding of the

 role data-flow_analysis plays in a compiler



 Redundancy in programs exists in several forms As_discussed in

 Section seccse it may exist in the form of common

 subexpressions where several

 evaluations of the expression produce the same value

 It may also exist in the

 form of a loop-invariant expression that evaluates to the same value

 in every iteration of the loop Redundancy may also be partial

 if it is found along some of the paths but not_necessarily along all paths

 Common subexpressions and loop-invariant_expressions

 can be_viewed as special_cases

 of partial_redundancy thus a single partial-redundancy-elimination

 algorithm can be devised to eliminate all the various forms of

 redundancy



 In the following we first discuss the different forms of redundancy

 in order to build_up our intuition about the problem

 We then

 describe the generalized redundancy-elimination problem and finally we

 present the algorithm This algorithm is particularly interesting

 because it involves solving multiple data-flow_problems in both

 the forward and backward directions



 The Sources of Redundancy



 Figure figpre-intro illustrates the

 three forms of redundancy common_subexpressions

 loop-invariant_expressions and partially_redundant expressions

 The figure shows the code both before and after each optimization

 figurehtb

 figureuullmanalsuch9figspre-introeps

 Examples of (a) global_common subexpression

 (b) loop-invariant code_motion (c) partial-redundancy_elimination

 figpre-intro

 figure



 Global_Common Subexpressions

 In Fig figpre-intro(a) the expression

 computed in block is redundant it

 has already

 been evaluated by the time the flow of control_reaches

 regardless of the path taken to get there As we

 observe in this example the value of the expression may be different

 on different paths We can optimize the code by storing the result of

 the computations of in blocks and

 in the same temporary_variable

 say and then assigning the value of to the variable

 in block

 instead of reevaluating the expression Had there been an

 assignment to either or after the last computation of

 but before block the expression in block would not

 be redundant



 Formally we say that an expression is (fully) redundant

 at point

 if it is an available expression in the sense of

 Section_ae-subsect at that point

 That is the expression has_been computed along all paths

 reaching and the variables and were not

 redefined after the last expression was evaluated

 The latter condition is necessary because even_though the expression

 is textually executed before reaching the point the

 value of computed at point

 would have_been different because the operands might have

 changed



 Finding Deep Common_Subexpressions

 Using available-expressions analysis to identify

 redundant expressions only works for expressions that are

 textually_identical For_example an application of common-subexpression

 elimination will recognize that t1

 in the code_fragment



 center

 t1 b_c a t1 d

 center

 has the same value as does t2 in



 center

 t2 b_c e t2 d

 center

 as_long as the variables and have not been_redefined

 in between It does_not however recognize that and are also

 the same It is possible to find such deep common_subexpressions by

 re-applying common_subexpression elimination until_no new common

 subexpressions are found on one round

 It is also possible to use the framework of Exercise all-eq-exer

 to catch deep common_subexpressions



 Loop-Invariant Expressions



 Fig figpre-intro(b) shows an example of a loop-invariant

 expression The expression is loop_invariant assuming

 neither the variable nor is redefined within the loop

 We can optimize the program by_replacing all the re-executions in a

 loop by a single calculation outside the loop We assign the

 computation to a temporary_variable say and then replace

 the expression in the loop by There is one more point we

 need to consider when performing code_motion optimizations such_as

 this We should not execute any

 instruction that would not have executed without the optimization

 For_example if it is possible to exit the loop without executing the

 loop-invariant

 instruction at all then we should not move the instruction out of the

 loop There_are two_reasons



 enumerate



 If the instruction raises an exception then executing it may throw an

 exception that would not have happened in the original_program



 When the loop exits early the optimized program takes more time

 than the original_program



 enumerate



 To ensure_that loop-invariant_expressions in while-loops can be

 optimized compilers typically represent the statement



 verbatim

 while c

 S



 verbatim

 in the same way as the statement

 verbatim

 if c

 repeat

 S

 until not c



 verbatim

 In this way loop-invariant_expressions can be placed just prior to the

 repeat-until construct



 Unlike common-subexpression_elimination where a redundant expression

 computation is simply dropped loop-invariant-expression elimination_requires

 an expression from inside the loop to move outside the loop Thus

 this optimization is generally known_as loop-invariant code

 motion Loop-invariant code_motion may need to be repeated because

 once a variable is determined to to have a loop-invariant value

 expressions using that variable may also become loop-invariant



 Partially Redundant Expressions



 An_example of a partially_redundant expression is shown in

 Fig figpre-intro(c) The expression in block

 is redundant on the path but not on the

 path We can eliminate the redundancy

 on the former path by placing a computation of in block

 All the results of are written into a temporary

 variable and the calculation in block is replaced with

 Thus like loop-invariant code_motion partial-redundancy_elimination

 requires the placement of new expression computations



 Can All Redundancy Be Eliminated

 secpre-limit



 Is it possible to eliminate all redundant_computations along every

 path The answer is no unless we are allowed to change the flow

 graph by creating new blocks



 ex

 excritical-edge

 In the example shown in Fig figcritical-edge(a) the expression

 of is computed redundantly in block if the

 program follows the execution_path

 However we cannot simply move the computation of to block

 because doing_so would create an extra computation of

 when the path is taken



 What we would like to do is to insert the computation of

 only along the edge from block to block We can do so by

 placing the instruction in a new block say and making the

 flow of control from go_through before it reaches

 The transformation is shown in Fig figcritical-edge(b)

 ex

 figurehtb

 figureuullmanalsuch9figscriticaleps

 is a critical_edge

 figcritical-edge

 figure



 We define a critical_edge of a flow_graph to be any edge leading

 from a node with more_than one successor to a node with more_than one

 predecessor By introducing new

 blocks along critical_edges we can always find a block to

 accommodate the desired expression placement

 For_instance the edge from to in

 Fig figcritical-edge(a) is critical because has two

 successors and has two predecessors



 Adding blocks may not be sufficient to allow the elimination of

 all redundant_computations As shown in Example expre-dup we

 may need to duplicate code so as to isolate the path where redundancy

 is found



 ex

 expre-dup

 In the example shown in Figure figpre-dup(a) the expression of

 is computed redundantly along the path

 We would like to remove the redundant

 computation of from block in this path and

 compute the expression only along the path

 However there is no single program point or edge

 in the source_program that corresponds uniquely to the latter path

 To create such a program point we can duplicate the pair of

 blocks and with one pair reached through and the

 other reached through as shown in

 Figure figpre-dup(b) The result of is saved in

 variable in block and moved to variable in the

 copy of reached from

 ex

 figurehtb

 figureuullmanalsuch9figspre-dupeps

 Code duplication to eliminate redundancies

 figpre-dup

 figure



 Since the number of paths is exponential in the number of

 conditional branches in the program eliminating all redundant

 expressions can greatly increase the size of the optimized code We

 therefore restrict our_discussion of redundancy-elimination

 techniques to those

 that may introduce additional blocks but that do_not duplicate

 portions of the control_flow graph



 The Lazy-Code-Motion Problem

 seclcm-conditions



 It is desirable for programs optimized with a

 partial-redundancy-elimination algorithm to have the following

 properties

 enumerate

 All redundant_computations of expressions that can be_eliminated

 without code duplication are eliminated



 The optimized program does_not perform any computation that is not in

 the original_program execution



 Expressions are computed at the latest possible time

 enumerate

 The last property is important because the values of expressions found to be

 redundant are usually held in registers until they are used

 Computing a value as late as possible minimizes its lifetime -

 the duration between the time the value is defined and

 the time it is last used which in turn minimizes its usage of a

 register

 We refer to the optimization of eliminating

 partial_redundancy with the goal of delaying the computations as much

 as possible as lazy_code motion



 To build_up our intuition of the problem we first discuss_how to

 reason_about partial_redundancy of a single expression along a single

 path For_convenience we assume for the rest of the discussion that

 every statement is a basic_block of its_own



 Full Redundancy



 An expression in block is redundant if along

 all paths_reaching has_been evaluated and the operands of

 have not been_redefined subsequently Let be the set of

 blocks each containing expression that renders in

 redundant The set of edges leaving the blocks in must

 necessarily form a cutset which if removed disconnects block

 from the entry of the program Moreover no operands of are

 redefined along the paths that lead from the blocks in to



 Partial Redundancy



 If an expression in block is only partially_redundant the

 lazy-code-motion algorithm attempts to render fully_redundant in

 by placing additional copies of the expressions in the flow_graph

 If the attempt is successful the optimized flow_graph will also have

 a set of basic_blocks each containing expression and whose

 outgoing_edges are a cutset between the entry and Like the

 fully_redundant case no operands of are redefined along the paths

 that lead from the blocks in to



 Anticipation of Expressions



 There is an additional

 constraint imposed on inserted expressions to ensure_that no extra

 operations are executed Copies of an expression must_be placed

 only at program points where the expression is anticipated We

 say that an expression is anticipated_at point if

 all paths_leading from the point eventually compute the value of

 the expression from the values of and that

 are available at that point



 Let_us now examine what it takes to eliminate partial_redundancy along

 an acyclic_path

 Suppose expression

 is evaluated only in blocks and

 and that the operands of are not redefined in blocks along the

 path There_are incoming_edges that join the path and there are

 outgoing_edges that exit the path We see that is not anticipated

 at the entry of block if and only if there_exists an outgoing

 edge leaving block

 that leads to an execution_path that does_not use the

 value of Thus anticipation limits how early an expression can

 be inserted



 We can create a cutset that includes the edge and

 that renders redundant in

 if

 is either

 available or anticipated_at the entry of If is anticipated

 but not

 available at the entry of we must place a copy of the expression



 along the incoming edge



 We have a choice of where to place the copies of the expression since

 there are usually several cutsets in the flow_graph that satisfy all

 the requirements In the above computation is introduced along the

 incoming_edges to the path of interest and so the expression is

 computed as close to the use as possible without_introducing

 redundancy Note_that these introduced operations may themselves be

 partially_redundant with other instances of the same expression in the

 program Such partial_redundancy may be_eliminated by moving

 these computations further up



 In summary anticipation of expressions limits how early an expression

 can be placed you cannot place an expression so early that it is not

 anticipated where you place it

 The earlier an expression is placed the more

 redundancy can be removed and among all solutions that

 eliminate the same redundancies the one that computes the expressions

 the latest minimizes the lifetimes of the registers holding the

 values of the expressions involved



 The Lazy-Code-Motion Algorithm



 This discussion thus motivates a

 four-step algorithm The first step uses anticipation to

 determine where expressions can be placed the second step finds the

 earliest cutset among those

 that eliminate as many redundant operations as

 possible without duplicating code and without_introducing any unwanted

 computations This step places the computations at program points

 where the values of their results are first anticipated The third

 step then pushes the cutset down to the point where any further delay

 would alter the semantics of the program or introduce redundancy The

 fourth and final_step is a simple pass to clean up the code by

 removing assignments to temporary_variables that are used only

 once Each step is accomplished with a data-flow pass the

 first and fourth are backward-flow problems the second and third are

 forward-flow problems



 Algorithm Overview

 enumerate

 Find all the expressions anticipated_at each program point using a

 backward_data-flow pass



 The second step places the computation where the values of the

 expressions are first anticipated_along some path After we have

 placed copies of an expression where the expression is first

 anticipated the expression would be available at program point

 if it has_been anticipated_along all paths_reaching

 Availability can be_solved using a forward_data-flow pass If we_wish

 to place the expressions at the earliest possible positions we can

 simply find those program points where the expressions are anticipated

 but are not available



 Executing an expression as_soon as it is anticipated may produce a

 value long before it is used An expression is postponable at

 a program point if the expression has_been anticipated and has yet to

 be used along any path reaching the program point Postponable

 expressions are found using a forward_data-flow pass We place

 expressions at those program points where they can no_longer be

 postponed



 A simple final backward_data-flow pass is used to eliminate

 assignments to temporary_variables that are used only once in the

 program

 enumerate



 Preprocessing Steps



 We_now present the full lazy-code-motion algorithm

 To keep the algorithm simple

 we assume that initially every statement is in a basic_block

 of its_own and we only introduce new computations of

 expressions at the beginnings of blocks To ensure_that this

 simplification does_not reduce the effectiveness of the technique we

 insert a new block between the source and the destination of an

 edge if the destination has more_than one_predecessor Doing_so

 obviously also takes care of all critical_edges in the program



 We abstract the semantics of each block with two sets

 is the set of expressions computed in and is

 the set of expressions killed that is the set of expressions any of whose

 operands are defined in Example_expre will be used

 throughout the discussion of the four data-flow_analyses whose

 definitions are summarized in Fig figpre-df



 ex

 expre

 In the flow_graph in Fig_figpre(a) the expression

 appears three times Because the block is part of a loop

 the expression may be computed many_times The

 computation in block is not only loop_invariant it is also a

 redundant expression since its value already has_been used in

 block For this example we need to compute

 only twice

 once in block and once along the path after and

 before The lazy_code motion algorithm will place the

 expression computations at the beginning of

 blocks and

 ex

 figurehtb

 figureuullmanalsuch9figspreeps

 Flow_graph of Example_expre

 figpre

 figure



 Anticipated Expressions



 Recall that an expression is anticipated_at

 a program point if all paths_leading from point eventually

 compute the value of the expression from the values of

 and that are available at that point



 In Fig_figpre(a) all the blocks anticipating on entry are

 shown as lightly_shaded boxes The expression is anticipated in

 blocks and It is not anticipated

 on entry to

 block because the value of is recomputed within the block

 and therefore the value of that would be computed

 at the beginning of is not

 used along any path

 The expression is not anticipated on entry to because

 it is unnecessary along the branch from to (although it

 would be used along the path )

 Similarly the expression is not anticipated_at the beginning

 of because of the branch from to The anticipation of an

 expression may oscillate along a path as illustrated by





 figure

 center

 tabularlll

 (a) Anticipated Expressions (b) Available_Expressions



 Domain_Sets of expressions Sets of expressions



 Direction Backwards_Forwards



 Transfer



 function

 (



 Boundary





 Meet_()



 Equations









 Initialization















 (c) Postponable Expressions (d) Used_Expressions



 Domain_Sets of expressions Sets of expressions



 Direction_Forwards Backwards



 Transfer



 function





 Boundary





 Meet_()



 Equations









 Initialization



 tabular

 center



 align

 earliestB anticipatedBin -_availableBin



 latestB_(earliestB postponableBin)



 (euseB (Ssucc(B)(earliestS postponableSin)))

 align



 Four data-flow passes in partial-redundancy_elimination

 figpre-df

 figure



 The data-flow_equations for the anticipated-expressions problem are shown

 in Fig figpre-df(a) The analysis is a backward

 pass An anticipated expression at the exit of a block is an

 anticipated expression on entry only if it is not in the set

 Also a block generates as new uses the set of

 expressions At the exit of the program none of the expressions are

 anticipated Since we are_interested in finding expressions that are

 anticipated_along every subsequent path the meet_operator is set

 intersection Consequently the interior_points must_be

 initialized to the universal_set as was discussed for the

 available-expressions problem in Section_ae-subsect



 Completing the Square

 Anticipated expressions (also called very busy expressions elsewhere) is

 a type of data-flow_analysis we have not seen previously

 While we have_seen backwards-flowing frameworks such_as live-variable

 analysis (Sect live-var-subsect) and we have_seen frameworks

 where the meet is intersection such_as available_expressions

 (Sect ae-subsect) this is the first example of a useful

 analysis that has both properties

 Almost all analyses we use can be placed in one of four groups

 depending_on whether they flow forwards or backwards and depending_on

 whether they use union or intersection for the meet

 Notice also that the union analyses always involve asking about

 whether there_exists a path along which something is true while the

 intersection analyses ask whether_something is true along all paths



 Available_Expressions



 At the end of this second step copies of an expression will be placed

 at program points where the expression is first anticipated If that

 is the case an expression will be available at program point

 if it is anticipated_along all paths_reaching This

 problem is similar to available-expressions described in

 Section_ae-subsect The transfer_function used here is slightly

 different though An expression is available on exit from a block

 if it is



 enumerate



 Either

 enumerate

 Available on entry or

 In the set of anticipated expressions

 upon_entry (ie it could be made available if we chose to

 compute it here)

 enumerate

 and



 Not killed in the block



 enumerate



 The data-flow

 equations for available_expressions are shown in

 Fig_figpre-df(b) To_avoid confusing the meaning of we

 refer to the result of an earlier analysis by appending

 to the name of the earlier analysis



 With the earliest_placement strategy the set of expressions placed_at

 block ie is defined as the set of anticipated

 expressions that are not_yet available That is



 earliestBanticipatedBin -_availableBin





 ex

 expre-redundant

 The expression in the flow_graph in

 Figure figpre-redundant is not anticipated_at the entry of

 block but is anticipated_at the entry of block It is

 however not necessary to compute the expression in block

 because the expression is already available due to block

 ex

 figurehtb

 figureuullmanalsuch9figspre-redundanteps

 Flow_graph for Example expre-redundant illustrating

 the use of availability

 figpre-redundant

 figure



 ex

 Shown with dark_shadows in Fig_figpre(a) are the blocks

 for which expression is not available they are

 and The early-placement positions are represented_by

 the lightly_shaded boxes with dark_shadows and are

 thus blocks and

 Note for instance that is considered available on entry to

 because there is a path along

 which is anticipated_at least once - at in this case -

 and since the beginning of neither_nor was

 recomputed

 ex



 Postponable Expressions



 The third_step postpones the computation of expressions as much as

 possible while preserving the original_program semantics and

 minimizing redundancy Example exearly illustrates the

 importance of this step



 ex

 exearly

 In the flow_graph shown in Figure figpre-early the expression

 is computed twice along the path

 The expression is anticipated even at the

 beginning of block If we compute the expression as_soon as it

 is anticipated we would have computed the expression in

 The result would have to be saved from the beginning through

 the execution of the loop comprising blocks and until it

 is used in block Instead we can delay the computation of

 expression until the beginning of and until the flow

 of control is about to transition from to

 ex

 figurehtb

 figureuullmanalsuch9figspre-earlyeps

 Flow_graph for Example exearly to illustrate the need

 for postponing an expression

 figpre-early

 figure





 Formally an expression is postponable to a

 program point if an early placement of is encountered

 along every_path from the entry_node to and there is no

 subsequent use of after the last such placement



 ex

 Let_us again consider expression in Fig figpre

 The two earliest points for are and note_that

 these are the two blocks that are both lightly and darkly shaded in

 Fig_figpre(a) indicating that is both anticipated

 and not available for these blocks and only these blocks

 We cannot_postpone from to because is

 used in

 We can postpone it from to however



 But we cannot_postpone from to

 The_reason is that although is not used in placing its

 computation at instead would lead to a redundant computation of

 along the path

 As we_shall see is one of the latest places we can compute



 ex



 The data-flow_equations for the postponable-expressions problem are

 shown in Fig figpre-df(c) The analysis is a forward pass

 We cannot_postpone an expression to the entry of the program so

 An expression is postponable to the exit

 of block if it is not used in the block and either it is

 postponable to the entry of or it is in

 An expression is not postponable to the

 entry of a block unless all its_predecessors include the

 expression in their sets at their exits Thus the meet

 operator is set_intersection and the interior_points must_be

 initialized to the top_element of the semilattice - the universal_set



 Roughly speaking an expression is placed_at the frontier where

 an expression transitions from being postponable to not being postponable

 More_specifically an expression may be placed_at the beginning of

 a block only if the expression is in 's earliest or

 postponable set upon_entry In_addition is in the

 postponement frontier of if one of the following holds

 enumerate



 is not in In other_words

 is in



 cannot be postponed to one of its successors In other_words

 there_exists a successor of such that is not in the earliest or

 postponable set upon_entry to that successor

 enumerate

 Expression can be placed_at the front of block in either of

 the above scenarios because of the new blocks introduced by the

 preprocessing step in the algorithm



 ex

 Fig figpre(b) shows the result of the analysis The

 light-shaded boxes represent the blocks whose set includes

 The dark_shadows indicate those that include in

 their set The latest_placements of the expressions are

 thus the entries of blocks and since



 enumerate



 is in the

 set of but not and



 's set

 includes and it uses



 enumerate

 The expression is stored

 into the temporary_variable in blocks and and

 is used in place of everywhere else as shown in the figure

 ex



 Used_Expressions

 Finally a backward_pass is used to determine if the temporary

 variables introduced are used beyond the block they are in We_say

 that an expression is used at point if there_exists a path

 leading_from that uses the expression before the value is

 reevaluated This analysis is essentially liveness_analysis (for

 expressions rather_than for variables)



 The data-flow_equations for the used expressions problem are shown

 in Fig figpre-df(d) The analysis is a backward

 pass A used expression at the exit of a block is a

 used expression on entry only if it is not in the set

 A block generates as new uses the set of expressions in

 At the exit of the program none of the expressions are

 used Since we are_interested in finding expressions that are

 used by any subsequent path the meet_operator is set

 union Thus the interior_points must_be

 initialized_with the top_element of the semilattice - the empty_set



 Putting it All_Together



 All the steps of the algorithm are summarized in

 Algorithm alglazy-code-motion



 alg

 alglazy-code-motion

 Lazy code_motion



 A flow_graph for which and have_been computed

 for each block



 A modified flow_graph satisfying the four lazy_code motion conditions

 in Section seclcm-conditions



 enumerate



 Insert an empty block along all edges entering a block with more

 than one_predecessor



 Find for all blocks as defined in

 Fig figpre-df(a)



 Find for all blocks as defined in

 Fig_figpre-df(b)



 Compute the earliest placements for all blocks



 earliestB anticipatedBin -_availableBin





 Find for all blocks as defined in

 Fig figpre-df(c)



 Compute the latest_placements for all blocks



 align

 latestB_(earliestB postponableBin)



 (euseB (S in succ(B)(earliestS postponableSin)))

 align

 Note_that denotes complementation with_respect to the set of all

 expressions computed by the program



 Find for all blocks as defined in

 Fig figpre-df(d)



 For each expression say computed by the program do the

 following

 enumerate

 Create a new_temporary say for

 For all blocks such that

 is in do the following

 enumerate

 Add t xy at the beginning of

 Replace every original by

 enumerate

 enumerate



 enumerate

 alg



 Summary



 Partial-redundancy elimination finds many different forms of redundant

 operations in one unified algorithm This algorithm illustrates_how

 multiple data-flow_problems can be used to find optimal expression placement



 enumerate



 The placement constraints are provided by the anticipated-expressions

 analysis which is a backwards data-flow_analysis with a

 set-intersection meet

 operator as it determines if expressions are used subsequent to

 each program point on all paths



 The earliest_placement of an expression is given by program points

 where the expression is anticipated but is not available Available

 expressions are found with a forwards data-flow_analysis with

 a set-intersection

 meet_operator that computes if an expression has_been anticipated

 before each program point along all paths



 The latest placement of an expression is given by program points where

 an expression can no_longer be postponed Expressions are

 postponable at a program point if for all paths_reaching the

 program point no use of the expression has_been encountered

 Postponable expressions are found

 with a forwards data-flow_analysis with a set-intersection meet

 operator



 Temporary assignments are eliminated unless they are used by some path subsequently We find used expressions with a

 backwards data-flow_analysis

 this time with a set-union meet_operator



 enumerate



 sexer

 pre-exer

 For the flow_graph in Fig pre-exer-fig



 itemize



 a)

 Compute anticipated for the beginning and end of each block





 b)

 Compute available for the beginning and end of each block





 c)

 Compute earliest for each block





 d)

 Compute postponable for the beginning and end of each block





 e)

 Compute used for the beginning and end of each block





 f)

 Compute latest for each block





 g)

 Introduce temporary_variable show where it is computed and where it

 is used





 itemize

 sexer



 figurehtfb



 fileuullmanalsuch9figspre-exereps

 Flow_graph for Exercise pre-exer

 pre-exer-fig

 figure



 exer

 Repeat_Exercise pre-exer for the flow_graph of

 Fig_fg1-fig (see the exercises to Section_secopt-sources)

 You_may limit your analysis to the expressions and



 exer



 vhexer

 The concepts discussed in this_section can also be applied to

 eliminate partially_dead code A definition of a variable is

 partially_dead if the variable is live_on some paths and not others

 We can optimize the program execution by only performing the

 definition along paths where the variable is live Unlike

 partial-redundancy_elimination where expressions are moved before the

 original the new definitions are placed after the original

 Develop an algorithm to move partially_dead code so expressions are

 evaluated only where they will_eventually be used

 vhexer

 Intermediate_Code for Procedures

 proc-3code-sect



 Procedures and their implementation will be discussed at length in

 Chapter_run-time-ch along with the run-time management of

 storage for names We use the term function in this_section for a

 procedure that returns a value We briefly discuss function

 declarations and three-address_code for function calls In

 three-address_code a function call is unraveled into the

 evaluation of parameters in preparation for a call followed_by

 the call itself For_simplicity we assume that parameters are

 passed by value parameter-passing methods are discussed in

 Section param-subsect



 ex

 Suppose that a is an array of integers and that f is

 a function from integers to integers Then the assignment



 center

 n f(ai)

 center



 might translate into the following three-address_code



 center

 tabularr_l

 1) i 4



 2) a



 3) param



 4) call f 1



 5) n

 tabular

 center



 The first two lines compute the value of the expression

 ai into temporary as discussed in

 Section expr-3code-sect Line 3 makes an actual

 parameter for the call of f on line_4

 That line also assigns the return value to

 temporary Line 5 assigns the result of f(ai)

 to n

 ex



 The productions in Fig fun-prod-fig allow function

 definitions and function calls (The syntax generates unwanted

 commas after the last parameter but is good enough for

 illustrating translation) Nonterminals and generate

 declarations and types respectively as in

 Section decl-sect A function definition generated_by

 consists of keyword define a return type the function

 name formal_parameters in parentheses and a function body

 consisting of a bracketed statement Nonterminal generates zero_or more

 formal_parameters where a formal_parameter consists of a type

 followed_by an_identifier Nonterminals and generate

 statements and expressions respectively The production for

 adds a statement that returns the value of an expression The

 production for adds function calls with actual_parameters

 generated_by An actual_parameter is an expression



 figurehtbf

 center

 tabularr_c l

 ''''



















 tabular

 center

 Adding functions to the source_language

 fun-prod-fig

 figure



 Function definitions and function calls can be translated using

 concepts that have_already been introduced in this_chapter



 itemize



 Function types The type of a function must encode the

 return type and the types of the formal_parameters Let void

 be a special type that represents no parameter or no return type

 The type of a function that returns an_integer is

 therefore function from void to integer Function

 types can be represented_by using a constructor fun applied

 to the return type and an ordered list of types for the

 parameters



 Symbol_tables Let be the top symbol_table when

 the function definition is reached The function name is entered

 into for use in the rest of the program The formal_parameters

 of a function can be handled in analogy with field names in a

 record (see Fig record-types-fig) In the production for

 after seeing define and the function name we push

 and set up a new symbol_table



 center





 center



 Call the new symbol_table Note_that top is

 passed as a parameter in so

 the new symbol_table can be linked to the previous one

 The new table is used to translate the function body We

 revert to the previous symbol_table after the function body is

 translated









 Type_checking Within expressions a function is

 treated_like any other operator The discussion of type_checking

 in Section coerce-3code-subsect therefore carries over

 including the rules for coercions For_example if f is a

 function with a parameter of type real then the integer is

 coerced to a real in the call



 Function calls When generating three-address

 instructions for a function call it

 is sufficient to generate the three-address_instructions for

 evaluating or reducing the parameters to addresses followed

 by a param instruction for each parameter If we do_not

 want to mix the parameter-evaluating instructions with the param instructions the attribute for each

 expression can be saved in a data_structure such_as a queue

 Once all the expressions are translated the param

 instructions can be generated as the queue is emptied



 itemize



 The procedure is such an important and frequently used programming

 construct that it is imperative for a compiler to generate good code for

 procedure_calls and returns The run-time routines that handle

 procedure parameter_passing calls and returns are part of the

 run-time support package Mechanisms for run-time support are

 discussed in Chapter_run-time-ch

 Program Representation



 We represent the program as a set of relations



 Domains

 figurehtb

 enumerate

 V - Variable domain

 I -_Invocation sites

 H -_Invocation of new (Heap objects)

 F - Field descriptor

 T - Type descriptor

 N - Name of method

 M - Method

 C - Context

 enumerate

 Domains of the relations used to represent the program

 figdomains

 figure



 The domains are listed in Figure figdomains



 We use a flow-sensitive intraprocedural pass to translate the program

 into a simplified representation based_on access paths This pass

 factors away local_variables combines locations that are

 indistinguishable to the analysis and splits variables that lead to

 imprecision The details of the intraprocedural pass are covered in

 earlier publications



 enumerate

 V - Variable domain V is the domain of all

 variables in the program There is a variable for each allocation_site

 formal_parameter return value thrown exception and dereference in

 the program There is also a special global variable for use

 in accessing static variables

 I -_Invocation sites I is the domain of all

 invocation_sites in the program An invocation_site is a method

 invocation of the form r p0m(p1 pk)

 H -_Invocation of new H is the domain of

 all invocations to object_creation methods The analysis signifies

 a heap_object by its creation_site H I

 For the purpose of this formulation calls to object initializers

 are separate from the object_creation

 F - Field descriptor F is the domain of field

 descriptors in the program Field descriptors are used when loading

 from a field (a bf) or storing to a field (af b) There is

 a special field descriptor to denote an array_access

 T - Type descriptor T is the domain of type

 descriptors (ie classes) in the program Type descriptors specify

 the declared types of variables and heap_objects Types can also

 contain methods and fields

 N - Name of method N is the domain of method

 names used in invocations In an invocation r p0m(p1 pk)

 m is the method name

 M - Method M is the domain of methods in the

 program

 C - Context C is the context domain which will

 be covered in Section contextsensitive

 enumerate



 Initial Relations



 Before the algorithm begins we start with a set of initial relations

 which are extracted directly from the program



 enumerate

 vP - Variable points-to (V H) vP signifies the set of variable points-to_relations directly

 present in the program ie a pair vP iff

 there is an invocation_site to an object_creation method that

 assigns the return value to



 item

 S - Stores (V F V) S is

 the set of store_instructions in the program A triple iff

 there is a statement v1f v2 in the program

 L - Loads (V F V) L is

 the set of load instructions in the program A triple iff

 there is a statement v2 v1f in the program

 vT - Variable type (V T) vT

 encodes the declared type of a variable Variables can only point to

 objects that have a type which is assignable to the variable's

 type(Assignability is similar to the subtype relation with

 allowances for interfaces null values and arrays It is covered in

 depth in the Java specificationjavaspec) A pair

 vT iff variable v is declared with type t

 hT - Heap type (H T) hT encodes

 the type of a object_created at a particular creation_site In Java

 the type created by a new instruction is known

 statically(For situations_where the type of the created

 object is not known precisely - for example the return value of a

 native method or the use of reflection - we make the hT

 relation contain the set of possible types that can be created) A

 pair hT iff the object_created at h has type t

 aT - Assignable types relation (T T)

 aT encodes the information from the class_hierarchy about

 assignability (the supertypesubtype relation) A pair

 aT iff type is assignable to type

 cha - Class hierarchy information (T N

 M) cha encodes virtual_method dispatch information

 from the class_hierarchy A triple cha iff is

 the target of dispatching the method name on type

 actual - Actual parameters for invocation_sites (I

 V) actual records the values

 passed as parameters at an invocation_site The triple

 actual iff is passed as parameter number at invocation

 site

 formal - Formal parameters for methods (M

 V) formal records the formal_parameters

 of a method The triple formal iff formal_parameter

 of method is represented_by variable

 IE -_Invocation edges (I M)

 IE records the invocation_edges whose targets are statically

 bound In Java some calls are static or non-virtual Additionally

 local type analysis combined with analysis of the class_hierarchy

 allows_us to determine that some calls are single-target

 A pair IE iff invocation_site calls method



 mI - Method invocations (M I N)

 mI contains virtual invocation_sites A triple

 mI iff method contains an virtual invocation_site with

 method name

 mV - Method variables(M V) mV

 records which methods contain which variables A pair

 mV iff method contains variable

 enumerate





 tocchapter8Code Generation1

 lof10

 lot10

 codegen-ch81

 loffigure81Position of code generator1

 codegen-role-fig811

 tocsection81Issues in the Design of a Code Generator2

 issues-sect812

 tocsubsection811Input to the Code Generator3

 tocsubsection812The Target Program3

 tocsubsection813Instruction Selection4

 instr-select-subsect8134

 tocsubsection814Register Allocation6

 reg-pair-ex816

 loffigure82Two three-address_code sequences7

 two-code-seq-fig827

 loffigure83Optimal machine-code sequences7

 optimal-seq-fig837

 tocsubsection815Evaluation Order7

 tocsection82The Target Language8

 target-sect828

 tocsubsection821A Simple Target Machine Model8

 tocsubsection822Program and Instruction Costs11

 tocsubsection823Exercises for Section 8212

 tocsection83Addresses in the Target Code14

 rt-storage-sect8314

 tocsubsection831Subdivision of Run-Time Memory14

 loffigure84Typical subdivision of run-time memory into code and data areas14

 run-time-layout-fig8414

 tocsubsection832Static Allocation16

 proc-code-ex8316

 tocsubsection833Stack Allocation17

 loffigure85Target code for static allocation18

 target-code-fig8518

 stack-alloc-ex8419

 loffigure86Target code for stack allocation20

 stack-code-fig8620

 tocsubsection834Run-Time Addresses for Names21

 tocsubsection835Exercises for Section 8321

 tocsection84Basic Blocks and Flow Graphs23

 bb-sect8423

 tocsubsection841Basic Blocks23

 bb-alg8523

 identity-matrix-ex8624

 loffigure87Intermediate code to set a matrix to an identity matrix25

 identity-matrix-code-fig8725

 loffigure88Source code for Fig8625

 identity-matrix-code2-fig8825

 tocsubsection842Next-Use Information25

 nu-alg8726

 tocsubsection843Flow Graphs26

 fg-subsect84326

 loffigure89Flow graph from Fig8627

 identity-fg-fig8927

 fg-ex8827

 tocsubsection844Representation of Flow Graphs28

 tocsubsection845Loops28

 loops-subsect84528

 tocsubsection846Exercises for Section 8429

 mm-code-exer84129

 loffigure810A matrix-multiplication algorithm29

 mm-exer-fig81029

 primes-code-exer84229

 loffigure811Code to sieve for primes30

 primes-exer-fig81130

 tocsection85Optimization of Basic Blocks30

 bb-opt-sect8530

 tocsubsection851The DAG Representation of Basic Blocks30

 dag-bb-subsect85130

 tocsubsection852Finding Local Common Subexpressions31

 dag1-ex81031

 loffigure8128956plus2minus4plus3plusminus23plusminus2plusm for basic_block in Example8932

 dag1-fig81232

 dag2-ex81132

 tocsubsection853Dead Code Elimination32

 loffigure8138956plus2minus4plus3plusminus23plusminus2plusm for basic_block in Example81033

 dag2-fig81333

 tocsubsection854The Use of Algebraic Identities33

 alg-identities-subsect85433

 tocsubsection855Representation of Array References34

 array-dag1-ex81335

 loffigure814The 8956plus2minus4plus3plusminus23plusminus2plusminusDAG for a sequence of array assignments35

 array-dag1-fig81435

 loffigure815A node that kills a use of an array need not have that array as a child36

 array-dag2-fig81536

 tocsubsection856Pointer Assignments and Procedure Calls36

 tocsubsection857Reassembling Basic_Blocks From DAGs37

 tocsubsection858Exercises for Section 8538

 dag1-exer85138

 dag2-exer85639

 tocsection86A Simple Code Generator40

 simple-cg-sect8640

 tocsubsection861Register and Address Descriptors40

 tocsubsection862The Code-Generation Algorithm41

 simple-cg-alg-subsect86241

 tocsubsubsectionMachine Instructions for Operations41

 tocsubsubsectionMachine Instructions for Copy Statements42

 tocsubsubsectionEnding the Basic Block42

 tocsubsubsectionManaging Register and Address Descriptors42

 simple-cg-ex81643

 loffigure816Instructions generated and the changes in the register and address descriptors44

 simple-cg-fig81644

 tocsubsection863Design of the Function getReg45

 tricky-reg-item3c45

 tocsubsection864Exercises for Section 8646

 cg1-exer86146

 cg2-exer86447

 tocsection87Peephole Optimization47

 peephole-sect8747

 tocsubsection871Eliminating Redundant Loads and Stores47

 tocsubsection872Eliminating Unreachable Code48

 tocsubsection873Flow-of-Control Optimizations49

 tocsubsection874Algebraic Simplification and Reduction in Strength50

 tocsubsection875Use of Machine Idioms50

 tocsubsection876Exercises for Section 8750

 tocsection88Register Allocation and Assignment51

 ra-sect8851

 tocsubsection881Global Register Allocation51

 tocsubsection882Usage Counts52

 benefit-eq8152

 inner-loop-ra-ex81752

 loffigure817Flow graph of an inner loop53

 fg-inner-loop-fig81753

 tocsubsection883Register Assignment for Outer Loops53

 tocsubsection884Register Allocation by Graph Coloring53

 coloring-subsect88453

 loffigure818Code sequence using global register assignment54

 global-ra-code-fig81854

 tocsubsection885Exercises for Section 8856

 tocsection89Instruction Selection by Tree Rewriting56

 tile-sect8956

 tocsubsection891Tree-Translation Schemes56

 tile-tree-ex81856

 loffigure819Intermediate-code tree for ai2222em2222emb1667em1667em157

 tile-tree-fig81957

 tile-rules-ex81958

 tiling-ex82058

 loffigure820Tree-rewriting rules for some target-machine instructions59

 tile-rules-fig82059

 tocsubsection892Pattern Matching by Parsing61

 lr-tree-parse-subsect89261

 loffigure821Syntax-directed translation_scheme constructed from Fig LaTeX Warning Reference 'tile-rules-fig' on page 62 undefined62

 tile-parse-fig82162

 tile-parse-ex82162

 tocsubsection893Routines for Semantic Checking63

 tocsubsection894General Tree Matching63

 loffigure822An instruction set for tree matching64

 tree-match-fig82264

 tree-match-ex82264

 tocsection810Optimal Code_Generation for Expressions65

 ershov-sect81065

 tocsubsection8101Ershov Numbers65

 leaf-lab-item165

 uneq-lab-item3a65

 eq-lab-item3b65

 loffigure823A tree labeled with Ershov numbers66

 labeled-tree-fig82366

 tocsubsection8102Generating Code From Labeled Trees66

 labeled-tree-alg82466

 loffigure824Optimal three-register code for the tree of Fig81868

 labeled-tree-code-fig82468

 tocsubsection8103Evaluating Expressions with an Insufficient Supply of Registers68

 labeled-tree-lim-alg82668

 loffigure825Optimal three-register code for the tree of Fig818 using only two registers70

 labeled-tree-lim-code-fig82570

 tocsubsection8104Exercises for Section 81070

 ershov-exer810170

 tocsection811Dynamic Programming Code-Generation71

 dyn-prog-sect81171

 tocsubsection8111Contiguous Evaluation71

 tocsubsection8112The Dynamic Programming Algorithm72

 dyn-prog-ex82873

 loffigure826Syntax tree for (a-b)c(de) with cost_vector at each node74

 dyn-costs-fig82674

 tocsection812Summary of Chapter 875

 tocsection813References for Chapter 876

 aj76177

 agt89277

 chai81377

 chai82477

 ch90577

 ct04677

 ersh58777

 ersh71877

 fl91977

 fhp921078

 gg781178

 hp031278

 lm691378

 pg881478

 schw731578

 su701678

 uullmanalsuch8ch8

 page79

 equation1

 enumi16

 enumii3

 enumiii0

 enumiv0

 footnote3

 mpfootnote0

 part0

 chapter8

 section13

 subsection0

 subsubsection0

 paragraph0

 subparagraph0

 figure26

 table0

 parentequation0

 theorem28

 exerz0











 Queries and Other Analyses

 secqueries



 The algorithms in sections secapproach seccg

 and seccontext generate vast amounts of results in the form of

 relations Using the same declarative programming interface we can

 conveniently query the results and extract exactly the information we

 are_interested in This_section shows a variety of queries and

 analyses that make use of pointer information and context_sensitivity



 We also give an example of using our cloning technique to get a

 polymorphic version of a monomorphic type analysis and an example of

 using a thread_context to compute thread_escape analysis





 Debugging a Memory Leak

 queriesleak



 Memory leaks can occur in Java when a reference to an object remains

 even after it will no_longer be used One common approach of debugging

 memory_leaks is to use a

 dynamic tool that locates the allocation_sites of memory-consuming

 objects Suppose that upon reviewing the information the programmer

 thinks objects allocated in line 57 in file ajava should

 have_been freed He may wish to know which objects may be holding

 pointers to the leaked objects and which operations may have stored the

 pointers He can consult the static analysis results by supplying the

 queries



 equationarraylcl

 whoPointsTo57(hf) - (hfajava57)



 whoDunnit(cv1fv2) -_(v1fv2)



 (c v2 ajava57)

 equationarray

 The first query finds the objects and their fields that may point to

 objects allocated at ajava57 the second finds the

 store_instructions and the contexts under which they are executed that

 create the references





 Heap Connectivity



 When using a reference-counting garbage_collector it is useful to

 know whether or not an object can be involved in a

 cyclerecycler acyclic objects do_not need to be

 cycle-collected To_find cyclic objects we simply compute a

 transitive_closure on the relation and see if an object can

 transitively point to itself



 equationarraylcl

 reachable(h1h2) - (h1fh2)



 reachable(h1h3) - reachable(h1h2) (h2fh3)

 equationarray

 The heap connectivity relation () can also be used

 to find the set of objects that can be accessed by unknown code or to

 guide partitioning strategies for garbage

 collectorsharris99earlyharris01dynamichirzel03connectivity





 Finding a Security Vulnerability

 queriessecurity







 The Java Cryptography Extension (JCE) is a library of cryptographic

 algorithmsJCE Misuse of the JCE API can lead to security

 vulnerabilities and a false sense of security For_example many

 operations in the JCE use a secret key that must_be supplied_by the

 programmer It is important that secret keys be cleared after they

 are used so they cannot be recovered by attackers with access to

 memory Since String objects are immutable and cannot be

 cleared secret keys should not be stored in String objects but

 in an array of characters or bytes instead



 To guard against misuse the function that accepts the secret key

 only allows arrays of characters or

 bytes as input However a programmer not versed in security issues

 may have stored the key in a String object and then use a

 routine in the String class to convert it to an array of

 characters We can write a query to audit programs for the presence

 of such idioms Let be an input relation

 specifying that variable is the return value of method We

 define a relation which indicates if the

 object was directly derived_from a String Specifically it

 records the objects that are returned by a call to a method in the

 String class An invocation to method

 is a vulnerability if the first

 argument points to an object derived_from a String



 equationarraylcl

 fromString(h) - (Stringm) Mret(mv)



 (vh)



 vuln(ci) - (iPBEKeySpecinit())



 (i1v) (cvh)



 fromString(h)

 equationarray

 Notice_that this query does_not only find cases where the object

 derived_from a String is immediately supplied to

 This query will also identify cases where

 the object has passed through many variables and heap_objects





 Limiting Access of Untrusted Code

 queriesuntrusted



 Java includes some unsafe_objects that provide direct unchecked

 access to memory or the file system the sunmiscUnsafe class

 defines one such example Obviously access to unsafe_objects must_be

 confined to trusted code Such confinement unfortunately is not

 enforced by the normal Java protection mechanism but is left up to

 the programmers This is an error-prone task because a seemingly

 unrelated change could cause

 the object to inadvertently become accessible to untrusted_code





 However the objects are used

 in multiple packages and therefore their usage cannot be enforced with

 the normal Java protection mechanism Instead they rely_on the code

 not to leak such objects to untrusted_code Because this property is

 totally unchecked it is very error-prone and sensitive to

 modification -





 We can use points-to_information to check if these unsafe_objects may

 have_been leaked to untrusted_code First we check if any safe

 method may expose unsafe_objects A non-private method could

 potentially be called from untrusted_code so unsafe_objects should

 never be returned by a non-private method Likewise unsafe_objects

 should never be written into non-private static fields Second we

 check if any unsafe code has access to unsafe_objects Variables in

 unsafe code should never refer to unsafe_objects Moreover no

 non-private fields from objects reachable from untrusted_code should

 point to unsafe_objects



 In the following we assume the availability of relations



 which contain the sets of unsafe_objects trusted

 methods private methods and private fields respectively Also let

 be an input relation specifying that variable belongs

 to method



 equationarraylcl

 untrustedV(v) - (mv) trustedM(m)



 untrustedV(v) - Mret(mv) privateM(m)



 untrustedH(h)_- untrustedV(v) (vh)



 untrustedH(h)_- (globalfh) privateF(f)



 untrustedH(h)_- untrustedH(h1) (h1fh)



 privateF(f)



 exposed(h) - unsafe(h) untrustedH(h)

 equationarray

 keeps_track of the variables that are accessible

 by untrusted_code It consists of variables of untrusted

 methods and return values of non-private methods

 holds heap_objects that are accessible by

 untrusted_code It includes objects_pointed to by untrusted variables

 objects written into a non-private global variable and objects that

 are transitively reachable from other untrusted objects Any unsafe

 object that is in the relation is potentially

 exposed





 Aliased Parameters



 We can easily use the points-to_information to discover if parameters

 are aliased under certain contexts In many_cases code can be

 optimized if it can be proven that parameters are not aliased Because

 it is context-sensitive this can be used to automatically redirect call

 sites to call specialized versions of methods



 eqnarray

 aliased(cmz1z2) - (mz1v1) (mz2v2)



 z1 z2 (cv1h) (cv2h)

 eqnarray







 Type Refinement

 queriesrefinement



 Libraries are written to handle the most general types of

 objects possible and their full generality is typically not used in

 many applications By analyzing the actual types of objects used in

 an application we can refine the types of the variables and

 object fields Type refinement can be used to reduce overheads in

 cast operations resolve virtual_method calls and gain better

 understanding of the program



 We_say that variable can be legally declared as written

 if is a supertype of the types of all the

 objects can point to The type of a variable is refinable if the

 variable can be declared to have a more_precise type To_compute the

 super types of we first find the

 types of objects_pointed to by We then intersect the supertypes

 of all the exact_types to get the desired solution we do so in

 Datalog by finding the complement of the union of the complement of

 the exact_types



 equationarraylcl

 varExactTypes(vt) - (vh) (ht)



 notVarType(vt) - varExactTypes(vtv) (ttv)



 varSuperTypes(vt) - notVarType(vt)



 refinable(vtc) - (vtd) varSuperTypes(vtc)



 (tdtc) td tc

 equationarray

 The above shows a context-insensitive type_refinement query We find

 for each variable the type to which it can be refined regardless of

 the context Even_if the end result is context-insensitive it is

 more_precise to take_advantage of the context-sensitive_points-to

 results available to determine the exact_types as shown in the first

 rule In Section resultseval we compare the accuracy of

 this context-insensitive query with a context-sensitive_version





 holds the types of objects_pointed to by

 We discover the supertypes of all such types by doing a double negation which

 effectively performs a universal quantification







 Querying Relations

 queriesqueries



 We can perform queries on the results of the analysis by selecting

 elements of the relations that match some criteria For_example to

 find the points-to set of a variable under all contexts we can

 select from the relation as such



 pointsTo(v0h) - (v0h)





 To_find the types of objects that can point to the object



 typesCanPoint(h0t) - (hfh0) (ht)





 The combination of these last two queries allows_us to infer polymorphic

 types based_on pointer information for variables and heap_objects

 (with recursion treated by collapsing cycles as described in

 Section contextcallpath)



 The possibilities for queries are almost endless Furthermore these

 types of inference_rules translate directly into BDD_operations and

 are very efficient to compute





 Interprocedural Data Flow



 By combining the points-to_relations that we compute with the initial

 relations from the program statements we can construct relations that

 model the flow of data across method calls and through pointer

 indirections



 eqnarray

 usedef(cv2v2cv1v1) - (cv1v1cv2v2)



 usedef(cv2v2cv1v1) - (vb2fv2) (cv2vb2hb)



 (vb1fv1) (cv1vb1hb)



 defuse(cv1v1cv2v2) - usedef(cv2v2cv1v1)



 eqnarray



 By taking the transitive_closure on the usedef or defuse

 relations we can obtain a program slice of the flow of data to

 or from a point The slice of the locations leading up to a variable

 is usedef while the slice of locations following a variable is

 defuse These slices are useful for taint

 analysisshankardetecting provenance analysislam-pact

 and flow analysisfahndrich00scalable



 eqnarray

 usedef(cv2v2cv1v1) - usedef(cv2v2cv1v1)



 usedef(cv3v3cv1v1) - usedef(cv3v3cv2v2)



 usedef(cv2v2cv1v1)



 defuse(cv1v1cv2v2) - defuse(cv1v1cv2v2)



 defuse(cv1v1cv3v3) - defuse(cv1v1cv2v2)



 defuse(cv2v2cv3v3)



 eqnarray





 Context-Sensitive Mod-Ref Analysis



 Mod-ref analysis is used to determine what fields of what objects may

 be modified or referenced by a statement or call

 siteWLandi93



 We can use the context-sensitive_points-to results to solve a context-sensitive

 version of this query We define to mean that is a

 local variable in The relation specifies the set of

 variables and contexts of methods that are transitively reachable from

 a method means that calling method with

 context can transitively call a method with local variable

 under_context



 equationarraylcl

 (cmcv) - (mv)



 (c1m1c3v3) - (m1i)(c1ic2m2)



 (c2m2c3v3)

 equationarray

 The first rule simply says_that a method in context can reach

 its local variable The second rule_says that if method in

 context calls method in context then in

 context can also reach all variables reached by method in

 context



 We can now define the mod and ref set of a method as_follows



 equationarraylcl

 mod(cmhf) - (cmcvv)



 (vf) (cvvh)



 ref(cmhf) - (cmcvv)



 (vf) (cvvh)

 equationarray

 The first rule_says that if method in context can reach a

 variable in context and if there is a store through that

 variable to field of object then in context can

 modify field of object The second rule for defining the ref

 relations is analogous

































































 Context-Sensitive Type Analysis

 queriesta



 Our cloning technique can be applied to add context_sensitivity to

 other context-insensitive algorithms The example we show here is the

 type_inference of variables and fields By not distinguishing between

 instances of heap_objects this analysis does_not generate results as

 precise as those extracted_from running the complete context-sensitive

 pointer_analysis as discussed in Section queriesrefinement but

 is much faster

 because there

 is less differentiation



 The basic type analysis is similar to -CFAshivers91 Each variable

 and field in the program has a set of concrete types that it can refer

 to The sets are propagated through calls returns loads and

 stores By using the path numbering_scheme in

 Algorithm_algcontext we can convert this basic analysis into

 one which is context-sensitive-in essence making the analysis

 into a -CFA analysis where is the depth of the call_graph and

 recursive_cycles are collapsed



 figurehtb

 alg Context-sensitive type analysis

 algta

 alg

 Domains



 Domains_from Algorithm_algcs





 Relations



 Relations from Algorithm_algcs plus

 arraylll

 output (contextC variableV typeT)



 output (fieldF targetT)



 (variableV typeT)

 array

 Rules

 0em

 equationarraylcl

 (vt) -_(vtv) (tvt)



 (cvt) -_0(vh) (ch) (ht) ctrule0



 (cv1v1t) - (cv1v1cv2v2)



 (cv2v2t) (v1t) ctrule1



 (ft) - (fv2) (v2t) ctrule2



 (vt) - (fv) (ft)



 (vt) ctrule3



 2l(c1v1c2v2)

 equationarray

 -55ex

 equationarraylcl

 - (c2ic1m) (mzv1)



 (izv2) ctArule

 equationarray

 figure



 description









 C_V T is the context-sensitive

 variable type relation

 means that variable in context can refer to an object

 of type This is the analogue of in the points-to_analysis





 F_T is the field type relation

 means that field can point to an object of type





 V T is the type_filter relation

 means that it is type-safe to assign an object of type to variable

 description



 Rule (ctrule0) initializes the relation based_on the

 initial local points-to_information contained in combining it

 with to get the type and to get the context_numbers

 Rule (ctrule1) does transitive_closure on the relation

 filtering with to enforce type safety

 Rules (ctrule2) and (ctrule3) handle stores and loads

 respectively They differ from their counterparts in the pointer

 analysis in that they do_not use the base object only the field

 Rule (ctArule) models the effects of parameter_passing

 in a context-sensitive manner



 Thread Escape Analysis

 queriesescape



 Our last example is a thread_escape analysis which determines if

 objects_created by one thread may be used by another The results of

 the analysis can be used for optimizations such_as synchronization

 elimination and allocating objects in thread-local heaps as_well as

 for understanding programs and checking for possible race conditions

 due to missing synchronizationschoi99escapeOOPSLA99Whaley

 This example_illustrates how we can vary context_sensitivity to

 fit the needs of the analysis



 We_say that an object allocated by a thread has escaped if it

 may be accessed by another thread This notion is stronger than

 most other formulations where an object is said to escape if it can be

 reached by another threadchoi99escapeOOPSLA99Whaley











 Java threads being subclasses of javalangThread are

 identified by their creation_sites In the special_case where a

 thread_creation can execute only once a thread can simply be named by

 the creation_site The thread that exists at virtual machine startup

 is an example of a thread that can only be created once A creation

 site reached via different call_paths or embedded in loops or

 recursive_cycles may generate multiple threads To distinguish

 between thread instances created at the same site we create two

 thread contexts to represent two separate thread instances If an

 object_created by one instance is not accessed by its clone then it

 is not accessed by any other instances created by the same call_site

 This scheme creates at most twice as many_contexts as there are thread

 creation_sites



 We clone the thread run()_method one for each thread_context

 and place these clones on the list of entry methods to be analyzed

 Methods (transitively) invoked by a context's run()_method all

 inherit the same context A clone of a method not only has its_own

 cloned variables but also its_own cloned object_creation sites

 In this way objects_created by separate threads are distinct from each

 other We run a points-to_analysis over this slightly expanded_call

 graph an object_created in a thread_context escapes if it is accessed

 by variables in another thread_context



 figurehtb

 alg Thread-sensitive pointer_analysis

 algescape

 alg

 Domains



 Domains_from Algorithm_algcs





 Relations



 Relations from Algorithm_algci-types plus

 arraylll

 input (cC heapH)



 input (cvC variableV chC heapH)



 output (cvC variableV chC heapH)



 output (cbC baseH fieldF ctC targetH)

 array

 Rules



 0em

 equationarraylcl

 (vh)_- (vtv)_(hth) (tvth) tvPfilterrule



 (c1vc2h) - (c1vc2h) cerule1



 (cvch) -_0(vh) (ch) cerule2



 (c2v1chh) -_(v1v2) (c2v2chh)



 (v1h) cerule3



 (c1h1fc2h2) -_(v1fv2) (cv1c1h1)



 (cv2c2h2) cerule4



 (cv2c2h2) -_(v1fv2) (cv1c1h1)



 (c1h1fc2h2)



 (v2h2) cerule5

 equationarray

 figure



 description



 C_H encodes the non-thread objects_created by a

 thread means that a thread with context may execute

 non-thread allocation_site in other_words there is a call path

 from the run()_method in context to allocation_site





 C_V C_H is the set of

 initial inter-thread points-to_relations This includes the

 points-to_relations for thread_creation sites and for the global

 object means that thread has an thread

 allocation_site and points to the newly_created thread

 context (There are usually two contexts assigned to each allocation

 site) All global objects across all contexts are given the same

 context



 C_V C_H is the

 thread-sensitive version of the variable points-to_relation

 means variable in context can point

 to heap_object created under_context



 C_H F C_H is the

 thread-sensitive version of the heap points-to_relation

 means that field of heap_object

 created under_context can point to heap_object created

 under_context

 description



 REDUNDANT

 These creation_sites are special in that they

 have different context_numbers for the variable and heap_object It

 also_incorporates the constraint that the global object is aliased

 across all contexts





 Rule (cerule1) incorporates the initial points-to_relations for

 thread_creation sites Rule (cerule2) incorporates the points-to

 information for non-thread creation_sites which have

 the context_numbers of threads that can reach the method The other

 rules are analogous to those of the context-sensitive_pointer analysis

 with an additional context attribute for the heap_objects



 From the analysis results we can easily determine which objects

 have escaped An object_created by thread_context has

 escaped written if it is accessed by a

 different context Complications involving unknown code such

 as native_methods could also be handled using this technique



 equationarraylcl

 escaped(ch) - (cvch) cv c

 equationarray



 Conversely an object_created by context is captured written

 if it has not escaped Any captured object

 can be allocated on a thread-local heap



 equationarraylcl

 captured(ch) - (cvch) escaped(ch)

 equationarray



 We can also use escape_analysis to eliminate unnecessary

 synchronizations We define a relation indicating

 if the program contains a synchronization operation performed on

 variable A synchronization for variable under_context is

 necessary written if

 and can point to an escaped_object



 equationarraylcl

 neededSyncs(cv) - syncs(v) (cvchh)



 escaped(chh)

 equationarray



 Notice_that is context-sensitive Thus we can

 distinguish when a synchronization is necessary only for certain

 threads and generate specialized versions of methods for those threads





 Finally we can compute all the objects reachable by a thread with the

 query

 eqnarray

 escaped(cmh2) - escaped(cmh1) (h1h2)

 eqnarray



 Register_Allocation and Assignment

 ra-sect



 Instructions involving only register operands are faster_than

 those involving memory operands On modern machines processor

 speeds are often an order of magnitude or_more faster_than memory

 speeds Therefore efficient utilization of registers is vitally

 important in generating good code This_section presents various

 strategies for deciding at each point in a program what values

 should reside in registers (register allocation) and in which

 register each value should reside (register assignment)



 One approach to register_allocation and assignment is to assign

 specific values in the target program to certain registers For

 example we could decide to assign base addresses to one group of

 registers arithmetic computations to another the top of the

 stack to a fixed register and so on



 This_approach has the advantage that it simplifies the design of a

 code_generator Its disadvantage is that applied too strictly it

 uses registers inefficiently certain registers may go unused over

 substantial portions of code while unnecessary loads and stores

 are generated into the other registers Nevertheless it is

 reasonable in most computing environments to reserve a few

 registers for base registers stack pointers and the like and to

 allow the remaining registers to be used by the code_generator as

 it_sees fit



 Global Register_Allocation



 The code_generation algorithm in Section_simple-cg-sect

 used registers to hold values for the duration of a single

 basic_block

 However all live_variables were stored at the end of each block

 To save some of these stores and corresponding loads we might

 arrange to assign registers to frequently used variables and

 keep these registers consistent across block boundaries

 (globally)

 Since programs_spend most of their time in inner_loops

 a natural approach to global register assignment is to try

 to keep a frequently used value in a fixed register

 throughout a loop

 For the time being assume that we know the loop structure of

 a flow_graph and that we know what values computed in a basic

 block are used outside that block

 The next chapter covers techniques for computing this information



 One strategy for global register_allocation is to assign some

 fixed_number of registers to hold the most active values in

 each inner_loop

 The selected values may be different in different loops

 Registers not already allocated may be used to hold values

 local to one block as in Section_simple-cg-sect

 This_approach has the drawback that the fixed_number of

 registers is not always the right number to make available

 for global register_allocation

 Yet the method is simple to implement and was used in

 Fortran H the optimizing Fortran compiler developed by

 IBM for the 360-series machines in the late 1960s



 With early C compilers a programmer could do some register_allocation

 explicitly by using register declarations to keep certain values

 in registers for the duration of a procedure Judicious use of

 register declarations did speed_up many programs but programmers

 were encouraged to first profile their programs to determine

 the program's hotspots before doing their own register_allocation



 Usage Counts



 In this_section we_shall assume that the savings to be realized by

 keeping a variable in a register for the duration of a loop

 is one_unit of cost for each reference to if is

 already in a register However if we use the approach in

 Section_simple-cg-sect to generate code for a block there

 is a good chance that after has_been computed in a block it

 will remain in a register if there are subsequent uses of in

 that block Thus we count a savings of one for each use of in

 loop that is not preceded by an assignment to in the same

 block We also save two units if we can avoid a store of at

 the end of a block Thus if is allocated a register we count

 a savings of two for each block in loop for which is live

 on exit and in which is assigned a value



 On the debit side if is live_on entry to the loop

 header we must load into its register just_before

 entering loop

 This load costs two units

 Similarly for each exit block of loop at which

 is live_on entry to some successor of outside of

 we must store at a cost of two

 However on the assumption that the loop is iterated

 many_times we may neglect these debits since they occur

 only once each time we enter the loop

 Thus an approximate formula for the benefit to be realized

 from allocating a register for within loop is



 equation

 benefit-eq blocks B in L use(x B) 2

 live(x B)

 equation

 where is the number of times is used in

 prior to any definition of is 1 if is

 live_on exit from and is assigned a value in and

 is 0 otherwise Note_that (benefit-eq)

 is approximate because not all blocks in a loop are executed with

 equal frequency and also because (benefit-eq) is based_on

 the assumption that a loop is iterated many_times On specific

 machines a formula analogous to (benefit-eq) but possibly

 quite different from it would have to be developed



 ex

 inner-loop-ra-ex Consider the basic_blocks in the

 inner_loop depicted in Fig fg-inner-loop-fig where jump

 and conditional_jump statements have_been omitted Assume

 registers R0_R1 and R2 are allocated to hold

 values throughout the loop Variables live_on entry into and on

 exit from each block are shown in Fig fg-inner-loop-fig for

 convenience immediately above and below each block respectively

 There_are some subtle points about live_variables that we address

 in the next chapter For_example notice that both e and

 f are live at the end of but of these only e

 is live_on entry to and only f on entry to In

 general the variables live at the end of a block are the union of

 those live at the beginning of each of its successor blocks



 figurehtfb



 Flow

 graph of an inner_loop fg-inner-loop-fig

 figure



 To evaluate (benefit-eq) for we observe that

 a is live_on exit from and is assigned a value there

 but is not live_on exit from or Thus

 Hence the value of

 (benefit-eq) for is 4 That is four units of

 cost can be saved by selecting a for one of the global

 registers The values of (benefit-eq) for b_c

 d e and f are 5 3 6 4 and 4 respectively

 Thus we may select a b and d for registers

 R0_R1 and R2 respectively Using R0 for

 e or f instead of a would be another choice with

 the same apparent benefit Figure global-ra-code-fig shows

 the assembly code generated from Fig fg-inner-loop-fig

 assuming that the strategy of Section_simple-cg-sect is used

 to generate code for each block We do_not show the generated code

 for the omitted conditional or unconditional_jumps that end each

 block in Fig fg-inner-loop-fig and we therefore do_not

 show the generated code as a single stream as it would appear in

 practice

 ex



 figurehtfb



 Code

 sequence using global register assignment

 global-ra-code-fig

 figure





 Register Assignment for Outer Loops



 Having assigned registers and generated code for inner_loops

 we may apply the same idea to progressively_larger enclosing loops

 If an outer_loop contains an inner_loop the names

 allocated registers in need not be allocated registers

 in

 However if we choose to allocate a register in but

 not we must load on entrance to and store

 on exit from

 We leave as an_exercise the derivation of a criterion for selecting

 names to be allocated registers in an outer_loop given that

 choices have_already been made for all loops nested_within



 Register_Allocation by Graph Coloring

 coloring-subsect



 When a register is needed for a computation but all available

 registers are in use the contents of one of the used registers

 must_be stored (spilled) into a memory_location in order

 to free up a register

 Graph coloring is a simple systematic technique for allocating

 registers and managing register spills



 In the method two passes are used In the first target-machine

 instructions are selected as though there are an_infinite number

 of symbolic registers in effect names used in the intermediate

 code become names of registers and the three-address_instructions

 become machine-language instructions If access to variables

 requires instructions that use stack pointers display pointers

 base registers or other quantities that assist access then we

 assume that these quantities are held in registers reserved for

 each purpose Normally their use is directly translatable into an

 access mode for an address mentioned in a machine_instruction If

 access is more_complex the access must_be broken into several

 machine_instructions and a temporary symbolic register (or

 several) may need to be created



 Once the instructions have_been selected a second pass assigns

 physical registers to symbolic ones The goal is to find an

 assignment that minimizes the cost of spills



 In the second pass for each procedure a register-interference graph is constructed in which the nodes are

 symbolic registers and an edge connects two nodes if one is live

 at a point where the other is defined For_example a

 register-interference graph for Fig fg-inner-loop-fig would

 have nodes for names a and d In block a

 is live at the second statement which defines d therefore

 in the graph there would be an edge between the nodes for a

 and d



 An attempt is made to color the register-interference graph using

 colors where is the number of assignable registers

 A graph is said to be colored if each node has_been assigned

 a color in such a way that no two adjacent nodes have the same color

 A color represents a register and the color makes sure that no two

 symbolic registers that can interfere with each other are assigned

 the same physical register



 Although the problem of determining whether a graph is

 -colorable is NP-complete in general the following heuristic

 technique can usually be used to do the coloring quickly in

 practice Suppose a node in a graph has fewer_than

 neighbors (nodes connected to by an edge) Remove and its

 edges from to obtain a graph A -coloring of can

 be extended to a -coloring of by assigning a color not

 assigned to any of its neighbors



 By repeatedly eliminating nodes having fewer_than edges from the

 register-interference graph either we obtain the empty graph in

 which case we can produce a -coloring for the original graph by

 coloring the nodes in the reverse order in which they_were

 removed or we obtain a graph in which each node has or_more

 adjacent nodes In the latter case a -coloring is no_longer

 possible At this point a node is spilled by introducing code to

 store and reload the register Chaitin has devised several

 heuristics for choosing the node to spill A general rule is to

 avoid introducing spill code into inner_loops



 exer

 Construct the register-interference graph for the

 program in Fig fg-inner-loop-fig

 exer



 exer

 Devise a register-allocation strategy on the assumption

 that we automatically store all registers

 on the stack before each procedure call and restore

 them after the return

 exer

 From Regular Expressions to Automata

 re-fa-sect



 The regular_expression is the notation of choice for describing lexical

 analyzers and other pattern-processing software as was reflected in

 Section lex-sect

 However implementation of that software requires the simulation of a

 DFA as in Algorithm dfa-alg or perhaps simulation of an NFA

 Because an NFA often has a choice of move on an input_symbol (as

 Fig_nfa1-fig does on input from state 0) or on (as

 Fig nfa2-fig does from state 0) or even a choice of making a

 transition on or on a real input_symbol its simulation is

 less straightforward than for a DFA

 Thus often it is important to convert an NFA to a DFA that accepts the

 same language



 In this_section we_shall first show_how to convert NFA's to DFA's

 Then we use this technique known_as the subset_construction to

 give a useful algorithm for simulating NFA's directly in situations

 (other_than lexical analysis) where the NFA-to-DFA conversion takes more

 time than the direct simulation

 Next we show_how to convert regular_expressions to NFA's from which a

 DFA can be constructed if desired

 We_conclude with a discussion of the time-space tradeoffs inherent in

 the various methods for implementing regular_expressions and see_how to

 choose the appropriate method for your application



 Conversion of an NFA to a DFA

 nfa-dfa-subsect



 The general_idea behind the subset_construction is that each state

 of the constructed DFA

 corresponds to a set of NFA_states

 After reading input the DFA is in that state which

 corresponds to the set of states that the NFA can reach from its start

 state following paths labeled



 It is possible that the number of DFA states is exponential in the

 number of NFA_states which could lead to difficulties when we try to

 implement this DFA

 However part of the power of the automaton-based approach to lexical

 analysis is that for real languages the NFA and DFA have approximately

 the same number of states and the exponential behavior is not seen



 alg

 subset-cons-alg

 The subset_construction of a DFA from an NFA



 An NFA



 A DFA_accepting the same language as



 Our algorithm constructs a transition table for

 Each state of is a set of NFA_states and we construct

 so will simulate in parallel all possible moves can make on

 a given input_string

 Our first problem is to deal_with -transitions of

 properly

 In Fig closure-ops-fig we see the definitions of several

 functions that describe basic computations on the states of that

 are needed in the algorithm

 Note_that is a single state of while is a set of states of





 figurehtfb



 center

 tabularll

 Operation Description



 Set of NFA_states reachable from NFA state



 on s alone



 Set of NFA_states reachable from some NFA state



 in set on s alone





 Set of NFA_states to which there is a transition on



 input_symbol from some state in



 tabular

 center



 Operations on NFA_states

 closure-ops-fig



 figure



 We must explore those sets of states that can be in after seeing

 some input_string

 As a basis before reading the first input_symbol can be in any of

 the states of where is its start_state

 For the induction suppose that can be in set of states after

 reading input_string

 If it next reads input then can immediately go to any of the

 states in

 However after_reading it may also make several s thus

 could be in any state of after_reading

 input

 Following these ideas the construction of the set of 's states Dstates and its transition function is shown in

 Fig subset-cons-fig



 figurehtfb



 center

 tabularl

 initially is the only state in Dstates and it is

 unmarked



 while_( there is an unmarked state in Dstates )



 mark



 for ( each input_symbol )







 if_( is not in Dstates )



 add as an unmarked state to Dstates















 tabular

 center



 The subset_construction

 subset-cons-fig



 figure



 The start_state of is and the accepting_states of

 are all those sets of 's states that include at_least one accepting

 state of

 To complete our description of the subset_construction we need only to

 show_how is computed for any set of NFA_states

 This process shown in Fig ecl-fig is a straightforward search

 in a graph from a set of states

 In this case imagine that only the -labeled edges are

 available in the graph

 alg



 figurehtfb



 center

 tabularl

 push all states of onto stack



 initialize to



 while_( stack is not empty )



 pop the top_element off stack



 for ( each state with an edge from to

 labeled )



 if_( is not in )



 add to



 push_onto stack











 tabular

 center



 Computing

 ecl-fig



 figure



 ex

 subset-cons-ex

 Figure nfa5-fig shows another NFA accepting

 it happens to be the one we_shall construct

 directly from this regular_expression in Section re-fa-sect

 Let_us apply_Algorithm subset-cons-alg to Fig nfa5-fig



 figurehtfb

 fileuullmanalsuch3figsnfa5eps

 NFA for

 nfa5-fig

 figure



 The start_state of the equivalent DFA is or

 since these are exactly the states reachable from

 state 0 via a path all of whose edges have label

 Note_that a path can have zero edges so state 0 is reachable from

 itself by an -labeled path



 The input alphabet is

 Thus our first step is to mark and compute

 and



 Among the states 0_1 2_4 and 7 only 2 and 7 have transitions on

 to 3 and 8 respectively

 Thus

 Also so we conclude



 center







 center

 Let_us call this set so



 Now we must compute

 Among the states in only 4 has a transition on and it goes to

 5

 Thus



 center



 center

 Let_us call the above set so



 figurehtfb



 center

 tabularc_c c_c

 NFA State DFA State























 tabular

 center



 Transition table for DFA

 subset-cons-result-fig



 figure



 If we continue this process with the unmarked sets and we

 eventually reach a point where all the states of the DFA are marked

 This conclusion is guaranteed since there are only

 different subsets of a set of eleven NFA_states

 The five different DFA states we actually construct their corresponding

 sets of NFA_states and the transition table for the DFA are shown

 in Fig subset-cons-result-fig and the transition graph for

 is in Fig_dfa2-fig

 State is the start_state and state which contains state 10 of

 the NFA is the only accepting_state



 figurehtfb

 fileuullmanalsuch3figsdfa2eps

 Result of applying the subset_construction to

 Fig nfa5-fig

 dfa2-fig

 figure



 Note_that has one more state than the DFA of Fig dfa1-fig for

 the same language

 States and have the same move function and so can be merged

 We discuss the matter of minimizing the number of states of a DFA in

 Section dfa-min-subsect

 ex



 Simulation of an NFA

 nfa-sim-subsect



 A strategy that has_been used in a number of text-editing programs is

 to construct an NFA from a regular_expression and then simulate the NFA

 using something_like an on-the-fly subset_construction

 The simulation is outlined below



 alg

 nfa-sim-alg

 Simulating an NFA



 An input_string terminated by an end-of-file character eof

 An NFA with start_state accepting_states and transition

 function



 Answer yes if accepts no otherwise



 The algorithm keeps a set of current states those that are reached

 from following a path labeled by the inputs read so_far

 If is the next_input character read by the function nextChar() then we first compute and then close that set

 using

 The algorithm is sketched in Fig_nfa-sim-fig

 alg



 figurehtfb



 center

 tabularr_l

 1)



 2) nextChar()



 3)_while ( eof )



 4)



 5) nextChar()



 6)



 7) if_( )_return yes



 8) else_return no



 tabular

 center



 Simulating an NFA

 nfa-sim-fig



 figure



 Efficiency of NFA Simulation

 nfa-eff-subsect



 If carefully implemented Algorithm nfa-sim-alg can be quite

 efficient

 As the ideas involved are useful in a number of similar algorithms

 involving search of graphs we_shall look_at this implementation in

 additional detail

 The data_structures we need are



 enumerate



 Two stacks each of which holds a set of NFA_states

 One of these stacks oldStates

 holds the current set of states ie the value of on the right

 side of line_(4) in Fig_nfa-sim-fig

 The second newStates holds

 the next set of states - on the left_side of line_(4)

 Unseen is a step where as we go_around the loop of lines_(3) through

 (6) newStates is transferred to oldStates



 A boolean array alreadyOn

 indexed by the NFA_states to indicate which states are in newStates

 While the array and stack hold the same information it is much faster

 to interrogate alreadyOn than to search for state on the

 stack newStates

 It is for this efficiency that we maintain both representations



 A two-dimensional_array holding the transition table of the NFA

 The entries in this table which are sets of states are represented_by

 linked_lists



 enumerate



 To implement line_(1) of Fig_nfa-sim-fig we need to set each

 entry in array alreadyOn to FALSE then for each state

 in push_onto oldStates and set alreadyOn to

 TRUE

 This operation on state and

 the implementation of line_(4) as_well are facilitated by a function we_shall

 call addState

 This function pushes state onto newStates sets alreadyOn to TRUE and calls itself recursively on the

 states in in order to further the computation of



 However to avoid duplicating work we must_be careful never to call

 addState on a state that is already on the stack newStates

 Figure addstate-fig sketches this function



 figurehtfb



 center

 tabularr_l

 9) addState()



 10) push_onto newStates



 11) alreadyOn TRUE



 12) for ( on )



 13) if_( alreadyOn )



 14) addState()



 15)



 tabular

 center



 Adding a new state which is known not to be on newStates

 addstate-fig



 figure



 We implement line_(4) of Fig_nfa-sim-fig by_looking at each state

 on oldStates

 We first find the set of states where is the next

 input and for each of those states

 that is not already on newStates we

 apply addState to it

 Note_that addState has the effect of computing the and

 adding all those states to newStates as_well if they_were not

 already on

 This sequence of steps is summarized in Fig step4-fig



 figurehtfb



 center

 tabularr_l

 16) for ( on oldStates )



 17) for ( on )



 18) if_( alreadyOn )



 19) addState



 20) pop from oldStates



 21)







 22) for ( on newStates )



 23) pop from newStates



 24) push_onto oldStates



 25) alreadyOn FALSE



 26)



 tabular

 center



 Implementation of step (4) of Fig_nfa-sim-fig

 step4-fig



 figure



 Now suppose that the NFA has states and transitions ie

 is the sum over all states of the number of symbols (or ) on

 which the state has a transition out

 Not counting the call to addState at line (19) of

 Fig step4-fig the time_spent in the loop of lines (16) through

 (21) is

 That is we can go_around the loop at most times and each step of

 the loop requires constant work except for the time_spent in

 addState

 The same is true of the loop of lines (22) through (26)



 Big-Oh Notation

 An expression like is a shorthand for at most some constant

 times

 Technically we say a function perhaps the running_time of some

 step of an algorithm is if there are constants

 and such that whenever it is true that



 A useful idiom is which means some constant

 The use of this big-oh notation enables us to avoid getting too

 far into the details of what we count as a unit of execution time yet

 lets_us express the rate at which the running_time

 of an algorithm grows



 During one execution of Fig step4-fig ie of step (4) of

 Fig_nfa-sim-fig it is only possible to call addState on a

 given state once

 The_reason is that whenever we call addState we set alreadyOn to TRUE at line (11) of Fig addstate-fig

 Once alreadyOn is TRUE the tests at line (13) of

 Fig addstate-fig and line (18) of Fig step4-fig prevent

 another call



 The time_spent in one call to addState exclusive of the time

 spent in recursive_calls at line (14) is for lines (10) and

 (11)

 For lines (12) and (13) the time depends_on how many s there

 are out of state

 We do_not know this number for a given state but we know that there are

 at most transitions in total out of all states

 As a result the aggregate time_spent in lines (12) and (13) over all calls to

 addState during one execution of the code of Fig step4-fig

 is

 The aggregate for the rest of the steps of addState is

 since it is a constant per call and there are at most calls



 We_conclude that implemented properly the time to execute line_(4)

 of Fig_nfa-sim-fig is

 The rest of the while-loop of lines_(3) through_(6) takes time

 per iteration

 If the input is of length then the total work in that loop is



 Line (1) of Fig_nfa-sim-fig

 can be executed in time since it is

 essentially the steps of Fig step4-fig with oldStates

 containing only the state

 Lines (2) (7) and (8) each take time

 Thus the running_time of Algorithm nfa-sim-alg properly

 implemented is

 That is the time taken is proportional to the length of the input times

 the size (nodes plus edges) of the transition graph



 Construction of an NFA from a Regular Expression

 re-nfa-subsect



 We_now give an algorithm for converting any regular_expression to

 an NFA that defines the same language

 The algorithm is syntax-directed in the sense that it works recursively

 up the parse_tree for the regular_expression

 For each subexpression the algorithm constructs an NFA with a single

 accepting_state



 alg

 my-alg

 The McNaughton-Yamada-Thompson algorithm to convert a regular_expression

 to an NFA



 A regular_expression over alphabet



 An NFA accepting



 Begin by parsing into its constituent subexpressions

 The rules for constructing an NFA consist of basis rules for handling

 subexpressions with no operators and inductive rules for constructing

 larger NFA's from the NFA's for the immediate subexpressions of a given

 expression



 For expression construct the NFA



 fileuullmanalsuch3figsmy-basis1eps



 Here is a new state the start_state of this NFA and is

 another new state the accepting_state for the NFA



 For any subexpression in construct the NFA



 fileuullmanalsuch3figsmy-basis2eps



 where again and are new states the start and accepting_states

 respectively

 Note_that in both of the basis constructions we construct a distinct

 NFA with new states for every occurrence of or some as

 a subexpression of



 Suppose and are NFA's for regular_expressions and

 respectively



 itemize



 a)

 Suppose

 Then the NFA for is constructed as in

 Fig my-union-fig

 Here and are new states the start and accepting_states of

 respectively

 There_are s from to the start states of and

 and each of their accepting_states have s to the accepting_state



 Note_that the accepting_states of and are not accepting in



 Since any path from to must pass_through either or

 exclusively and since the label of that path is not changed by the

 's leaving or entering we conclude that

 accepts which is the same as

 That is Fig my-union-fig is a correct construction for the union

 operator



 figurehtfb

 fileuullmanalsuch3figsmy-unioneps

 NFA for the union of two regular_expressions

 my-union-fig

 figure



 b)

 Suppose

 Then construct as in Fig my-cat-fig

 The start_state of becomes the start_state of and the

 accepting_state of is the only accepting_state of

 The accepting_state of and the start_state of are merged

 into a single state with all the transitions in or out of either

 state

 A path from to in Fig my-cat-fig must go first through

 and therefore its label will begin_with some string in

 The path then continues through so the path's label finishes

 with a string in

 As we_shall soon argue accepting_states never have edges out and

 start states never have edges in so it is not possible for a path to

 re-enter after leaving it

 Thus accepts exactly and is a correct NFA for





 figurehtfb

 fileuullmanalsuch3figsmy-cateps

 NFA for the concatenation of two regular_expressions

 my-cat-fig

 figure



 c)

 Suppose

 Then for we construct the NFA shown in

 Fig my-star-fig

 Here and are new states the start_state and lone accepting

 state of

 To get from to we can either follow the introduced path labeled

 which takes care of the one string in or we can go

 to the start_state of through that NFA then from its accepting

 state back to its start_state zero_or more times

 These options allow to accept all the strings in

 and so on so the entire set of strings accepted_by is





 figurehtfb

 fileuullmanalsuch3figsmy-stareps

 NFA for the closure of a regular_expression

 my-star-fig

 figure



 d)

 Finally suppose

 Then and we can use the NFA as

 itemize

 alg



 The method description in Algorithm_my-alg contains hints as to

 why the inductive construction works as it should

 We_shall not give a formal correctness proof but we_shall list several

 properties of the constructed NFA's in addition to the all-important

 fact that accepts language

 These properties are interesting in their own right and helpful in

 making a formal proof



 enumerate



 has at most twice as many states as there are operators and

 operands in

 This bound follows from the fact that each step of the

 algorithm creates at most two new

 states



 has one start_state and one accepting_state

 The accepting_state has no outgoing transitions and the start_state has

 no incoming transitions



 Each state of other_than the accepting_state has either one

 outgoing transition on a symbol in or up to two outgoing transitions

 both on



 enumerate



 figurehtfb

 fileuullmanalsuch3figsmy-treeeps

 Parse_tree for

 my-tree-fig

 figure



 ex

 my-ex

 Let_us use Algorithm_my-alg to construct an NFA for



 Figure my-tree-fig shows a parse_tree for that is analogous to the

 parse_trees constructed for arithmetic_expressions in

 Section parse-tree-subsect

 For subexpression the first we construct the NFA



 fileuullmanalsuch3figsmy-ex1eps



 State numbers have_been chosen for consistency with what_follows

 For we construct



 fileuullmanalsuch3figsmy-ex2eps



 We can now combine and using the construction of

 Fig my-union-fig to obtain the NFA for this NFA is

 shown in Fig my-ex3-fig



 figurehtfb

 fileuullmanalsuch3figsmy-ex3eps

 NFA for

 my-ex3-fig

 figure



 The NFA for is the same as that for

 The NFA for is then as shown in Fig my-ex4-fig

 We have used the construction in Fig my-star-fig to build this

 NFA from the NFA in Fig my-ex3-fig



 figurehtfb

 fileuullmanalsuch3figsmy-ex4eps

 NFA for

 my-ex4-fig

 figure



 Now_consider subexpression which is another

 We use the basis construction for again but we must use new states

 It is not permissible to reuse the NFA we constructed for even

 though and are the same expression

 The NFA for is



 fileuullmanalsuch3figsmy-ex5eps



 To obtain the NFA for we apply the construction of

 Fig my-cat-fig

 We merge states 7 and yielding the NFA of Fig my-ex6-fig

 Continuing in this fashion with new NFA's for the two subexpressions

 called and we eventually construct the NFA for

 that we first met in Fig nfa5-fig

 ex



 figurehtfb

 fileuullmanalsuch3figsmy-ex6eps

 NFA for

 my-ex6-fig

 figure



 Efficiency of String-Processing Algorithms

 str-eff-subsect



 We observed that Algorithm dfa-alg processes a string in time

 while in Section nfa-eff-subsect we concluded that we

 could simulate an NFA in time proportional to the product of and

 the size of the NFA's transition graph

 Obviously it is faster to have a DFA to simulate than an NFA so we

 might wonder whether it ever makes_sense to simulate an NFA



 One issue that may favor an NFA is that the subset

 construction can in the worst_case exponentiate the number of states

 While in principle the number of DFA states does_not influence the

 running_time of Algorithm dfa-alg should the number of states

 become so large that the transition table does_not fit in main memory

 then the true running_time would have to include disk IO and therefore

 rise noticeably



 ex

 exponential-ex

 Consider the family of languages described by regular_expressions of the

 form that is each language

 consists of strings of 's and 's such that the th character

 to the left of the right_end holds

 An -state NFA is easy to construct

 It stays in its initial_state under any input but also has the option

 on input of going to state 1

 From state 1 it goes to state 2 on any input and so on until in

 state it accepts

 Figure exp-nfa-fig suggests this NFA



 figurehtfb



 fileuullmanalsuch3figsexp-nfaeps

 An NFA that has many fewer states than the smallest equivalent

 DFA

 exp-nfa-fig

 figure



 However any DFA for the language must have at_least states

 We_shall not prove this fact but the idea is that if two strings of

 length can get the DFA to the same state then we can exploit the

 last position where the strings differ (and_therefore one must have

 the other ) to continue the strings identically until they are

 the same in the last positions

 The DFA will then be in a state where it must both accept

 and not accept

 Fortunately as we mentioned it is rare for lexical analysis to involve

 patterns of this type and we do_not expect to encounter DFA's with

 outlandish numbers of states in practice

 ex



 However lexical-analyzer generators and other string-processing systems

 often start with a regular_expression

 We are faced with a choice of converting the regular_expression to an

 NFA or DFA

 The additional cost of going to a DFA is thus the cost of executing

 Algorithm subset-cons-alg on the NFA (one could go directly from a regular

 expression to a DFA but the work is essentially the same)

 If the string-processor is one that will be executed many_times as is

 the case for lexical analysis then any cost of converting to a DFA is

 worthwhile

 However in other string-processing applications such_as grep

 where the user specifies one regular_expression and one or several files

 to be searched for the pattern of that expression it may be more

 efficient to skip the step of constructing a DFA and simulate the NFA

 directly



 Let_us consider the cost of converting a regular_expression to an NFA by

 Algorithm_my-alg

 A key step is constructing the parse_tree for

 In Chapter_parse-ch we_shall see several methods that are capable

 of constructing this parse_tree in linear time that is in time

 where stands_for the size of - the sum of

 the number of operators and operands in

 It is also easy to check that each of the basis and inductive

 constructions of Algorithm_my-alg takes constant time so the entire

 time_spent by the conversion to an NFA is



 Moreover as we observed in Section re-nfa-subsect the NFA we

 construct has at most states and at most transitions

 That is in terms of the analysis in Section nfa-eff-subsect we

 have and

 Thus simulating this NFA on an input_string takes time



 This time dominates the time taken by the NFA construction which is

 and therefore we conclude that it is possible to take a

 regular_expression and string and tell_whether is in

 in time



 The time taken by the subset_construction is highly dependent on the

 number of states the resulting DFA has

 To_begin notice that in the subset_construction of

 Fig subset-cons-fig the key step the construction of a set of

 states from a set of states and an input_symbol is very

 much like the construction of a new set of states from the old set of

 states in the NFA simulation of Algorithm nfa-sim-alg

 We already concluded that properly implemented this step takes time

 at most proportional to the number of states and transitions of the NFA



 Suppose we start with a regular_expression and convert it to an

 NFA

 This NFA has

 at most states and at most transitions

 Moreover there are at most input symbols

 Thus for every DFA state constructed we must construct at most

 new states and each one takes at most time

 The time to construct a DFA of states is thus



 In the common case_where is about the subset_construction

 takes time

 However in the worst_case as in Example exponential-ex this

 time is

 Figure string-options-fig summarizes the options when one is given

 a regular_expression and wants to produce a recognizer that will

 tell_whether one or_more strings are in



 figurehtfb



 center

 tabularccc

 Automaton Initial Per

 String



 NFA



 DFA typical case



 DFA worst_case



 tabular

 center



 Initial cost and per-string-cost of various methods of

 recognizing the language of a regular_expression

 string-options-fig



 figure



 If the per-string cost dominates as it does when we build a lexical

 analyzer we clearly prefer the DFA

 However in commands like grep where we run the automaton on only

 one string we generally prefer the NFA

 It is not until approaches that we would even think about

 converting to a DFA



 There is however a mixed strategy that is about as good as the better

 of the NFA and the DFA strategy for each expression and string

 Start off simulating the NFA but remember the sets of NFA_states (ie

 the DFA states) and

 their transitions as we compute them

 Before processing the current set of NFA_states and the current_input

 symbol check to see whether we have_already computed this transition

 and use the information if so



 exer

 Convert to DFA's the NFA's of



 itemize



 a) Fig nfa2-fig

 b) Fig nfa3-fig

 c) Fig nfa4-fig



 itemize

 exer



 exer

 use Algorithm nfa-sim-alg to simulate the NFA's



 itemize



 a) Fig nfa3-fig

 b) Fig nfa4-fig



 itemize

 on input

 exer



 exer

 re-dfa-exer

 Convert the following regular_expressions to deterministic_finite

 automata using algorithms my-alg and subset-cons-alg



 itemize



 a)

 b)

 c)

 d)



 itemize

 exer

 Region-Based_Analysis



 The iterative_data-flow analysis algorithm we have discussed so_far is

 just one approach to solving_data-flow problems Here we discuss

 another approach called region-based_analysis In the iterative-analysis

 approach we create transfer_functions for basic_blocks then

 find the fixedpoint solution by repeated_passes over the blocks Instead of

 creating transfer_functions just for individual basic_blocks a

 region-based_analysis finds transfer_functions that summarize the

 execution of progressively_larger regions of the program

 Ultimately transfer_functions for

 entire procedures are constructed and then applied to get the desired

 data-flow_values directly



 While a data-flow_framework using an iterative_algorithm is

 specified_by a semilattice of data-flow_values and a family of

 transfer_functions closed_under composition region-based_analysis

 requires more

 A region-based framework

 includes both a semilattice of data-flow_values and a

 semilattice of transfer_functions that must possess

 a meet_operator a composition

 operator and a closure_operator

 We_shall see what all these elements entail in

 Section



 A region-based_analysis is particularly useful for data-flow_problems

 where paths that have cycles may change the data-flow_values

 The closure_operator allows the effect of a loop to be summarized more

 effectively than does iterative analysis The technique is also useful for

 interprocedural_analysis where transfer_functions associated_with a

 procedure may be treated_like the transfer_functions associated_with

 basic_blocks



 For_simplicity we_shall only consider forward_data-flow problems in

 this_section We first illustrate_how region-based_analysis works by

 using the familiar example of reaching_definitions

 In Section we show a

 more compelling use of this technique when we

 study the analysis of induction_variables



 Regions



 In region-based_analysis code is organized as a hierarchy of regions which are (roughly) portions of a flow_graph that have

 only one point of entry You_should find this concept of viewing

 code as a hierarchy of regions intuitive because a block-structured

 procedure is naturally organized as a hierarchy of regions Each

 statement in a block-structured program is a region as control_flow

 can only enter at the beginning of a statement Each level of

 statement nesting corresponds to a level in the region_hierarchy



 Formally a region of a control_flow graph is a

 subgraph with only a single point of entry More formally a region

 is a collection of nodes and edges such that







 There is a header in that dominates all the nodes in



 If some node can reach a node in without_going

 through then is also in



 is the set of all the control_flow edges between nodes and

 in except (possibly) for some that enter







 Clearly a natural_loop is a region but a region does_not necessarily

 have a back_edge and need not contain any cycles For

 example in Fig nodes and together_with

 the edge

 form a region so do nodes and with

 edges and



 However the subgraph with

 nodes and with edge does_not form a region

 because control

 may enter the subgraph at both nodes and

 More_precisely neither_nor dominates the other so

 condition (1) for a region is violated

 Even_if we picked say to be the header we

 would violate condition (2) since we can reach from without

 going_through and is not in the region





 figureuullmanalsuch9figsregionseps

 Examples of regions



 Region Hierarchies for Reducible Flow_Graphs



 In what_follows we_shall assume the flow_graph is reducible

 If occasionally we must deal_with nonreducible_flow graphs then we can

 use a technique called node_splitting discussed in

 Section



 To construct a hierarchy of regions we identify the natural_loops

 Recall from Section that in a reducible_flow

 graph any two natural_loops are either_disjoint or one is nested

 within the other

 The process of parsing a reducible_flow graph into its hierarchy of

 loops begins_with every block as a region by itself

 We call these regions leaf_regions

 Then we order the natural_loops from the inside out ie_starting

 with the innermost_loops

 To process a loop we replace the entire loop by a node in two steps







 First the body of the loop (all nodes and edges except the back

 edges to the header) is replaced_by a node representing a region

 Edges to the header of now enter the node for

 Edges from any exit of loop are replaced_by edges from to the

 same destination

 However if the edge is a back_edge then it becomes a loop on

 We call a body_region



 Next we construct a region that represents the entire_natural

 loop

 We call a loop region

 The only_difference between and is that the latter includes

 the back_edges to the header of loop

 Put_another way when replaces in the flow_graph all we have

 to do is remove the edge from to itself



 We proceed this way reducing larger and larger loops to single nodes

 first with a looping edge and then without

 Since loops of a reducible_flow graph are nested or disjoint the loop

 region's node can represent all the nodes of the natural_loop

 in the series of flow_graphs

 that are constructed by this reduction process



 Eventually all natural_loops are reduced to single nodes

 At that point the flow_graph may be reduced to a single_node or there

 may be several nodes remaining with no loops ie the reduced flow

 graph is an acyclic_graph of more_than one node

 In the former_case we are done constructing the region_hierarchy while

 in the latter case we construct one more body_region for the entire

 flow_graph





 figureuullmanalsuch9figsrd-regioneps

 (a) An_example flow_graph for the reaching_definitions problem

 and (b) Its region_hierarchy





 Consider the control_flow graph in Fig (a) There is one

 back_edge in this flow_graph which leads from to The

 hierarchy of regions is shown in Fig (b) the

 edges shown are the edges in the region flow_graphs There_are

 altogether 8 regions







 Regions are leaf_regions representing

 basic_blocks through

 respectively Every basic_block is also an exit block in its

 region



 Body region

 represents the body of the only loop in the flow_graph it consists

 of regions and and three_interregion edges

 and It has two exit blocks and

 since they both have outgoing_edges not contained in the region

 Figure (a) shows the flow_graph with reduced to

 a single_node

 Notice_that although the edges and

 have both been replaced_by edge it is important to

 remember that the latter edge represents the two former edges since we

 shall have to propagate transfer_functions across this edge eventually

 and we need to know that what comes out of both blocks and

 will reach the header of



 Loop region

 represents the entire_natural loop It includes one

 subregion and one back_edge

 It has also two exit

 nodes again and

 Figure (b) shows the flow_graph after the entire

 natural_loop is reduced to



 Finally body_region is the top_region

 It includes three regions

 and three_interregion edges

 and

 When we reduce the flow_graph to it becomes a single_node

 Since there are no back_edges to its header there is no need

 for a final_step reducing this body_region to a loop region







 fileuullmanalsuch9figsreducingeps

 Steps in the reduction of the flow_graph of Fig to a single region



 To summarize the process of decomposing reducible_flow

 graphs hierarchically we offer the following algorithm





 Constructing a bottom-up order of regions of a reducible_flow graph



 A reducible_flow graph



 A list of regions of that can be used in region-based data-flow

 problems







 Begin the list with all the leaf_regions consisting of single blocks of

 in any order



 Repeatedly choose a natural_loop such that if there are any natural

 loops contained_within then these loops have had their body and

 loop regions added to the list already

 Add first the region consisting of the body of (ie without

 the back_edges to the header of ) and then the loop region of



 If the entire_flow graph is not itself a natural_loop add at the end

 of the list the region consisting of the entire_flow graph





 Where Reducible Comes_From

 We_now see_why reducible_flow graphs were given that name

 While we_shall not prove this fact the original definition of

 reducible_flow graph involving the back_edges of the graph is

 equivalent to several definitions in which we mechanically reduce the

 flow_graph to a single_node

 The process of collapsing natural_loops described_here is one of them

 Another interesting definition is that the reducible_flow graphs are

 all and only those that can be reduced to a single_node by the

 following two transformations









 Remove an edge from a node to itself





 If node has a single predecessor and is not the entry of

 the flow_graph combine and







 Overview of a Region-Based_Analysis



 For each region and for each block within we

 compute a transfer_function that summarizes the effect

 of executing all possible paths within

 that lead from the entry of

 to the exit of

 This computation begins_with regions that are single blocks where

 is just the transfer_function for the block

 itself



 We then proceed up the region_hierarchy computing transfer_functions

 for progressively_larger regions

 If is a body_region then the edges belonging

 to form an acyclic_graph on the subregions of

 We may proceed to compute the transfer_functions in a

 topological_order of the subregions

 If is a loop region then we only need to account for the effect of

 the back_edges to the header of

 How we perform each of these computations will be seen in

 Algorithm



 Once we have computed the transfer_functions for the region

 that is the

 entire_flow graph we can apply the transfer_function

 to obtain for

 each block in the entire_flow graph

 If we_want the values as we normally do for forward-flow

 problems then we compute these values by_taking the meet of

 over all the predecessors of block



 Necessary Assumptions About Transfer_Functions



 In order for region-based_analysis to work we need to make certain

 assumptions about properties of the set of transfer_functions in the

 framework

 Specifically we need three

 primitive operations on transfer_functions composition meet and

 closure only the first is required for data-flow_frameworks that use

 the iterative_algorithm



 Composition



 The transfer_function of a sequence of

 nodes can be derived by_composing the functions representing the

 individual nodes Let and be transfer_functions of nodes

 and The effect of executing followed_by is

 represented_by

 Function composition has_been discussed in

 Section and an example using reaching_definitions

 was shown in Section To review let and

 be the

 and sets for

 Then





 f2 f1 (x) gen2 ((gen1 (x_- kill1))

 - kill2)



 (gen2 (gen1 -_kill2)) (x_- (kill1_kill2))

 Thus the set and sets for

 are and

 respectively



 Meet



 Here the transfer_functions themselves are values of a semilattice

 with a meet_operator

 The meet of two transfer_functions and

 is defined by

 where is the

 meet_operator for data-flow_values

 The meet_operator on transfer_functions

 is used to combine the effect of alternative paths of

 execution with the same end points Where it is not ambiguous from

 now on we_shall refer to the meet_operator of transfer_functions

 also as

 For the reaching-definitions_framework we have





 (f1 f2)(x) f1(x)_f2(x)



 (gen1 (x_- kill1))

 (gen2 (x_- kill2))



 (gen1 gen2) (x_- (kill1_kill2))

 That is the and sets for are

 and

 respectively



 Closure



 If represents the transfer_function of a

 cycle then represents the effect of going_around the cycle

 times In the case_where the number of iterations is not known we

 have to assume that the loop may be executed 0 or_more times We

 represent the transfer_function of such a loop by the closure of which is

 defined by









 Note_that must_be the identity transfer

 function since it represents the

 effect of going zero times_around the loop ie_starting at the entry

 and not moving

 If we let represent the identity transfer_function then we can

 write











 Suppose the transfer_function in a reaching_definitions framework

 has a set and a set Then





 f2(x) f(f(x))



 (gen ((gen (x_- kill)) -_kill)



 gen_(x -_kill)



 f3(x) f(f2(x))



 gen_(x -_kill)



 and so on any is

 That is going_around a loop doesn't affect the transfer_function if it

 is of the - form

 Thus





 f(x) I f1(x)_f2(x)



 x (gen (x_- kill))



 gen x

 That is the and sets for are and

 respectively Intuitively since we might not go_around a loop at all

 anything in will reach the entry to the loop

 In all subsequent_iterations the

 reaching_definitions include those in the set



 An Algorithm for Region-Based_Analysis



 The following algorithm solves a forward data-flow-analysis problem on

 a reducible_flow graph according to some framework that satisfies the

 assumptions of Section

 Recall that is notation for the transfer_function that

 transforms data-flow_values at the entry to region into the correct

 value at the exit of block which must_be one of the blocks

 contained_within region





 Region-Based_Analysis



 A data-flow_framework with the properties outlined in

 Section and a reducible_flow graph



 Data-flow values for each basic_block of







 Use_Algorithm to construct the bottom-up sequence

 of regions of say



 For each region in the bottom-up order do the following



 If is a leaf region corresponding to block let

 the transfer_function associated_with block by the

 flow_graph

 If is a body_region perform the computation of

 Fig (a) with

 If is a loop region perform the computation of Fig (b)

 with



 For each block apply the transfer_function for the

 entire_flow graph and its block to



 For each block of compute



















 Details of region-based data-flow computations





 Thus in lines_(1) and (2) of Fig (a) we arrange

 to visit each subregion of a body_region in some topological_order

 Line (3) computes to be the transfer_function representing all

 paths from the header of to the header of

 Notice_that all the predecessors must_be in regions that precede

 in the topological_order constructed at line_(1)

 Thus will already have_been computed and can be used in

 line_(3)

 Line (4)computes the transfer_function for within

 That transfer_function is followed_by whatever transfer_function

 expresses the transformation within going from its header to the

 end of



 For loop regions we perform the steps of lines (5) through_(7)

 Line (5) computes to represent the effect of going_around the loop of

 region once

 Then line_(7) reflects the principle that each path from the

 header of to staying wholly within consists of







 A path that

 goes zero_or

 more times_around the loop (expressed by ) followed_by



 A path

 from the header to within the body_region













 Computing transfer_functions for the flow_graph in

 Fig (a) using region-based_analysis





 Let_us apply_Algorithm to the flow_graph in

 Fig (a)

 Step 1 constructs an order in which the regions are visited this

 order will be the numerical order of their subscripts



 The transfer_functions constructed in Step_2 are summarized in

 Fig

 Since there are many computations many of them similar we_shall give

 only some examples of the calculation

 We encourage you to work through the entire example step by step



 We are working a reaching-definitions_problem and this framework

 involves

 transfer_functions of the -

 type

 The values of the and sets for the five basic_blocks

 are summarized

 below







 Remember the simplified rules for - transfer_functions

 from Section







 To take the meet of transfer_functions take the union of the 's

 and the intersection of the 's



 To compose transfer_functions take the union of both the 's and

 the 's

 However as an exception an expression that is generated_by the first

 function not generated_by the second but killed

 by the second is not in the of the result



 To take the closure of a transfer_function retain its and replace

 the by





 The first five regions are basic_blocks

 Thus their transfer_functions







 are just

 the transfer_functions for the basic_blocks themselves

 These and sets appear in Fig (a)



 Next consider the transfer_functions for body_region which is

 blocks and without the back_edge

 The order of processing these blocks will be the only topological_order



 First has no predecessors within remember that the edge

 goes outside

 Thus for we compose the identity_function with

 which just gives_us ie the transfer

 function for block itself



 Block has one_predecessor within_namely

 We thus compute to be the meet of the single transfer_function

 and we compose this function with the transfer_function

 for within its_own region

 Notice_that we have_already computed so we may use it to

 compute transfer_functions for and the subregions of that

 appear later in the topological_order for



 Last for we must compute

 because both and are predecessors of within

 This transfer_function is composed with the transfer_function

 to get the desired function

 Notice for example that is not_killed in this transfer_function

 because the path does_not redefine variable



 Now_consider loop region

 It has transfer_functions for the same blocks as

 Since there is only one back_edge to the header of

 the transfer_function that we compute is just

 Then we must compute which takes the set

 from and has set

 This transfer_function is composed with each of the three transfer

 functions of to get the corresponding transfer_functions of

 Notice for instance how is in the set for

 because of paths like or even





 Finally consider the entire_flow graph

 Its subregions are and which we_shall consider in

 that topological_order

 Transfer_function is just which in turn is



 The_reason is that has no predecessors so we obtain

 by_composing the identity_function with



 The header of which is has only one_predecessor so

 we compose with each of the transfer_functions for

 and within to obtain their transfer_functions within



 Lastly we consider

 Its header has two predecessors within_namely and



 Therefore we compute and compose that

 with to get

 Notice_that is the identity_function since both its

 and sets are empty



 We are now done with step 2 of Algorithm

 In step 3 we must apply each of the transfer_functions for to

 since is the entry_node of the flow_graph

 The boundary conditions for the reaching-definitions_framework tells_us

 that is the empty_set ie no definitions reach

 the entry

 Thus for each block which is the

 same as the set for

 These sets are summarized in

 Fig













 Final steps of region-based flow analysis





 The algorithm completes with step 4

 Here we simply compute for each block

 the meet (of data-flow_values not of transfer

 functions) of the out-sets of its_predecessors

 These results also are summarized in

 Fig

 Note_that these values are exactly what we would get had we applied

 iterative_data-flow analysis to the same flow_graph as must_be the

 case of course



 Efficiency of Region-Based_Analysis



 It is quite tricky to compare the iterative_algorithm with the

 hierarchical_algorithm

 To_begin consider the iterative_approach

 We observed in Section that the

 depth of a reducible_flow graph (as_defined in Section ) is

 typically 3 and therefore or about

 5 passes_through the flow_graph suffice

 Thus if there are nodes there will be or about

 computations of data-flow

 values - half for the 's and half for the 's

 Some of these computations may be more_complex than_others especially

 as a node may have a large_number of predecessors and therefore take

 extra time to compute its value

 However as we_wish to compare the running_time of two algorithms on the

 same flow_graph it is reasonable to count only the values computed and

 not worry_about the time needed to compute each one



 For the sake of comparison assume we are applying a data-flow_framework

 whose transfer_functions are of the - type

 Thus the data-flow_values can be represented_by bit_vectors as

 discussed in Section

 We may therefore estimate the cost of the iterative_algorithm as the

 computation of bit_vectors



 If we look_at the hierarchical_algorithm applied to a flow_graph of

 nodes and depth we observe several things







 Since we compute only 's in step 2 of Algorithm

 the work at a node that is in regions (other_than the region that is

 the node itself) involves computing values



 However each of those values is a transfer_function not a data-flow

 value and we require two bit_vectors to represent a transfer_function -

 one for and one for

 Thus bit-vectors are computed



 If the flow graph's depth is then there are at most regions

 in which any node finds itself body regions loop regions

 surrounding them and perhaps a final body_region representing the

 entire_flow graph

 Thus in step 2 the number of bit_vectors computed for all nodes is no more

 than and it could be less



 The third_step involves no computation since the 's are just the

 components of the corresponding transfer_function



 The last step of Algorithm computes data-flow

 values consisting of one bit_vector each





 Therefore the total_number of bit_vectors computed by the hierarchical

 algorithm is at most

 That figure appears to be

 almost twice the bit_vectors computed by the

 iterative_algorithm

 Further the

 hierarchical_algorithm is conceptually more_complex especially if

 there are nonreducible_flow graphs with which to contend

 However there are two important reasons_why the region-based approach

 turns_out to be more rather_than less efficient than the iterative

 approach







 a)

 The estimates of bit_vectors computed assume each node is at the maximum

 depth for the flow_graph In reality many nodes of a flow_graph will

 appear inside only one loop or no loops and the innermost_loops tend

 to be small

 Thus the average_number of regions in which a node appears

 may be considerably less_than our

 assumed regions



 b)

 One can avoid computing if is not the entire_flow graph

 and is not an exit of

 region ie there is no node out of that goes to a node not

 in

 The_reason is that if is a node which is an exit of then one

 can skip_over nodes like and compute for some region

 containing if we use the transfer_function for from the header

 of to

 For_example if of Fig were some interior

 region and not the entire_flow graph then we would have no need for

 in the computation of transfer_functions for

 We could compute by_composing (because

 is the only predecessor of the header of ) with





 Handling Nonreducible Flow_Graphs



 If nonreducible_flow graphs are expected to be common for the programs

 to be processed by a compiler or other program-processing software then

 we recommend using an iterative rather_than a hierarchy-based approach

 to data-flow_analysis

 However if we need only to be prepared for the occasional nonreducible

 flow_graph then the following node-splitting technique is

 adequate



 Construct regions from natural_loops to the extent possible

 If the flow_graph is nonreducible we_shall find that the resulting

 graph of regions has cycles but no back_edges so we cannot parse the

 graph any further

 A_typical situation is suggested in Fig (a)

 which has the same structure as the nonreducible_flow graph of

 Fig but the nodes in

 Fig may actually be complex regions as

 suggested by the smaller nodes within







 fileuullmanalsuch9figsnode-splittingeps

 Duplicating a region to make a nonreducible_flow graph become

 reducible



 We pick some region that has more_than one_predecessor and is not the

 header of the entire_flow graph

 If has predecessors make copies of the entire_flow graph

 and connect each predecessor of 's header to a different copy of



 Remember that only the header of a region could possibly have a

 predecessor outside that region

 It_turns out although we_shall not prove it that such node_splitting

 results in a reduction by at_least one in the number of regions after

 new back_edges are identified and their regions constructed

 The resulting graph may still not be reducible but by alternating a

 splitting phase with a phase where new natural_loops are identified and

 collapsed to regions we eventually are left with a single region ie

 the flow_graph has_been reduced





 The splitting shown in Fig (b) has turned the

 edge into a back_edge since now dominates



 These two regions may thus be_combined into one

 The resulting three regions - and the new region form

 an acyclic_graph and therefore may be_combined into a single body

 region

 We have thus successfully reduced the entire_flow graph to a single

 region

 In_general additional splits may be necessary and in the worst_case

 the total_number of basic_blocks could become exponential in the number

 of blocks in the original flow_graph



 We must also think about how the result of the data-flow_analysis on the

 split flow_graph relates to the answer we desire for the original flow

 graph

 There_are two approaches we might consider







 Splitting regions may be beneficial for the optimization process and we

 can simply revise the flow_graph to have copies of certain basic_blocks

 Since each duplicated block is entered along only a subset of the paths

 that reached the original the data-flow_values at these duplicated

 blocks will tend to contain more specific information than was available

 at the original

 For_instance fewer definitions may reach each of the duplicated_blocks

 that reach the original block



 If we_wish to retain the original flow_graph with no splitting then

 after analyzing the split flow_graph we look_at each split block

 and its corresponding set of blocks

 We may compute

 and

 similarly for the 's







 For the flow_graph of Fig (see the exercises for

 Section )







 Find all the possible regions You_may however omit from

 the list the regions consisting of a single_node and no edges



 Give the set of nested regions constructed by

 Algorithm



 Give a - reduction of the flow_graph as

 described in the box on Where 'Reducible' Comes_From in

 Section







 Repeat_Exercise on the following flow_graphs







 a)

 Fig



 b)

 Fig



 c)

 Your_flow graph from Exercise



 d)

 Your_flow graph from Exercise







 Show that a flow_graph is reducible if and only it can be transformed to

 a single_node using the operations and described in the box

 in Section

 Region-Based_Analysis



 The iterative_data-flow analysis algorithm we have discussed so_far is

 but one of many_ways we can use to solve data-flow_problems In

 iterative_data-flow analysis we create transfer_functions for basic

 blocks then find the fix point solution using an iterative method

 Region-based_analysis is an alternate approach to iterative

 analysis Instead of just creating transfer_functions for individual

 basic_blocks the region-based_analysis finds transfer_functions that

 summarize entire procedures and then apply them to get the data-flow

 values



 Whereas a data-flow_framework using an iterative_algorithm is

 specified_by a semi-lattice of data-flow_values and a family of

 transfer_functions closed_under composition a region-based framework

 specification includes a semi-lattice of data-flow_values a

 semi-lattice of transfer_functions with a meet_operator a composition

 operator and a closure_operator



 Region Flow_Graphs



 Region-based_analysis organizes the code as a hierarchy of regions where regions correspond to some code that has only a single

 point of entry The nesting structure in structured programs is a

 perfect example of a region-based hierarchy Each nesting level of

 a structured program makes an region Nested in each statement are

 possibly more statements this corresponds to the hierarchy where each

 region may contain more subregions



 To handle general control_flow rather_than just structured programs

 let_us define the concept of a region in terms of nodes and edges

 in a control_flow graph A region of a control_flow graph is

 defined as a subgraph consisting of



 a set of nodes that includes a header

 which dominates all the other nodes in the region and



 all the edges between the nodes in the graph except (possibly) for

 some that enter the header



 We note_that a node representing a basic_block is itself a region

 since the flow of control can only enter through the top of a basic

 block We refer to basic_blocks as leaf_regions Like natural_loops

 regions in a flow_graph must either be disjoint or one is nested in

 another Thus regions form a hierarchy If a region contains

 region we say that is a parent region of and that

 is a subregion of Except for the case of leaf_regions every

 node in a region belongs to a subregion and every edge is either an

 edge in a subregion or it is an edge leading_from a basic_block in one

 subregion to the header of another (not_necessarily distinct)

 subregion A basic_block with outgoing control_flow edges that lead

 to basic_blocks outside a region is also known_as an exit node

 for that region The control_flow of a region can be represented_by a

 region flow_graph where



 each subregion is represented_by a node

 there_exists an edge if and only if there is an edge from

 an exit of subregion to the header of subregion



 Given a program we can create many different region hierarchies For

 example as shown in Fig we can form a region out of

 two adjacent basic_blocks and the edge connecting them as_long as

 there are no other flow edges that lead to the second one Or we can

 make regions out of nested conditional_statements with each region

 corresponding to a conditional_statement



 In the region-based approach we_shall use in this_chapter we create

 regions out of basic_blocks the bodies of the natural_loops the

 natural_loops themselves and finally the entire procedure itself

 The body of a natural_loop consisting of all the nodes in a natural

 loop and all the edges that connect between these nodes is clearly a

 region Similarly the loop body plus all the back_edges back to the

 header of the loop itself form yet another region representing the

 bloop Finally since a procedure has a single point of entry all its

 nodes and edges also make up a region



 The top_region of the hierarchy is the region representing the entire

 procedure The subregions contained in the top_region are the

 outermost natural_loops as_well basic_blocks not found inside any

 loops The top_region also includes all the edges from the exits of

 basic_blocks or outermost_loops to the headers of the subregions

 Each loop region consists of a single subregion representing its body

 and a set of back_edges A region representing a body of a loop is

 similar to that of a procedure region except that it consists of only

 basic_blocks and further inner_loops nested inside the loop





 Consider the flow_graph in Fig Now each of the

 basic_blocks in the flow_graph is a leaf region There is one back

 edge in this flow_graph which leads from to Thus our

 region graph hierarchy consists of



 corresponding to the basic_blocks

 respectively

 Every basic_block is also an exit node for its region

 represents the body of the only loop in the program it consists

 of regions and the region flow_graph has 3 edges

 and This region has two exit

 nodes and

 represents the natural_loop in the program It includes one

 subregion and one back_edge It has also two exit

 nodes again and

 Finally is the top_region It includes regions

 and The region flow_graph contains edges

 It has one exit node







 figureuullmanalsuch9figsrd-regioneps

 (a) A example flow_graph for the reaching_definitions prblem

 and (b) its region_hierarchy



 If a control_flow graph is reducible then the region flow_graphs of

 all non-loop regions are acyclic This is because the only retreating

 edges in a graph are back_edges and they are all included in the loop

 regions In_fact the term reducibility refers to the ability

 to reduce a control_flow graph recursively into region flow_graphs

 such that cycles are only formed_by edges that lead to the headers of

 the regions Irreducible flow_graphs can be made reducible by node

 splitting

 NODE SPLITTING NEEDS TO BE EXPANDED

 In the following we will assume that all our control_flow graphs are

 or have_been made reducible



 Overview of a Region-Based_Analysis



 If for each basic_block we can compute a transfer_function that

 summarizes the effect of executing all possible paths that lead to the

 basic_block then applying the transfer_function to the boundary value

 at the beginning of a program will produce the desired data-flow value

 at the beginning of the basic_block directly An efficient way of

 performing this computation is to take_advantage of the concept of

 regions A region-based_analysis typically consists of two phases

 the first is a bottom-up phase that computes transfer_functions

 starting_with the leaf_regions working up to the top_region the

 second is a top-down phase that applies the transfer_functions

 starting_with the top_region and working down to the leaf_regions



 In the first phase for each region we analyze only the execution

 paths that are internal to a region We compute the transfer

 functions that correspond to the effect of all possible_executions

 that lead from the header of the region to the header of the

 subregions This information is internal to the region and is used

 in the second phase We also compute similar transfer_functions

 leading to the end of each of the region's exit blocks These

 transfer_functions constitute a summary of the effect of the

 region Only a subregion's summary information is used to compute the

 transfer_functions for a region We start with the leaf nodes which

 are the basic_blocks and work our way up to the top_region



 Thus this first phase creates a tree structure of transfer_functions

 there is a transfer_function that leads from the header of a region to

 the header of each of its subregion By composing the transfer

 functions down a path of a tree to a basic_block we get the transfer

 function that represents the execution from the top of the program to

 that particular basic_block The data-flow value at the entry of a

 basic_block can be found by simply applying such a function to the

 boundary value at the entry of a program Thus we can first find all

 the transfer_functions that summarize all executions leading to each

 basic_block then applying them to get all the data-flow value



 We can optimize this procedure further again by exploiting the region

 hierarchy We note_that once we know the data-flow value at the

 header of the region we can compute the data-flow_values of the

 header of its subregion by_applying the transfer_function that leads

 from the header of the region to the header of the subregion This

 avoids the need of performing many function compositions Thus the

 second phase of the algorithm starts with the top_region The

 data-flow value associated_with the header of the top_region is simply

 the boundary value of the data-flow_problem We apply the transfer

 functions computed for each region to the data-flow value at the

 header of the region to calculate the data-flow_values at the header

 of each subregion This procedure is repeated for all the subregions

 to compute the data-flow value at the entry of each basic_block





 Primitive Transfer Function Operations



 To_compute the transfer_functions for each region we need three

 primitive operations on transfer_functions composition meet and

 closure





 Composition operation The transfer_function of a sequence of

 nodes can be derived by_composing the functions representing the

 individual nodes Let and be transfer_functions of nodes

 and The effect of executing followed_by is

 represented_by



 This has_been discussed in Section and an example

 using reaching_definitions has_been shown in Section

 Let be the gen and kill set for and

 be the gen and kill set for



 f2 f1 (x) gen2 ((gen1 (x_- kill1)) - kill2)



 (gen2 (gen1 -_kill2)) (x_- (kill1_kill2))

 Thus the gen set and kill set for

 are and

 respectively



 Meet operation

 Here the transfer_functions themselves are values of a semi-lattice

 with a meet_operator

 The meet of two transfer_functions and

 is defined such that



 The meet_operator is used to combine the effect of alternate paths of

 execution with the same end points Where it is not ambigious from

 now on we_shall just refer to the meet_operator of transfer_functions

 also as



 Consider the example of the reaching_definitions problem where a

 transfer_function is represented_by its gen set and kill set

 and respectively



 (f1 f2)(x) (f1 f2)(x)



 f1(x)_f2(x)



 (gen1 (x_- kill1)) (gen2 (x_- kill2))



 (gen1 gen2) (x_- (kill1_kill2))

 That is the gen and kill_sets for are

 and

 respectively



 Closure operation If represents the transfer_function of a

 cycle then represents the effect of going_around the cycle

 times In the case_where the number of iterations is not known we

 have to assume that the loop may be executed 0 or_more times We

 represent the transfer_function of such a loop with where is

 the kleene star operator Let be the identity_function then



 We also refer to the as the closure

 operation



 Suppose the transfer_function in a reaching_definitions framework

 has a set and a set Then



 f2(x) f(f(x))



 (gen ((gen (x_- kill)) -_kill)



 gen_(x -_kill)



 f3(x) f(f2(x))



 gen_(x -_kill)



 fn(x) gen_(x -_kill) n 1

 That is the set of definitions_reaching the entry of a cyclic path is

 the same after the first second or any other number of iterations



 f(x) I f1(x)_f2(x)



 gen x

 That is the gen and kill_sets for are and

 respectively In the case_where is not executed at all the

 reaching_definitions are and in all subsequent_iterations the

 reaching_definitions may include those in the set



 A Region-Based_Analysis



 We use the notation to denote the transfer_function

 summarizing the effect of all possible_executions that start from the

 header of to the header of the subregion Similarly we use

 the notation to denote the transfer_function

 summarizing the effect of all possible_executions that start from the

 header of the region to the exit of the basic_block

 For each region we_wish to compute for all

 that are subregions of and for all that are

 exit nodes of We note_that if is a node in subregion

 then must also be an exit node of





 A Region-Based Analyis





 The boundary value at the entry of the procedure

 A set of representing the

 transfer_functions of basic_block

 A hierarchy of regions

 each region has a set of exit basic_blocks and a set of

 region flow_graph edges

 is the top_region



 Data-flow values at the entry of each basic_block



 The algorithm for region-based_analysis is shown in

 Fig









 A region-based_analysis algorithm



 In the bottom-up traversal as we visit a region we can assume

 that all its subregions have_already been_visited That is the

 value for all exit nodes in



 If the region is a loop region then it can contain only one

 subregion which is its loop body The transfer

 function summarizes the effect of executing 0 or_more iterations of

 the loop We first compute the effect of executing one iteration

 which is computed by finding the meet of all the for

 all that can branch back to the beginning of the loop We then

 find the closure of that function to compute



 If the region is not a loop region then it may have multiple

 subregions The region flow_graph edges are acyclic thus we can

 visit the nodes in topological_order guaranteeing that a node is not

 visited until all its_predecessors have_been visited As we visit

 each region we compute the transfer_function to the header of

 the region as_well as to the end of each of its exit

 nodes



 The first subregion is a special_case because it shares the same entry

 point as the header Thus the transfer_function is simply

 the identity_function For all other nodes the is the

 meet of all the transfer_functions representing the execution from the

 header of the region to the end of the basic_blocks that can reach the

 header of Because of the topological ordering these functions

 have_already all been_computed



 The calculation of the transfer_functions for the exit nodes of a

 region is the same for both acyclic and cyclic regions The function

 can be computed by_composing the transfer_function

 representing the computation up the header of the subregion

 with the transfer_function from the header of to

 the end of the exit node These functions may be used

 in line_(4) in the subsequent_iterations of the loop in lines (2-14)

 if region is acyclic If the exit node of is also an exit

 node for then this information will be used in the subsequent

 iterations in the outer_loop from lines (1-16) In the case of cyclic

 regions an exit node in its subregion may branch back to the top of

 the loop or out of the loop or both The loop in lines (12-13)

 will find the transfer_functions for all the exit nodes which may

 include information that may not be used in this algorithm





 The result of applying_Algorithm to the flow_graph in

 Fig is shown in Fig In the first

 phase the regions are visited from the bottom up The transfer

 functions computed for the leaf_regions are simply the transfer

 functions of the basic_blocks



 We then compute the transfer_functions associated_with region

 Note_that

 The gen set is the union of the gen

 sets of and the kill set is the

 intersection of the kill_sets of and

 is not in the kill set because the

 definition is only conditionally executed



 Next we process the region

 The closure of a function retains a function's

 gen set but has an empty kill set thus the gen set for

 is and the kill set is empty

 There_are two exits_out of the loop via node and When

 leaving through node the definition is killed whereas

 leaving through node the definition is killed In either

 case is killed The gen sets are the same for both exits



 Region contains three subregions and



 since the gen set of

 contains the gen set of and the

 kill set of is a subset of the kill set of





 In the top down phase the regions are visited in a top down manner

 is set to the boundary value an empty_set We then visit

 the top_region followed_by and We compute the

 input value to each of the subregions for each of the region visited

 It is easy to see that the value is the same computed using an

 iterative analysis





 (a) Phase 1 bottom-up analysis to compute transfer_functions







 (b) Phase 2 top-down analysis to compute data-flow_values





 Region-Based_Analysis of Reaching_Definitions





 Region-Based_Analysis



 The iterative_data-flow analysis algorithm we have discussed so_far is

 just one approach to solving_data-flow problems Here we discuss

 another approach called region-based_analysis Recall

 that in the iterative-analysis

 approach we create transfer_functions for basic_blocks then

 find the fixedpoint solution by repeated_passes over the blocks Instead of

 creating transfer_functions just for individual blocks a

 region-based_analysis finds transfer_functions that summarize the

 execution of progressively_larger regions of the program

 Ultimately transfer_functions for

 entire procedures are constructed and then applied to get the desired

 data-flow_values directly



 While a data-flow_framework using an iterative_algorithm is

 specified_by a semilattice of data-flow_values and a family of

 transfer_functions closed_under composition region-based_analysis

 requires more elements

 A region-based framework

 includes both a semilattice of data-flow_values and a

 semilattice of transfer_functions that must possess

 a meet_operator a composition

 operator and a closure_operator

 We_shall see what all these elements entail in

 Section



 A region-based_analysis is particularly useful for data-flow_problems

 where paths that have cycles may change the data-flow_values

 The closure_operator allows the effect of a loop to be summarized more

 effectively than does iterative analysis The technique is also useful for

 interprocedural_analysis where transfer_functions associated_with a

 procedure call may be treated_like the transfer_functions associated_with

 basic_blocks



 For_simplicity we_shall consider only forward_data-flow problems in

 this_section We first illustrate_how region-based_analysis works by

 using the familiar example of reaching_definitions

 In Section we show a

 more compelling use of this technique when we

 study the analysis of induction_variables



 Regions



 In region-based_analysis a program is viewed_as a hierarchy of regions which are (roughly) portions of a flow_graph that have

 only one point of entry We should find this concept of viewing

 code as a hierarchy of regions intuitive because a block-structured

 procedure is naturally organized as a hierarchy of regions Each

 statement in a block-structured program is a region as control_flow

 can only enter at the beginning of a statement Each level of

 statement nesting corresponds to a level in the region_hierarchy



 Formally a region of a flow_graph is a

 collection of nodes and edges such that







 There is a header in that dominates all the nodes in



 If some node can reach a node in without_going

 through then is also in



 is the set of all the control_flow edges between nodes and

 in except (possibly) for some that enter







 Clearly a natural_loop is a region but a region does_not necessarily

 have a back_edge and need not contain any cycles For

 example in Fig nodes and together_with

 the edge

 form a region so do nodes and with

 edges and



 However the subgraph with

 nodes and with edge does_not form a region

 because control

 may enter the subgraph at both nodes and

 More_precisely neither_nor dominates the other so

 condition (1) for a region is violated

 Even_if we picked say to be the header we

 would violate condition (2) since we can reach from without

 going_through and is not in the region





 figureuullmanalsuch9figsregionseps

 Examples of regions



 Region Hierarchies for Reducible Flow_Graphs



 In what_follows we_shall assume the flow_graph is reducible

 If occasionally we must deal_with nonreducible_flow graphs then we can

 use a technique called node_splitting that will be discussed in

 Section



 To construct a hierarchy of regions we identify the natural_loops

 Recall from Section that in a reducible_flow

 graph any two natural_loops are either_disjoint or one is nested

 within the other

 The process of parsing a reducible_flow graph into its hierarchy of

 loops begins_with every block as a region by itself

 We call these regions leaf_regions

 Then we order the natural_loops from the inside out ie_starting

 with the innermost_loops

 To process a loop we replace the entire loop by a node in two steps







 First the body of the loop (all nodes and edges except the back

 edges to the header) is replaced_by a node representing a region

 Edges to the header of now enter the node for

 An edge from any exit of loop is replaced_by an edge from to the

 same destination

 However if the edge is a back_edge then it becomes a loop on

 We call a body_region



 Next we construct a region that represents the entire_natural

 loop

 We call a loop region

 The only_difference between and is that the latter includes

 the back_edges to the header of loop

 Put_another way when replaces in the flow_graph all we have

 to do is remove the edge from to itself



 We proceed this way reducing larger and larger loops to single nodes

 first with a looping edge and then without

 Since loops of a reducible_flow graph are nested or disjoint the loop

 region's node can represent all the nodes of the natural_loop

 in the series of flow_graphs

 that are constructed by this reduction process



 Eventually all natural_loops are reduced to single nodes

 At that point the flow_graph may be reduced to a single_node or there

 may be several nodes remaining with no loops ie the reduced flow

 graph is an acyclic_graph of more_than one node

 In the former_case we are done constructing the region_hierarchy while

 in the latter case we construct one more body_region for the entire

 flow_graph





 figureuullmanalsuch9figsrd-regioneps

 (a) An_example flow_graph for the reaching_definitions problem

 and (b) Its region_hierarchy





 Consider the control_flow graph in Fig (a) There is one

 back_edge in this flow_graph which leads from to The

 hierarchy of regions is shown in Fig (b) the

 edges shown are the edges in the region flow_graphs There_are

 altogether 8 regions







 Regions are leaf_regions representing

 blocks through

 respectively Every block is also an exit block in its

 region



 Body region

 represents the body of the only loop in the flow_graph it consists

 of regions and and three_interregion edges

 and It has two exit blocks and

 since they both have outgoing_edges not contained in the region

 Figure (a) shows the flow_graph with reduced to

 a single_node

 Notice_that although the edges and

 have both been replaced_by edge it is important to

 remember that the latter edge represents the two former edges since we

 shall have to propagate transfer_functions across this edge eventually

 and we need to know that what comes out of both blocks and

 will reach the header of



 Loop region

 represents the entire_natural loop It includes one

 subregion and one back_edge

 It has also two exit

 nodes again and

 Figure (b) shows the flow_graph after the entire

 natural_loop is reduced to



 Finally body_region is the top_region

 It includes three regions

 and three_interregion edges

 and

 When we reduce the flow_graph to it becomes a single_node

 Since there are no back_edges to its header there is no need

 for a final_step reducing this body_region to a loop region







 fileuullmanalsuch9figsreducingeps

 Steps in the reduction of the flow_graph of Fig to a single region



 To summarize the process of decomposing reducible_flow

 graphs hierarchically we offer the following algorithm





 Constructing a bottom-up order of regions of a reducible_flow graph



 A reducible_flow graph



 A list of regions of that can be used in region-based data-flow

 problems







 Begin the list with all the leaf_regions consisting of single blocks of

 in any order



 Repeatedly choose a natural_loop such that if there are any natural

 loops contained_within then these loops have had their body and

 loop regions added to the list already

 Add first the region consisting of the body of (ie without

 the back_edges to the header of ) and then the loop region of



 If the entire_flow graph is not itself a natural_loop add at the end

 of the list the region consisting of the entire_flow graph





 Where Reducible Comes_From

 We_now see_why reducible_flow graphs were given that name

 While we_shall not prove this fact the definition of

 reducible_flow graph used in this_book

 involving the back_edges of the graph is

 equivalent to several definitions in which we mechanically reduce the

 flow_graph to a single_node

 The process of collapsing natural_loops described in

 Section is one of them

 Another interesting definition is that the reducible_flow graphs are

 all and only those graphs that can be reduced to a single_node by the

 following two transformations









 Remove an edge from a node to itself





 If node has a single predecessor and is not the entry of

 the flow_graph combine and







 Overview of a Region-Based_Analysis



 For each region and for each subregion within we

 compute a transfer_function that summarizes the effect

 of executing all possible paths

 leading_from the entry of

 to the entry of while staying within

 We_say that a block within is an exit block

 of region

 if it has an outgoing_edge to some block outside We also compute

 a transfer_function for each exit block of

 denoted that summarizes the effect

 of executing all possible paths within

 leading_from the entry of

 to the exit of



 We then proceed up the region_hierarchy computing transfer_functions

 for progressively_larger regions We begin_with regions that are

 single blocks where is just the identity_function and

 is the transfer_function for the block itself

 As we move up the hierarchy







 If is a body_region then the edges belonging

 to form an acyclic_graph on the subregions of

 We may proceed to compute the transfer_functions in a

 topological_order of the subregions



 If is a loop region then we only need to account for the effect of

 the back_edges to the header of





 Eventually we reach the top of

 the hierarchy and compute the

 transfer_functions for region

 that is the entire_flow graph

 How we perform each of these computations will be seen in

 Algorithm



 The next step is to compute the data-flow_values at the entry and exit

 of each block We process the regions in the reverse order

 starting_with region and working our way down the hierarchy

 For each region we compute the data-flow_values at the entry For

 region we apply

 to get the data-flow

 values at the entry of the subregions in We repeat until

 we reach the basic_blocks at the leaves of the region_hierarchy



 Necessary Assumptions About Transfer_Functions



 In order for region-based_analysis to work we need to make certain

 assumptions about properties of the set of transfer_functions in the

 framework

 Specifically we need three

 primitive operations on transfer_functions composition meet and

 closure only the first is required for data-flow_frameworks that use

 the iterative_algorithm



 Composition



 The transfer_function of a sequence of

 nodes can be derived by_composing the functions representing the

 individual nodes Let and be transfer_functions of nodes

 and The effect of executing followed_by is

 represented_by

 Function composition has_been discussed in

 Section and an example using reaching_definitions

 was shown in Section To review let and

 be the

 and sets for

 Then





 f2 f1 (x) gen2 ((gen1 (x_- kill1))

 - kill2)



 (gen2 (gen1 -_kill2)) (x_- (kill1_kill2))

 Thus the and sets for

 are and

 respectively

 The same idea works for any transfer_function of the gen-kill form

 Other transfer_functions may also be closed but we have to consider

 each case separately



 Meet



 Here the transfer_functions themselves are values of a semilattice

 with a meet_operator

 The meet of two transfer_functions and

 is defined by

 where is the

 meet_operator for data-flow_values

 The meet_operator on transfer_functions

 is used to combine the effect of alternative paths of

 execution with the same end points Where it is not ambiguous from

 now on we_shall refer to the meet_operator of transfer_functions

 also as

 For the reaching-definitions_framework we have





 (f1 f2)(x) f1(x)_f2(x)



 (gen1 (x_- kill1))

 (gen2 (x_- kill2))



 (gen1 gen2) (x_- (kill1_kill2))

 That is the and sets for are

 and

 respectively

 Again the same argument applies to any set of gen-kill transfer

 functions



 Closure



 If represents the transfer_function of a

 cycle then represents the effect of going_around the cycle

 times In the case_where the number of iterations is not known we

 have to assume that the loop may be executed 0 or_more times We

 represent the transfer_function of such a loop by the closure of which is

 defined by









 Note_that must_be the identity transfer

 function since it represents the

 effect of going zero times_around the loop ie_starting at the entry

 and not moving

 If we let represent the identity transfer_function then we can

 write











 Suppose the transfer_function in a reaching_definitions framework

 has a set and a set Then





 f2(x) f(f(x))



 gen ((gen (x_- kill)) -_kill)



 gen_(x -_kill)



 f3(x) f(f2(x))



 gen_(x -_kill)



 and so on any is

 That is going_around a loop doesn't affect the transfer_function if it

 is of the - form

 Thus





 f(x) I f1(x)_f2(x)



 x (gen (x_- kill))



 gen x

 That is the and sets for are and

 respectively Intuitively since we might not go_around a loop at all

 anything in will reach the entry to the loop

 In all subsequent_iterations the

 reaching_definitions include those in the set



 An Algorithm for Region-Based_Analysis



 The following algorithm solves a forward data-flow-analysis problem on

 a reducible_flow graph according to some framework that satisfies the

 assumptions of Section

 Recall that and

 refer to

 transfer_functions that

 transform data-flow_values at the entry to region into the correct

 value at the entry of subregion and the exit of the exit block

 respectively





 Region-based_analysis



 A data-flow_framework with the properties outlined in

 Section and a reducible_flow graph



 Data-flow values for each block of







 Use_Algorithm to construct the bottom-up sequence

 of regions of say where is the

 topmost region



 Perform the bottom-up analysis to

 compute the transfer_functions summarizing the effect of executing a

 region

 For each region in the bottom-up order do the following





 If is a leaf region corresponding to block let

 and

 the transfer_function associated_with block

 If is a body_region perform the computation of

 Fig (a)

 If is a loop region perform the computation of Fig (b)



 Perform the top-down_pass to find the data-flow_values at the

 beginning of each region







 For each region in in the top-down order

 compute







 where is the immediate enclosing region of





 Let_us first look_at the details of how the bottom-up analysis works

 In line_(1) of Fig (a) we

 visit the subregions of a body_region in some topological_order

 Line (2) computes the transfer_function representing all the possible

 paths from the header of to the header of

 then in lines_(3) and (4) we compute

 the transfer_functions representing all the possible

 paths from the header of to the exits of - that is

 to the exits of all

 blocks that have successors outside

 Notice_that all the predecessors in

 must_be in regions that precede

 in the topological_order constructed at line_(1)

 Thus will have_been computed already

 in line_(4) of a previous iteration through the outer_loop



 For loop regions we perform the steps of lines_(1) through (4) in

 Fig (b) Line (2) computes the effect of going

 around the loop body_region zero_or more times Lines (3) and (4)

 compute the effect at the exits of the loop after one or_more iterations



 In the top-down_pass of the algorithm

 step 3(a) first assigns the boundary_condition

 to the input of the top-most region

 Then if is immediately contained

 in we can simply_apply the transfer_function to

 the data-flow value to compute













 Details of region-based data-flow computations





 Let_us apply_Algorithm to find reaching_definitions

 in the flow_graph in

 Fig (a)

 Step 1 constructs the bottom-up order in which the regions are visited this

 order will be the numerical order of their subscripts





 The values of the and sets for the five blocks

 are summarized

 below





 Remember the simplified rules for - transfer_functions

 from Section





 To take the meet of transfer_functions take the union of the 's

 and the intersection of the 's



 To compose transfer_functions take the union of both the 's and

 the 's

 However as an exception an expression that is generated_by the first

 function not generated_by the second but killed

 by the second is not in the of the result



 To take the closure of a transfer_function retain its and replace

 the by



 The first five regions are blocks

 respectively For

 is

 the identity_function and is

 the transfer_function for block



















 Computing transfer_functions for the flow_graph in

 Fig (a) using region-based_analysis



 The rest of the transfer_functions constructed in Step_2

 of Algorithm are

 summarized in Fig

 Region consisting of regions and

 represents the loop body and thus does_not include the back_edge

 The order of processing these regions will be the

 only topological_order First has no

 predecessors within remember that the edge

 goes outside Thus is the identity

 function(Strictly speaking we mean but

 when a region like is a single block it is often clearer if we

 use the block name rather_than the region name in this context)

 and is the transfer_function for

 block itself



 The header of region has one_predecessor within_namely

 The transfer_function to its entry is simply the transfer_function to the

 exit of which has already_been computed

 We compose this function with the transfer_function

 of within its_own region to compute the transfer_function to the

 exit of



 Last for the transfer_function to the entry of

 we must compute



 because both and are predecessors of the header of



 This transfer_function is composed with the transfer_function

 to get the desired function

 Notice for example that is not_killed in this transfer_function

 because the path does_not redefine variable



 Now_consider loop region It contains only one subregion

 which represents its loop body

 Since there is only one back_edge to the header of

 the transfer_function representing the execution of the loop

 body

 0 or_more times is just

 the set is and the

 set is

 There_are two exits_out of region blocks and



 Thus

 this transfer_function is composed with each of the transfer

 functions of to get the corresponding transfer_functions of

 Notice for instance how is in the set for

 because of paths like or even





 Finally consider the entire_flow graph

 Its subregions are and which we_shall consider in

 that topological_order

 As before the transfer_function is simply

 the identity_function and the

 transfer_function is just which in turn is





 The header of which is has only one_predecessor

 so the transfer_function to its entry is simply the transfer_function

 out of in region

 We compose with the transfer_functions to

 the exits of

 and within to obtain their corresponding transfer_functions within



 Lastly we consider

 Its header has two predecessors within_namely and



 Therefore we compute

 to get Since the transfer_function of block

 is the identity_function



 Step_3 computes the actual reaching_definitions from the transfer

 functions In step 3(a) since there are no

 reaching_definitions at the beginning of the program

 Figure shows_how step 3(b) computes the rest of

 the data-flow

 values The step starts with the subregions of Since the

 transfer_function from the start of to the start of each of its

 subregion has_been computed a single application of the transfer

 function finds the data-flow value at the start each subregion We

 repeat the steps until we get the data-flow_values of the leaf

 regions which are simply the individual basic_blocks

 Note_that the data-flow_values shown in Figure are exactly what we would get had we applied

 iterative_data-flow analysis to the same flow_graph as must_be the

 case of course



































































 Final steps of region-based flow analysis







 NOT SURE IF WE WANT TO DWELL ON THIS



 Efficiency of Region-Based_Analysis



 It is quite tricky to compare the iterative_algorithm with the

 hierarchical_algorithm

 To_begin consider the iterative_approach

 We observed in Section that the

 depth of a reducible_flow graph (as_defined in Section ) is

 typically 3 and therefore or about

 5 passes_through the flow_graph suffice

 Thus if there are nodes there will be or about

 computations of data-flow

 values - half for the 's and half for the 's

 Some of these computations may be more_complex than_others especially

 as a node may have a large_number of predecessors and therefore take

 extra time to compute its value

 However as we_wish to compare the running_time of two algorithms on the

 same flow_graph it is reasonable to count only the values computed and

 not worry_about the time needed to compute each one



 For the sake of comparison assume we are applying a data-flow_framework

 whose transfer_functions are of the - type

 Thus the data-flow_values can be represented_by bit_vectors as

 discussed in Section

 We may therefore estimate the cost of the iterative_algorithm as the

 computation of bit_vectors



 If we look_at the hierarchical_algorithm applied to a flow_graph of

 nodes and depth we observe several things







 Since we compute only 's in step 2 of Algorithm

 the work at a node that is in regions (other_than the region that is

 the node itself) involves computing values



 However each of those values is a transfer_function not a data-flow

 value and we require two bit_vectors to represent a transfer_function -

 one for and one for

 Thus bit-vectors are computed



 If the flow graph's depth is then there are at most regions

 in which any node finds itself body regions loop regions

 surrounding them and perhaps a final body_region representing the

 entire_flow graph

 Thus in step 2 the number of bit_vectors computed for all nodes is no more

 than and it could be less



 The third_step involves no computation since the 's are just the

 components of the corresponding transfer_function



 The last step of Algorithm computes data-flow

 values consisting of one bit_vector each





 Therefore the total_number of bit_vectors computed by the hierarchical

 algorithm is at most

 That figure appears to be

 almost twice the bit_vectors computed by the

 iterative_algorithm

 Further the

 hierarchical_algorithm is conceptually more_complex especially if

 there are nonreducible_flow graphs with which to contend

 However there are two important reasons_why the region-based approach

 turns_out to be more rather_than less efficient than the iterative

 approach







 a)

 The estimates of bit_vectors computed assume each node is at the maximum

 depth for the flow_graph In reality many nodes of a flow_graph will

 appear inside only one loop or no loops and the innermost_loops tend

 to be small

 Thus the average_number of regions in which a node appears

 may be considerably less_than our

 assumed regions



 b)

 One can avoid computing if is not the entire_flow graph

 and is not an exit of

 region ie there is no node out of that goes to a node not

 in

 The_reason is that if is a node which is an exit of then one

 can skip_over nodes like and compute for some region

 containing if we use the transfer_function for from the header

 of to

 For_example if of Fig were some interior

 region and not the entire_flow graph then we would have no need for

 in the computation of transfer_functions for

 We could compute by_composing (because

 is the only predecessor of the header of ) with







 Handling Nonreducible Flow_Graphs



 If nonreducible_flow graphs are expected to be common for the programs

 to be processed by a compiler or other program-processing software then

 we recommend using an iterative rather_than a hierarchy-based approach

 to data-flow_analysis

 However if we need only to be prepared for the occasional nonreducible

 flow_graph then the following node-splitting technique is

 adequate



 Construct regions from natural_loops to the extent possible

 If the flow_graph is nonreducible we_shall find that the resulting

 graph of regions has cycles but no back_edges so we cannot parse the

 graph any further

 A_typical situation is suggested in Fig (a)

 which has the same structure as the nonreducible_flow graph of

 Fig but the nodes in

 Fig may actually be complex regions as

 suggested by the smaller nodes within







 fileuullmanalsuch9figsnode-splittingeps

 Duplicating a region to make a nonreducible_flow graph become

 reducible



 We pick some region that has more_than one_predecessor and is not the

 header of the entire_flow graph

 If has predecessors make copies of the entire_flow graph

 and connect each predecessor of 's header to a different copy of



 Remember that only the header of a region could possibly have a

 predecessor outside that region

 It_turns out although we_shall not prove it that such node_splitting

 results in a reduction by at_least one in the number of regions after

 new back_edges are identified and their regions constructed

 The resulting graph may still not be reducible but by alternating a

 splitting phase with a phase where new natural_loops are identified and

 collapsed to regions we eventually are left with a single region ie

 the flow_graph has_been reduced





 The splitting shown in Fig (b) has turned the

 edge into a back_edge since now dominates



 These two regions may thus be_combined into one

 The resulting three regions - and the new region - form

 an acyclic_graph and therefore may be_combined into a single body

 region

 We thus have reduced the entire_flow graph to a single

 region

 In_general additional splits may be necessary and in the worst_case

 the total_number of basic_blocks could become exponential in the number

 of blocks in the original flow_graph



 We must also think about how the result of the data-flow_analysis on the

 split flow_graph relates to the answer we desire for the original flow

 graph

 There_are two approaches we might consider







 Splitting regions may be beneficial for the optimization process and we

 can simply revise the flow_graph to have copies of certain blocks

 Since each duplicated block is entered along only a subset of the paths

 that reached the original the data-flow_values at these duplicated

 blocks will tend to contain more specific information than was available

 at the original

 For_instance fewer definitions may reach each of the duplicated_blocks

 that reach the original block



 If we_wish to retain the original flow_graph with no splitting then

 after analyzing the split flow_graph we look_at each split block

 and its corresponding set of blocks

 We may compute

 and

 similarly for the 's







 For the flow_graph of Fig (see the exercises for

 Section )







 Find all the possible regions You_may however omit from

 the list the regions consisting of a single_node and no edges



 Give the set of nested regions constructed by

 Algorithm



 Give a - reduction of the flow_graph as

 described in the box on Where 'Reducible' Comes_From in

 Section







 Repeat_Exercise on the following flow_graphs







 a)

 Fig



 b)

 Fig



 c)

 Your_flow graph from Exercise



 d)

 Your_flow graph from Exercise







 Prove that every natural_loop is a region





 Show that a flow_graph is reducible if and only it can be transformed to

 a single_node using







 a)

 The operations and described in the box

 in Section



 b)

 The region definition introduced in Section







 Show that when you apply node_splitting to a nonreducible_flow graph

 and then perform - reduction on the resulting split

 graph you wind_up with strictly fewer nodes than you started with





 What happens if you apply node-splitting and - reduction

 alternately to reduce a complete directed

 graph of nodes

 Related Work

 secrelated















 This_paper describes a scalable cloning-based points-to_analysis that

 is context-sensitive field-sensitive inclusion-based and implemented

 using_BDDs Our program analyses expressed in Datalog are

 translated by into a BDD implementation automatically We

 also presented example queries using our system to check for

 vulnerabilities infer types and find objects that escape a thread

 Due to space constraints we can only describe work that is very closely

 related to ours



 There is a good deal of related work associated_with each of the above

 topics we_shall attempt to only describe the most relevant here

















 Cloning-based context-sensitive_analysis

 Cooper proposed a summary-based interprocedural data-flow

 analysis where a bottom-up analysis is used to create summaries for

 each method and actual data-flow_values are propagated in a top-down

 manner to all the different procedurescooper92procedure Where

 data-flow_values differ for different call_paths they create separate

 clones Our_approach simply creates a clone for every context we_wish

 to evaluate and directly compute their results by_applying a

 context-insensitive algorithm on the cloned_call graph The approach

 is like inlining except that the actual code is not duplicated

 function boundaries are retained some of the calling_contexts can

 invoke the same clone and consequently we can handle recursion This

 approach is the closest to Emami's invocation-graph

 approachMEmami94 as discussed below





 Scalable pointer analyses Most of the scalable algorithms

 proposed are context-insensitive and flow-insensitive The first

 scalable pointer_analysis proposed was a unification-based algorithm

 due to Steensgaardsteensgaard96pointsto Das extended

 the unification-based approach to include

 one-level-flowdas00unificationbased and one level of

 context sensitivitydas01estimating Subsequently a number of

 inclusion-based algorithms have_been shown to scale to large

 programsberndl03heintze01ultrafastlhotak03whaley02sas



 A number of context-sensitive but flow-insensitive analyses have_been

 developed recentlyfahndrich00scalablefoster00polymorphic The

 C pointer_analysis due to Fahndrich et

 alfahndrich00scalable has_been demonstrated to work on a

 200K-line gcc program Unlike ours their algorithm is

 unification-based and field-independent meaning that fields in a

 structure are modeled as having the same_location Their

 context-sensitive_analysis discovers targets of function pointers

 on-the-fly Our algorithm first computes the call_graph using a

 context-insensitive_pointer alias_analysis there are significantly

 more indirect calls in Java programs the target of our

 technique due to virtual_method invocations Their algorithm uses

 CFL-reachability queries to implement context sensitivityRHS95

 Instead of computing context-sensitive solutions on demand we compute

 all the context-sensitive_results and represent them in a form

 convenient for further analysis



 Other context-sensitive_pointer analysis Some of the earlier

 attempts of context-sensitive_analysis are

 flow-sensitiveMEmami94WLandi93OOPSLA99WhaleyRWilson95

 Our analysis is similar to the work by Emami in that they also

 compute context-sensitive_points-to results directly for all the

 different_contexts Their analysis is flow-sensitive ours uses flow

 sensitivity only in summarizing each method intraprocedurally While

 our technique treats all members of a strongly_connected component in

 a call_graph as one_unit their technique only ignores subsequent

 invocations in recursive_cycles On the other_hand their technique

 has only been_demonstrated to work for programs under 3000 lines

 To be fair their work was done in 1994 when machines were

 much slower and had much less memory



 As_discussed in Section secintro using summaries

 is another common approach to context_sensitivity It is difficult to

 compute a compact summary if a fully flow-sensitive result is desired

 One_solution is to use the concept of partial transfer_functions

 which create summaries for observed calling

 contextsWilsonThesisRWilson95 The same summary can be

 reused by multiple contexts that share the same relevant alias

 patterns This technique has_been shown to handle C programs up to

 20000 lines



 One_solution is to allow only weak updatesOOPSLA99Whaley

 that is a write to a variable only adds a value to the contents of

 the variable without removing the previously held value This greatly

 reduces the power of a flow-sensitive analysis This_approach has

 been used to handle programs up to 70000 lines of code However on

 larger programs the representation still becomes too large to deal

 with Because the goal of the prior work was escape_analysis it was

 not necessary to maintain precise points-to_relations for locations

 that escape so the algorithm achieved scalability by collapsing

 escaped nodes



 BDD-based pointer_analysis BDDs have recently been used in a

 number of program analyses such_as predicate

 abstractionBebop2000 shape

 analysismanevich02compactlyBultan02 and in particular

 points-to analysisberndl03zhu02 Zhu proposed a summary-based

 context-sensitive_points-to analysis for C programs and reported

 preliminary experimental_results on C programs with less_than 5000

 lineszhu02 Berndl showed that BDDs

 can be used to compute context-insensitive inclusion-based_points-to

 results for large Java programs efficiently In the same conference

 this_paper is presented Zhu and Calman describe a cloning-based

 context-sensitive_analysis for C pointers assuming that only the safe

 C subset is used The largest program reported in their experiment

 has about 25000 lines and contextszhu04



 High-level languages and tools for program analysis The

 use of Datalog and other logic_programming languages has previously

 been_proposed for describing program

 analysesdawson96practicalreps94demandUllmanDatabase Our

 system implements Datalog using BDDsbddbddb and has

 been used to compute context-sensitive_points-to results and other

 advanced analyses Other examples of systems that translate program

 analyses and queries written in logic_programming languages into

 implementations using_BDDs include Toupietoupie and

 CrocoPatcrocopat Jedd is a Java language extension that

 provides a relational algebra abstraction over

 BDDslhotak03jedd







 is a

 CrocoPat also based_on BDDS allows users to

 query program relations using a language based_on predicate

 calculuscrocopat





 Many

 other systems have attempted to provide a uniform framework in which

 to implement program analysis including set- and

 type-constraintsaiken98toolkit graph rewrite

 systemsassmann96how and



 -calculusrauzy95toupie

 which uses non-binary decision_diagrams



 Filtering with vPfilter is like type-based alias

 analysisdiwan98typebased

 Aggregate analysis to detect when a polymorphic type is used in a monomorphic waydiwan96simple







 could be_solved efficiently by

 representing the constraints in BDD form and expressing the inference

 rules as BDD_operations We extend this work by_adding the ability to

 compute the call_graph on-the-fly and context-sensitivity





 Space limitations preclude us from reviewing the large volume of

 research on pointer_alias analysis in detail Since this_paper

 presents a context-sensitive_pointer alias_analysis that can handle

 large_programs we_shall first overview scalable algorithms proposed

 to date and then discuss the more_recent research in

 context-sensitive_pointer alias_analysis





 Experimental Results

 secresults



 figurehtb

 small

 tabularllrrrrrr l

 Name Description Classes Methods Bytecodes Vars Allocs 2cCS Paths



 freetts speech synthesis system

 215 723 48K 8K 3K 16 bits



 nfcchat scalable distributed chat client

 283 993 61K 11K 3K 23 bits



 jetty HTTP Server and Servlet container

 309 1160 66K 12K 3K 20 bits



 openwfe java workflow engine

 337 1215 74K 14K 4K 22 bits



 joone Java neural net framework

 375 1531 92K 17K 4K 24 bits



 jboss J2EE application server

 348 1554 104K 17K 4K 29 bits



 jbossdep J2EE deployer

 431 1924 119K 21K 5K 29 bits



 sshdaemon SSH daemon

 485 2053 115K 24K 5K 32 bits



 pmd Java source code analyzer

 394 1971 140K 19K 4K 79 bits



 azureus Java bittorrent client

 498 2714 167K 24K 5K 31 bits



 freenet anonymous peer-to-peer file sharing system

 667 3200 210K 38K 8K 25 bits



 sshterm SSH terminal

 808 4059 241K 42K 8K 39 bits



 jgraph mathematical graph-theory objects and algorithms

 1041 5753 337K 59K 10K 37 bits



 umldot makes UML class diagrams from Java code

 1189 6505 362K 65K 11K 49 bits



 jbidwatch auction site bidding sniping and tracking tool

 1474 8262 489K 90K 16K 46 bits



 columba graphical email client with internationalization

 2020 10574 572K 111K 19K 44 bits



 gantt plan projects using Gantt charts

 1834 10487 597K 117K 20K 44 bits



 jxplorer ldap browser

 1927 10702 645K 133K 22K 32 bits



 jedit programmer's text editor

 1788 10934 667K 124K 20K 26 bits



 megamek networked BattleTech game

 1265 8970 668K 123K 21K 49 bits



 gruntspud graphical CVS client

 2277 12846 687K 145K 24K 31 bits



 tabular

 small

 Information about the benchmarks we used to test our analysesThe sizes

 are based_on reachable code as determined by Algorithm algci-fly

 figbenchmarks

 figure



 In this_section we present some experimental_results of using

 on the Datalog algorithms presented in this_paper We

 describe our testing methodology and benchmarks present the analysis times

 evaluate the results of the analyses

 and

 provide some insight on our experience

 of developing these analyses and the tool



 Methodology

 resultsmethod







 The input to is more or less the Datalog_programs exactly as

 they are presented in this_paper (We added a few rules to handle

 return values and threads and added annotations for the physical

 domain assignments of input relations) The input relations were

 generated with the Joeq compiler infrastructurejoeq The

 entire implementation is only 2500 lines of code

 uses the JavaBDD libraryjavabdd an open-source library based

 on the BuDDy librarybuddy The entire system is available as

 open-sourcebddbddb and we hope that others will find it

 useful





 Due to a limitation in the

 implementation we used a maximum of 63 bits for the context domain

 any contexts numbered beyond were_merged into a single

 context



















 All experiments were performed on a 22GHz Pentium 4 with Sun JDK

 14204 running on Fedora Linux Core 1 For

 the context-insensitive and context-sensitive experiments

 respectively we used initial BDD table sizes of 4M and 12M the

 tables could grow by 1M and 3M after each garbage_collection the BDD

 operation cache sizes were 1M and 3M









 To test the scalability and applicability of the algorithm we applied

 our technique to 21 of the most_popular Java projects on Sourceforge

 as of November 2003 We simply walked down the list of 100 Java

 projects sorted by activity selecting the ones that would compile

 directly as standalone applications They are all real applications

 with tens of thousands of users each As far as we know these are

 the largest benchmarks ever reported for any context-sensitive Java

 pointer_analysis As a point of comparison the largest benchmark in

 the specjvm suite javac would rank only 13th in our list



 For each application we chose an applicable main() method as

 the entry point to the application We included all class

 initializers thread run methods and finalizers We ignored null

 constants in the analysis-every points-to set is automatically

 assumed to include null Exception objects of the same type were

 merged We treated reflection and native_methods as returning unknown

 objects Some native_methods and special fields were modeled

 explicitly



 A short description of each of the benchmarks is included in

 Figure figbenchmarks along with their vital statistics The

 number of classes methods and bytecodes were those discovered by the

 context-insensitive on-the-fly call_graph construction algorithm so

 they include only the reachable parts of the program and the class

 library



 The number of context-sensitive (CS) paths is for the most part

 correlated to the number of methods in the program with the exception

 of pmd pmd has an astounding paths in the call

 graph which requires 79 bits to represent pmd has different

 characteristics because it contains code generated_by the parser

 generator JavaCC Many machine-generated methods call the same class

 library routines leading to a particularly egregious exponential

 blowup The JavaBDD library only supports physical_domains up to 63

 bits contexts numbered beyond were_merged into a single

 context The large_number of paths also caused the algorithm to

 require many more rule applications to reach a fixpoint solution



 Analysis Times

 resultstimes



 figurehtb

 small

 tabularlrrrrrrrrrrrrr

 7cContext-insensitive pointers

 4cContext-sensitive

 2cThread-sensitive





 2-12

 Name

 2cno type_filter

 2cwith type_filter

 3cwith cg discovery

 2cpointer analysis

 2ctype analysis

 2cpointer analysis





 time_mem time_mem iter time_mem time_mem

 time_mem time_mem



 freetts 1 3 1 3 20 2_4 1 6 1 6 1 4



 nfcchat 1 4 1 4 23 4 6 2 12 2 12 1 6



 jetty 1 5 1 5 22 4 7 3 12 2 10 1 6



 openwfe 1 5 1 6 23 5 8 4 14 2 14 1 7



 joone 2 7 1 7 24 7 10 4 18 3 18 1 9



 jboss 2 7 2 7 30 8 10 7 24 4 22 2 9



 jbossdep 2 9 2 9 26 7 12 9 30 5 26 3 11



 sshdaemon 2 9 2 10 26 13 14 12 34 6 28 3 13



 pmd 1 7 1 7 33 9 10 297 111 19 36 1 9



 azureus 2 10 2 10 29 13 15 9 32 6 30 2 12



 freenet 7 16 5 16 40 41 23 21 38 10 32 6 21



 sshterm 8 17 5 17 31 37 25 50 86 18 60 7 23



 jgraph 17 27 11 25 42 78 37 119 134 33 20 13 35



 umldot 17 30 11 29 34 97 43 457 304 63 130 16 41



 jbidwatch 31 43 20 40 32 149 58 580 394 68 140 25 56



 columba 43 55 27 49 42 273 73 807 400 123 178 38 72



 gantt 41 59 26 51 39 261 76 1122 632 113 174 34 71



 jxplorer 57 68 39 60 41 390 88 337 198 78 118 51 83



 jedit 61 61 38 54 37 278 80 113 108 60 82 50 76



 megamek 40 57 26 51 34 201 76 1101 600 100 224 34 73



 gruntspud 66 76 41 67 35 389 99 312 202 86 130 58 95



 tabular

 small

 Analysis times and peak memory_usages for each of the

 benchmarks and analyses Time is in seconds and memory is in

 megabytes

 figtimes

 figure



 We measured the analysis times and memory_usage for each of the

 algorithms presented in this_paper (Figure figtimes) The

 algorithm with call_graph discovery in each iteration computes a

 call_graph based_on the points-to_relations from the previous

 iteration The number of iterations taken for that algorithm is also

 included here



 All timings reported are wall-clock times from a cold start and

 include the various overheads for Java garbage_collection BDD garbage

 collection growing the node table etc The memory numbers reported

 are the sizes of the peak number of live BDD nodes during the course

 of the algorithm We measured peak BDD memory_usage by setting the

 initial table size and maximum table size increase to 1MB and only

 allowed the table to grow if the node table was more_than 99 full

 after a garbage collection(To avoid garbage collections it

 is recommended to use more memory Our timing runs use the default

 setting of 80)



 The context-insensitive analyses

 (Algorithms algci and algci-types)

 are remarkably fast

 the type-filtering version was able to complete in under 45 seconds on

 all benchmarks

 It is interesting to notice that introducing type_filtering actually

 improved the analysis time and memory_usage Along with being more

 accurate the points-to sets are much_smaller in the type-filtered

 version leading to faster analysis times













 For Algorithm algci-fly the call_graph discovery sometimes

 took over 40 iterations to complete but it was very effective in

 reducing the size of the call_graph as compared to CHAlhotak03

 The complexity of the call_graph discovery algorithm seems to vary with the

 number of virtual call_sites that need resolving-jedit and megamek

 have many methods declared as final but jxplorer has none leading to

 more call_targets to resolve and longer analysis times



 The analysis times and memory_usages of our context-sensitive

 points-to_analysis (Algorithm algcs) were on the whole very

 reasonable It can analyze most of the small and medium size

 benchmarks in a few minutes and it successfully finishes analyzing

 even the largest benchmarks in under 19 minutes This is rather

 remarkable considering that the context-sensitive formulation is

 solving up to times as many relations as the

 context-insensitive version Our scheme of numbering the contexts

 consecutively allows the BDD to efficiently represent the similarities

 between calling_contexts The analysis times are most directly

 correlated to the number of paths in the call_graph From the

 experimental data presented here it appears that the analysis time of

 the context-sensitive algorithm scales approximately with

 where is the number of paths in the call_graph more

 experiments are necessary to determine if this trend persists across

 more programs



 -

 The context-sensitive type analysis_(Algorithm algta) is as

 expected quite a bit faster and less memory-intensive than the

 context-sensitive_points-to analysis Even_though it uses the same

 number of contexts it is an order of magnitude faster_than

 the context-sensitive_points-to analysis This is because in the type

 analysis the number of objects that can be pointed to is much_smaller

 which greatly increases sharing in the BDD The thread-sensitive

 pointer_analysis (Algorithm algescape) has analysis times and

 memory_usages that are roughly comparable to those of the

 context-insensitive_pointer analysis even_though it includes thread

 context information This is because the number of thread_creation

 sites is relatively_small and we use at most two contexts per thread



 Evaluation of Results

 resultseval





 It is difficult to come_up with meaningful metrics with which to

 evaluate points-to_analysis in general and even more so with

 context-sensitive_analysis Taking full advantage of context

 sensitivity requires a context-sensitive query Turning a

 context-sensitive_analysis result back into a context-insensitive

 version by projecting away the context causes us to lose much of the

 benefit of context_sensitivity Most straightforward metrics such_as

 points-to set size are meaningless because they depend_on the

 precision used one could obtain the best result by simply

 representing everything by a single object

 Our formulation of context_sensitivity is also unique in that it can

 distinguish_between huge numbers of different_contexts - far far

 too_many to iterate over This makes it difficult to collect some

 kinds of statistics



 Instead we evaluate the analyses in terms of some of the actual uses

 of the analysis information described in Section secqueries

 First we evaluated the escape_analysis of

 Section queriesescape with the thread-sensitive analysis

 (Algorithm algescape)





 An in-depth analysis of the accuracy of the analyses with_respect to

 each of the queries in Section secqueries is beyond the scope

 of this_paper Instead we show the results of two specific queries thread

 escape_analysis (Section queriesescape) and type_refinement

 (Section queriesrefinement)



 The results of the escape_analysis are shown in

 Figure figescape The first two columns give the number of

 captured and escaped_object creation_sites respectively

 The next two columns give the number of unneeded and needed

 synchronization operations





 The single-threaded benchmarks have only one escaped_object the

 global object from which static variables are accessed In the

 multi-threaded benchmarks the analysis is effective in finding 30-50

 of the allocation_sites to be captured and 15-30 of the

 synchronization operations to be unnecessary These are static

 numbers to fully evaluate the results would_require dynamic execution

 counts which is outside of the scope of this_paper



 The results of the type_refinement query are shown in

 Figure figrefined We tested the query across six different

 analysis variations From left to right they are context-insensitive

 pointer_analysis without and with type_filtering context-sensitive

 pointer_analysis and context-sensitive type analysis with the context

 projected away and context-sensitive_pointer and type analysis on the

 fully cloned graph Projecting away the context in a

 context-sensitive_analysis makes the result context-insensitive

 however it can still be more_precise than context-insensitive

 analysis because of the extra precision at the intermediate steps of

 the analysis We measured the percentages of variables that can point

 to multiple types and variables whose types can be refined



 -

 Including the type_filtering makes the algorithm strictly more

 precise Likewise the context-sensitive_pointer analysis is strictly

 more_precise than both the context-insensitive_pointer analysis and

 the context-sensitive type analysis We can see this trend in the

 results As the precision increases the percentage of multi-typed

 variables drops and the percentage of refinable

 variables increases The context-insensitive_pointer analysis and the

 context-sensitive type analysis are not directly comparable in some

 cases the pointers are more_precise in other cases the context-sensitive

 types are more_precise



 When we do_not project_away the context the context-sensitive_results

 are remarkably precise-the percentage of multi-typed variables is

 never_greater than 1 for the pointer_analysis and 2 for the type

 analysis Projecting away the context loses much of the benefit of

 context_sensitivity but is still noticeably more_precise than using

 a context-insensitive_analysis



 figurehtb

 small

 tabularlrrrr

 2cheap objects 2csync operations



 Name



 freetts 2349 1 43 0



 nfcchat 1845 2369 52 46



 jetty 2059 2408 47 89



 openwfe 3275 1 57 0



 joone 1640 1908 34 75



 jboss 3455 2836 112 105



 jbossdep 1838 2298 32 94



 sshdaemon 12822 22669 468 1244



 pmd 3428 1 47 0



 azureus 8131 9183 226 229



 freenet 5078 9737 167 309



 sshterm 16118 24483 767 3642



 jgraph 25588 48356 1078 5124



 umldot 38930 69332 2146 8785



 jbidwatch 97234 143384 2243 11438



 columba 111578 174329 3334 18223



 gantt 106814 156752 2377 11037



 jxplorer 188192 376927 4127 18904



 jedit 446896 593847 7132 36832



 megamek 179221 353096 3846 22326



 gruntspud 248426 497971 5902 25568



 tabular

 small

 Results of escape_analysis

 figescape

 figure



 figurehtb

 small

 tabularlrrrrrrrrrrrr

 4cContext-insensitive pointers

 4cProjected context-sensitive

 4cContext-sensitive





 2-13

 Name

 2cno type_filter

 2cwith type_filter

 2cpointer analysis

 2ctype analysis

 2cpointer analysis

 2ctype analysis





 s_m r s_m r s_m r s_m r s_m r s_m r



 multi_refine multi_refine multi_refine multi_refine multi_refine multi_refine







 freetts 949 51 411 977 23 416 980 20 419 975 25 413 999_01 444 997 03 440



 nfcchat 876 124 364 914 86 370 918 82 374 914 86 369 999_01 459 993 07 453



 jetty 874 126 362 923 77 371 927 73 374 923 77 371 999_01 454 994_06 448



 openwfe 879 121 369 930 70 377 934 66 380 930 70 376 999_01 455 995_05 448



 joone 881 119 375 932 68 381 936 64 384 933 67 381 999_01 458 995_05 450



 jboss 866 134 378 921 79 387 926 74 393 922 78 387 999_01 473 993 07 464



 jbossdep 898 102 403 960 40 426 930 70 400 925 75 394 998 02 476 992 08 466



 sshdaemon 893 107 393 940 60 403 942 58 405 941 59 404 999_01 468 994_06 461



 pmd 904 96 423 938 62 431 941 59 434 938 62 431 999_01 521 994_06 481



 azureus 900 100 436 939 61 441 940 60 443 938 62 441 999_01 508 991 09 497



 freenet 879 121 391 937 63 400 941 59 405 937 63 401 999_01 470 992 08 460



 sshterm 853 147 408 911 89 420 915 85 425 910 90 421 994_06 513 984 16 499



 jgraph 839 161 432 904 96 451 907 93 454 903 97 452 993 07 547 981 19 532



 umldot 843 157 423 906 94 436 910 90 439 906 94 436 994_06 530 980 20 512



 jbidwatch 851 149 423 914 86 434 918 82 437 914 86 434 994_06 520 983 17 505



 columba 843 157 423 910 90 437 914 86 441 911 89 439 994_06 524 982 18 510



 gantt 850 150 434 918 82 447 921 79 450 918 82 447 995_05 530 983 17 514



 jxplorer 848 152 431 921 79 443 923 77 446 920 80 444 995_05 525 984 16 508



 jedit 846 154 436 919 81 447 921 79 449 919 81 447 994_06 531 984 16 515



 megamek 867 133 446 929 71 451 932 68 453 928 72 452 995_05 533 986 14 516



 gruntspud 846 154 440 923 77 455 925 75 457 922 78 455 995_05 536 986 14 521





 freetts 51 411 23 416 20 419 25 413 01 444 03 440



 nfcchat 124 364 86 370 82 374 86 369 01 459 07 453



 jetty 126 362 77 371 73 374 77 371 01 454 06 448



 openwfe 121 369 70 377 66 380 70 376 01 455 05 448



 joone 119 375 68 381 64 384 67 381 01 458 05 450



 jboss 134 378 79 387 74 393 78 387 01 473 07 464



 jbossdep 102 403 74 395 70 400 75 394 02 476 08 466



 sshdaemon 107 393 60 403 58 405 59 404 01 468 06 461



 pmd 96 423 62 431 59 434 62 431 01 521 06 481



 azureus 100 436 61 441 60 443 62 441 01 508 09 497



 freenet 121 391 63 400 59 405 63 401 01 470 08 460



 sshterm 147 408 89 420 85 425 90 421 06 513 16 499



 jgraph 161 432 96 451 93 454 97 452 07 547 19 532



 umldot 157 423 94 436 90 439 94 436 06 530 20 512



 jbidwatch 149 423 86 434 82 437 86 434 06 520 17 505



 columba 157 423 90 437 86 441 89 439 06 524 18 510



 gantt 150 434 82 447 79 450 82 447 05 530 17 514



 jxplorer 152 431 79 443 77 446 80 444 05 525 16 508



 jedit 154 436 81 447 79 449 81 447 06 531 16 515



 megamek 133 446 71 451 68 453 72 452 05 533 14 516



 gruntspud 154 440 77 455 75 457 78 455 05 536 14 521



 tabular

 small

 Results of the type_refinement query Numbers are percentages

 Columns labeled multi and refine refer to multi-type variables and refinable-type variables respectively

 figrefined

 figure



 Experience

 resultsexperience



 All the experimental_results reported here are generated using

 At the early stages of our research we hand-coded every

 points-to_analysis using BDD_operations directly and spent a

 considerable amount of time tuning their performance Our

 context-numbering scheme is the reason why the analysis would finish

 at all on even small programs Every one of the optimizations

 described in Section approachbdd was first carried_out

 manually After considerable effort megamek still took over three

 hours to analyze and jxplorer did_not complete at all

 The incrementalization was very

 difficult to get correct and we found a subtle bug months after the

 implementation was completed We did_not incrementalize the outermost

 loops as it would have_been too tedious and error-prone It was also

 difficult to experiment with different rule application orders



 To get even better performance and more_importantly to make it

 easier to develop new queries and analyses we created We

 automated and extended the optimizations we have used in our manual

 implementation and implemented a few new ones to empirically choose

 the best BDD_library parameters The end result is that code

 generated_by outperforms our manually tuned context-sensitive

 pointer_analysis by as much as an order of magnitude Even better we

 could use to quickly and painlessly develop new analyses that

 are highly_efficient such_as the type analysis in

 Section queriesta and the thread_escape analysis in

 Section queriesescape





 Through the development of

 we were_able to speed_up the algorithm by an order of magnitude and

 also quickly and easily develop new analyses and queries



 When developing instead of trying to solve the specific

 problems for each analysis we took it upon ourselves to solve

 problems in a general way so that other analyses could potentially

 benefit For_example rather_than requiring the user to

 incrementalize by hand will automatically incrementalize the

 input program transforming the rules into a form that is more

 suitable to incrementalization identifying loop-invariant relations

 and minimizing rename_operations Rather_than requiring the user

 to specify an iteration order analyzes the rules to come_up

 with a fast iteration order Rather_than relying on the user to

 guide the assignment of attributes to physical_domains tries

 to find the best assignment itself by_taking into_account the loop

 structure and variable_ordering Rather_than requiring the user to

 specify the variable_ordering will experimentally try all

 partial domain orders for each operation keeping_track of the time

 spent on each and coming up with the best overall ordering for the

 algorithm can also automatically sweep the BDD_library

 parameters to find the fastest node table size and cache size



 By confronting problems in this manner we were_able to develop a

 robust and efficient tool for specifying and solving analyses

 Because automatically incrementalized parts of the algorithm

 that we did_not automatically found a better variable_ordering than

 we did manually optimized the rule ordering and domain assignment

 and automatically found better BDD node table and cache sizes it was

 able to improve the execution time of the context-sensitive_pointer

 analysis by an order of magnitude Even better because we solved the

 problems in a general way we could use to quickly and

 painlessly develop highly_efficient versions of other analyses such

 as the type analysis in Section queriesta or the thread_escape

 analysis in Section queriesescape





























 Data Reuse

 ch11reuse



 From array_access functions we derive two kinds of information useful

 for locality optimization and parallelization



 enumerate



 Data reuse for locality optimization we_wish to

 identify sets of iterations that access the same data or the same

 cache_line



 Data_dependence for correctness of parallelization and locality loop

 transformations we_wish to identify all the data_dependences in

 the code Recall that two (not_necessarily distinct)_accesses have

 a data_dependence if instances of the accesses may refer to the same

 memory_location and at_least one of them is a write



 enumerate

 In many_cases whenever we identify iterations that reuse the same

 data there are data_dependences between them



 Whenever there is a data_dependence obviously the same data is

 reused For_example in matrix_multiplication the same element in

 the output array is written times The write operations must

 be executed in the original execution order(There is a subtle

 point here Because of the commutativity of addition we would get the same

 answer to the sum regardless of the order in which we performed the sum

 However this case is very special In_general it is far too complex

 for the compiler to determine what computation is being performed by a

 sequence of arithmetic steps followed_by writes and we cannot rely_on

 there being any algebraic rules that will help us reorder the steps

 safely)

 we can exploit the reuse

 by allocating a register to hold one element of the output array while

 it is being computed



 However not all reuse can be_exploited in locality optimizations

 here is an example illustrating this issue



 ex

 exnoreuse

 Consider the following loop



 verbatim

 for_(i 0 i_n i)

 Z7i3 Z3i5

 verbatim

 We observe that the loop writes to a different location at each

 iteration so there are

 no reuses or dependences on the different write operations The loop

 however reads locations and writes

 locations The read and write iterations

 access the same elements 17 38 and 59 and so on That is the

 integers of the form for

 are all those integers that can be written both as

 and as for some integers and

 However this reuse occurs rarely and cannot be_exploited easily if at

 all

 ex



 Data_dependence is different from reuse analysis in that one of the

 accesses_sharing a data_dependence must_be a write access More

 importantly data_dependence needs to be both correct and precise It

 needs to find all dependences for correctness and it should not find

 spurious dependences because they can cause unnecessary serialization



 With data reuse we only need to find where most of the exploitable

 reuses are This problem is much simpler so we take_up this topic here

 in this_section and tackle data_dependences in the next We simplify

 reuse analysis by ignoring loop_bounds because they seldom change the

 shape of the reuse Much of the reuse exploitable by affine

 partitioning resides among instances of the same array_accesses and

 accesses that share the same coefficient_matrix (what we have typically

 called F in the affine index function)

 As shown above

 access patterns like and have no reuse of

 interest



 Types of Reuse



 We first start with Example exreuse to illustrate the different

 kinds of data reuses In the following we need to distinguish

 between the access as an instruction in a program eg x Zij from the execution of this instruction many_times as we

 execute the loop_nest For emphasis we may refer to the statement

 itself as a static_access while the various iterations of the

 statement as we execute its loop_nest are called dynamic

 accesses



 Reuses can be classified as self versus group If

 iterations reusing the same data come from the same static_access we

 refer to the reuse as self reuse if they come from different

 accesses we refer to it as group_reuse The reuse is temporal

 if the same exact location is referenced it is spatial if the

 same cache_line is referenced



 ex

 exreuse

 Consider the following loop_nest



 verbatim

 float Zn

 for_(i 0 i_n i)

 for_(j 0 j_n j)

 Zj1 (Zj Zj1 Zj2)3

 verbatim

 Accesses and each have self-spatial

 reuse because consecutive_iterations of the same access refer to

 contiguous array_elements Presumably contiguous elements are very

 likely to reside on the same cache_line

 In_addition they all have self-temporal_reuse

 since the exact elements are used over and over again in each

 iteration in the outer_loop In_addition they all have the same

 coefficient_matrix and thus have group_reuse There is group_reuse

 both temporal and spatial between the different accesses Although

 there are accesses in this code if the reuse can be_exploited

 we only need to bring in about cache_lines into the cache where

 is the number of words in a cache_line We drop a factor of

 due to self-spatial_reuse a factor of to due to spatial_locality

 and finally a factor of 4 due to group_reuse

 ex



 In the following we show_how we can use linear_algebra to extract the

 reuse information from affine array_accesses We are_interested in

 not just finding how much potential savings there are but also which

 iterations are reusing the data so that we can try to move them close

 together to exploit the reuse



 Self Reuse



 There can be substantial savings in memory accesses by exploiting self

 reuse If the data referenced by a static_access has dimensions

 and the access is nested in a loop deep for some

 then the same data can be reused times where

 is the number of iterations in each loop For_example if a 3-deep

 loop_nest accesses one column of an array then there is a potential

 savings factor of accesses It_turns out that the dimensionality

 of an access corresponds to the concept of the rank of the

 coefficient_matrix in the access and we can find which iterations

 refer to the same_location by finding the null_space of the

 matrix as explained below



 Rank of a Matrix



 The rank of a matrix_F is the largest_number of columns (or

 equivalently rows) of F that are linearly_independent A set of

 vectors is linearly_independent if none of the vectors can be

 written as a linear_combination of finitely many other vectors in the

 set



 ex

 rank-ex

 Consider the matrix





 arrayl_l l

 1_2 3



 5 7 9



 4_5 6



 2 1_0



 array





 Notice_that the second row is the sum of the first and third rows while the

 fourth row is the third row minus twice the first row However the

 first and third rows are linearly_independent neither is a multiple of

 the other

 Thus the rank of the matrix is 2



 We could also draw this conclusion by_examining the columns The third

 column is twice the second column minus the first column On the other

 hand any two columns are linearly_independent Again we conclude that

 the rank is 2

 ex



 ex

 Let_us look_at the array_accesses in Fig array-accesses-fig The

 first access has dimension 1 because the rank of the

 matrix is 1 That is the one row is linearly_independent as

 is the first column



 The second access has dimension 2 The_reason is that

 the matrix





 arrayl_l

 1_0



 0_1



 array





 has two independent_rows (and_therefore two independent columns of

 course)

 The third access is of dimension 1 because the matrix





 arrayl_l

 0_1



 0_1



 array





 has_rank 1 Note_that the two rows are identical so only one is

 linearly_independent Equivalently the first column is 0 times the second

 column so the columns are not independent

 Intuitively in a large square array

 the only elements_accessed lie along a one-dimensional line just above

 the main diagonal



 The fourth access has dimension 0 because a matrix of all

 0's has_rank 0 Note_that for such a matrix we cannot find a linear

 sum of even one row that is nonzero

 Finally the last access has dimension 2 Note

 that in the matrix for this access





 arrayl_l

 0_0



 1_0



 2 1



 array





 the last two rows are linearly_independent neither is a multiple of the

 other

 However the first row is a linear sum of the other two rows with both

 coefficients 0

 ex



 Null Space of a Matrix



 A reference in a -deep_loop nest with rank

 accesses data elements in iterations so on average

 iterations must refer to the same array_element Which

 iterations access the same data Suppose an access in this loop_nest is

 represented_by matrix-vector combination F and f

 Let i and be two

 iterations that refer to the same array_element Then



 Rearranging terms we get



 F(i-i') 0





 There is a well-known concept from linear_algebra that characterizes

 when i and satisfy the above equation

 The set of all solutions to the equation

 is called the null_space of F Thus two iterations

 refer to the same array_element if the difference of their loop-index

 vectors belongs to the null_space of matrix_F



 It is easy to see

 that the null vector

 always satisfies

 That is two iterations surely refer to the same array_element if

 their difference is 0 in other_words if they are really the same

 iteration Also the null_space is truly a vector space That is if

 and then

 and



 If the matrix_F is fully ranked that is its

 rank is then the null_space of F

 consists of only the null vector In

 that case iterations in a loop_nest all refer to different data In

 general the dimension of the null_space also known_as the nullity is If then for each element

 there is a -dimensional space

 of iterations that access that element



 The null_space can be represented_by its basis_vectors A

 -dimensional null_space is represented_by independent vectors

 any vector that can be_expressed as a linear_combination of the basis

 vectors belongs to the null_space



 ex

 null-space-ex

 Let_us reconsider the matrix of Example rank-ex





 arrayl_l l

 1_2 3



 5 7 9



 4_5 6



 2 1_0



 array





 We determined in that example that the rank of the matrix is 2 thus the

 nullity is To_find

 a basis for the null_space which in this case must_be a single nonzero

 vector of length 3 we may suppose a vector in the null_space to be

 and try to solve the equation





 arrayl_l l

 1_2 3



 5 7 9



 4_5 6



 2 1_0



 array





 arrayl

 x



 y



 z



 array







 arrayl

 0



 0



 0



 array





 If we multiply the first two rows by the vector of unknowns we get the

 two equations



 center

 tabularr_l l

 0



 0



 tabular

 center

 We could write the equations that come from the third and fourth rows as

 well but because there are no three linearly_independent rows we know

 that the additional equations add no new constraints on and

 For_instance the equation we get from the third row

 can be obtained_by subtracting the first equation from the second



 We must eliminate as many variables as we can from the above equations

 Start by using the first equation to solve for that is

 Then substitute for in the second equation to get or

 Since and it follows that

 Thus the vector is really We may pick any

 nonzero value of to form the one and only basis_vector for

 the null_space For_example we may choose and use as

 the basis of the null_space

 ex



 figurehtb



 center

 tabularlcccc

 Access AffineExpression

 Rank Null- Basisof



 ity NullSpace







 Xi-1



































 1 1























 Yij







































 2 0











 Yjj1







































 1 1























 Y12







































 0 2





































 Z1i2ij











































 2 0







 tabular

 center



 Rank and nullity of affine accesses

 fignullity

 figure



 ex

 exnullity

 The rank nullity and null_space for each of the references in

 Example exaccesses are shown in Fig fignullity

 Observe that the sum of the rank and nullity in all the cases is

 the depth of the loop_nest 2 Since the accesses and

 have a rank of 2 all iterations refer to different

 locations



 Accesses and both have rank-1 matrices

 so iterations refer to the same_location In the former

 case entire rows in the iteration_space refer to the same_location

 In other_words iterations that differ only in the dimension share

 the same_location which is succinctly represented_by the basis of the

 null_space 01 For entire columns in the iteration

 space refer to the same_location and this fact is succinctly

 represented_by the basis of the null_space 10



 Finally the

 access refers to the same_location in all the

 iterations The null_space corresponding has 2 basis_vectors 01

 10 meaning that all pairs of iterations in the loop_nest refer to

 exactly the same_location

 ex



 Self-Spatial Reuse

 self-spatial-subsect



 The analysis of spatial_reuse depends_on the data layout of the

 matrix C matrices are laid_out in row-major_order and Fortran

 matrices are laid_out in column-major_order

 In other_words array_elements

 and are contiguous in C and

 and are contiguous in Fortran Without

 loss of generality in the rest of the chapter we_shall adopt the C

 (row-major) array layout



 As a first approximation we consider two array_elements to share the

 same cache_line if and only if they share the same row in a

 two-dimensional_array More_generally in an array of dimensions we

 take array_elements to share a cache_line if they differ only in the

 last dimension Since for a typical array and cache many array

 elements can fit in one cache_line there is significant speedup to be

 had by accessing an entire row in order even_though strictly speaking

 we occasionally have to wait to load a new cache_line



 The trick to discovering and taking_advantage of self-spatial_reuse is

 to drop the last_row from the coefficient_matrix F If the resulting

 truncated

 matrix has_rank that is less_than the depth of the loop_nest then we can

 assure_spatial locality by making sure that the innermost_loop varies

 only the last coordinate of the array



 ex

 spatial-reuse-ex

 Consider the last access in

 Fig fignullity

 If we delete the last_row we are left with the truncated_matrix





 arrayrr

 0_0



 1_0



 array





 The rank of this matrix is evidently 1 and since the loop_nest has

 depth 2 there is the opportunity for spatial_reuse In this case since

 is the inner-loop_index the inner_loop visits contiguous

 elements of the array stored in row-major_order Making the

 inner-loop_index will not yield spatial

 locality since as changes both the second

 and third dimensions change

 ex



 The general rule for determining whether there is self-spatial_reuse is

 as_follows As always we assume that the loop_indexes correspond to

 columns of the coefficient_matrix in order with the outermost_loop first

 and the innermost_loop last Then in order for there to be spatial

 reuse the vector must_be in the null_space of the

 truncated_matrix

 The_reason is that if this vector is in the null_space then when we fix

 all loop_indexes but the innermost one we know that all dynamic

 accesses during one run through the inner_loop vary in only the last

 array index If the array is stored in row-major_order then these

 elements are all near one another perhaps in the same cache_line



 ex

 Note_that (transposed as a column vector) is in the null_space

 of the truncated_matrix of Example spatial-reuse-ex Thus as

 mentioned there we expect that with as the inner-loop_index there

 will be spatial_locality On the other_hand if we reverse the order of

 the loops so is the inner_loop then the coefficient_matrix becomes





 arrayrr

 0_0



 0_1



 array





 Now is not in the null_space of this matrix Rather the null

 space is generated_by the basis_vector Thus as we suggested

 in Example spatial-reuse-ex we do_not expect spatial_locality if

 is the inner_loop



 We should observe however that the test for being

 in the null_space is not quite sufficient to assure_spatial locality

 For_instance suppose the access were not but

 Then only every fiftieth element of would be

 accessed during one run of the inner_loop and we would not reuse a

 cache_line unless it were long enough to hold more_than 50 elements

 ex



 Group Reuse



 We compute group_reuse only among accesses in a loop sharing the same

 coefficient_matrix

 Given two

 dynamic_accesses and

 reuse of the same data requires that



 Fi1 f1 Fi2 f2



 or



 F(i1-i2) (f2-f1)



 Suppose v is one solution to this equation Then if w is

 any vector in the null_space of F is also a

 solution and in fact those are all the solutions to the equation



 ex

 group-reuse1-ex

 The following 2-deep_loop nest



 verbatim

 for_(i 1_i n_i)

 for_(j 1_j n_j)

 Zij Zi-1j

 verbatim

 has two array

 accesses

 and

 Observe that these two accesses are both characterized by the coefficient

 matrix





 arrayl_l

 1_0



 0_1



 array





 like the second access in Fig fignullity

 This matrix has_rank 2 so there is no self-temporal_reuse



 However each access exhibits self-spatial_reuse As described in

 Section self-spatial-subsect when we delete the bottom row of the

 matrix we are left with only the top row which has_rank 1

 Since is in the null_space of this truncated_matrix we expect

 spatial_reuse As each incrementation of inner-loop_index

 increases the second array index by one we in fact do access adjacent

 array_elements and will make maximum use of each cache_line



 Although there is no self-temporal_reuse for either access observe

 that the two references and access

 almost the same set of array_elements That is

 there is group-temporal reuse because the data read by access

 is

 the same as the data written by access

 except for the case

 This simple pattern applies to the entire iteration_space and can

 be_exploited to improve data_locality in the code Formally

 discounting the loop_bounds the two accesses and

 refer to the same_location

 in iterations and respectively provided





 arrayrr

 1_0



 0_1



 array





 arrayr

 i1



 j1



 array







 arrayr

 0



 0



 array







 arrayrr

 1_0



 0_1



 array





 arrayr

 i2



 j2



 array







 arrayr

 -1



 0



 array





 Rewriting the terms we get





 arrayrr

 1_0



 0_1



 array





 arrayr

 i1 - i2



 j1 - j2



 array







 arrayr

 -1



 0



 array





 That is and



 Notice_that the reuse occurs along the -axis of the iteration_space

 That is the iteration occurs iterations (of the inner

 loop) after the

 iteration

 Thus many iterations are executed before the data written is

 reused This data may or may not still be in the cache If the cache

 manages to hold two consecutive rows of matrix then access

 does_not miss in the cache and the total_number of cache

 misses for the entire loop_nest is where is the number of

 elements per cache_line Otherwise there will be

 twice as many misses since both static accesses require a new cache

 line for each dynamic_accesses

 ex



 ex

 group-reuse-ex

 Suppose there are two accesses

 center

 and

 center

 in a 3-deep loop_nest with indexes and from the outer to

 the inner_loop Then two accesses and

 reuse the same element whenever





 arrayrrr

 1_0 0



 0_1 0



 1 1_0



 array





 arrayr

 i1



 j1



 k1



 array







 arrayr

 0



 0



 0



 array







 arrayrrr

 1_0 0



 0_1 0



 1 1_0



 array





 arrayr

 i2



 j2



 k2



 array







 arrayr

 1



 -1



 0



 array







 One_solution to this equation for a vector

 is that is

 and (It is interesting to

 observe that although there is a solution in this case there would be

 no solution if we changed one of the third components from to

 That is in the example as given both accesses touch those

 array_elements that lie in the 2-dimensional subspace defined by the

 third component is the sum of the first two components If we changed

 to none of the elements_touched by the second access

 would lie in and there would be no reuse at all)

 However the null_space of the matrix



 F



 arrayrrr

 1_0 0



 0_1 0



 1 1_0



 array





 is generated_by the basis_vector that is the third loop

 index can be arbitrary Thus v the solution to the

 above equation is any vector for some Put_another

 way a dynamic access to in a loop_nest with

 indexes and

 is reused not only by other dynamic_accesses

 with the same values of and and a different

 value of but also by dynamic_accesses with

 loop_index values and any value of

 ex



 Although we_shall not do so here we

 can reason_about group-spatial reuse analogously

 As per the discussion of self-spatial_reuse we simply drop

 the last dimension from consideration



 The extent of reuse is different for the different categories of

 reuse Self-temporal reuse gives the most benefit a reference with

 a -dimensional null_space reuses the same data times The

 extent of self-spatial_reuse is limited by the length of the cache

 line Finally the extent of group_reuse is limited by the number of

 references in a group sharing the reuse



 exer

 Compute the ranks of each of the matrices in Fig comp-ranks-fig

 Give both a maximal_set of linearly_independent columns and a maximal_set

 of linearly_independent rows

 exer



 figurehtfb



 center

 tabularc_c c









































































 (a)_(b) (c)



 tabular

 center



 Compute the ranks and null spaces of these matrices

 comp-ranks-fig



 figure



 exer

 Find a basis for the null_space of each matrix in

 Fig comp-ranks-fig

 exer



 exer

 Assume that the iteration_space has dimensions (variables) and

 For each of the accesses below describe the subspaces that refer

 to the following single elements of the array



 itemize

 a)

 b)

 c)

 d)

 itemize

 exer



 hexer

 Suppose array is stored in row-major_order and accessed inside the

 following loop_nest



 verbatim

 for_(i 0 i_100 i)

 for_(j 0 j_100 j)

 for (k 0 k 100 k)

 some access to A

 verbatim

 Indicate for each of the following accesses whether it is possible to

 rewrite the loops so that the access to exhibits self-spatial_reuse

 that is entire cache_lines are used consecutively Show_how to rewrite

 the loops if so Note the rewriting of the loops may involve both

 reordering and introduction of new loop_indexes However you may not

 change the layout of the array eg by changing it to column-major

 order

 Also note in general reordering of loop_indexes may be legal or

 illegal depending_on criteria we develop in the next section However

 in this case_where the effect of each access is simply to set an array

 element to 0 you do_not have to worry_about the effect of reordering

 loops as far as the semantics of the program is concerned



 itemize



 a) Ai1ikj 0

 b) Ajkii 0

 c) Aijkijk 0

 d) Aij-kijik 0



 itemize

 hexer



 exer

 In Section self-spatial-subsect we commented that we get spatial

 locality if the innermost_loop varies only as the last coordinate of an

 array_access However that assertion depended on our_assumption that

 the array was stored in row-major_order What condition would assure

 spatial_locality if the array were stored in column-major_order

 exer



 hexer

 In Example group-reuse-ex we observed that the existence of

 reuse between two similar accesses depended heavily on the particular

 expressions for the coordinates of the array Generalize our

 observation there to determine for which functions there is

 reuse between the accesses and



 hexer



 hexer

 In Example group-reuse1-ex we suggested that there will be more

 cache_misses than necessary if rows of the matrix are so long that

 they do_not fit in the cache If that is the case how could you

 rewrite the loop_nest in order to guarantee group-spatial reuse

 hexer

 The Role of the Lexical_Analyzer

 lex-role-sect



 As the first phase of a compiler the main task of the lexical_analyzer

 is to read the input_characters of the source_program

 group them into lexemes and produce as output a sequence of

 tokens for each lexeme in the source_program

 The stream of tokens is sent to the parser for syntax analysis

 It is common for the lexical_analyzer to interact with the symbol_table

 as_well

 When the lexical_analyzer discovers a lexeme constituting an_identifier

 it needs to enter that lexeme into the symbol_table

 In some_cases information regarding the kind of identifier may be

 read from the symbol_table by the lexical_analyzer to assist it in

 determining the proper token it must pass to the parser



 These interactions are suggested in Fig lex-parse-fig

 Commonly the interaction is implemented_by having the parser call the

 lexical_analyzer

 The call suggested by the getNextToken command causes the

 lexical_analyzer to read characters from its input until it can

 identify the next lexeme and produce for it the next token

 which it returns to the parser



 figurehtfb

 fileuullmanalsuch3figslex-parseeps

 Interactions between the lexical_analyzer and the parser

 lex-parse-fig

 figure



 Since the lexical_analyzer is the part of the compiler that reads the

 source text it may perform certain other tasks besides identification of

 lexemes

 One such task is stripping out comments and whitespace

 (blank newline tab and perhaps other characters that are used to

 separate tokens in the input)

 Another task is correlating error_messages generated_by

 the compiler with the source_program

 For_instance the lexical_analyzer may keep_track of the number of

 newline characters seen so it can associate a line number with each

 error message

 In some compilers the lexical_analyzer makes a copy of the source

 program with the error_messages inserted at the appropriate positions

 If the source_program uses a macro-preprocessor the expansion of macros

 may also be performed by the lexical_analyzer



 Sometimes lexical_analyzers are divided_into a cascade of two

 processes



 itemize



 a)

 Scanning consists of the simple processes that do_not require

 tokenization of the input such_as deletion of comments and compaction of

 consecutive whitespace characters into one



 b)

 Lexical analysis proper is the more_complex portion which

 produces tokens from the output of the scanner



 itemize



 Lexical Analysis Versus Parsing

 lex-issues-subsect



 There_are a number of reasons_why the analysis portion of a compiler is

 normally separated into lexical analysis and parsing (syntax analysis)

 phases



 enumerate



 Simplicity of design is the most_important consideration

 The separation of lexical and syntactic analysis often allows_us to

 simplify at_least one of these tasks

 For_example a parser that had to deal_with comments and whitespace as

 syntactic units would be considerably more_complex than one that can

 assume comments and whitespace have_already been removed by the lexical

 analyzer

 If we are designing a new language separating lexical and syntactic

 concerns can lead to a cleaner overall language design



 Compiler efficiency is improved

 A separate lexical_analyzer allows_us to apply specialized techniques

 that serve only the lexical task not the job of parsing

 In_addition specialized buffering techniques for reading input

 characters can speed_up the compiler significantly



 Compiler portability is enhanced

 Input-device-specific peculiarities can be

 restricted to the lexical_analyzer



 enumerate



 Tokens Patterns and Lexemes

 lexemes-subsect



 When discussing lexical analysis we use three related but distinct

 terms



 itemize



 A token is a pair consisting of a token_name and an

 optional attribute value

 The token_name is an abstract symbol representing a kind of lexical unit

 eg a particular keyword or a sequence of input_characters

 denoting an_identifier

 The token names are the input symbols that the parser processes

 In what_follows we_shall generally write the name of a token in

 boldface

 We will often refer to a token by its token_name



 A pattern is a description of the form that the lexemes of a token

 may take

 In the case of a keyword as a token the pattern is just the

 sequence of characters that form the keyword

 For identifiers and some other tokens the pattern is a more_complex

 structure that is matched by many strings



 A lexeme is a sequence of characters in the source_program that

 matches the pattern for a token and is identified by the lexical

 analyzer as an instance of that token



 itemize



 ex

 lexeme-ex

 Figure lexeme-fig gives some typical tokens their informally

 described patterns and some sample lexemes

 To_see how these concepts are used in practice in the C statement



 verbatim

 printf(Total

 verbatim

 both printf and score are lexemes matching the pattern for

 token_id and Total d92n is a lexeme matching

 literal

 ex



 figurehtfb



 center

 tabularlll

 token informal Description

 Sample Lexemes



 if characters i f if



 else characters e l s e else



 comparison or or or or or





 id letter followed_by letters and digits pi score D2



 number any numeric constant 314159 0

 602e23



 literal anything but surrounded_by 's

 core dumped



 tabular

 center



 Examples of tokens

 lexeme-fig

 figure



 In many programming_languages the following classes cover most or all

 of the tokens



 enumerate



 One token for each keyword

 The pattern for a keyword is the same as the keyword itself



 Tokens for the operators either individually or in classes such_as the

 token comparison mentioned in Fig lexeme-fig



 One token representing all identifiers



 One or_more tokens representing constants such_as numbers and literal strings



 Tokens for each punctuation symbol such_as left and right

 parentheses comma and semicolon



 enumerate



 Attributes for Tokens

 token-attr-subsect



 When more_than one lexeme can match a pattern the lexical_analyzer must

 provide the subsequent compiler phases

 additional information_about the particular lexeme that

 matched

 For_example the pattern for token number matches both 0 and 1

 but it is extremely important for the code_generator to know which

 lexeme was found in the source_program

 Thus in many_cases the lexical_analyzer returns to the parser not only

 a token_name but an attribute value that describes the lexeme represented_by

 the token the token_name influences parsing_decisions while the attribute

 value influences translation of tokens after the parse



 We_shall assume that tokens have at most one associated attribute

 although this attribute may have a structure that combines several pieces

 of information

 The most_important example is the token_id where we need to

 associate with the token a great deal of information

 Normally information_about an_identifier - eg its lexeme its

 type and the location at which it is first found (in case an error

 message about that identifier must_be issued) - is kept in the symbol

 table

 Thus the appropriate attribute value for an_identifier is a pointer to the

 symbol-table_entry for that identifier



 Tricky Problems When Recognizing Tokens

 Usually given the pattern describing the lexemes of a token

 it is relatively

 simple to recognize matching lexemes when they occur on the input

 However in some languages it is not immediately apparent when we have

 seen an instance of a lexeme corresponding to a token

 The following example is taken from Fortran in the fixed-format still

 allowed in Fortran 90

 In the statement



 center

 DO 5 I 125

 center

 it is not apparent that the first lexeme is DO5I an instance of

 the identifier token until we see the dot following the 1

 Note_that blanks in fixed-format Fortran are ignored (an archaic

 convention)

 Had we seen a comma instead of the dot we would have had a

 do-statement

 center

 DO 5 I 125

 center

 in which the first lexeme is the keyword DO



 ex

 attrib-ex

 The token names and associated attribute values for the Fortran statement



 verbatim

 E M C 2

 verbatim

 are written below as a sequence of pairs



 center

 tabularl

 id pointer to symbol-table_entry for E



 assignop



 id pointer to symbol-table_entry for M



 multop



 id pointer to symbol-table_entry for C



 expop



 number integer value 2



 tabular

 center



 Note_that in certain pairs especially operators punctuation and

 keywords there is no need for an attribute value

 In this example the token number has_been given an

 integer-valued attribute

 In_practice a typical compiler would instead store a character string

 representing the constant and use as an attribute value for number a

 pointer to that string

 ex



 Lexical Errors

 lex-error-subsect



 It is hard for a lexical_analyzer to tell without the aid of other

 components that there is a source-code error

 For_instance if the string fi is encountered for the first time

 in a C program in the context



 verbatim

 fi ( a f(x))

 verbatim

 a lexical_analyzer cannot_tell whether fi is a misspelling of the

 keyword if or an undeclared function identifier

 Since fi is a valid lexeme for the token_id the lexical

 analyzer must return the token_id to the parser and let some other

 phase of the compiler - probably the parser in this case - handle

 an error due to transposition of the letters



 However suppose a situation arises in which the lexical_analyzer is

 unable to proceed because none of the patterns for tokens matches any

 prefix of the remaining_input

 The simplest recovery strategy is panic mode recovery

 We delete successive characters from the remaining_input until the

 lexical_analyzer can find a well-formed token at the beginning of what

 input is left

 This recovery technique may confuse the parser but in an interactive

 computing environment it may be quite adequate



 Other possible error-recovery actions are



 enumerate



 Delete one character from the remaining_input



 Insert a missing character into the remaining_input



 Replace a character by another character



 Transpose two adjacent characters



 enumerate

 Transformations like these may be tried

 in an attempt to repair the input

 The simplest such strategy is to see whether a prefix of the remaining

 input can be transformed into a valid lexeme by a single transformation

 This strategy makes_sense since in practice most lexical errors involve

 a single character

 A more general correction strategy is to find the smallest number of

 transformations needed to convert the source_program into one that

 consists only of valid lexemes but this approach is considered too

 expensive in practice to be worth the effort



 sexer

 Divide the following C program



 verbatim

 float limitedSquare(x) float_x

 returns x-squared but never more_than 100

 return (x-100x100)100xx



 verbatim

 into appropriate lexemes using the

 discussion of Section lexemes-subsect as a guide Which lexemes

 should get associated lexical values What should those values be

 sexer



 hsexer

 Tagged languages_like HTML or XML are different from conventional programming

 languages in that the punctuation (tags) are either very numerous (as in

 HTML) or a user-definable set (as in XML) Further tags can often have

 parameters Suggest how to divide the following HTML document



 verbatim

 Here is a photo of Bmy houseB

 PIMG SRC housegifBR

 See A HREF morePixhtmlMore PicturesA if you

 liked that oneP

 verbatim

 into

 appropriate lexemes Which lexemes should get associated lexical

 values and what should those values be

 hsexer

 Stack Allocation of Space

 stack-alloc-sect



 Almost all compilers for languages that use procedures

 functions or methods as

 units of user-defined actions manage at_least part of their

 run-time memory as a stack

 Each time a procedurefootnoteRecall we use

 procedure as a generic term for function procedure

 method or subroutinefootnote

 is called space for its local_variables is pushed_onto a stack

 and when the procedure terminates that space is popped_off

 the stack As we_shall see this arrangement

 not only allows space to be shared by procedure_calls whose

 durations do_not overlap in time but it allows_us to compile code

 for a procedure in such a way that the relative_addresses of its

 nonlocal variables are always the same regardless of the sequence

 of procedure_calls



 Activation Trees

 rec-subsect



 Stack allocation would not be feasible if procedure_calls or activations of procedures did_not nest in time The following

 example_illustrates nesting of procedure_calls



 ex

 Figure qs1-fig contains a sketch of a program that reads

 nine integers into an array and sorts them using the

 recursive quicksort algorithm



 figurehtfb

 center

 tabularl

 'int a11'



 'void readArray() ' Reads 9 integers into ''



 '_int i'



 '_'



 ''



 'int partition(int m int n) '



 ' 'Picks a separator value and partitions so that



 '_' are less_than and are



 ' 'equal to or greater_than Returns ''



 '_'



 ''



 'void quicksort(int m int n) '



 '_int i'



 ' if (n m) '



 ' i partition(m n)'



 ' quicksort(m i-1)'



 ' quicksort(i1 n)'



 '_'



 ''



 'main() '



 ' readArray()'



 ' a0 -9999'



 ' a10 9999'



 ' quicksort(19)'



 ''



 tabular

 center

 Sketch of a quicksort program

 qs1-fig



 figure



 A Version of QuicksortThe sketch of a quicksort

 program in Fig qs1-fig uses two auxiliary functions readArray and partition The function readArray is

 used only to load the data into the array The first and last

 elements of are not used for data but rather for

 sentinels set in the main function We assume is set to

 a value lower than any possible data value and is set to

 a value higher_than any data value



 The function partition divides a portion of the array

 delimited by the arguments and so the low elements of

 through are at the beginning and the high elements

 are at the end although neither group is necessarily in sorted

 order We_shall not go into the way partition works except

 that it may rely_on the existence of the sentinels One possible

 algorithm for partition is suggested by the more detailed

 code in Fig_figqs



 Recursive procedure quicksort first decides if it needs to

 sort more_than one element of the array Note_that one element is

 always sorted so quicksort has nothing to do in that

 case If there are elements to sort quicksort first calls

 partition which returns an index to separate the low

 and high elements These two groups of elements are then sorted by

 two recursive_calls to quicksort



 The main function has three tasks It calls readArray sets

 the sentinels and then calls quicksort on the entire data

 array Figure qs2-fig suggests a sequence of calls that

 might result from an execution of the program In this execution

 the call to returns 4 so through

 hold elements less_than its chosen separator value

 while the larger elements are in through

 ex



 figurehtfb

 center

 tabularl

 'enter main()'



 ' enter readArray()'



 ' leave readArray()'



 ' enter quicksort(19)'



 ' enter partition(19)'



 ' leave partition(19)'



 ' enter quicksort(13)'



 '_'



 ' leave quicksort(13)'



 ' enter quicksort(59)'



 '_'



 ' leave quicksort(59)'



 ' leave quicksort(19)'



 'leave main()'



 tabular

 center

 Possible activations for the program of Fig qs1-fig

 qs2-fig



 figure



 In this example as is true in general procedure activations are

 nested in time If an activation of procedure_calls procedure

 then that activation of must end before the activation of

 can end There_are three common cases



 enumerate



 The activation of terminates normally Then in

 essentially any language control resumes just after the point of

 at which the call to was made



 The activation of or some procedure called either

 directly or indirectly aborts ie it becomes impossible for

 execution to continue In that case ends simultaneously with





 The activation of terminates because of an exception

 that cannot handle Procedure may handle the exception in

 which case the activation of has terminated while the

 activation of continues although not_necessarily from the

 point at which the call to was made If cannot handle the

 exception then this activation of terminates at the same time

 as the activation of and presumably the exception will be

 handled by some other open activation of a procedure



 enumerate



 We therefore can represent the activations of procedures during

 the running of an entire program by a tree called an activation tree Each node corresponds to one activation and the

 root is the activation of the main procedure that initiates

 execution of the program At a node for an activation of

 procedure the children correspond to activations of the

 procedures called by this activation of We show these

 activations in the order that they are called from left to right

 Notice_that one child must finish before the activation to its

 right can begin



 ex

 One possible activation tree that completes the sequence of calls and

 returns suggested in Fig qs2-fig is shown in

 Fig activation-tree-fig

 Functions are represented_by the first letters of their names

 Remember that this tree is only one possibility since the arguments of

 subsequent calls and also the number of calls along any branch is

 influenced by the values returned by partition

 ex



 figurehtfb





 Activation tree representing calls during an execution of

 quicksort activation-tree-fig

 figure



 The use of a run-time_stack is enabled by several useful

 relationships between the activation tree and the behavior of the

 program



 enumerate



 The sequence of procedure_calls corresponds to a preorder_traversal of

 the activation tree



 The sequence of returns corresponds to a postorder_traversal of the

 activation tree



 Suppose that control lies within a particular activation of

 some procedure corresponding to a node of the activation

 tree Then the activations that are currently open (live)

 are those that correspond to node and its ancestors The order

 in which these activations were called is the order in which they

 appear along the path to starting_at the root and they will

 return in the reverse of that order



 enumerate



 Activation Records

 activation-rec-subsect



 Procedure calls and returns are usually managed by a run-time

 stack called the control stack Each live activation has an

 activation_record (sometimes called a frame) on the control stack

 with the root of the activation tree at the bottom and the entire

 sequence of activation_records on the stack corresponding to the

 path in the activation tree to the activation where control

 currently resides The latter activation has its record at the top

 of the stack



 ex

 If control is currently in the activation of the tree of

 Fig activation-tree-fig then the activation_record for

 is at the top of the control stack Just below is the

 activation_record for the parent of in the

 tree Below that is the activation_record and at the

 bottom is the activation_record for the main function and

 root of the activation tree

 ex



 We_shall conventionally draw control stacks with the bottom of the

 stack higher_than the top so the elements in an activation_record

 that appear lowest on the page are actually closest to the top of

 the stack



 The contents of activation_records vary with the language being

 implemented Here is a list of the kinds of data that might appear

 in an activation_record (see Fig act-rec1-fig for a summary

 and possible order for these elements)



 figurehtfb



 A

 general activation_record act-rec1-fig

 figure



 enumerate



 Temporary values such_as those arising from the evaluation

 of expressions in cases where those temporaries cannot be held in

 registers



 Local data belonging to the procedure whose activation

 record this is



 A saved machine status with information_about the state of

 the machine just_before the call to the procedure This

 information typically includes the return_address (value of

 the program counter to which the called procedure must return)

 and the contents of registers that were used by the calling

 procedure and that must_be restored when the return occurs



 An access_link may be needed to locate data needed by

 the called procedure but found elsewhere eg in another

 activation_record Access links are discussed in

 Section access-link-subsect



 A control link pointing to the activation_record of

 the caller



 Space for the return value of the called function if any

 Again not all called procedures return a value and if one does

 we may prefer to place that value in a register for efficiency



 The actual_parameters used by the calling_procedure

 Commonly these values are not placed in the activation_record but

 rather in registers when possible for greater efficiency

 However we show a space for them to be completely general



 enumerate



 ex

 Figure act-rec2-fig shows snapshots of the run-time_stack as

 control_flows through the activation tree of

 Fig activation-tree-fig Dashed lines in the partial trees

 go to activations that have ended Since array is global

 space is allocated for it before execution begins_with an

 activation of procedure main as shown in

 Fig act-rec2-fig(a)



 figurehtfb



 Downward-growing

 stack of activation_records act-rec2-fig

 figure



 When control_reaches the first call in the body of main

 procedure is activated and its activation_record is pushed

 onto the stack (Fig act-rec2-fig(b)) The activation_record

 for contains space for local variable Recall that the top

 of stack is at the bottom of diagrams When control returns from

 this activation its record is popped leaving just the record for

 main on the stack



 Control then reaches the call to (quicksort) with actual

 parameters 1 and 9 and an activation_record for this call is

 placed on the top of the stack as in Fig act-rec2-fig(c)

 The activation_record for contains space for the parameters

 and and the local variable following the general

 layout in Fig act-rec1-fig Notice_that space once used by

 the call of is reused on the stack No trace of data local to

 will be available to When returns the

 stack again has only the activation_record for main



 Several activations occur between the last two snapshots in

 Fig act-rec2-fig A recursive call to was made

 Activations and have begun and ended during the

 lifetime of leaving the activation_record for

 on top (Fig act-rec2-fig(d)) Notice_that when a procedure

 is recursive it is normal to have several of its activation

 records on the stack at the same time

 ex



 Calling Sequences

 calling-seq-subsect



 Procedure calls are implemented_by what are known_as calling

 sequences which consists of code that allocates an activation

 record on the stack and enters information into its fields A return sequence is similar code to restore the state of the

 machine so the calling_procedure can continue its execution after

 the call



 Calling sequences and the layout of activation_records may differ

 greatly even among implementations of the same language The code

 in a calling_sequence is often divided between the calling

 procedure (the caller) and the procedure it calls (the

 callee) There is no exact division of run-time tasks between

 caller and callee the source_language the target_machine and

 the operating_system impose requirements that may favor one

 solution over another In_general if a procedure is called from

 different points then the portion of the calling_sequence

 assigned to the caller is generated times However the

 portion assigned to the callee is generated only once Hence it

 is desirable to put as much of the calling_sequence into the

 callee as possible - whatever the callee can be relied upon to

 know We_shall see however that the callee cannot know

 everything



 When designing calling sequences and the layout of activation

 records the following principles are helpful



 enumerate



 Values communicated between caller and callee are generally

 placed_at the beginning of the callee's activation_record so they

 are as close as possible to the caller's activation_record The

 motivation is that the caller can compute the values of the actual

 parameters of the call and place them on top of its_own activation

 record without_having to create the entire activation_record of

 the callee or even to know the layout of that record Moreover

 it allows for the use of procedures that do_not always take the

 same number or type of arguments such_as C's printf

 function The callee knows where to place the return value

 relative to its_own activation_record while however many

 arguments are present will appear sequentially below that place on

 the stack



 Fixed-length items are generally placed in the middle From

 Fig act-rec1-fig such items typically include the control

 link the access_link and the machine status fields If exactly

 the same components of the machine status are saved for each call

 then the same code can do the saving and restoring for each

 Moreover if we standardize the machine's status information then

 programs such_as debuggers will have an easier time deciphering

 the stack_contents if an error occurs



 Items whose size may not be known early enough are placed_at

 the end of the activation_record Most local_variables have a

 fixed length which can be determined by the compiler by_examining

 the type of the variable However some local_variables have a

 size that cannot be determined until the program executes the

 most_common example is a dynamically sized array where the value

 of one of the callee's parameters determines the length of the

 array Moreover the amount of space needed for temporaries

 usually depends_on how successful the code-generation phase is in

 keeping temporaries in registers Thus while the space needed for

 temporaries is eventually known to the compiler it may not be

 known when the intermediate_code is first generated



 We must locate the top-of-stack pointer judiciously A

 common approach is to have it point to the end of the fixed-length

 fields in the activation_record Fixed-length data can then be

 accessed by fixed offsets known to the intermediate-code

 generator relative to the top-of-stack pointer A consequence of

 this approach is that variable-length fields in the activation

 records are actually above the top-of-stack Their offsets

 need to be calculated at_run time but they too can be accessed

 from the top-of-stack pointer by using a positive offset



 enumerate



 figurehtfb





 Division of tasks between caller and callee

 caller-callee-fig

 figure



 An_example of how caller and callee might cooperate in managing

 the stack is suggested by Fig caller-callee-fig A register

 topsp points to the end of the machine-status field in the

 current top activation_record This position within the callee's

 activation_record is known to the caller so the caller can be

 made responsible_for setting topsp before control is

 passed to the callee The calling_sequence and its division

 between caller and callee are as_follows



 enumerate



 The caller evaluates the actual_parameters



 The caller stores a return_address and the old value of topsp into the callee's activation_record The caller then

 increments topsp to the position shown in

 Fig caller-callee-fig That is topsp is moved past

 the caller's local data and temporaries and the callee's

 parameters and status fields



 The callee saves the register values and other status information



 The callee initializes its local data and begins execution



 enumerate

 A suitable corresponding return sequence is



 enumerate



 The callee places the return value next to the parameters

 as in Fig act-rec1-fig



 Using information in the machine-status field the callee restores topsp and other registers and then branches to the return_address

 that the caller placed in the status field



 Although topsp has_been decremented the caller

 knows where the return value is relative to the current value of

 topsp the caller therefore may use that value



 enumerate



 The above calling and return sequences allow the number of

 arguments of the called procedure to vary from call to call (eg

 as in C's printf function) Note_that at_compile time the

 target code of the caller knows the number and types of arguments

 it is supplying to the callee Hence the caller knows the size of

 the parameter area The target code of the callee however must

 be prepared to handle other calls as_well so it waits until it is

 called and then examines the parameter field Using the

 organization of Fig caller-callee-fig information

 describing the parameters must_be placed next to the status field

 so the callee can find it For_example in the printf

 function of C the first argument describes the remaining

 arguments so once the first argument has_been located the callee

 can find whatever other arguments there are



 Variable-Length Data on the Stack



 The run-time memory-management system must deal frequently with

 the allocation of space for objects the sizes of which are not known at

 compile_time but which are local to a procedure and thus may be

 allocated on the stack In modern languages objects whose size

 cannot be determined at_compile time are allocated space in the

 heap the storage structure that we discuss in

 Section heap-sect However it is also possible to allocate

 objects arrays or other structures of unknown size on the stack

 and we discuss here how to do so The_reason to prefer placing

 objects on the stack if possible is that we avoid the expense of

 garbage collecting their space Note_that the stack can be used

 only for an object if it is local to a procedure and becomes

 inaccessible when the procedure returns



 A common strategy for allocating variable-length arrays (ie

 arrays whose size depends_on the value of one or_more parameters

 of the called procedure) is shown in Fig var-array-fig The

 same scheme works for objects of any type if they are local to the

 procedure called and have a size that depends_on the parameters of

 the call



 In Fig var-array-fig procedure has three local arrays

 whose sizes we suppose cannot be determined at_compile time The

 storage for these arrays is not part of the activation_record for

 although it does appear on the stack Only a pointer to the

 beginning of each array appears in the activation_record itself

 Thus when is executing these pointers are at known offsets

 from the top-of-stack pointer so the target code can access array

 elements through these pointers



 figurehtfb





 Access to dynamically allocated arrays

 var-array-fig

 figure







 Also shown in Fig var-array-fig is the activation_record

 for a procedure called by The activation_record for

 begins after the arrays of and any variable-length arrays of

 are located beyond that



 Access to the data on the stack is through two pointers top

 and topsp Here top marks the actual top of stack

 it points to the position at which the next activation_record will

 begin The second topsp is used to find local

 fixed-length fields of the top activation_record For consistency

 with Fig caller-callee-fig we_shall suppose that topsp points to the end of the machine-status field In

 Fig var-array-fig topsp points to the end of this

 field in the activation_record for From there we can find

 the control-link field for which leads us to the place in the

 activation_record for where topsp pointed when was

 on top



 The code to reposition top and topsp can be

 generated at_compile time in terms of sizes that will become

 known at_run time When returns topsp can be restored

 from the saved control link in the activation_record for The

 new value of top is (the old unrestored value of) topsp minus the length of the machine-status control and

 access_link return-value and parameter fields (as in

 Fig act-rec1-fig) in 's activation_record This length

 is known at_compile time to the callee although it may depend_on

 the caller if the number of parameters can vary across calls to





 sexer

 act-tree-exer

 Suppose that the program of Fig qs1-fig uses a partition

 function that always picks as the separator Also

 when the array is reordered assume that the order is

 preserved as much as possible That is first come all the elements

 less_than in their original order then all elements equal to

 and finally all elements greater_than in their original order



 itemize

 a)

 Draw the activation tree when the numbers are

 sorted

 b)

 What is the largest_number of activation_records that ever appear

 together on the stack

 itemize

 sexer



 exer

 Repeat_Exercise act-tree-exer when the initial order of the

 numbers is

 exer



 exer

 fib-act-exer

 In Fig fib-act-exer-fig is C code to compute Fibonacci numbers

 recursively Suppose that the activation_record for includes the

 following elements in order (return value argument local

 local ) there will normally be other elements in the activation

 record as_well The questions below assume that the initial call is





 itemize

 a) Show the complete activation tree

 b)

 What does the stack

 and its activation_records look_like

 the first time is about to return

 c)

 What does the stack

 and its activation_records look_like

 the fifth time is about to return

 itemize

 exer



 figurehtfb

 center

 tabularl

 'int f(int n) '



 '_int t s'



 ' if (n 2) return 1'



 ' s f(n-1)'



 ' t f(n-2)'



 ' return st'



 ''



 tabular

 center

 Fibonacci program for Exercise fib-act-exer

 fib-act-exer-fig

 figure



 exer

 Here is a sketch of two C functions and



 center

 tabularl

 'int f(int x) int i' 'return i1' ''



 'int g(int y) int j' 'f(j1)' ''



 tabular

 center



 That is function calls Draw the top of the

 stack starting_with the activation_record for after

 calls and is about to return You can consider only

 return values parameters control links and space for local

 variables you do_not have to consider stored state or temporary

 or local values not shown in the code sketch Answer the following

 questions



 itemize

 a) Which function creates the space on the stack for each

 element

 b) Which function writes the value of each element

 c) To which activation_record does the element belong

 itemize

 exer



 sexer

 In a language that passes parameters by reference there is a function

 that does the following



 center

 'x x 1 y y 2 return xy'

 center



 If is assigned the value 3 and then is

 called what is returned

 sexer



 exer

 The C function is defined by



 center

 tabularl

 'int f(int x py ppz) '



 ' ppz 1 py 2 x 3 return xpyppz'



 ''

 tabular

 center



 Variable is a pointer to variable is a

 pointer to and is an_integer currently with value 4 If

 we call what is returned

 exer

 Addresses in the Target Code

 rt-storage-sect



 In this_section we show_how names in the IR can be

 converted into addresses in the target code

 by_looking at code_generation for simple procedure_calls

 and returns using static and stack allocation

 In Section stor-org-sect we described how each

 executing program_runs in its_own logical address space

 that was partitioned_into four code and data areas



 enumerate



 A statically_determined area Code that holds the

 executable target code The size of the target code can be determined

 at_compile time



 A statically_determined data area Static for holding global

 constants and other data generated_by the compiler The size of

 the global constants and compiler data can also be determined at_compile time



 A dynamically managed area Heap for holding data

 objects that are allocated and freed during program execution

 The size of the Heap cannot be determined at_compile time



 A dynamically managed area Stack for holding activation

 records as they are created and destroyed during procedure_calls

 and returns Like the Heap the size of the Stack

 cannot be determined at_compile time



 enumerate



 Static Allocation



 To illustrate code_generation for simplified procedure_calls and

 returns we_shall focus_on the following three-address_statements



 itemize



 call callee



 return



 halt



 action which is a placeholder for other three-address_statements



 itemize



 The size and layout of activation_records are determined by the

 code_generator via the information_about names stored in the

 symbol_table We_shall first illustrate_how to store the return

 address in an activation_record on a procedure call and how to

 return control to it after the procedure call For_convenience we

 assume the first location in the activation_record holds the return

 address



 Let_us first consider the code needed to implement the simplest

 case static allocation Here a call callee statement

 in the intermediate_code can be_implemented by a sequence of two

 target-machine_instructions



 flushleft

 tabularl

 ' ST 'calleestaticArea' '



 ' BR 'calleecodeArea

 tabular

 flushleft



 The ST instruction saves the return_address at the beginning

 of the activation_record for callee and the BR

 transfers_control to the target code for the called procedure callee The attribute calleestaticArea is a

 constant that gives the address of the beginning of the activation

 record for callee and the attribute calleecodeArea

 is a constant referring to the address of the first instruction of

 the called procedure callee in the Code area

 of the run-time memory



 The operand '' in the ST

 instruction is the literal return_address it is the address of

 the instruction following the BR instruction We assume that

 ''here is the address of the current instruction and

 that the three constants plus the two instructions in the calling

 sequence have a length of 5 words or 20 bytes



 The code for a procedure ends with a return to the calling

 procedure except that the first procedure has no caller so its

 final instruction is HALT which returns control to the

 operating_system A return statement can be

 implemented_by a simple jump instruction



 flushleft

 ' BR 'calleestaticArea

 flushleft

 which transfers_control to the address saved at the beginning of

 the activation_record for callee



 ex

 proc-code-ex

 Suppose we have the following three-address_code



 flushleft

 tabularp125in l

 '_'code for c



 '_'



 ' call p'



 '_'



 ' halt'



 '_'code for p



 '_'



 ' return'



 tabular

 flushleft



 Figure target-code-fig shows the target program for this

 three-address_code We use the pseudoinstruction ACTION to

 represent the sequence of machine_instructions to execute the

 statement action which represents three-address_code that

 is not relevant for this discussion We arbitrarily start the code

 for procedure c at address 100 and for procedure p at

 address 200 We assume that each ACTION instruction takes 20

 bytes We further assume that the activation_records for these

 procedures are statically allocated starting_at locations 300 and

 364 respectively



 The instructions starting_at address 100 implement the statements



 flushleft

 ' '' call p '' halt'



 flushleft

 of the first procedure c Execution therefore starts with

 the instruction at address 100 The ST

 instruction at address 120 saves the return_address 140 in the

 machine-status field which is the first word in the activation

 record of p The BR instruction at address 132

 transfers_control the first instruction in the target code of the

 called procedure p



 figurehtfb

 center

 tabular r_l l

 '_'code for



 100 '_'code for



 120 ST 364 140 ' 'save return_address 140 in location 364



 132 BR 200 '_'call



 140



 160 HALT '_'return to operating_system







 '_'code for



 200



 220 BR 364 '_'return to address saved in location 364







 ' '300-363 hold activation_record for



 300 '_'return address



 304 ' 'local data for







 ' '364-451 hold activation_record for



 364 '_'return address



 368 ' 'local data for



 tabular

 center



 Target code for static allocation

 target-code-fig

 figure



 After executing the jump instruction at

 location 220 is executed Since location 140 was saved at address

 364 by the call sequence above 364 represents 140 when the

 BR statement at address 220 is executed Therefore when

 procedure p terminates control returns to address 140 and

 execution of procedure c resumes

 ex





 Stack Allocation



 Static allocation can become stack allocation by using relative

 addresses for storage in activation_records In stack

 allocation however the position of an activation_record for a procedure is

 not known until run_time This position is usually stored in a

 register so words in the activation_record can be accessed as

 offsets from the value in this register The indexed address mode

 of our target_machine is convenient for this purpose



 Relative addresses in an activation_record can be taken as offsets

 from any known position in the activation_record as we saw in

 Chapter_run-time-ch For_convenience we_shall use positive

 offsets by maintaining in a register SP a pointer to the

 beginning of the activation_record on top of the stack When a

 procedure call occurs the calling_procedure increments SP

 and transfers_control to the called procedure After control

 returns to the caller we decrement SP thereby deallocating

 the activation_record of the called procedure



 The code for the first procedure initializes the stack by

 setting SP to the start of the stack area in memory



 flushleft

 tabularp25in l

 ' LD SP 'stackStart ' 'initialize the stack



 '_'code for the first procedure



 ' HALT' ' 'terminate execution



 tabular

 flushleft



 A procedure call sequence increments SP saves the

 return_address and transfers_control to the called procedure



 flushleft

 tabularp25in l

 ' ADD SP_SP 'callerrecordSize ' 'increment stack pointer



 ' ST 0(SP) '_' 'save return_address



 ' BR 'calleecodeArea ' 'jump to the callee



 tabular

 flushleft



 The operand ''callerrecordSize represents the size of

 an activation_record so the ADD instruction makes SP

 point to the next activation_record The operand

 '' in the ST instruction is the

 address of the instruction following BR it is saved in the

 address pointed to by SP



 The return sequence consists of two_parts The called procedure

 transfers_control to the return_address using



 flushleft

 tabularp25in l

 ' BR 0(SP)' '_'return to caller

 tabular

 flushleft





 The_reason for using 0(SP) in the BR instruction is

 that we need two levels of indirection 0(SP) is the address

 of the first word in the activation_record and 0(SP) is the

 return_address saved there



 The second part of the return sequence is in the caller which

 decrements SP thereby restoring SP to its previous

 value That is after the subtraction SP points to the

 beginning of the activation_record of the caller



 flushleft

 tabularp25in l

 ' SUB SP_SP 'callerrecordSize ' 'decrement stack pointer



 tabular

 flushleft



 Chapter_run-time-ch contains a broader discussion of calling

 sequences and the tradeoffs in the division of labor between the

 calling and called procedures



 ex

 stack-alloc-ex

 The program in Fig stack-alloc-ex-fig

 is an abstraction of the quicksort

 program in the previous chapter

 Procedure is recursive so more_than one activation of

 can be alive at the same time



 figurehtfb



 center

 tabularp125in l

 '_'code for m



 '_'



 ' call q'



 '_'



 ' halt'



 '_'code for p



 '_'



 ' return'



 '_'code for q



 '_'



 ' call p'



 '_'



 ' call q'



 '_'



 ' call q'



 ' return'



 tabular

 center



 Code for Example stack-alloc-ex

 stack-alloc-ex-fig



 figure



 Suppose that the sizes of the activation_records for procedures

 m p and q have_been determined to be msize psize and qsize respectively The first word

 in each activation_record will hold a return_address We

 arbitrarily assume that the code for these procedures starts at

 addresses 100 200 and 300 respectively and that the stack

 starts at address 600 The target program is shown in

 Figure stack-code-fig



 figure

 center

 tabular r_l l

 '_'code for m



 100 LD SP 600 ' 'initialize the stack



 108 '_'code for



 128 ADD SP_SP msize '_'call sequence begins



 136 ST 0(SP) 152 ' 'push return_address



 144 BR 300 '_'call q



 152 SUB SP_SP msize ' 'restore SP



 160



 180 HALT







 '_'code for p



 200



 220 BR 0(SP) '_'return







 '_'code for q



 300 ' 'contains a conditional_jump to 456



 320 ADD SP_SP qsize



 328 ST 0(SP) 344 ' 'push return_address



 336 BR 200 '_'call p



 344 SUB SP_SP qsize



 352



 372 ADD SP_SP qsize



 380 ST 0(SP) 396 ' 'push return_address



 388 BR 300 '_'call q



 396 SUB SP_SP qsize



 404



 424 ADD SP_SP qsize



 432 ST 0(SP) 440 ' 'push return_address



 440 BR 300 '_'call q



 448 SUB SP_SP qsize



 456 BR 0(SP) '_'return







 600 ' 'stack starts here



 tabular

 center



 Target code for stack allocation

 stack-code-fig

 figure



 We assume that contains a conditional_jump to

 the address 456 of the return sequence from q otherwise

 the recursive_procedure q is condemned to call itself

 forever



 Let msize psize and qsize be 20 40 and 60

 respectively The first instruction at address 100 initializes the

 SP to 600 the starting address of the stack SP holds

 620 just_before control transfers from m to q because

 msize is 20 Subsequently when q calls p the

 instruction at address 320 increments SP to 680 where the

 activation_record for p begins SP reverts to 620

 after control returns to q If the next two recursive_calls

 of q return immediately the maximum value of SP

 during this execution is 680 Note_however that the last stack

 location used is 739 since the activation_record of q

 starting_at location 680 extends for 60 bytes

 ex





 Run-Time Addresses for Names



 The storage-allocation strategy and the layout of local data in an

 activation_record for a procedure determine how the storage for

 names is accessed In Chapter_inter-ch we assumed that a

 name in a three-address statement is really a pointer to a

 symbol-table_entry for that name This_approach has a significant

 advantage it makes the compiler more portable since the front

 end need not be changed even when the compiler is moved to a

 different machine where a different run-time organization is

 needed On the other_hand generating the specific sequence of

 access steps while generating intermediate_code can be of

 significant advantage in an optimizing_compiler since it lets the

 optimizer take_advantage of details it would not see in the simple

 three-address statement



 In either case names must eventually be replaced_by code to

 access storage locations We thus consider some elaborations of

 the simple three-address copy_statement x 0 After the

 declarations in a procedure are processed suppose the

 symbol-table_entry for x contains a relative_address 12 for

 x For_example consider the case in which x is in a statically

 allocated area beginning at address static Then the actual

 run-time address of x is Although the

 compiler can eventually determine the value of

 at_compile time the position of the static area may not be

 known when intermediate_code to access the name is generated In

 that case it makes_sense to generate three-address_code to

 compute with the understanding that this

 computation will be carried_out during the code_generation phase

 or possibly by the loader before the program_runs The assignment

 x 0 then translates_into



 verbatim

 static12 0

 verbatim

 If the static area starts at address 100 the target code for this

 statement is



 verbatim

 LD 112 0

 verbatim



 exer

 Generate_code for the following three-address_statements assuming

 stack allocation where register SP points to the top of the

 stack



 verbatim

 call p

 call q

 return

 call r

 return

 return

 verbatim

 exer



 exer

 Generate_code for the following three-address_statements assuming

 stack allocation where register SP points to the top of the

 stack



 itemize



 a) x 1

 b) x a

 c) x a 1

 d) x a b

 e) The two statements



 verbatim

 x b_c

 y a x

 verbatim

 itemize

 exer

 exer

 Generate_code for the following three-address_statements again

 assuming stack allocation and assuming a and b are

 arrays whose elements are 4-byte values



 itemize



 a) The four-statement sequence



 verbatim

 x_ai

 y bj

 ai_y

 bj x

 verbatim



 b) The three-statement sequence



 verbatim

 x_ai

 y bi

 z x_y

 verbatim



 c) The three-statement sequence



 verbatim

 x_ai

 y bx

 ai_y

 verbatim

 itemize

 exer

 Scope Rules

 scope-sect



 The scope of a declaration of is the context in which

 uses of refer to this declaration A language uses static

 scope or lexical scope if it is possible to determine the

 scope of a declaration by_looking only at the program Otherwise

 the language uses dynamic_scope With dynamic_scope as the

 program_runs the same use of could refer to any of several

 declarations of



 Most_languages such_as C and its family use static_scope



 Names Identifiers and Variables Although the

 terms name and variable often refer to the same_thing we

 use them carefully to distinguish_between compile-time names and

 the run-time locations denoted_by names



 Usually a name is simply an_identifier An identifier is a

 string of characters typically letters or digits that refers to

 (identifies) an entity such_as a data object a function a

 class or a type Identifiers correspond to tokens in our

 discussions of lexical analysis and parsing



 All identifiers are names but not all names are identifiers For

 example a name lexscan might denote the method scan

 belonging to an object lex Here lex and scan

 are identifiers while lexscan is a name but not an

 identifier Composite names like lexscan are called

 qualified names



 A variable refers to a particular location of the store It

 is common for the same identifier to be declared more_than once

 each such declaration introduces a new variable Even_if each

 identifier is declared just once an_identifier local to a

 recursive_procedure will refer to different locations of the store

 at different times as we_shall see



 Environments and States

 two-stage-subsect



 Clearly the execution of an assignment such_as changes

 the value denoted_by the name Specifically the assignment

 changes the value in the location denoted_by It may be less

 clear that the location denoted_by can change at_run time see

 Section dyn-subsect on dynamic_scope or

 Section rec-subsect on recursive_procedure activations



 The association of names with locations in memory and then with

 values can be described by two mappings that change as the program

 runs (see Fig two-stage-fig)



 figurehtfb



 Two-stage mapping from names to values

 two-stage-fig

 figure



 enumerate



 The environment is a mapping from names to locations

 in store Since variables refer to locations (l-values)

 we could alternatively define an environment as a mapping from

 names to variables



 The state is a mapping from locations in store to

 their values That is the state maps l-values to their

 corresponding r-values



 enumerate



 Environments change according to the scope rules of a language



 ex

 Consider the sketch of a C program in Fig env-change-fig

 Integer is declared a global variable and also declared as a

 variable local to function When is executing the

 environment adjusts so that name refers to the location

 reserved for the that is local to and any use of

 such_as the assignment i 3 shown explicitly refers to

 that location Typically the local is given a place on the

 run-time_stack



 figurehtfb

 center

 tabularl_l l





 'int i' '' global ''







 'void f('') '



 'int i' '' local ''







 'i 3' '' use of local ''







 ''







 'x i1' '' use of global ''



 tabular

 center

 Two declarations of the name

 env-change-fig

 figure



 Whenever a function other_than is executing uses of

 cannot refer to the that is local to Uses of name in

 must_be within the scope of some other declaration of An

 example is the explicitly shown statement x i1 which is

 inside some procedure whose definition is not shown The in

 i1 presumably refers to the global



 In most languages declarations precede use so a function that

 comes before the global cannot refer to it

 ex



 Procedures Functions and Methods To_avoid

 saying procedures functions or methods each time we_want to

 talk_about a subprogram that may be called we_shall usually refer

 to all of them as procedures The exception is that when

 talking explicitly of programs in languages_like C that have only

 functions we_shall refer to them as functions



 A function generally returns a value of some type (the return

 type) while a procedure does_not return any value C and

 similar languages which have only functions treat procedures as

 functions that have a special return type void to signify no

 return value Object-oriented languages use methods which can be

 either functions or procedures but are associated_with a

 particular class



 The environment and the state mappings in Fig two-stage-fig

 are dynamic but there are a few exceptions



 enumerate

 Static versus dynamic binding (of names to locations)

 Most binding of names to locations is dynamic and we discuss

 several approaches to this binding throughout the chapter Some

 declarations such_as the global in Fig env-change-fig

 can be given a location in the store once and for all as the

 compiler generates object codefootnoteTechnically the C

 compiler will assign a location in virtual_memory for the global

 leaving it to the loader and the operating_system to

 determine where in the physical_memory of the machine will be

 located However we will not worry_about relocation issues

 such_as these which have no impact on compiling Instead we

 treat the address space that the compiler uses for its output code

 as if it gave real locations in the storefootnote



 Static versus dynamic binding (of names to values)

 The binding of locations to values (the second stage in

 Fig two-stage-fig) is generally dynamic as_well since we

 cannot_tell the value in a location until we run the program

 Declared constants are an exception For_instance the C

 definition

 verbatim

 define ARRAYSIZE 1000

 verbatim

 binds the name ARRAYSIZE to the value 1000 statically We

 can determine this binding by_looking at the statement and we

 know that it is impossible for this binding to change when the

 program executes



 enumerate



 Static Scope Block Structure

 static-scope-subsect



 Most_languages including C and its family use static_scope The

 scope rules for C are based_on program structure the scope of a

 declaration is determined implicitly by where the declaration

 appears in the program Later languages (C Java C) also

 provide explicit control over scopes through the use of keywords

 like public private and protected



 In this_section we consider static-scope rules for a language with

 blocks where a block is a grouping of declarations and

 statements C uses braces and to

 delimit a block the tradition of using begin and end

 dates_back to Algol



 ex

 To a first approximation the C static-scope policy is as_follows



 enumerate



 A C program consists of a sequence of top-level declarations

 of variables and functions



 Functions may have variable declarations within them where

 variables include local_variables and parameters The scope of

 each such declaration is restricted to the function in which it

 appears



 The scope of a top-level declaration of a name consists

 of the entire program that follows with the exception of

 statements within a function that also has a declaration of



 enumerate



 The additional detail regarding the C static-scope policy deals

 with variable declarations within statements We examine such

 declarations next and in Example block-structure-ex

 ex



 In C the syntax of blocks is given by



 center

 tabularr_c l

 statement block



 block declarations

 statements

 tabular

 center



 Note_that this syntax allows blocks to be nested inside each

 other This nesting property is referred to as block

 structure The C family of languages has block structure except

 that a function may not be defined inside another function This

 restriction on function definitions has significant implications

 for run-time storage management which we discuss in

 Section st-nonest-subsect



 We_say that a declaration belongs to a block if

 is the most_closely nested block containing that is is

 located within but not within any block that is nested_within





 The static-scope rule for variable declarations in a

 block-structured languages is as_follows If declaration of

 name belongs to block then the scope of is all of

 except for any blocks nested to some depth within

 in which is redeclared Here is redeclared in if

 some other declaration of the same name belongs to



 An_equivalent way to express this rule is to focus_on a use of a

 name Let be all the blocks that surround

 this use of with the smallest nested_within

 which is nested_within and so on Search for the

 smallest such that there is a declaration of belonging to

 This use of refers to the declaration in

 Alternatively this use of is within the scope of the declaration in



 figurehtfb

 Blocks

 in a C program block-structure-fig

 figure



 ex

 block-structure-ex The C program in

 Fig block-structure-fig has four blocks with several

 definitions of variables and As an memory aid each

 declaration initializes its variable to the number of the block to

 which it belongs



 For_instance consider the declaration int a 1 in block

 It's scope is all of except for those blocks nested

 (perhaps deeply) within that have their own declaration of

 nested immediately_within does_not have a

 declaration of but does does_not have a

 declaration of so block is the only place in the entire

 program that is outside the scope of the declaration of the name

 that belongs to That is this scope includes and

 all of except for the part of that is within

 The scopes of all five declarations are summarized in

 Fig bs-scopes-fig



 figurehtfb



 center

 tabularll



 Declaration Scope





 int a 1



 int b 1



 int b 2



 int a 3



 int b 4





 tabular

 center



 Scopes of declarations in

 Example block-structure-ex bs-scopes-fig



 figure



 From another point of view let_us consider the output statement

 in block and bind the variables and used there to

 the proper declarations The list of surrounding blocks in order

 of size is Note_that does_not surround the

 point in question has a declaration of so it is to

 this declaration that this use of refers and the value of

 printed is 4 However does_not have a declaration of

 so we next look_at That block does_not have a declaration

 of either so we proceed to Fortunately there is a

 declaration int a 1 belonging to that block so the value

 of printed is 1 Had there been no such declaration the

 program would have_been erroneous

 ex



 Explicit Access Control



 Classes and records introduce a new scope for their members If

 p is an object of a class with a field x then the use

 of x in px refers to field x in the class

 definition In Section record-types we used a separate

 symbol_table to hold information_about field names in a record

 The idea of using a separate symbol_table extends to classes



 In analogy with block structure the scope of a public member

 declaration in a class extends to any subclass

 except if has a local declaration of the same name



 Through the use of keywords_like public private and

 protected object-oriented languages such_as C provide

 explicit control over access to member names in a superclass

 These keywords support encapsulation by restricting access

 Thus private names are purposely given a scope that includes only

 the method declarations and definitions associated_with that class

 and any friend classes Protected names are accessible to

 subclasses Public names are accessible from outside the class



 In C a class definition may be separated from the definitions

 of some or all of its methods Therefore a name associated

 with the class may have a region of the code that is outside

 its scope followed_by another region (a method definition) that

 is within its scope In_fact regions inside and outside the scope

 may alternate until all the methods have_been defined



 Declarations and Definitions The apparently

 similar terms declaration and definition for

 programming-language concepts are actually quite different

 Declarations tell_us about the types of things while definitions

 tell_us about their values Thus int i is a declaration of

 while i 1 is a definition of



 The difference is more significant when we deal_with methods or

 other procedures In C a method is declared in a class

 definition by giving the types of the arguments and result of the

 method (often called the signature for the method The

 method is then defined ie the code for executing the method is

 given in another place Similarly it is common to define a

 procedure in one file and declare it in other files where the

 procedure is used



 Dynamic Scope

 dyn-subsect



 Technically any scoping policy is dynamic if it is based_on

 factor(s) that can be known only when the program executes The

 term dynamic_scope however usually refers to the following

 policy a use of a name refers to the declaration of in

 the most_recently called function with such a declaration In this

 sense dynamic_scope is found only in special situations in

 programming_languages such_as during macro expansion



 We_now consider two examples of dynamic policies macro expansion

 in the C preprocessor and method resolution in object-oriented

 programming



 ex

 In the simple C program in Fig dyn-scope-fig identifier

 is a macro that stands_for expression But what is

 We cannot resolve statically that is in terms of the

 program text



 figurehtfb

 center

 tabularl

 define a (x1)



 int x 2



 void b() int x 1 printf(

 void c() printf(

 void_main() b() c()

 tabular

 center

 A macro whose names must_be scoped dynamically

 dyn-scope-fig

 figure





 In_fact in order to interpret we must use the usual

 dynamic-scope rule We examine all the function calls that are

 currently active and we take the most_recently called function

 that has a declaration of It is to this declaration that the

 use of refers



 In the example of Fig dyn-scope-fig the function

 main first calls function As executes it prints the

 value of the macro Since must_be substituted for

 we resolve this use of to the declaration int x1 in

 function The_reason is that has a declaration of so

 the in the printf in refers to this Thus

 the value printed is 1



 After finishes and is called we again need to print the

 value of macro However the only accessible to is the

 global The print statement in thus refers to this

 declaration of and value 2 is printed

 ex



 Analogy Between Static and Dynamic Scoping While

 there could be any number of static or dynamic policies for

 scoping there is an interesting relationship_between the normal

 (block-structured) static_scoping rule and the normal dynamic

 policy In a sense the dynamic rule is to time as the static rule

 is to space While the static rule asks us to find the declaration

 whose unit (block) most_closely surrounds the physical location of

 the use the dynamic rule asks us to find the declaration whose

 unit (function invocation) most_closely surrounds the time of the

 use



 Dynamic scope resolution is also essential for polymorphic

 procedures those that have two or_more definitions for the same

 name depending only on the types of the arguments In some

 languages such_as ML (see_Section ml-intro-subsect) it is

 possible to statically determine types for all uses of names in

 which case the compiler can replace each use of a procedure name

 by a reference to the code for the proper procedure However

 in other languages such_as Java and C there are times when the

 compiler cannot make that determination



 ex

 A distinguishing feature of object-oriented_programming is the

 ability of each object to invoke the appropriate method in

 response to a message In other_words the procedure called when

 is executed depends_on the class of the object denoted_by

 at that time A_typical example is as_follows



 enumerate



 There is a class with a method_named



 is a subclass of and has its_own method_named





 There is a use of of the form where is an

 object of class



 enumerate

 Normally it is impossible to tell at_compile time whether

 will be of class or of the subclass If the method

 application occurs several_times it is highly likely that some

 will be on objects denoted_by that are in class but not

 while others will be in class It is not until run-time

 that it can be decided which definition of is the right one

 Thus the code generated_by the compiler must determine the class

 of the object and call one or the other procedure named

 ex







 sexer

 bs1-exer For the block-structured C code of

 Fig bs-exer-fig(a) indicate the values assigned to

 and

 sexer



 figurehtfb



 Block-structured code bs-exer-fig

 figure



 exer

 bs2-exer Repeat_Exercise bs1-exer for the code of

 Fig bs-exer-fig(b)

 exer



 exer

 scope-exer For the block-structured code of

 Fig scope-exer-fig assuming the usual static_scoping of

 declaratons give the scope for each of the twelve declarations

 exer



 figurehtfb

 center

 tabularl

 '_int w x_y z_Block B1 '



 '_int x z_Block B2 '



 '_int w x Block B3 '



 '_'



 '_int w x Block B4 '



 '_int y_z Block B5 '



 '_'



 ''



 tabular

 center

 Block structured code for Exercise scope-exer

 scope-exer-fig



 figure



 exer

 What is printed by the following C code



 verbatim

 define a (x1)

 int x 2

 void b() x a printf(

 void c() int x 1 printf(

 void_main() b() c()

 verbatim

 exer

 Syntax-Directed Definitions

 sdd-sect



 A syntax-directed_definition (SDD) is a context-free

 grammar together_with attributes and rules Attributes are

 associated_with grammar_symbols and rules are associated_with

 productions If is a symbol and is one of its attributes

 then we write to denote the value of at a particular

 parse-tree_node labeled If we implement the nodes of the

 parse_tree by records or objects then the attributes of can

 be_implemented by data fields in the records that represent the

 nodes for Attributes may be of any kind numbers types

 table references or strings for instance The strings may even

 be long sequences of code say code in the intermediate language used

 by a compiler



 Inherited and Synthesized Attributes

 inher-synth-subsect



 We_shall deal_with two kinds of attributes for nonterminals



 enumerate



 A synthesized_attribute for a nonterminal at a

 parse-tree_node is defined by a semantic_rule associated_with

 the production at Note_that the production must have as

 its head A synthesized_attribute at node is defined only in

 terms of attribute values at the children of and at itself



 An inherited_attribute for a nonterminal at a

 parse-tree_node is defined by a semantic_rule associated_with

 the production at the parent of Note_that the production

 must have as a symbol in its body An inherited_attribute

 at node is defined only in terms of attribute

 values at 's parent itself and 's siblings



 enumerate

 While we do_not allow an inherited_attribute at node to be

 defined in terms of attribute values at the children of node

 we do allow a synthesized_attribute at node to be defined in

 terms of inherited_attribute values at node itself



 Terminals can have synthesized_attributes but not inherited

 attributes Attributes for terminals have lexical values that are

 supplied_by the lexical_analyzer there are no semantic_rules in

 the SDD itself for computing the value of an attribute for a terminal



























 An Alternative Definition of Inherited Attributes

 No additional translations are enabled if we allow an inherited

 attribute at a node to be defined in terms of attribute

 values at the children of as_well as at itself at its

 parent and at its siblings Such rules can be simulated by

 creating additional attributes of say

 These are synthesized_attributes that copy the needed attributes

 of the children of the node_labeled We then compute as

 an inherited_attribute using the attributes

 in place of attributes at the children Such attributes are rarely

 needed in practice



 ex

 sdd-desk-calc-ex The SDD in

 Fig_sdd-desk-calc-fig is based_on our familiar grammar for

 arithmetic_expressions with operators and It evaluates

 expressions terminated by an endmarker n



 In the SDD each of the nonterminals has a single synthesized_attribute

 called We also suppose that the terminal digit has

 a synthesized_attribute lexval which is an_integer value returned by

 the lexical_analyzer



 figurehtfb



 center

 tabularl_ll

 Production_Semantic Rules



 1) n



 2)



 3)



 4)



 5)



 6)



 7)



 tabular

 center



 Syntax-directed definition of a simple desk_calculator

 sdd-desk-calc-fig



 figure



 The rule for production 1 sets

 to which we_shall see is the numerical value of the

 entire expression



 Production 2 also has one rule which

 computes the attribute for the head as the sum of the

 values at and At any parse-tree_node labeled

 the value of for is the sum of the values of at

 the children of node_labeled and



 Production 3 has a single rule that defines the

 value of for to be the same as the value of at

 the child for Production 4 is similar to the second

 production its rule multiplies the values at the children instead

 of adding them The rules for productions 5 and 6 copy values at a

 child like that for the third production Production 7 gives

 the value of a digit that is the numerical value of the

 token digit that the lexical_analyzer returned

 ex



 An SDD that involves only synthesized_attributes is called S-attributed the SDD in Fig_sdd-desk-calc-fig has this

 property In an S-attributed SDD each rule computes an

 attribute for the nonterminal at the head of a production from

 attributes taken from the body of the production



 For_simplicity the examples in this_section have semantic_rules

 without side_effects In_practice it is convenient to allow

 SDD's to have limited side_effects such_as printing the result

 computed by a desk_calculator or interacting with a symbol_table

 Once the order of evaluation of attributes is discussed in

 Section eval-sdd-sect we_shall allow semantic_rules to

 compute arbitrary functions possibly involving side_effects



 An S-attributed SDD can be_implemented naturally in conjunction

 with an LR_parser In_fact the SDD in

 Fig_sdd-desk-calc-fig mirrors the Yacc program of

 Fig yacc-calc-fig which illustrates translation during LR

 parsing The difference is that in the rule for production 1 the

 Yacc program prints the value as a side_effect instead

 of defining the attribute



 An SDD without side_effects is sometimes_called an attribute grammar The rules in an attribute grammar define the

 value of an attribute purely in terms of the values of other

 attributes and constants



 Evaluating an SDD at the Nodes of a Parse Tree

 ann-pt-subsect



 To visualize the translation specified_by an SDD it

 helps to work with parse_trees even_though a translator need not

 actually build a parse_tree Imagine therefore that the rules of

 an SDD are applied by first constructing a parse_tree and then

 using the rules to evaluate all of the attributes at each of the

 nodes of the parse_tree A parse_tree showing the value(s) of

 its attribute(s) is called an annotated_parse tree



 How do we construct an annotated_parse tree In what order do we

 evaluate attributes Before we can evaluate an attribute at a node

 of a parse_tree we must evaluate all the attributes upon which

 its value depends For_example if all attributes are synthesized

 as in Example sdd-desk-calc-ex then we must evaluate

 the attributes at all of the children of a node before we can

 evaluate the attribute at the node itself



 With synthesized_attributes we can evaluate attributes in any

 bottom-up order such_as that of a postorder_traversal of the

 parse_tree the evaluation of S-attributed definitions is

 discussed in Section s-attrib-subsect



 For SDD's with both inherited and synthesized_attributes there

 is no guarantee that there is even one order in which to

 evaluate attributes at nodes For_instance consider nonterminals

 and with synthesized and inherited_attributes and

 respectively along with the production and rules



 center

 tabularc l

 Production_Semantic Rules









 tabular

 center

 These rules are circular it is impossible to evaluate either

 at a node or at the child of without first

 evaluating the other The circular dependency of and

 at some pair of nodes in a parse_tree is suggested by

 Fig attr-cycle-fig



 figurehtfb

 The

 circular dependency of and on one another

 attr-cycle-fig

 figure



 It is computationally difficult to determine_whether or not there exist

 any circularities in any of the parse_trees that a given SDD

 could have to translatefootnoteWithout going into

 details while the problem is decidable it cannot be_solved by a

 polynomial-time algorithm even if since it

 has exponential time complexityfootnote Fortunately there

 are useful subclasses of SDD's that are sufficient to guarantee that

 an order of evaluation exists as we_shall see in

 Section eval-sdd-sect



 ex

 ann-calc-ex Figure ann-calc-fig shows an annotated

 parse_tree for the input_string constructed

 using the grammar and rules of Fig_sdd-desk-calc-fig The

 values of lexval are presumed supplied_by the lexical

 analyzer Each of the nodes for the nonterminals has attribute

 computed in a bottom-up order and we see the resulting

 values associated_with each node For_instance at the node with a

 child_labeled after computing and at

 its first and third children we apply the rule that says

 is the product of these two values or 15

 ex



 figurehtfb



 Annotated parse_tree for

 ann-calc-fig

 figure



 Inherited_attributes are useful when the structure of a parse_tree

 does_not match the abstract_syntax of the source code The

 next example shows_how inherited_attributes can be used to

 overcome such a mismatch due to a grammar designed for parsing

 rather_than translation



 exsdd-ll-term-ex

 The SDD in Fig sdd-ll-term-fig computes terms like

 and The top-down parse of input begins_with the

 production Here generates the digit

 but the operator is generated_by Thus the left_operand

 appears in a different subtree of the parse_tree from An

 inherited_attribute will therefore be used to pass the operand to

 the operator



 The grammar in this example is an excerpt from a

 non-left-recursive version of the familiar expression grammar we

 used such a grammar as a running_example to illustrate top-down

 parsing in Section top-down-sect



 figurehtfb

 center

 tabularl_ll

 Production_Semantic Rules



 15pt1)







 15pt2)







 15pt3)



 15pt4)



 tabular

 center

 An SDD based_on a grammar suitable for top-down

 parsing sdd-ll-term-fig

 figure



 Each of the nonterminals and has a synthesized_attribute

 the terminal digit has a synthesized_attribute

 lexval The nonterminal has two attributes an

 inherited_attribute inh and a synthesized_attribute syn



 The semantic_rules are based_on the idea that the left_operand of

 the operator is inherited More_precisely the head of

 the production inherits the left_operand of

 in the production_body Given a term the root of the

 subtree for inherits Then the root of the subtree

 for inherits the value of and so on if there are

 more factors in the term Once all the factors have_been

 accumulated the result is passed back up the tree using

 synthesized_attributes



 To_see how the semantic_rules are used consider the annotated

 parse_tree for in Fig ann-ll-term-fig The leftmost

 leaf in the parse_tree labeled digit has attribute value

 where the is supplied_by the lexical

 analyzer Its parent is for production 4

 The only semantic_rule associated_with this production defines

 which equals



 figurehtfb



 Annotated parse_tree for ann-ll-term-fig

 figure



 At the second child of the root the inherited_attribute

 is defined by the semantic_rule

 associated_with production 1

 Thus the left_operand 3 for the operator is passed from left to

 right across the children of the root



 The

 production at the node for is (We

 retain the subscript in the annotated_parse tree to

 distinguish_between the two nodes for ) The inherited

 attribute is defined by the semantic_rule

 associated_with

 production 2



 With and we get

 At the lower node for the production is

 The semantic_rule

 defines The

 syn attributes at the nodes for pass the value up

 the tree to the node for where

 ex





 exer

 sdd-desk-calc1-exer For the SDD of

 Fig_sdd-desk-calc-fig give annotated_parse trees for the

 following expressions



 itemize



 a)



 b)



 c)



 itemize

 exer





 exer

 sdd-desk-calc2-exer Extend the SDD of

 Fig sdd-ll-term-fig to handle expressions as in

 Fig_sdd-desk-calc-fig

 exer



 exer

 sdd-desk-calc3-exer Repeat

 Exercise sdd-desk-calc1-exer using your SDD from

 Exercise sdd-desk-calc2-exer

 exer

 Syntax-Directed Translation Schemes

 sdt-sect



 Syntax-directed_translation schemes

 are a complementary notation to syntax-directed_definitions

 All of the applications of syntax-directed_definitions in

 Section apps-sdd can be_implemented using

 syntax-directed_translation schemes



 From Section sdt-intro-subsect a syntax-directed

 translation_scheme (SDT) is a context-free_grammar with program

 fragments embedded within production_bodies The program_fragments

 are called semantic_actions and can appear at any position

 within a production_body By convention we place curly_braces

 around actions if braces are needed as grammar_symbols then we

 quote them



 Any SDT can be_implemented by first building a parse_tree and

 then performing the actions in a left-to-right depth-first_order

 that is during a preorder_traversal An_example appears in

 Section sdt-inside-subsect



 Typically SDT's are implemented_during

 parsing without building a parse_tree In this_section we focus

 on the use of SDT's to implement two important classes of

 SDD's



 enumerate



 The underlying_grammar is LR-parsable and the SDD is

 S-attributed



 The underlying_grammar is LL-parsable and the SDD is

 L-attributed



 enumerate

 We_shall see_how in both these cases the semantic_rules in an

 SDD can be converted into an SDT with actions that are

 executed at the right time During parsing an action in a

 production_body is executed as_soon as all the grammar_symbols to

 the left of the action have_been matched



 's that can be_implemented during_parsing can be

 characterized by introducing distinct marker_nonterminals in

 place of each embedded action each marker has only one

 production If the grammar with marker

 nonterminals can be_parsed by a given

 method then the SDT can be_implemented

 during_parsing



 Postfix Translation Schemes

 bus-subsect



 By far the simplest SDD implementation occurs_when we can parse

 the grammar bottom-up and the SDD is S-attributed In that

 case we can construct an SDT in which each action is placed_at

 the end of the production and is executed along with the reduction

 of the body to the head of that production SDT's with all

 actions at the right ends of the production_bodies are called postfix SDT's



 ex

 sdt-desk-calc-ex The postfix SDT in

 Fig sdt-desk-calc-fig implements the desk_calculator SDD

 of Fig_sdd-desk-calc-fig with one change the action for

 the first production prints a value The remaining actions are

 exact counterparts of the semantic_rules Since the underlying

 grammar is LR and the SDD is S-attributed these actions can

 be correctly performed along with the reduction steps of the

 parser

 ex



 figurehtfb



 center

 tabularl_l l_l





























 tabular

 center



 Postfix SDT implementing the desk_calculator

 sdt-desk-calc-fig



 figure



 Parser-Stack Implementation of Postfix SDT's

 stack-postfix-subsect



 Postfix SDT's can be_implemented during LR_parsing by executing

 the actions when reductions occur The attribute(s) of each

 grammar symbol can be put on the stack in a place where they can

 be found during the reduction The best plan is to place the

 attributes along with the grammar_symbols (or the LR states that

 represent these symbols) in records on the stack itself



 In Fig bus-stack-fig the parser stack contains records

 with a field for a grammar symbol (or parser state) and below it

 a field for an attribute The three grammar_symbols are

 on top of the stack perhaps they are about to be reduced

 according to a production like Here we show

 as the one attribute of and so on In_general we can

 allow for more attributes either by making the records large

 enough or by putting pointers to records on the stack With small

 attributes it may be simpler to make the records large_enough

 even if some fields go unused some of the time However if one or

 more attributes are of unbounded size - say they are character

 strings - then it would be better to put a pointer to the

 attribute's value in the stack record and store the actual value

 in some larger shared storage area that is not part of the stack



 figurehtfb





 Parser stack with a field for synthesized_attributes

 bus-stack-fig

 figure



 If the attributes are all synthesized and the actions occur at

 the ends of the productions then we can compute the attributes

 for the head when we reduce the body to the head If we reduce by

 a production such_as then we have all the

 attributes of and available at known positions on

 the stack as in Fig bus-stack-fig After the action

 and its attributes are at the top of the stack in the position of

 the record for



 ex

 stack-sdt-desk-calc-ex Let_us rewrite the actions of the

 desk-calculator SDT of Example sdt-desk-calc-ex so that

 they manipulate the parser stack explicitly Such stack

 manipulation is usually done automatically by the parser



 figurehtfb



 center

 tabularl_l l

 Production Actions



 n







 15pt











 15pt











 15pt











 tabular

 center



 Implementing the desk_calculator on a bottom-up_parsing

 stack stack-sdt-desk-calc-fig



 figure



 Suppose that the stack is kept in an array of records called

 with a cursor to the top of the

 stack Thus refers to the top record

 on the stack to the record below

 that and so on Also we assume that each record has a field

 called which holds the attribute of whatever grammar

 symbol is represented in that record Thus we may refer to the

 attribute that appears at the third position on the stack

 as The entire SDT is shown

 in Fig stack-sdt-desk-calc-fig



 For_instance in the second production we go

 two positions below the top to get the value of and we find

 the value of at the top The resulting sum is placed where the

 head will appear after the reduction that is two positions

 below the current top The_reason is that after the reduction the

 three topmost stack symbols are replaced_by one After computing

 we pop two symbols off the top of the stack so the

 record where we placed will now be at the top of the

 stack



 In the third production no action is necessary

 because the length of the stack does_not change and the value of

 at the stack top will simply become the value of

 The same observation applies to the productions

 and Production

 is slightly_different Although the value does

 not change two positions are removed_from the stack during the

 reduction so the value has to move to the position after the

 reduction



 Note_that we have omitted the steps that manipulate the first

 field of the stack records - the field that gives the LR state

 or otherwise represents the grammar symbol If we are performing

 an LR parse the parsing_table tells_us what the new state is

 every time we reduce see Algorithm lr parsing-alg Thus we

 may simply place that state in the record for the new top of

 stack

 ex



 SDT's With Actions Inside Productions

 sdt-inside-subsect



 An action may be placed_at any position within the body of a

 production It is performed immediately_after all symbols to its

 left are processed Thus if we have a production

 the action is done after we have

 recognized (if is a terminal) or all the terminals derived

 from (if is a nonterminal) More_precisely



 itemize



 If the parse is bottom-up then we perform action as

 soon_as this occurrence of appears on the top of the parsing

 stack



 If the parse is top-down we perform just_before we

 attempt to expand this occurrence of (if a nonterminal) or

 check for on the input (if is a terminal)



 itemize



 's that can be_implemented during_parsing include postfix

 SDT's and a class of SDT's considered in

 Section l-att-sect that implements L-attributed definitions

 Not all SDT's can be_implemented during_parsing as we_shall see

 in the next example



 ex

 prefix-sdt-ex As an extreme example of a problematic

 SDT suppose that we turn our desk-calculator running_example

 into an SDT that prints the prefix form of an expression

 rather_than evaluating the expression The productions and actions

 are shown in Fig prefix-sdt-fig



 figurehtfb

 center

 tabularl_l l_l

 1) n



 2)



 3)



 4)



 5)



 6)



 7) digit



 tabular

 center



 Problematic SDT for infix-to-prefix translation during

 parsing prefix-sdt-fig



 figure



 Unfortunately it is impossible to implement this SDT during

 either top-down or bottom-up_parsing because the parser would

 have to perform critical actions like printing instances of

 or long before it knows whether these symbols will

 appear in its input



 Using marker_nonterminals and for the actions in

 productions 2 and 4 respectively on input that is a digit a shift-reduce

 parser (see_Section shift-reduce-subsect) has conflicts

 between reducing by reducing by

 and shifting the digit

 ex



 Any SDT can be_implemented as_follows



 enumerate



 Ignoring the actions parse the input and produce a parse

 tree as a result



 Then examine each interior_node say one for production

 Add additional children to for the actions

 in so the children of from left to right have

 exactly the symbols and actions of



 Perform a preorder_traversal (see_Section dfs-subsect)

 of the tree and as_soon as a node_labeled

 by an action is visited perform that action



 enumerate

 For_instance Fig parse-prefix-fig shows the parse_tree for

 expression with actions inserted If we visit the nodes

 in preorder we get the prefix form of the expression





 figurehtfb



 Parse

 tree with actions embedded parse-prefix-fig

 figure



 Eliminating Left Recursion From SDT's

 sdt-left-rec-elim-subsect



 Since no grammar with left_recursion can be_parsed

 deterministically top-down we examined left-recursion elimination

 in Section left-rec-elim-subsect When the grammar is part

 of an SDT we also need to worry_about how the actions are

 handled



 First consider the simple case in which the only thing we care

 about is the order in which the actions in an SDT are

 performed For_example if each action simply prints a string we

 care only about the order in which the strings are printed In

 this case the following principle can guide us



 itemize



 When transforming the grammar treat the actions as if they_were

 terminal_symbols



 itemize

 This principle is based_on the idea that the grammar

 transformation preserves the order of the terminals in the

 generated string The actions are therefore executed in the same

 order in any left-to-right parse top-down or bottom-up



 The trick for eliminating left_recursion is to take two

 productions



 center



 center

 that generate strings consisting of a and any number of

 's and replace them by productions that generate the same

 strings using a new nonterminal (for remainder) of the

 first production



 center

 tabularl









 tabular

 center

 If does_not begin_with then no_longer has a

 left-recursive production In regular-definition terms with both

 sets of productions is defined by

 See Section left-rec-elim-subsect for the handling of

 situations_where has more recursive or nonrecursive

 productions



 ex

 sdt-postfix-ex Consider the following -productions from

 an SDT for translating infix expressions into_postfix notation



 center

 tabularl_l l_l









 tabular

 center

 If we apply the standard transformation to the remainder of

 the left-recursive production is



 center



 center

 and the body of the other production is

 If we introduce for the remainder of we get the set of

 productions



 center

 tabularl_l l













 tabular

 center

 ex



 When the actions of an SDD compute attributes rather_than

 merely printing output we must_be more careful about how we

 eliminate_left recursion from a grammar However if the SDD is

 S-attributed then we can always construct an SDT by placing

 attribute-computing actions at appropriate positions in the new

 productions



 We_shall give a general schema for the case of a single recursive

 production a single nonrecursive production and a single

 attribute of the left-recursive nonterminal the generalization to

 many productions of each type is not hard but is notationally

 cumbersome Suppose that the two productions are



 center

 tabularl_l l









 tabular

 center

 Here is the synthesized_attribute of left-recursive

 nonterminal and and are single grammar_symbols with

 synthesized_attributes and respectively These could

 represent a string of several grammar_symbols each with its_own

 attribute(s) since the schema has an arbitrary function

 computing in the recursive production and an arbitrary

 function computing in the second production In each

 case and take as arguments whatever attributes they are

 allowed to access if the SDD is S-attributed



 We want to turn the underlying_grammar into



 center

 tabularl_l l









 tabular

 center



 Figure left-rec-elim-sdt-fig suggests what the SDT on the

 new grammar must do In (a) we see the effect of the postfix

 SDT on the original grammar We apply once corresponding

 to the use of production and then apply as many

 times as we use the production Since

 generates a remainder of 's its translation depends_on the

 string to its left a string of the form Each use

 of the production results in an application of





 For we use an inherited_attribute to accumulate the

 result of successively applying starting_with the value of





 figurehtfb





 Eliminating left_recursion from a postfix SDT





 left-rec-elim-sdt-fig

 figure



 In_addition has a synthesized_attribute not shown in

 Fig left-rec-elim-sdt-fig This attribute is first computed

 when ends its generation of symbols as signaled by the

 use of production is then copied up the

 tree so it can become the value of for the entire

 expression The case_where generates is

 shown in Fig left-rec-elim-sdt-fig and we see that the

 value of at the root of (a) has two uses of So does

 at the bottom of tree (b) and it is this value of

 that gets copied up that tree



 To accomplish this translation we use the following SDT



 center

 tabularl_l l













 tabular

 center

 Notice_that the inherited_attribute is evaluated immediately

 before a use of in the body while the synthesized_attributes

 and are evaluated at the ends of the productions

 Thus whatever values are needed to compute these attributes will

 be available from what has_been computed to the left



 SDT's for L-Attributed Definitions

 l-attr-sdt-subsect



 In Section bus-subsect we converted S-attributed SDD's

 into_postfix SDT's with actions at the right ends of

 productions As_long as the underlying_grammar is LR postfix

 SDT's can be_parsed and translated bottom-up



 Now we consider the more general case of an L-attributed_SDD

 We_shall assume that the underlying_grammar can be_parsed

 top-down for if not it is frequently impossible to perform the

 translation in connection_with either an LL or an LR_parser With

 any grammar the technique below can be_implemented by_attaching

 actions to a parse_tree and executing them during preorder

 traversal of the tree



 The rules for turning an L-attributed_SDD into an SDT are as

 follows



 enumerate



 Embed the action that computes the inherited_attributes for

 a nonterminal immediately_before that occurrence of in the

 body of the production If several inherited_attributes for

 depend_on one another in an acyclic fashion order the evaluation

 of attributes so that those needed first are computed first



 Place the actions that compute a synthesized_attribute for the head of a

 production at the end of the body of that production



 enumerate



 We_shall illustrate these principles with two extended examples

 The first involves typesetting It illustrates_how the techniques

 of compiling can be used in language processing for applications

 other_than what we normally think of as programming_languages The

 second example is about the generation of intermediate_code for a

 typical programming-language construct a form of while-statement



 ex

 eqn-ex This example is motivated by languages for

 typesetting mathematical formulas Eqn is an early example

 of such a language ideas from Eqn are still found in the

 typesetting system which was used to produce this_book



 We_shall concentrate_on only the capability to define subscripts

 subscripts of subscripts and so on ignoring superscripts

 built-up fractions and all other mathematical features

 In the Eqn language one

 writes a sub i sub j to set

 the expression A simple grammar for boxes

 (elements of text bounded by a rectangle) is



 center



 center

 Corresponding to these four productions a box can be either



 enumerate



 Two boxes juxtaposed with the first to the left of

 the other



 A box and a subscript box The second box appears in a

 smaller size lower and to the right of the first box



 A parenthesized box for grouping of boxes and subscripts

 Eqn and both use curly_braces for grouping but we

 shall use ordinary round parentheses to avoid confusion with the

 braces that surround actions in SDT's



 A text_string that is any string of characters



 enumerate

 This grammar is ambiguous but we can still use it to parse

 bottom-up if we make subscripting and juxtaposition

 right associative with sub taking precedence_over

 juxtaposition



 Expressions will be typeset by constructing larger boxes out of

 smaller ones In Fig tex-box-fig the boxes for and

 are about to be juxtaposed to form the box for

 The left box for is itself constructed from

 the box for and the subscript The subscript is

 handled by shrinking its box by about 30 lowering it and

 placing it after the box for Although we_shall treat

 as a text_string the rectangles within its box show_how

 it can be constructed from boxes for the individual letters



 figurehtfb

 fileuullmanalsuch5figstex-boxeps

 Constructing larger boxes from smaller ones

 tex-box-fig

 figure



 In this example we concentrate_on the vertical geometry of boxes

 only The horizontal geometry - the widths of boxes - is also

 interesting especially when different characters have different

 widths

 It may not be readily apparent but each of the distinct

 characters in Fig tex-box-fig has a different width



 The values associated_with the vertical geometry of boxes are as

 follows



 itemize



 a) The point size is used to set text within a box

 We_shall assume that characters not in subscripts are set in

 10 point type the size of type in this_book Further we assume

 that if a box has point size then its subscript box has the

 smaller point size Inherited attribute will

 represent the point size of block This attribute must_be

 inherited because the context determines by how much a given box

 needs to be shrunk due to the number of levels of

 subscripting



 b) Each box has a baseline which is a vertical

 position that corresponds to the bottoms of lines of text not

 counting any letters like g that extend below the normal

 baseline In Fig tex-box-fig the dotted line represents

 the baseline for the boxes and the entire

 expression The baseline for the box containing the subscript

 is adjusted to lower the subscript



 c) A box has a height which is the distance from the

 top of the box to the baseline Synthesized attribute

 gives the height of box



 d) A box has a depth which is the distance from the

 baseline to the bottom of the box Synthesized attribute

 gives the depth of box



 itemize



 The SDD in Fig tex-box-sdd-fig gives rules for computing

 point sizes heights and depths Production 1 is used to assign

 the initial value 10



 figurehtfb

 center

 tabularl_l l_Production Semantic_Rules



 12pt1)



 15pt2)

















 15pt3)

















 15pt4)













 15pt5)









 tabular

 center

 SDD for typesetting boxes tex-box-sdd-fig

 figure



 Production 2 handles juxtaposition Point sizes are copied down

 the parse_tree that is two sub-boxes of a box inherit the same

 point size from the larger box Heights and depths are computed up

 the tree by_taking the maximum That is the height of the larger

 box is the maximum of the heights of its two components and

 similarly for the depth



 Production 3 handles subscripting and is the most subtle In this

 greatly simplified example we assume that the point size of a

 subscripted box is 70 of the point size of its parent Reality

 is much more_complex since subscripts cannot shrink indefinitely

 in practice after a few levels the sizes of subscripts shrink

 hardly at all Further we assume that the baseline of a subscript

 box drops by 25 of the parent's point size again reality is

 more_complex



 Production 4 copies attributes appropriately when parentheses are

 used Finally production 5 handles the leaves that represent text

 boxes In this matter too the true situation is complicated so

 we merely show two unspecified functions getHt and getDp that examine tables created with each font to

 determine the maximum height and maximum depth of any characters

 in the text_string The string itself is presumed to be provided

 as the attribute lexval of terminal text



 Our last_task is to turn this SDD into an SDT following the

 rules for an L-attributed_SDD which Fig tex-box-sdd-fig

 is The appropriate SDT is shown in Fig_tex-box-sdt-fig

 For readability since production_bodies become long we split

 them across lines and line up the actions Production bodies

 therefore consist of the contents of all lines up to the head of

 the next production

 ex



 figurehtfb



 center

 tabularl_l l_r l

 Production 3lActions



 1)







 15pt2)















 15pt3)



 sub











 15pt4) to 5in(











 15pt5) to 5intext









 tabular

 center



 SDT for typesetting boxes tex-box-sdt-fig



 figure



 Our next example concentrates on a simple while-statement and the

 generation of intermediate_code for this type of statement

 Intermediate_code will be treated_as a string-valued_attribute

 Later we_shall explore techniques that involve

 the writing of pieces of a string-valued_attribute as we parse

 thus avoiding the copying of long strings to build even longer

 strings The technique was_introduced in

 Example sdt-postfix-ex where we generated the postfix form

 of an infix expression on-the-fly rather_than computing it as

 an attribute

 However in our first formulation we create a string-valued_attribute

 by concatenation



 ex

 while-ex For this example we only need one production

 center



 center

 Here is the nonterminal that generates all kinds of

 statements presumably including if-statements assignment

 statements and others In this example stands_for a

 conditional expression - a boolean_expression that evaluates to true or false



 In this flow-of-control example the only things we ever generate

 are labels All the other intermediate-code instructions are

 assumed to be generated_by parts of the SDT that are not shown

 Specifically we generate explicit instructions of the form

 where is an_identifier to indicate that

 is the label of the instruction that follows We assume that

 the intermediate_code is like that introduced in

 Section 3code-subsect



 The meaning of our while-statement is that the conditional is

 evaluated If it is true control goes to the beginning of the

 code for If false then control goes to the code that

 follows the while-statement's code The code for must_be

 designed to jump to the beginning of the code for the

 while-statement when finished the jump to the beginning of the

 code that evaluates is not shown in Fig while-sdd-fig



 We use the following attributes to generate the proper

 intermediate_code



 enumerate



 The inherited_attribute labels the beginning

 of the code that must_be executed after is finished



 The synthesized_attribute is the sequence of

 intermediate-code steps that implements a statement and ends

 with a jump to



 The inherited_attribute labels the beginning

 of the code that must_be executed if is true



 The inherited_attribute labels the

 beginning of the code that must_be executed if is false



 The synthesized_attribute is the sequence of

 intermediate-code steps that implements the condition and

 jumps either to or to

 depending_on whether is true or false



 enumerate



 figurehtfb

 center

 tabularl_l

























 tabular

 center

 SDD for while-statements while-sdd-fig

 figure



 The SDD that computes these attributes for the while-statement

 is shown in Fig while-sdd-fig A number of points merit

 explanation



 itemize



 The function generates new labels



 The variables and hold labels that we need in the

 code is the beginning of the code for the while-statement

 and we need to arrange that jumps there after it finishes

 That is why we set to is the

 beginning of the code for and it becomes the value of

 because we branch there when is true



 Notice_that is set to

 because when the condition is false we execute whatever code must

 follow the code for



 We use as the symbol for concatenation of

 intermediate-code fragments The value of thus

 begins_with the label then the code for condition

 another label and the code for



 itemize



 This SDD is L-attributed When we convert it into an SDT

 the only remaining issue is how to handle the labels and

 which are variables and not attributes If we treat actions

 as dummy nonterminals then such variables can be treated_as the

 synthesized_attributes of dummy nonterminals Since and

 do_not depend_on any other attributes they can be assigned to the

 first action in the production The resulting SDT with embedded

 actions that implements this L-attributed_definition is shown in

 Fig while-sdt-fig

 ex



 figurehtfb



 center

 tabularl_l l_l

 while











 tabular

 center



 SDT for while-statements while-sdt-fig



 figure



 sexer

 We mentioned in Section stack-postfix-subsect that it is possible

 to deduce from the LR state on the parsing stack what grammar symbol

 is represented_by the state How_would we discover this information

 sexer



 exer

 Rewrite the following SDT



 center

 tabularl









 tabular

 center

 so that the underlying_grammar becomes non-left-recursive Here

 and are actions and 0 and 1 are terminals

 exer



 hexer

 The following SDT computes the value of a string of 0's and 1's

 interpreted as a positive binary integer



 center

 tabularl_l l













 tabular

 center

 Rewrite this SDT so the underlying_grammar is not

 left_recursive and yet the same value of is computed for

 the entire input_string

 hexer



 hexer

 genl-while-exer Write L-attributed SDD's analogous to

 that of Example while-ex for the following productions each

 of which represents a familiar flow-of-control construct as in

 the programming_language C You_may need to generate a

 three-address statement to jump to a particular label in

 which case you should generate goto



 itemize



 a) if else





 b) do while



 c)





 Note_that any statement in the list can have a jump from

 its middle to the next statement so it is not sufficient simply

 to generate code for each statement in order



 itemize

 hexer



 exer

 Convert each of your_SDD's from Exercise_genl-while-exer

 to an SDT in the manner of Example while-ex

 exer



 exer

 Modify the SDD of Fig tex-box-sdd-fig to include a

 synthesized_attribute the length of a box The

 length of the concatenation of two boxes is the sum of the lengths

 of each Then add your new rules to the proper positions in the

 SDT of Fig_tex-box-sdt-fig

 exer



 exer

 Modify the SDD of Fig tex-box-sdd-fig to include

 superscripts denoted_by operator sup between boxes If box

 is a superscript of box then position the baseline of

 06 times the point size of above the baseline of

 Add the new production and rules to the SDT of

 Fig_tex-box-sdt-fig

 exer

 Short-Pause Garbage_Collection

 short-pause-sect



 Simple trace-based collectors do stop-the-world-style

 garbage_collection

 which may introduce long pauses into the execution of user

 programs We can reduce the length of the pauses by performing

 garbage_collection one part at a time We can divide the work in

 time by interleaving garbage_collection with the mutation or we

 can divide the work in space by collecting a subset of the garbage

 at a time The former is known_as incremental collection

 and the latter is known_as partial collection



 An incremental collector breaks up the reachability analysis into

 smaller units allowing the mutator to run between these execution

 units The reachable set changes as the mutator executes so

 incremental collection is complex As we_shall see in

 Section incremental-sect finding a slightly conservative

 answer can make tracing more_efficient



 The best known of partial-collection algorithms is generational_garbage collection it partitions objects according

 to how long they have_been allocated and collects the newly

 created objects more often because they tend to have a shorter

 lifetime An alternative algorithm the train_algorithm

 also collects a subset of garbage at a time and is best applied

 to more mature_objects These two algorithms can be used together

 to create a partial collector that handles younger and older

 objects differently We discuss the basic algorithm behind

 partial collection in Section part-coll-subsect and then

 describe in more_detail how the generational and train algorithms

 work



 Ideas from both incremental and partial collection can be adapted

 to create an algorithm that collects objects in parallel on a

 multiprocessor see Section secgcparallel



 Incremental Garbage_Collection

 incremental-sect



 Incremental collectors are conservative While a garbage_collector

 must not collect objects that are not garbage it does_not have to

 collect all the garbage in each round We refer to the garbage

 left behind after collection as floating garbage Of_course

 it is desirable to minimize floating garbage In_particular an

 incremental collector should not leave behind any garbage that was

 not reachable at the beginning of a collection cycle If we can be

 sure of such a collection guarantee then any garbage not

 collected in one round will be collected in the next and no

 memory is leaked because of this approach to garbage_collection



 In other_words incremental collectors play it safe by

 overestimating the set of reachable_objects They first process

 the program's root_set atomically without interference from the

 mutator After finding the initial set of unscanned objects the

 mutator's actions are interleaved with the tracing step During

 this period any of the mutator's actions that may change

 reachability are recorded succinctly in a side table so that the

 collector can make the necessary adjustments when it resumes

 execution If space is exhausted before tracing completes the

 collector completes the tracing_process without allowing the

 mutator to execute In any event when tracing is done space is

 reclaimed atomically



 Precision of Incremental Collection



 Once an object becomes_unreachable it is not possible for the

 object to become reachable again Thus as garbage_collection and

 mutation proceed the set of reachable_objects can only



 enumerate



 Grow due to new objects allocated after garbage_collection

 starts and



 Shrink by losing references to allocated_objects



 enumerate



 Let the set of reachable_objects at the beginning of garbage

 collection be let New be the set of allocated_objects

 during garbage_collection and let Lost be the set of

 objects that have become_unreachable due to lost references since

 tracing began The set of objects reachable when tracing completes

 is



 (R New) - Lost





 It is expensive to reestablish an object's reachability every

 time a mutator loses a reference to the object so incremental

 collectors do_not attempt to collect all the garbage at the end of

 tracing Any garbage left behind - floating garbage - should

 be a subset of the Lost objects Expressed formally the

 set of objects found by tracing must satisfy



 (R New) - Lost S (R

 New)





 Simple Incremental Tracing



 We first describe a straightforward tracing algorithm that finds

 the upper_bound The behavior of the

 mutator is modified during the tracing as_follows



 itemize



 All references that existed before garbage_collection are

 preserved that is before the mutator overwrites a reference its

 old value is remembered and treated_like an additional unscanned

 object containing just that reference



 All objects_created are considered reachable immediately and

 are placed in the Unscanned_state



 itemize



 This scheme is conservative but correct because it finds the

 set of all the objects reachable before garbage_collection plus

 New the set of all the newly_allocated objects However

 the cost is high because the algorithm intercepts all write

 operations and remembers all the overwritten references Some of

 this work is unnecessary because it may involve objects that are

 unreachable at the end of garbage_collection We could avoid some

 of this work and also improve the algorithm's precision if we

 could detect when the overwritten references point to objects that

 are unreachable when this round of garbage_collection ends The

 next algorithm goes fairly far in these two directions



 Incremental Reachability Analysis

 incr-reach-subsect



 If we interleave the mutator with a basic tracing algorithm such

 as Algorithm_algmark-sweep then some reachable_objects may

 be misclassified as unreachable The problem is that the actions

 of the mutator can violate a key invariant of the algorithm

 namely a Scanned object can only contain references to

 other Scanned or Unscanned objects never to Unreached objects Consider the following scenario



 enumerate



 The garbage_collector finds object reachable and scans

 the pointers within thereby putting in the Scanned state



 The mutator stores a reference to an Unreached (but

 reachable) object into the Scanned object It

 does so by copying a reference to from an object that is

 currently in the Unreached or Unscanned_state



 The mutator loses the reference to in object It

 may have overwritten 's reference to before the reference

 is scanned or may have become_unreachable and never have

 reached the Unscanned_state to have its references scanned



 enumerate

 Now is reachable through object but the garbage

 collector may have_seen neither the reference to in nor

 the reference to in



 The key to a more_precise yet correct incremental trace is that

 we must note all copies of references to currently unreached

 objects from an object that has not been scanned to one that has

 To intercept problematic transfers of references the algorithm

 can modify the mutator's action during tracing in any of the

 following ways



 itemize



 Write Barriers Intercept writes of references into a

 Scanned object when the reference is to an Unreached object In this case classify as reachable and

 place it in the Unscanned set Alternatively place the

 written object back in the Unscanned set so we can

 rescan it



 Read Barriers Intercept the reads of references in

 Unreached or Unscanned objects Whenever the mutator

 reads a reference to an object from an object in either the

 Unreached or Unscanned_state classify as

 reachable and place it in the Unscanned set





 Transfer Barriers Intercept the loss of the original

 reference in an Unreached or Unscanned object

 Whenever the mutator overwrites a reference in an Unreached

 or Unscanned object save the reference being overwritten

 classify it as reachable and place the reference itself in the

 Unscanned set



 itemize



 None of the options above finds the smallest set of reachable

 objects If the tracing_process determines an object to be

 reachable it stays reachable even_though all references to it are

 overwritten_before tracing completes That is the set of

 reachable_objects found is between

 and



 Write barriers are the most efficient of the options outlined

 above Read barriers are more_expensive because typically there

 are many more reads than there are writes Transfer barriers are

 not competitive because many objects die young this approach

 would retain many unreachable_objects



 Implementing Write Barriers



 We can implement write barriers in two ways The first approach

 is to remember during a mutation phase all new references

 written into the Scanned objects We can place all these

 references in a list the size of the list is proportional to the

 number of write operations to Scanned objects unless

 duplicates are removed_from the list Note_that references on the

 list may later be overwritten themselves and potentially could be

 ignored



 The second more_efficient approach is to remember the locations

 where the writes occur We may remember them as a list of

 locations written possibly with duplicates eliminated Note it

 is not important that we pinpoint the exact locations written as

 long_as all the locations that have_been written are rescanned

 Thus there are several techniques that allow_us to remember less

 detail about exactly where the rewritten locations are



 itemize



 Instead of remembering the exact address or the object and

 field that is written we can remember just the objects that hold

 the written fields



 We can divide the address space into fixed-size blocks

 known_as cards and use a bit array to remember the cards

 that have_been written into



 We can choose to remember the pages that contain the written

 locations We can simply protect the pages containing Scanned objects Then any writes into Scanned objects will

 be detected without executing any explicit instructions because

 they will cause a protection violation and the operating_system

 will raise a program exception



 itemize



 In_general by coarsening the granularity at which we remember the

 written locations less storage is needed at the expense of

 increasing the amount of rescanning performed In the first

 scheme all references in the modified objects will have to be

 rescanned regardless of which reference was actually modified In

 the last two schemes all reachable_objects in the modified cards

 or modified pages need to be rescanned at the end of the tracing

 process



 Combining Incremental and Copying Techniques



 The above methods are sufficient for mark-and-sweep garbage

 collection Copying collection is slightly more_complicated

 because of its interaction with the mutator Objects in the Scanned or Unscanned states have two addresses one in the

 From_semispace and one in the To_semispace As in

 Algorithm algcopying-collector we must keep a mapping from

 the old address of an object to its relocated address



 There_are two choices for how we update the references First we

 can have the mutator make all the changes in the From space

 and only at the end of garbage_collection do we update all the

 pointers and copy all the contents over to the To space

 Second we can instead make changes to the representation in the

 To space Whenever the mutator dereferences a pointer to

 the From space the pointer is translated to a new location

 in the To space if one exists All the pointers need to be

 translated to point to the To space in the end



 Partial-Collection Basics

 part-coll-subsect



 The fundamental fact is that objects typically die young It

 has_been found that usually between 80 and 98 of all

 newly_allocated objects die within a few million instructions or

 before another megabyte has_been allocated That is objects

 often become_unreachable before any garbage_collection is invoked

 Thus is it quite cost effective to garbage collect new objects

 frequently



 Yet objects that survive a collection once are likely to survive

 many more collections With the garbage_collectors described so

 far the same mature_objects will be found to be reachable over

 and over again and in the case of copying collectors copied over

 and over again in every round of garbage_collection Generational

 garbage_collection works most frequently on the area of the heap

 that contains the youngest objects so it tends to collect a lot

 of garbage for relatively little work The train_algorithm on the

 other_hand does_not spend a large proportion of time on young

 objects but it does limit the pauses due to garbage_collection

 Thus a good combination of strategies is to use generational

 collection for young objects and once an object becomes

 sufficiently mature to promote it to a separate heap that is

 managed by the train_algorithm



 We refer to the set of objects to be collected on one round of

 partial collection as the target set and the rest of the

 objects as the stable set Ideally a partial collector

 should reclaim all objects in the target set that are unreachable

 from the program's root_set However doing_so would_require

 tracing all objects which is what we try to avoid in the first

 place Instead partial collectors conservatively reclaim only

 those objects that cannot be reached through either the root_set

 of the program or the stable set Since some objects in the stable

 set may themselves be unreachable it is possible that we_shall

 treat as reachable some objects in the target set that really have

 no path from the root_set



 We can adapt the garbage_collectors described in

 Sections secgcsimple and secgcrelocate to work in

 a partial manner by changing the definition of the root_set

 Instead of referring to just the objects held in the registers

 stack and global variables the root_set now also includes all the

 objects in the stable set that point to objects in the target set

 References from target objects to other target objects are traced

 as before to find all the reachable_objects We can ignore all

 pointers to stable objects because these objects are all

 considered reachable in this round of partial collection



 To identify those stable objects that reference target objects we

 can adopt techniques similar to those used in incremental garbage

 collection In incremental collection we need to remember all

 the writes of references from scanned objects to unreached objects

 during the tracing_process Here we need to remember all the

 writes of references from the stable objects to the target objects

 throughout the mutator's execution Whenever the mutator stores

 into a stable object a reference to an object in the target set

 we remember either the reference or the location of the write We

 refer to the set of objects holding references from the stable to

 the target objects as the remembered_set for this set of

 target objects As_discussed in Section incr-reach-subsect

 we can compress the representation of a remembered_set by

 recording only the card or page in which the written object is

 found



 Partial garbage_collectors are often implemented as copying

 garbage_collectors Noncopying collectors can also be_implemented

 by using linked_lists to keep_track of the reachable_objects The

 generational scheme described below is an example of how

 copying may be_combined with partial collection



 Generational Garbage_Collection

 secgcgenerational



 Generational garbage_collection is an effective way to exploit the

 property that most objects die young The heap_storage in

 generational_garbage collection is separated into a series of

 partitions We_shall use the convention of numbering them

 with the lower-numbered partitions holding the

 younger objects Objects are first created in partition 0 When

 this partition fills up it is garbage collected and its

 reachable_objects are moved into partition 1 Now with

 partition 0 empty again we resume allocating new objects in that

 partition When partition 0 again

 fillsfootnoteTechnically partitions do_not fill

 since they can be expanded with additional disk blocks by the

 memory_manager if desired However there is normally a limit on

 the size of a partition other_than the last We_shall refer to

 reaching this limit as filling the partitionfootnote

 it is garbage collected and its reachable_objects copied_into

 partition 1 where they join the previously copied objects This

 pattern repeats until partition 1 also fills up at which point

 garbage_collection is applied to partitions 0 and 1



 In_general each round of garbage_collection is applied to all

 partitions numbered or below for some the proper

 to choose is the highest-numbered partition that is currently

 full Each time an object survives a collection (ie it is found

 to be reachable) it is promoted to the next higher partition from

 the one it occupies until it reaches the oldest partition the

 one numbered



 Using the terminology introduced in

 Section part-coll-subsect when partitions and below are

 garbage collected the partitions from 0 through make up the

 target set and all partitions above comprise the stable set

 To support finding root sets for all possible partial collections

 we keep for each partition a remembered_set consisting

 of all the objects in partitions above that point to objects

 in set The root_set for a partial collection invoked on set

 includes the remembered_sets for partition and below



 In this scheme all partitions below are collected whenever we

 collect There_are two_reasons for this policy



 enumerate



 Since younger generations contain more garbage and are

 collected more often anyway we may as_well collect them along

 with an older generation



 Following this strategy we need to remember only the

 references pointing from an older generation to a newer

 generation That is neither writes to objects in the youngest

 generation nor promoting objects to the next generation causes

 updates to any remembered_set If we were to collect a partition

 without a younger one the younger generation would become part of

 the stable set and we would have to remember references that

 point from younger to older generations as_well



 enumerate



 In summary this scheme collects younger generations more often

 and collections of these generations are particularly

 cost effective since objects die young Garbage_collection

 of older generations takes more time since it includes the

 collection of all the younger generations and collects

 proportionally less garbage Nonetheless older generations do

 need to be collected once in a while to remove unreachable

 objects The oldest generation holds the most mature_objects its

 collection is expensive because it is equivalent to a full

 collection That is generational collectors occasionally require

 that the full tracing step be performed and therefore can

 introduce long pauses into a program's execution An alternative

 for handling mature_objects only is discussed next



 The Train Algorithm



 While the generational approach is very efficient for the handling

 of immature objects it is less efficient for the mature_objects

 since mature_objects are moved every time there is a collection

 involving them and they are quite unlikely to be garbage A

 different approach to incremental collection called the train_algorithm was developed to improve the handling of mature

 objects It can be used for collecting all garbage but it is

 probably better to use the generational approach for immature

 objects and only after they have survived a few rounds of

 collection promote them to another heap managed by the train

 algorithm Another advantage to the train_algorithm is that we

 never have to do a complete garbage_collection as we do

 occasionally for generational_garbage collection



 To motivate the train_algorithm let_us look_at a simple example of

 why it is necessary in the generational approach to have

 occasional all-inclusive rounds of garbage_collection

 Figure part-cycle-fig shows two mutually linked objects in

 two partitions and where Since both objects have

 pointers from outside their partition a collection of only

 partition or only partition could never collect either of

 these objects Yet they may in fact be part of a cyclic_garbage

 structure with no links from the outside In_general the

 links between the objects shown may involve many objects and

 long chains of references



 figurehtfb



 A

 cyclic structure across partitions that may be cyclic_garbage

 part-cycle-fig

 figure



 In generational_garbage collection we eventually collect

 partition and since we also collect at that time

 Then the cyclic structure will be completely contained in the

 portion of the heap being collected and we can tell if it truly

 is garbage However if we never have a round of collection that

 includes both and we would have a problem with cyclic

 garbage just as we did with reference_counting for garbage

 collection



 The train_algorithm uses fixed-length partitions called cars a car might be a single disk block provided there are no

 objects larger_than disk blocks or the car size could be larger

 but it is fixed once and for all Cars are organized into trains There is no limit to the number of cars in a train and

 no limit to the number of trains There is a lexicographic order

 to cars first order by train number and within a train order by

 car number as in Fig train-fig



 figurehtfb





 Organization of the heap for the train_algorithm

 train-fig

 figure



 There_are two ways that garbage is collected by the train

 algorithm



 itemize



 The first car in lexicographic order (that is the first

 remaining car of the first remaining train) is collected in one

 incremental garbage-collection step This step is similar to

 collection of the first partition in the generational algorithm

 since we maintain a remembered list of all pointers from

 outside the car Here we identify objects with no references at

 all as_well as garbage cycles that are contained completely

 within this car Reachable objects in the car are always moved to

 some other car so each garbage-collected car becomes empty and

 can be removed_from the train



 Sometimes the first_train has no external references That

 is there are no pointers from the root_set to any car of the

 train and the remembered_sets for the cars contain only

 references from other cars in the train not from other trains In

 this situation the train is a huge collection of cyclic_garbage

 and we delete the entire train



 itemize



 Remembered Sets



 We_now give the details of the train_algorithm Each car has a

 remembered_set consisting of all references to objects in the car

 from



 itemize



 a) Objects in higher-numbered cars of the same train and



 b) Objects in higher-numbered trains



 itemize

 In_addition each train has a remembered_set consisting of all

 references from higher-numbered trains That is the remembered

 set for a train is the union of the remembered_sets for its cars

 except for those references that are internal to the train It is

 thus possible to represent both kinds of remembered_sets by

 dividing the remembered_sets for the cars into internal (same

 train) and external (other trains) portions



 Note_that references to objects can come from anywhere not just

 from lexicographically higher cars However the two

 garbage-collection processes deal_with the first car of the first

 train and the entire first_train respectively Thus when it is

 time to use the remembered_sets in a garbage_collection there is

 nothing earlier from which references could come and therefore

 there is no point in remembering references to higher cars at any

 time We must_be careful of course to manage the remembered

 sets properly changing them whenever the mutator modifies

 references in any object



 Managing Trains



 Our objective is to draw out of the first_train all objects that

 are not cyclic_garbage Then the first_train either becomes

 nothing but cyclic_garbage and is therefore collected at the next

 round of garbage_collection or if the garbage is not cyclic then

 its cars may be collected one at a time



 We therefore need to start new trains occasionally even_though

 there is no limit on the number of cars in one train and we could

 in principle simply add new cars to a single train every time we

 needed more space For_example we could start a new train after

 every object creations for some That is in general a

 new object is placed in the last car of the last train if there

 is room or in a new car that is added to the end of the last

 train if there is no room However periodically we instead

 start a new train with one car and place the new object there



 Garbage Collecting a Car



 The heart of the train_algorithm is how we process the first car

 of the first_train during a round of garbage_collection

 Initially the reachable set is taken to be the objects of that

 car with references from the root_set and those with references in

 the remembered_set for that car We then scan these objects as in

 a mark-and-sweep collector but we do_not scan any reached objects

 outside the one car being collected After this tracing some

 objects in the car may be identified as garbage There is no need

 to reclaim their space because the entire car is going to

 disappear anyway



 However there are likely to be some reachable_objects in the car

 and these must_be moved somewhere else The rules for moving an

 object are



 itemize



 If there is a reference in the remembered_set from any other

 train (which will be higher-numbered than the train of the car

 being collected) then move the object to one of those trains If

 there is room the object can go in some existing car of the train

 from which a reference emanates or it can go in a new last car

 if there is no room



 If there is no reference from other trains but there are

 references from the root_set or from the first_train then move

 the object to any other car of the same train creating a new

 last car if there is no room If possible pick a car from which

 there is a reference to help bring cyclic structures to a single

 car



 itemize



 After moving all the reachable_objects from the first

 car we delete that car



 Panic Mode



 There is one problem with the rules above In order to be_sure

 that all garbage will_eventually be collected we need to be_sure

 that every train eventually becomes the first_train and if this

 train is not cyclic_garbage then eventually all cars of that

 train are removed and the train disappears one car at a time

 However by rule_(2) above collecting the first car of the first

 train can produce a new last car It cannot produce two or_more

 new cars since surely all the objects of the first car can fit in

 the new last car However could we be in a situation where each

 collection step for a train results in a new car being added and

 we never get finished with this train and move on to the other

 trains



 The answer is unfortunately that such a situation is possible

 The problem arises if we have a large cyclic nongarbage

 structure and the mutator manages to change references in such a

 way that we never see at the time we collect a car any

 references from higher trains in the remembered_set If even one

 object is removed_from the train during the collection of a car

 then we are OK since no new objects are added to the first_train

 and therefore the first_train will surely run out of objects

 eventually However there may be no garbage at all that we can

 collect at a stage and we run the risk of a loop where we

 perpetually garbage collect only the current first_train



 To_avoid this problem we need to behave differently whenever we

 encounter a futile garbage_collection that is a car from

 which not even one object can be deleted as garbage or moved to

 another train In this panic mode we make two changes



 enumerate



 When a reference to an object in the first_train is

 rewritten we maintain the reference as a new member of the root

 set



 When garbage collecting if an object in the first car has a

 reference from the root_set including dummy references set up by

 point (1) then we move that object to another train even if it

 has no references from other trains It is not important which

 train we move it to as_long as it is not the first_train



 enumerate

 In this way if there are any references from outside the first

 train to objects in the first_train these references are

 considered as we collect every car and eventually some object

 will be removed_from that train We can then leave panic mode and

 proceed normally sure that the current first_train is now smaller

 than it was



 sexer

 incr-gc-exer Suppose that the network of objects from

 Fig gc2-exer-fig is managed by an incremental algorithm

 that uses the four lists Unreached Unscanned Scanned and

 Free as in Baker's algorithm To be specific the Unscanned_list is managed as a queue and when more_than one

 object is to be placed on this list due to the scanning of one

 object we do so in alphabetical order Suppose also that we use

 write barriers to assure that no reachable object is made garbage

 Starting with and on the Unscanned_list suppose the

 following events occur



 itemize

 is scanned The pointer

 is rewritten to be is scanned

 is scanned The pointer

 is rewritten to be

 itemize

 Simulate the entire incremental garbage_collection assuming no

 more pointers are rewritten Which objects are garbage Which

 objects are placed on the Free_list



 sexer



 exer

 Repeat_Exercise incr-gc-exer on the assumption that



 itemize



 a) Events and are interchanged in order

 b) Events and occur before and





 itemize

 exer



 sexer

 train-exer Suppose the heap consists of exactly the nine

 cars on three trains shown in Fig train-fig (ie ignore

 the ellipses) Object in car 11 has references from cars 12

 23 and 32 When we garbage collect car 11 where might wind

 up

 sexer



 exer

 Repeat_Exercise train-exer for the cases that has



 itemize

 a) Only references from cars 22 and 31 b) No

 references other_than from car 11

 itemize

 exer



 exer

 Suppose the heap consists of exactly the nine cars on three trains

 shown in Fig train-fig (ie ignore the ellipses) We are

 currently in panic mode Object in car 11 has only one

 reference from object in car 12 That reference is

 rewritten When we garbage collect car 11 what could happen to



 exer

 A Simple Code Generator

 simple-cg-sect



 In this_section we_shall consider an algorithm that generates

 code for a single basic_block It considers each three-address

 instruction in turn and keeps_track of what values are in what

 registers so it can avoid generating unnecessary loads and stores



 One of the primary issues during code_generation is deciding how

 to use registers to best advantage There_are four principal uses

 of registers



 itemize



 In most machine architectures some or all of the operands

 of an operation must_be in registers in order to perform the

 operation



 Registers make good temporaries - places to hold the

 result of a subexpression while a larger expression is being

 evaluated or_more generally a place to hold a variable that is

 used only within a single basic_block



 Registers are used to hold (global) values that are

 computed in one basic_block and used in other blocks for example

 a loop_index that is incremented going_around the loop and is used

 several_times within the loop



 Registers are often used to help with run-time storage

 management for example to manage the run-time_stack including

 the maintenance of stack pointers and possibly the top elements of

 the stack itself



 itemize

 These are competing needs since the number of registers available

 is limited



 The algorithm in this_section assumes that some set of registers

 is available to hold the values that are used within the block

 Typically this set of registers does_not include all the

 registers of the machine since some registers are reserved for

 global variables and managing the stack We assume that the basic

 block has already_been transformed into a preferred sequence of

 three-address_instructions by transformations such_as combining

 common_subexpressions We further assume that for each operator

 there is exactly one machine_instruction that takes the necessary

 operands in registers and performs that operation leaving the

 result in a register The machine_instructions are of the form



 itemize



 LD reg mem



 ST mem reg



 OP reg reg reg

 itemize



 Register and Address Descriptors



 Our code-generation algorithm considers each three-address

 instruction in turn and decides what loads are necessary to get

 the needed operands into registers After generating the loads

 it generates the operation itself Then if there is a need to

 store the result into a memory_location it also generates that

 store



 In order to make the needed decisions we require a data_structure that

 tells_us what program variables currently have their value in a

 register and which register or registers if so We also need to know

 whether the memory_location for a given variable currently has the

 proper value for that variable since a new value for the variable may

 have_been computed in a register and not_yet stored The desired data

 structure has the following descriptors



 enumerate



 For each available register

 a register_descriptor keeps_track of the variable names whose

 current value is in that register Since we_shall use only those

 registers that are available for local use within a basic_block we

 assume that initially all register descriptors are empty As the code

 generation progresses each register will hold the value of zero_or more

 names



 For each program variable an address_descriptor keeps_track of

 the location or locations where the current value of that variable can

 be found The location might be a register a memory address a stack

 location or some set of more_than one of these The information can be

 stored in the symbol-table_entry for that variable name



 enumerate



 The Code-Generation Algorithm

 simple-cg-alg-subsect



 An essential part of the algorithm is a function

 which selects registers for each memory

 location associated_with the three-address_instruction

 Function getReg has access to the register and address

 descriptors for all the variables of the basic_block and may also

 have access to certain useful data-flow information such_as the

 variables that are live_on exit from the block We_shall discuss

 getReg after presenting the basic algorithm While we do

 not know the total_number of registers available for local data

 belonging to a basic_block we assume that there are enough

 registers so that after freeing all available registers by

 storing their values in memory there are enough registers to

 accomplish any three-address operation



 In a three-address_instruction such_as xyz we

 shall treat as a generic_operator and ADD as the

 equivalent machine_instruction We do_not therefore take

 advantage of commutativity of Thus when we implement the

 operation the value of y must_be in the second register

 mentioned in the ADD instruction never the third A

 possible improvement to the algorithm is to generate code for

 both xyz and xzy whenever

 is a commutative operator and pick the better

 code sequence



 Machine Instructions for Operations



 For a three-address_instruction such_as do the following



 enumerate



 Use to select registers for

 and Call these and



 If is not in (according to the register_descriptor

 for ) then issue an instruction_LD where

 is one of the memory_locations for (according to the

 address_descriptor for )



 Similarly if is not in issue an instruction

 LD where is a location for



 Issue the instruction ADD



 enumerate



 Machine Instructions for Copy Statements



 There is an important special_case a three-address copy_statement of the form

 We assume that getReg will always choose the same

 register for both and If is not already in that

 register then generate the machine_instruction LD If was already in we do_nothing It

 is only necessary that we adjust the register_descriptor for

 so that it includes as one of the values found there



 Ending the Basic Block



 As we have described the algorithm variables used by the block

 may wind_up with their only location being a register If the

 variable is a temporary used only within the block that is fine

 when the block ends we can forget about the value of the

 temporary and assume its register is empty However if the

 variable is live_on exit from the block or if we don't_know which

 variables are live_on exit then we need to assume that the value

 of the variable is needed later In that case for each variable

 whose address_descriptor does_not say that its value is

 located in the memory_location for we must generate the

 instruction ST where is a register in which 's

 value exists at the end of the block



 Managing Register and Address Descriptors



 As the code-generation algorithm issues load store and other

 machine_instructions it needs to update the register and

 address descriptors The rules are as_follows



 enumerate



 For the instruction_LD

 enumerate

 Change the register_descriptor for register so it holds

 only

 Add to the address_descriptor for

 Remove from the address_descriptor of variables other_than

 enumerate



 For the instruction ST change the address

 descriptor for to include its_own memory_location



 For an operation such_as ADD

 implementing a three-address_instruction

 enumerate

 Change the register_descriptor for so that it holds only

 Change the address_descriptor for so that its only location is

 Note_that the memory_location for is not now in the

 address_descriptor for

 Remove from the address_descriptor of variables other_than



 enumerate



 When we process a copy_statement after generating the

 load for into register if needed and after managing

 descriptors as for all load statements (per rule 1)

 enumerate

 Add to the register_descriptor for

 Change the address_descriptor for so that its only location is



 enumerate



 enumerate



 ex

 simple-cg-ex

 Let_us translate the basic_block consisting of the

 three-address_statements



 verbatim

 t a - b

 u a - c

 v t u

 a d

 d v u

 verbatim

 Here we assume that t u and v are

 temporaries local to the block while a b_c

 and d are variables that are live_on exit from the block

 Since we have not_yet discussed how the function getReg

 might work we_shall simply assume that there are as many

 registers as we need but that when a register's value is no

 longer needed (for example it holds only a temporary all of

 whose uses have_been passed) then we reuse its register



 A summary of all the machine-code instructions generated is in

 Fig simple-cg-fig The figure also shows the register and

 address descriptors before and after the translation of each

 three-address_instruction



 figurehtfb



 Instructions generated and the changes in the register

 and address descriptors simple-cg-fig

 figure



 For the first three-address_instruction ta-b we

 need to issue three instructions since nothing is in a register

 initially Thus we see a and b loaded into registers

 R1 and R2 and the value t produced in register

 R2 Notice_that we can use R2 for t because the

 value b previously in R2 is not needed within the

 block Since b is presumably live_on exit from the block

 had it not been in its_own memory_location (as indicated by its

 address descriptor) we would have had to store R2 into b first The decision to do so had we needed R2 would be

 taken by getReg



 The second instruction ua-c does_not require a

 load of a since it is already in register_R1

 Further we can reuse R1 for the result u since the

 value of a previously in that register is no_longer needed

 within the block and its value is in its_own memory_location if

 a is needed outside the block Note_that we change the

 address_descriptor for a to indicate that it is no_longer in

 R1 but is in the memory_location called a



 The third instruction vtu requires only the

 addition Further we can use R3 for the result v

 since the value of c in that register is no_longer needed

 within the block and c has its value in its_own memory

 location



 The copy instruction ad requires a load of d

 since it is not in a register We show register R2's descriptor

 holding both a and d The addition of a to the

 register_descriptor is the result of our processing the copy_statement and

 is not the result of any machine_instruction



 The fifth instruction dvu uses two values that

 are in registers Since u is a temporary whose value is no

 longer needed we have chosen to reuse its register_R1 for

 the new value of d Notice_that d is now in only R1 and is not in its_own memory_location The same holds for

 a which is in R2 and not in the memory_location

 called a As a result we need a coda to the machine

 code for the basic_block that stores the live-on-exit variables

 a and d into their memory_locations We show these as

 the last two instructions

 ex



 Design of the Function getReg



 Lastly let_us consider how to implement for a

 three-address_instruction There_are many options although

 there are also some absolute prohibitions against choices that

 lead to incorrect code due to the loss of the value of one or_more

 live_variables We begin our examination with the case of an

 operation step for which we again use as the generic

 example First we must pick a register for and a register for

 The issues are the same so we_shall concentrate_on picking

 register for The rules are as_follows



 enumerate



 If is currently in a register pick a register already

 containing as Do not issue a machine_instruction to

 load this register as none is needed



 If is not in a register but there is a register that is

 currently empty pick one such register as



 The difficult case occurs_when is not in a register and

 there is no register that is currently empty We need to pick one

 of the allowable registers anyway and we need to make it safe to

 reuse Let be a candidate register and suppose is one of

 the variables that the register_descriptor for says is in

 We need to make_sure that 's value either is not really needed

 or that there is somewhere else we can go to get the value of

 The possibilities are

 enumerate

 If the address_descriptor for says_that is somewhere

 besides then we are OK

 If is the variable being

 computed by instruction and is not also one of the other

 operands of instruction ( in this example) then we are OK

 The_reason is that in this case we know this value of is

 never again going to be used so we are free to ignore it

 tricky-reg-item Otherwise if is not used later (that

 is after the instruction there are no further uses of

 and if is live_on exit from the block then is recomputed

 within the block) then we are OK

 If we are not OK by one

 of the first three cases then we need to generate the store

 instruction to place a copy of in its_own

 memory_location This operation is called a spill

 enumerate

 Since may hold several variables at the moment we repeat the above

 steps for each such variable At the end 's score is the

 number of store_instructions we needed to generate Pick one of the

 registers with the lowest score

 enumerate



 Now_consider the selection of the register The issues and

 options are almost as for so we_shall only mention the

 differences



 enumerate



 Since a new value of is being computed a register that

 holds only is always an acceptable choice for This

 statement holds even if is one of and since our

 machine_instructions allows two registers to be the same in one

 instruction



 If is not used after instruction in the sense described for

 variable in item (tricky-reg-item) and holds only

 after being loaded if necessary then can also be

 used as A similar option holds regarding and



 enumerate



 The last matter to consider specially is the case when is a

 copy instruction We pick the register as above

 Then we always choose



 exer

 cg1-exer

 For each of the following C assignment statements



 itemize



 a) x a bc

 b) x a(bc) - d(ef)

 c) x_ai 1

 d) ai bci

 e) aij bik ckj

 f) p q



 itemize

 generate three-address_code assuming that all array_elements are

 integers taking four_bytes each In parts (d) and (e) assume

 that a b and c are constants giving the

 location of the first (0th) elements of the arrays with those

 names as in all previous examples of array_accesses in this

 chapter

 exer



 hexer

 Repeat_Exercise cg1-exer parts (d) and (e) assuming that

 the arrays a b and c are located via pointers

 pa pb and pc respectively pointing to the

 locations of their_respective first elements

 hexer



 exer

 Convert your three-address_code from Exercise cg1-exer

 into machine code for the

 machine_model of this_section

 You_may use as many registers as you need

 exer



 exer

 cg2-exer

 Convert your three-address_code from Exercise cg1-exer into

 machine code using the simple code-generation algorithm of this

 section assuming three registers are available

 Show the register and address descriptors after each step

 exer



 exer

 Repeat_Exercise cg2-exer but assuming only two registers are

 available

 exer

 Introduction to LR_Parsing Simple LR

 slr-sect



 The most prevalent type of bottom-up parser today is based_on a

 concept called LR() parsing the L is for left-to-right

 scanning of the input the R for constructing a rightmost

 derivation in reverse and the for the number of input symbols

 of lookahead that are used in making parsing_decisions The cases

 or are of practical interest and we_shall only

 consider LR_parsers with here When () is omitted

 is assumed to be 1



 This_section introduces the basic concepts of LR_parsing and the

 easiest method for constructing shift-reduce parsers called

 simple LR (or SLR for short) Some familiarity with the basic

 concepts is helpful even if the LR_parser itself is constructed

 using an automatic parser_generator We begin_with items and

 parser states the diagnostic output from an LR_parser

 generator typically includes parser states which can be used to

 isolate the sources of parsing conflicts



 Section_lr-parsers-sect introduces two more_complex

 methods - canonical-LR and LALR - that are used in the

 majority of LR_parsers



 Why LR Parsers

 lr-why-subsect



 LR_parsers are table-driven much like the nonrecursive

 LL parsers of Section nonrec-pp-subsect A grammar for which

 we can construct a parsing_table using one of the methods in this

 section and the next is said to be an LR grammar

 Intuitively for a grammar to be LR it is sufficient that a

 left-to-right shift-reduce_parser be_able to recognize handles

 of right-sentential_forms when they appear on top of the stack



 LR_parsing is attractive for a variety of reasons



 itemize



 LR_parsers can be constructed to recognize virtually all

 programming-language constructs for which context-free_grammars

 can be written Non-LR context-free_grammars exist but these can

 generally be avoided for typical programming-language constructs



 The LR-parsing method is the most general nonbacktracking

 shift-reduce_parsing method known yet it can be_implemented as

 efficiently as other more primitive shift-reduce methods (see

 the bibliographic notes)



 An LR_parser can detect a syntactic error as_soon as it is

 possible to do so on a left-to-right_scan of the input



 The class of grammars that can be_parsed using LR methods is a

 proper superset of the class of grammars that can be_parsed with

 predictive or LL methods For a grammar to be LR() we must_be

 able to recognize the occurrence of the right_side of a

 production in a right-sentential_form

 with input symbols of lookahead This requirement is far

 less stringent than that for LL() grammars where we must_be

 able to recognize the use of a production seeing only the first

 symbols of what its right_side derives Thus

 it should not be surprising that LR_grammars can

 describe more languages than LL_grammars



 itemize



 The principal drawback of the LR method is that it is too_much

 work to construct an LR_parser by hand for a typical

 programming-language grammar A specialized tool an LR_parser

 generator is needed Fortunately many such generators are

 available and we_shall discuss one of the most commonly_used ones Yacc in Section yacc-sect Such a generator takes a

 context-free_grammar and automatically produces a parser for that

 grammar If the grammar contains ambiguities or other constructs

 that are difficult to parse in a left-to-right_scan of the input

 then the parser_generator locates these constructs and provides

 detailed diagnostic messages



 Items and the LR(0) Automaton

 items-subsect



 How does a shift-reduce_parser know when to shift and when to

 reduce For_example with stack_contents T and next

 input_symbol in Fig shift-reduce-actions-fig how does

 the parser know that T on the top of the stack is not a

 handle so the appropriate action is to shift and not to reduce

 T to E



 An LR_parser makes shift-reduce decisions by maintaining states to

 keep_track of where we are in a parse States represent

 sets of items An LR(0)_item (item for short) of a

 grammar is a production of with a dot at some position of

 the body Thus production XYZ yields

 the four items

 center

















 center

 The production generates only one item





 Representing Item Sets

 A parser_generator that

 produces a bottom-up parser may need to represent items and sets

 of items conveniently Note_that an item can be represented_by a

 pair of integers the first of which is the number of one of the

 productions of the underlying_grammar and the second of which is

 the position of the dot Sets of items can be represented_by a

 list of these pairs However as we_shall see the necessary sets

 of items often include closure items where the dot is at the

 beginning of the body These can always be reconstructed from the

 other items in the set and we do_not have to include them in the

 list



 Intuitively an item indicates how much of a production we have

 seen at a given point in the parsing process For_example the

 item indicates that we hope to see a string

 derivable_from next on the input Item

 indicates that we have just seen on the input a string_derivable

 from and that we hope next to see a string_derivable from

 Item indicates that we have_seen the

 body and that it may be time to reduce to



 One collection of sets of LR(0)_items called the canonical

 LR(0)_collection provides the basis for constructing

 a deterministic_finite automaton that is used to make parsing

 decisions Such an automaton is called an LR(0)

 automatonfootnoteTechnically the automaton misses being

 deterministic according to the definition of

 Section dfa-subsect because we do_not have a dead_state

 corresponding to the empty_set of items As a result there are

 some state-input pairs for which no next state

 existsfootnote In_particular each state of the LR(0)

 automaton represents a set of items in the canonical

 LR(0)_collection The automaton for the expression

 grammar_(expr-gram-display) shown in

 Fig_lr-states-fig will serve as the running_example for

 discussing the canonical_LR(0) collection for a grammar





 figurehtfb

 center



 LR(0)_automaton for the expression grammar

 (expr-gram-display) lr-states-fig

 center

 figure



 To construct the canonical_LR(0) collection for a grammar we

 define an augmented_grammar and two functions and

 If is a grammar with start_symbol then the augmented_grammar for is with a new start_symbol

 and production The purpose of this new

 starting production is to indicate to the parser when it should

 stop parsing and announce acceptance of the input That is

 acceptance occurs_when and only when the parser is about to reduce

 by



 Closure of Item Sets



 If is a set of items for a grammar then is

 the set of items constructed from by the two rules



 enumerate



 Initially add every item in to



 If is in and

 is a production then add the item

 to if it is not already

 there Apply this rule until_no more new items can be added to





 enumerate



 Intuitively in

 indicates that at some point in the parsing process we think we

 might next see a substring derivable_from as input The

 substring derivable_from will have a prefix derivable

 from by_applying one of the -productions We therefore add

 items for all the -productions that is if

 is a production we also include

 in



 ex

 closure-ex

 Consider the augmented expression grammar

 center

 tabularr_c l

















 tabular

 center



 If is the set of one item then

 contains the set of items in

 Fig_lr-states-fig



 To_see how the closure is computed is put in

 by rule (1) Since there is an immediately to

 the right of a dot we add the -productions with dots at the

 left ends and Now there is

 a immediately to the right of a dot in the latter item so we

 add and Next the

 to the right of a dot forces us to add and

 but no other items need to be added

 ex



 The closure can be computed as in Fig closure-fig A

 convenient_way to implement the function is to keep a

 boolean array indexed by the nonterminals of such

 that is set to true if and when we add the item

 for each -production





 figurehtfb



 center

 tabularl

 SetOfItems







 repeat



 for ( each item in

 )



 for ( each production of )



 if_( is not in )



 add to



 until_no more items are added to on one round



 return







 tabular

 center



 Computation of

 closure-fig



 figure



 Note_that if one -production is added to the closure of

 with the dot at the left end then all -productions will be

 similarly added to the closure Hence it is not necessary in some

 circumstances actually to list the items

 added to by A list

 of the nonterminals whose productions were so added will

 suffice We divide all the sets of items of interest into two

 classes



 enumerate



 Kernel items the initial item

 and all items whose dots are not at the left

 end



 Nonkernel items all items with their dots at the left end

 except for



 enumerate

 Moreover each set of items of interest is formed_by taking the

 closure of a set of kernel_items the items added in the closure

 can never be kernel_items of course Thus we can represent the

 sets of items we are really interested in with very little storage

 if we throw away all nonkernel items knowing that they could be

 regenerated by the closure process In Fig_lr-states-fig

 nonkernel items are in the shaded part of the box for a state



 The Function



 The second useful function is where is a set of

 items and is a grammar symbol is defined to be

 the closure of the set of all items

 such that

 is in Intuitively the function is used to define the

 transitions in the LR(0)_automaton for a grammar The states of

 the automaton correspond to sets of items and

 specifies the transition from the state for under input



 ex

 goto-ex If is the set of two items

 then contains the

 items



 center

 tabularl





















 tabular

 center



 We computed by_examining for items with

 immediately to the right of the dot is not

 such an item but is We moved the dot over

 the to get and then took the closure of

 this singleton set

 ex



 We are now_ready for the algorithm to construct the canonical

 collection of sets of LR(0)_items for an augmented_grammar

 - the algorithm is shown in Fig set-of-items-fig



 figurehtfb



 center

 tabularl

 void







 repeat



 for ( each set of items in )



 for ( each grammar symbol )



 if_( is not empty and not in )



 add to



 until_no new sets of items are added to on a

 round







 tabular

 center



 Computation of the canonical_collection of sets of LR(0)_items

 set-of-items-fig

 figure



 ex

 The canonical_collection of sets of LR(0)_items for grammar

 (expr-gram-display) and the function are shown in

 Fig_lr-states-fig is encoded by the

 transitions in the figure

 ex



 Use of the LR(0) Automaton



 The central idea_behind Simple LR or SLR_parsing is the

 construction from the grammar of the LR(0)_automaton The states

 of this automaton are the sets of items from the canonical_LR(0)

 collection and the transitions are given by the function

 The LR(0)_automaton for the expression grammar

 (expr-gram-display) appeared earlier in

 Fig_lr-states-fig



 The start_state of the LR(0)_automaton is

 where is

 the start_symbol of the augmented_grammar All states are

 accepting_states We_say state to refer to the state

 corresponding to the set of items



 How can LR(0)_automata help with shift-reduce decisions

 Shift-reduce decisions can be made as_follows Suppose that the

 string of grammar_symbols takes the LR(0)_automaton from

 the start_state 0 to some state Then shift on next_input

 symbol if state has a transition on Otherwise we

 choose to reduce the items in state will tell_us which

 production to use



 The LR-parsing algorithm to be introduced in

 Section lr-alg-subsect uses its stack to keep_track of

 states as_well as grammar_symbols in fact the grammar symbol can

 be recovered from the state so the stack_holds states The next

 example gives a preview of how an LR(0)_automaton and a stack of

 states can be used to make shift-reduce_parsing decisions



 ex

 Figure viable-fig illustrates the actions of a shift-reduce

 parser on input using the LR(0)_automaton in

 Fig_lr-states-fig We use a stack to hold states for

 clarity the grammar_symbols corresponding to the states on the

 stack appear in column SYMBOLS At line_(1) the

 stack_holds the start_state 0 of the automaton the corresponding

 symbol is the bottom-of-stack marker



 figurehtfb

 center

 tabularcllrl

 LINE stack symbols input action



 (1) shift to 5height 12pt depth 0pt width 0pt



 (2) reduce by



 (3) reduce by



 (4) shift to 7



 (5) shift to 5



 (6) reduce by



 (7) reduce by



 (8) reduce by



 (9) accept



 tabular

 center

 The parse of

 viable-fig

 figure



 The next_input symbol is and state 0 has a transition on

 to state 5 We therefore shift At line (2) state 5

 (symbol ) has_been pushed_onto the stack There is no

 transition from state 5 on input so we reduce From item

 in state 5 the reduction is by

 production



 With symbols a reduction is implemented_by popping the body of

 the production from the stack (on line (2) the body is )

 and pushing the head of the production (in this case ) With

 states we pop state 5 for symbol which brings state 0 to

 the top and look for a transition on the head of the

 production In Fig_lr-states-fig state 0 has a transition

 on to state 3 so we push_state 3 with corresponding symbol

 see line_(3)



 As_another example consider line_(5) with state_7 (symbol )

 on top of the stack This state has a transition to state 5 on

 input so we push_state 5 (symbol ) State 5 has no

 transitions so we reduce by When we pop state 5

 for the body state_7 comes to the top of the stack Since

 state_7 has a transition on to state 10 we push_state 10

 (symbol )

 ex



 The LR-Parsing Algorithm

 lr-alg-subsect



 A schematic of an LR_parser is shown in

 Fig lr-parser-model-fig It consists of an input an

 output a stack a driver program and a parsing_table that has

 two_parts ( and ) The driver program is the same

 for all LR_parsers only the parsing_table changes from one parser

 to another The parsing program reads characters from an input

 buffer one at a time Where a shift-reduce_parser would shift a

 symbol an LR_parser shifts a state Each state summarizes

 the information contained in the stack below it



 figurehtfb

 center



 center

 Model of an LR parserlr-parser-model-fig

 figure



 The stack_holds a sequence of states where

 is on top In the SLR_method the stack_holds states from

 the LR(0)_automaton the canonical-LR and LALR methods are

 similar By construction each state has a corresponding grammar

 symbol Recall that states correspond to sets of items and that

 there is a transition from state to state if

 All transitions to state must_be for the same grammar

 symbol Thus each state except the start_state has a

 unique grammar symbol associated_with itfootnoteThe

 converse need not hold that is more_than one state may have the

 same grammar symbol See for example states and in the

 LR(0)_automaton in Fig_lr-states-fig which are both

 entered by transitions on E or states and which

 are both entered by transitions on Tfootnote



 Structure of the LR_Parsing Table



 The parsing_table consists of two_parts a parsing-action function

 and a goto function



 enumerate



 The function takes as arguments a state and a

 terminal (or the input endmarker) The value of

 can have one of four forms

 enumerate

 Shift where is a state

 The action taken by the parser effectively shifts input to the

 stack but uses state to represent

 Reduce

 The action of the parser effectively reduces

 on the top of the stack to head

 Accept

 The parser accepts the input and finishes parsing

 Error

 The parser discovers an error in its input and takes some corrective

 action

 We_shall have more to say about how such error-recovery routines work in

 Sections lr-error-subsect and yacc-error-subsect

 enumerate



 We extend the function defined on sets of items to

 states if then also maps a

 state and a nonterminal to state



 enumerate



 LR-Parser Configurations



 To describe the behavior of an LR_parser it helps to have a notation

 representing the complete state of the parser its stack and the

 remaining_input

 A configuration of an LR_parser is a pair



 center



 center

 where the first component is the stack_contents (top on the right) and

 the second_component is the remaining_input

 This configuration represents the right-sentential_form



 center



 center

 in essentially the same way as a shift-reduce_parser would the

 only_difference is that instead of grammar_symbols the stack

 holds states from which grammar_symbols can be recovered

 That is is the grammar symbol represented_by state

 Note_that the start_state of the parser does_not represent a

 grammar symbol and serves as a bottom-of-stack marker as_well as

 playing an important role in the parse



 Behavior of the LR Parser



 The next move of the parser from the configuration

 above is determined by reading the

 current_input symbol and the state on top of the stack

 and then consulting the entry in the parsing

 action table The configurations resulting after each of the four

 types of move are as_follows



 enumerate



 If shift the parser executes a

 shift_move it shifts the next state onto the stack entering

 the configuration



 center



 center

 The symbol need not be held on the stack since it can be

 recovered from if needed (which in practice it never is) The

 current_input symbol is now



 If reduce then the

 parser executes a reduce move entering the configuration



 center



 center

 where is the length of

 and Here the parser first

 popped state symbols off the stack exposing state

 The parser then pushed the entry for

 onto the stack The current_input symbol is not changed in a

 reduce move For the LR_parsers we_shall construct

 the sequence of grammar_symbols corresponding to the

 states popped_off the stack will always match the right

 side of the reducing production



 The output of an LR_parser is generated after a reduce move by

 executing the semantic_action associated_with the reducing

 production For the time being we_shall assume the output

 consists of just printing the reducing production



 If accept parsing is completed



 If error the parser has discovered an

 error and calls an error_recovery routine

 enumerate



 The LR-parsing algorithm is summarized below All LR_parsers

 behave in this fashion the only_difference between one LR_parser

 and another is the information in the and

 fields of the parsing_table



 alg

 lr parsing-alg

 LR-parsing algorithm



 An input_string and an LR-parsing table with functions

 and for a grammar



 If is in the reduction steps of

 a bottom-up_parse for otherwise

 an error indication



 Initially the parser has on its stack where

 is the initial_state and in the input_buffer The parser

 then executes the program in Fig lr-alg-fig

 alg



 figurehtfb



 center

 tabularl

 let be the first symbol of



 while(1) repeat forever



 let be the state on top of the stack



 if_( shift )



 push_onto the stack



 let be the next_input symbol



 else if_( reduce )



 pop symbols off the stack



 let state now be on top of the stack



 push_onto the stack



 output the production



 else if_( accept ) break parsing is done



 else call error-recovery routine







 tabular

 center



 LR-parsing program

 lr-alg-fig



 figure



 exaction-goto-ex

 Figure action-goto-fig shows the and

 functions of an LR-parsing table for the expression grammar

 (expr-gram-display) repeated_here with the productions

 numbered



 center

 tabularl_l p l_l

 (1) (4)



 (2) (5)



 (3) (6)



 tabular

 center

 The codes for the actions are



 enumerate



 means shift and stack state



 means reduce by the production numbered



 acc

 means accept



 blank means error



 enumerate



 figurehtfb

 center

 tabularcc_c c_c c_l lc c r

 0pt0ptSTATE 6c 3c



 2-11

 -2ptid E_T F



 1pt

 0 s5_s4 1_2 3



 1 s6 acc



 2 r2 s7 r2_r2



 3_r4 r4_r4 r4



 4 s5_s4 8 2_3



 5 r6_r6 r6_r6



 6 s5_s4 9 3



 7 s5_s4 10



 8 s6 s11



 9 r1 s7 r1_r1



 10 r3_r3 r3_r3



 11 r5_r5 r5_r5



 tabular

 Parsing_table for expression grammar

 action-goto-fig

 center

 figure

 Note_that the value of for terminal is found in

 the field connected with the shift_action on input for

 state The field gives for nonterminals

 Although we have not_yet explained how the entries for

 Fig_action-goto-fig were selected we_shall deal_with this

 issue shortly



 On input the sequence of stack and input

 contents is shown in Fig lr-parser-moves-fig Also shown

 for clarity are the sequences of grammar_symbols corresponding to

 the states held on the stack For_example at line_(1) the

 LR_parser is in state 0 the initial_state with no grammar symbol

 and with id the first input_symbol The action in row 0 and

 column id of the action field of Fig_action-goto-fig

 is s5 meaning shift by pushing state 5 That is what has happened

 at line (2) the state symbol 5 has_been pushed_onto the stack

 and id has_been removed_from the input



 Then becomes the current_input symbol and the action of

 state 5 on input is to reduce by One state

 symbol is popped_off the stack State 0 is then exposed Since the

 goto of state 0 on F is 3 state 3 is pushed_onto the stack

 We_now have the configuration in line_(3) Each of the remaining

 moves is determined similarly

 ex



 figurehtfb

 center

 tabularr_l l_r l

 STACK_SYMBOLS INPUT_ACTION



 1pt(1) 0 shift



 (2) 0 5 reduce by



 (3) 0 3 reduce by



 (4) 0 2 shift



 (5) 0 2 7 shift



 (6) 0 2 7 5 reduce by



 (7) 0 2 7 10 reduce by





 (8) 0 2 reduce by



 (9) 0_1 shift



 (10) 0_1 6 shift



 (11) 0_1 6 5 reduce by



 (12) 0_1 6 3 reduce by



 (13) 0_1 6 9 reduce by



 (14) 0_1 accept



 tabular

 Moves of an LR_parser on

 lr-parser-moves-fig

 center

 figure



 Constructing SLR-Parsing Tables



 The SLR_method for constructing parsing_tables is a good starting

 point for studying LR_parsing We_shall refer to the parsing_table

 constructed by this method as an SLR table and to an LR_parser

 using an SLR-parsing table as an SLR_parser The other two methods

 augment the SLR_method with lookahead information



 The SLR_method begins_with LR(0)_items and LR(0)_automata

 introduced in Section bottom-up-sect That is given a

 grammar we augment to produce with a new start

 symbol From we construct the canonical_collection

 of sets of items for together_with the function



 The and entries in the parsing_table are then

 constructed using the following algorithm It requires us to know

 for each nonterminal of a grammar (see

 Section top-down-sect)



 alg

 slr-table-alg

 Constructing an SLR-parsing table



 An_augmented grammar



 The SLR-parsing table functions and for





 enumerate



 Construct the

 collection of sets of LR(0)_items for



 State is constructed from The parsing actions for state

 are determined as_follows



 enumerate



 If is in and

 then set to shift

 Here must_be a terminal



 If is in then set

 to reduce for all

 in () here may not be



 If is in then set

 to accept

 enumerate

 If any conflicting actions result from the above rules we say the

 grammar is not SLR(1) The algorithm_fails to produce a parser in

 this case



 The goto_transitions for state are constructed for all

 nonterminals using the rule If

 then



 All entries not defined by rules (2) and (3) are made error



 The initial_state of the parser is the one constructed from the

 set of items containing



 enumerate

 alg



 The parsing_table consisting of the and

 functions determined by Algorithm slr-table-alg is called

 the SLR(1) table for G An LR_parser using the SLR(1) table

 for is called the SLR(1) parser for and a grammar having

 an SLR(1) parsing_table is said to be SLR(1) We usually

 omit the (1) after the SLR since we_shall not deal here

 with parsers having more_than one symbol of lookahead



 ex

 Let_us construct the SLR table for the augmented expression

 grammar The canonical_collection of sets of LR(0)_items for the

 grammar was shown in Fig_lr-states-fig First consider the

 set of items



 center

 tabularl





























 tabular

 center



 The item

 gives rise to the entry shift 4 and the

 item to the entry

 shift 5 Other items in yield no

 actions Now_consider



 center

 tabularl









 tabular

 center

 The first item yields accept and the second

 yields shift 6 Next consider



 center

 tabularl









 tabular

 center

 Since the first item makes



 center



 reduce

 center

 The second item makes shift 7 Continuing

 in this fashion we obtain the and tables that

 were shown in Fig_action-goto-fig In that figure the

 numbers of productions in reduce actions are the same as the order

 in which they appear in the original grammar

 (expr-gram-display) That is

 is number 1 is 2

 and so on

 ex



 ex

 not-slr-ex

 Every SLR(1) grammar is unambiguous but there are many

 unambiguous_grammars that are not SLR(1) Consider the grammar

 with productions



 displaylvalue-display

 317ptstabularr c_l













 tabular

 (lvalue-display)

 display

 Think of and as standing for l-value and r-value respectively and as an operator indicating

 contents of(As in Section checking-subsect

 an l-value

 designates a location and an r-value is a value that can be

 stored in a location) The canonical_collection of sets of LR(0)

 items for grammar_(lvalue-display) is shown in

 Fig not-slr-fig



 figurehtfb

 center

 tabularr_l p r_l









































































 tabular

 Canonical LR(0)_collection for grammar_(lvalue-display)

 not-slr-fig

 center

 figure



 Consider the set of items The first item in this set makes

 be shift 6 Since

 contains (to see_why consider the derivation )

 the second item sets

 to reduce

 Since there is

 both a shift and a reduce entry in state

 2 has a shiftreduce_conflict on input_symbol



 Grammar (lvalue-display) is not ambiguous This shiftreduce

 conflict arises from the fact that the SLR_parser construction

 method is not powerful enough to remember enough left context to

 decide what action the parser should take on input having

 seen a string reducible to The canonical and LALR methods to

 be discussed next will succeed on a larger collection of

 grammars including grammar_(lvalue-display) Note_however

 that there are unambiguous_grammars for which every LR_parser

 construction method will produce a parsing_action table with

 parsing_action conflicts Fortunately such grammars can generally

 be avoided in programming_language applications

 ex



 Viable Prefixes

 valid-items-subsect



 Why can LR(0)_automata be used to make shift-reduce decisions

 The LR(0)_automaton for a grammar characterizes the strings of

 grammar_symbols that can appear on the stack of a shift-reduce

 parser for the grammar The stack_contents must_be a prefix of a

 right-sentential_form If the stack_holds and the rest of

 the input is then a sequence of reductions will take

 to In terms of derivations



 Not all prefixes of right-sentential_forms can appear on the

 stack however since the parser must not shift past the handle

 For_example suppose



 Then at various times during the parse the stack will hold

 and but it must not hold since is

 a handle which the parser must reduce to before shifting





 The prefixes of right sentential_forms that can appear on the

 stack of a shift-reduce_parser are called viable_prefixes

 They are defined as_follows a viable_prefix is a prefix of a

 right-sentential_form that does_not continue past the right_end of

 the rightmost handle of that sentential_form By this definition

 it is always possible to add terminal_symbols to the end of a

 viable_prefix to obtain a right-sentential_form







 SLR_parsing is based_on the fact that LR(0)_automata recognize

 viable_prefixes We_say item

 is valid for a viable_prefix if there is a

 derivation In

 general an item will be valid for many viable_prefixes



 The fact that is valid for

 tells_us a lot about_whether to shift or reduce

 when we find on the parsing stack In_particular

 if then it suggests that we have not_yet

 shifted the handle onto the stack so shift is our move If

 then it looks as if

 is the handle and we should reduce by this production

 Of_course two valid_items may tell_us to do different things for

 the same viable_prefix Some of these conflicts can be_resolved by

 looking_at the next_input symbol and others can be_resolved by

 the methods of Section ambig-gram-sect but we should not

 suppose that all parsing_action conflicts can be_resolved if the

 LR method is applied to an arbitrary grammar



 We can easily compute the set of valid_items for each viable

 prefix that can appear on the stack of an LR_parser In_fact it

 is a central theorem of LR-parsing theory that the set of valid

 items for a viable_prefix is exactly the set of items

 reached from the initial_state along the path labeled in

 the LR(0)_automaton for the grammar In essence the set of valid

 items embodies all the useful information that can be gleaned from

 the stack While we_shall not prove this theorem here we_shall

 give an example



 ex

 valid-items-ex Let_us consider the augmented expression

 grammar again whose sets of items and function are

 exhibited in Fig_lr-states-fig Clearly the string

 is a viable_prefix of the grammar The automaton of

 Fig_lr-states-fig will be in state_7 after having read

 State 7 contains the items



 center

 tabularl













 tabular

 center

 which are precisely the items_valid for To_see why

 consider the following three rightmost_derivations



 center

 tabularc_c l c_c l c_c l























 tabular

 center

 The first derivation shows the validity of

 the second the validity of and

 the third the validity of It can be shown

 that there are no other valid_items for although we

 shall not prove that fact here

 ex



 Items as States of an NFA A nondeterministic

 finite_automaton for recognizing viable_prefixes can be

 constructed by treating the items themselves as states There is a

 transition from to

 labeled and there is a

 transition from to

 labeled Then

 for set of items (states of ) is exactly the

 -closure of a set of NFA_states defined in

 Section nfa-dfa-subsect Thus gives the

 transition from on symbol in the DFA constructed from

 by the subset_construction Viewed in this way the procedure

 ) in Fig set-of-items-fig is just the subset

 construction itself applied to the NFA with items as states



 exer

 Describe all the viable_prefixes for the following grammars



 itemize



 a)

 The grammar of Exercise more-cfgs-exer(a)



 b)

 The grammar of Exercise_cfg-exer



 c)

 The grammar of Exercise more-cfgs-exer(c)



 itemize

 exer



 sexer

 first-slr-exer

 Construct the SLR sets of items for the (augmented) grammar of

 Exercise_cfg-exer Compute the function for these sets of

 items Show the parsing_table for this grammar Is

 the grammar SLR

 sexer



 exer

 Show the actions of your parsing_table from

 Exercise first-slr-exer on the input

 exer



 exer

 For each of the (augmented) grammars of

 Exercise more-cfgs-exer(a)-(g)



 itemize



 a) Construct the SLR sets of items and their function

 b) Indicate any action_conflicts in your sets of items

 c) Construct the SLR-parsing table if one exists



 itemize

 exer



 exer

 Show that the following grammar



 centertabularl c_l













 tabularcenter

 is LL(1) but not SLR(1)

 exer



 exer

 Show that the following grammar



 centertabularl c_l









 tabularcenter

 is SLR(1) but not LL(1)

 exer



 vhexer

 Consider the family of grammars defined by



 centertabularl c_l l

 for



 for and





 tabularcenter

 Show that



 itemize



 a) has productions

 b) has sets of LR(0)_items

 c) is SLR(1)



 itemize

 What does this analysis say about how large LR_parsers can get



 vhexer



 hexer

 We suggested that individual items could be regarded as states of a

 nondeterministic_finite automaton while sets of valid_items are the

 states of a deterministic_finite automaton (see the box on Items as

 States of an NFA in Section valid-items-subsect) For

 the grammar of Exercise_cfg-exer



 itemize



 a) Draw the transition_diagram (NFA) for the valid_items of this

 grammar according to the rule given in the box cited above



 b) Apply the subset_construction

 (Algorithm subset-cons-alg) to your NFA from part (a) How does

 the resulting DFA compare to the set of LR(0)_items for the grammar



 c)_Show that in all cases the subset_construction

 applied to the NFA that comes_from the valid_items for a grammar

 produces the LR(0) sets of items



 itemize

 hexer



 hexer

 The following is an ambiguous_grammar



 centertabularl_l l









 tabularcenter

 Construct for this grammar its collection of sets of LR(0)_items

 If we try to build an LR-parsing table for the grammar there are

 certain conflicting actions What are they Suppose we tried to use

 the parsing_table by nondeterministically choosing a possible action whenever

 there is a conflict Show all the possible sequences of actions on

 input

 hexer

 Solving Using BDDs



 Overview of the BDD Data Structure





















 BDDs (Binary Decision Diagrams) are a representation of a boolean

 function Given a set of input truth values a BDD

 returns a boolean value true or false Equivalently the BDD can be

 thought of as a set of binary strings of length a binary_string

 is in the set if and only if the value for that string is true



 A BDD is represented as a directed acyclic_graph (DAG) with a single

 root node and two terminal nodes representing the constants one and

 zero Each non-terminal node in the DAG represents an input variable

 and has exactly two_successors - a high_edge and a low_edge A high

 outgoing_edge represents the case when the variable marking the node

 is true and a negative outgoing_edge represents the case when the

 variable marking the node is false Paths in the DAG from the root to

 a terminal node represent that the value of the function on the truth

 values on the path is the value contained in the terminal node To

 evaluate a BDD for a specific input one simply starts at the root

 node and for each node follows the high_edge if the input

 variable is true and the low_edge if the input variable is

 false The value of the terminal node that we reach is the value of

 the BDD for that input



 The variant of BDDs that we use are called reduced ordered binary

 decision_diagrams or ROBDDs Ordered refers to the constraint

 that on all paths through the graph the variables respect a given

 linear order Reduced refers to the

 additional constraints that no two distinct nodes have the same

 variable name and low and high successor and no node has identical

 low and high successors In ROBDDs for any given function

 there is exactly one node which

 represents that function Because this is true for all nodes in the

 ROBDD the ROBDD exhibits shared substructure where many paths

 can share the same nodes This leads to an efficient_representation

 of similar functions



 Many operations can be performed efficiently on BDDs some examples

 are boolean_functions such_as andor and existential quantification

 Furthermore boolean_functions can be_combined with quantification

 in a single operation that is more_efficient than the composition

 of the two One of the most useful combinations is combining the

 boolean and function (set intersection) with existential

 quantification This operation is commonly referred to as a

 relational product and we use it extensively when applying

 inference_rules

 Specifically the relational product is defined as



 relprod(XYV1) (v2v3) v1 (v1 v2) X (v1 v3) Y)





 Most operations on BDDs take time that is linear with_respect to the

 number of nodes in the representations of the input parameters and of

 the result In BDDs as the cardinality of the sets increase the

 representation can actually get smaller because more bits

 become don't-cares



 TODO Talk about encoding sets and relations in BDD



 TODO Talk about variable_ordering and NP-completeness



 Translating Inference_Rules into BDD Operations



 Incrementalization

 The Source Language

 source-lang-sect



 A program in the language consists of a block with optional

 declarations and statements Token basic represents basic

 types



 tabularp6in c_l

 program block



 block 123 decls_stmts 125



 decls decls decl



 decl type id



 type type num basic



 stmts stmts stmt



 tabular

 Treating assignments as statements rather_than as operators

 within expressions simplifies translation



 tabularp6in c lstmt loc bool



 if_( bool ) stmt



 if_( bool ) stmt_else stmt



 while_( bool ) stmt



 do stmt while_( bool )



 break



 block



 loc loc bool id

 tabular

 The productions for expressions handle associativity and

 precedence of operators They use a nonterminal for each level of

 precedence and a nonterminal factor for parenthesized

 expressions identifiers array references and constants



 tabularp6in c_l

 bool bool join join



 join join equality equality



 equality equality rel equality rel rel



 rel expr_expr expr_expr expr_expr



 expr_expr expr



 expr_expr term expr - term_term



 term_term unary term unary_unary



 unary_unary - unary factor



 factor ( bool ) loc num real true false

 tabular

 a

 b

 c



 Specification of Tokens

 spec-token-sect



 Regular_expressions are an important notation for specifying lexeme

 patterns

 While they cannot express all possible patterns they are very effective

 in specifying those types of patterns that we actually need for tokens

 In this_section we_shall study the formal notation for regular

 expressions and in Section lex-sect we_shall see_how these

 expressions are used in a lexical-analyzer generator

 Then Section re-fa-sect shows_how to build the lexical_analyzer

 by converting regular_expressions to automata that perform the

 recognition of the specified tokens



 Strings and Languages

 alph-lang-subsect



 An alphabet is any finite set of symbols

 Typical examples of symbols are letters digits and punctuation

 The set is the binary alphabet

 ASCII is an important

 example of an alphabet it is used in many software systems

 Unicode which includes approximately 100000 characters from

 alphabets around the world is another important example of an

 alphabet



 A string over an alphabet is a finite sequence of symbols drawn

 from that alphabet

 In language theory the terms sentence and word are often used

 as synonyms for string

 The length of a string usually written is the number of

 occurrences of symbols in

 For_example banana is a string of length six

 The empty_string denoted is the string of length

 zero



 Terms for Parts of Strings

 The following string-related terms are commonly_used



 enumerate



 A prefix of string is any string obtained_by removing zero_or

 more symbols from the end of

 For_example ban banana and are prefixes of

 banana



 A suffix of string is any string obtained_by removing zero_or

 more symbols from the beginning of

 For_example nana banana and are suffixes of

 banana



 A substring of is obtained_by deleting any prefix and any

 suffix from

 For_instance banana nan and are

 substrings of banana



 The proper prefixes suffixes and substrings of a string are

 those prefixes suffixes and substrings respectively of that are

 not or not equal to itself



 A subsequence of is any string formed_by deleting zero_or more

 not_necessarily consecutive positions of

 For_example baan is a subsequence of banana



 enumerate





 A language is any countable set of strings over some fixed alphabet

 This definition is very broad

 Abstract languages_like the empty_set or

 the set containing only the empty_string are languages

 under this definition

 So too are the set of all syntactically well-formed C programs and the

 set of all grammatically correct English sentences although the latter

 two languages are difficult to specify exactly

 Note_that the definition of language does_not require that any

 meaning be ascribed to the strings in the language

 Methods for defining the meaning of strings are discussed in

 Chapter_sdt-ch



 If and are strings then the concatenation of and

 denoted is the string formed_by appending to

 For_example if dog and house then doghouse

 The empty_string is the identity under concatenation that is for any

 string



 If we think of concatenation as a product we can define the

 exponentiation of strings as_follows

 Define to be and for all define to be



 Since it follows that

 Then and so on



 Operations on Languages

 lang-ops-subsect



 In lexical analysis the most_important operations on languages are

 union concatenation and closure which are defined formally in

 Fig ops-def-fig

 Union is the familiar operation on sets

 The concatenation of languages is all strings formed_by taking a

 string from the first language and a string from the second language in

 all possible ways and concatenating them

 The (Kleene) closure

 of a language denoted is the set of strings you get by

 concatenating zero_or more times

 Note_that the concatenation of zero times is defined to

 be and inductively is

 Finally the positive closure denoted

 is the same as the Kleene closure but

 without the term

 That is will not be in unless it is in itself



 figurehtfb



 center

 tabularcc

 operation definition and Notation



 Union of and is in or is

 in



 Concatenation of and is in and

 is in



 Kleene closure of



 Positive closure of



 tabular

 center



 Definitions of operations on languages

 ops-def-fig



 figure



 ex

 let-dig-ex

 Let be the set of letters



 and let be the set of digits



 We may think of and in two essentially equivalent ways

 One_way is that and are respectively the alphabets of

 uppercase and lowercase letters and of digits

 The second way is that and are languages all of whose strings

 happen to be of length one

 Here are some other languages that can be constructed from languages

 and

 using the operators of Fig ops-def-fig



 enumerate



 is the set of letters and digits - strictly speaking the

 language with 62 strings of length one each of which strings is either

 one letter or one digit



 is the set of 520 strings of length two each consisting of one

 letter followed_by one digit



 is the set of all 4-letter strings



 is the set of all strings of letters including the

 empty_string



 id-set-item

 is the set of all strings of letters and digits

 beginning with a letter



 is the set of all strings of one or_more digits



 enumerate

 ex



 Regular Expressions

 re-def-subsect



 Suppose we wanted to describe the set of valid C identifiers

 It is almost exactly the language described in item (id-set-item)

 above the only_difference is that the underscore is included among the

 letters



 In Example let-dig-ex we were_able to describe identifiers

 by giving names to sets of letters and digits and using the language

 operators union concatenation and closure

 This process is so useful that a notation called regular

 expressions has come into common use for describing all the languages

 that can be built from these operators applied to the symbols of some

 alphabet

 In this notation if letter is established to stand_for any letter

 or the underscore and digit

 is established to stand_for any digit then we could

 describe the language of C identifiers by



 center



 center

 The vertical bar above means union the parentheses are used to group

 subexpressions the star means zero_or more occurrences of and the

 juxtaposition of letter with the remainder of the expression

 signifies concatenation



 The regular_expressions are built recursively out of smaller regular

 expressions using the rules described below

 Each regular_expression denotes a language which is also

 defined recursively from the languages denoted_by 's subexpressions

 Here are the rules that define the regular_expressions over some

 alphabet and the languages that those expressions denote



 There_are two rules that form the basis



 enumerate



 is a regular_expression and is

 that is the language whose sole member is the empty_string



 If is a symbol in then is a regular_expression and

 that is the language with one string of length one

 with in its one position

 Note_that by convention we use italics for symbols and boldface for

 their corresponding regular expressionfootnoteHowever when

 talking_about specific characters from the ASCII character set we_shall

 generally use teletype font for both the character and its regular

 expressionfootnote



 enumerate



 There_are four parts to the induction whereby larger regular_expressions

 are built from smaller ones

 Suppose and are regular_expressions denoting languages

 and respectively



 enumerate



 is a regular_expression denoting the language





 is a regular_expression denoting the language



 is a regular_expression denoting



 is a regular_expression denoting

 This last rule_says that we can add additional pairs of parentheses

 around expressions without_changing the language they denote



 enumerate



 As defined regular_expressions often contain unnecessary pairs of

 parentheses

 We may drop certain pairs of parentheses if we adopt the conventions

 that



 itemize



 a)

 The unary operator has highest precedence and is left_associative



 b)

 Concatenation has second highest precedence and is left_associative



 c)

 has lowest precedence and is left_associative



 itemize

 Under these conventions for example we may replace the regular

 expression by

 Both expressions denote the set of strings that are either a single

 or are zero_or more 's followed_by one



 ex

 reg-exp-ex

 Let



 enumerate



 The regular_expression denotes the language



 denotes the language of all

 strings of length two over the alphabet

 Another regular_expression for the same language is





 denotes the language consisting

 of all strings of zero_or more 's that is





 denotes the set of all strings consisting of zero_or more

 instances of or that is all strings of 's and

 's

 Another regular_expression for the same language is



 denotes the language that

 is the string and all strings consisting of zero_or more 's and

 ending in



 enumerate

 ex



 A language that can be defined by a regular_expression is

 called a regular set

 If two regular_expressions and denote the same regular set we

 say they are equivalent and write

 For_instance

 There_are a number of algebraic laws for regular_expressions each law

 asserts that expressions of two different forms are equivalent

 Figure re-laws-fig shows some of the algebraic laws that hold for

 arbitrary regular_expressions and



 figurehtfb



 center

 tabularcl

 law description



 is commutative



 is associative



 Concatenation is associative



 Concatenation distributes

 over



 is the identity for

 concatenation



 is guaranteed in a

 closure



 is idempotent



 tabular

 center



 Algebraic laws for regular_expressions

 re-laws-fig



 figure



 Regular Definitions

 reg-def-subsect



 For notational_convenience we may wish to give names to certain regular

 expressions and use those names in subsequent expressions as if the names

 were themselves symbols

 If is an alphabet of basic symbols then a regular

 definition is a sequence of definitions of the form



 center

 tabularl_c l

















 tabular

 center

 where



 enumerate



 Each is a new symbol not in and not the same as

 any other of the 's and



 Each is a regular_expression over the alphabet





 enumerate

 By restricting to and the previously defined 's we

 avoid recursive definitions and we can construct a regular_expression

 over alone for each

 We do so by first replacing uses of in (which cannot use any

 of the 's except for ) then replacing uses of and

 in by and (the substituted) and so on

 Finally in

 we replace each for by the

 substituted version of each of which has only symbols of





 ex

 re-id-ex

 C identifiers are strings of letters digits and underscores

 Here is a regular definition for the language of C identifiers

 We_shall conventionally use italics for the symbols defined in regular

 definitions



 center

 tabularr_c l

 letter A B Z

 a b z



 digit 0_1 9



 id letter letter digit



 tabular

 center

 ex



 ex

 re-numbers-ex

 Unsigned numbers (integer or floating point) are strings such_as 5280 001234

 6336E4 or 189E-4

 The regular definition



 center

 tabularr_c l

 digit 0_1 9



 digits digit digit



 optionalFraction digits



 optionalExponent ( E (_) digits )



 number digits optionalFraction

 optionalExponent



 tabular

 center

 is a precise specification for this set of strings

 That is an optionalFraction is either a decimal point (dot) followed_by

 one or_more digits or it is missing (the empty string)

 An optionalExponent if not missing is the letter E followed_by

 an_optional or sign followed_by one or_more digits

 Note_that at_least one digit must follow the dot so number does

 not match 1 but does match 10

 ex



 Extensions of Regular Expressions

 re-short-subsect



 Since Kleene introduced regular_expressions with the basic

 operators for union concatenation and Kleene closure in the 1950s

 many extensions have_been added to regular_expressions to enhance

 their ability to specify string patterns

 Here we mention a few notational extensions that were first

 incorporated into Unix utilities such_as Lex that are particularly

 useful in the specification lexical_analyzers

 The references to this_chapter contain a discussion of

 some regular-expression variants in use today



 enumerate



 One or_more instances

 The unary postfix operator represents the positive closure

 of a regular_expression and its language

 That is if is a regular_expression then denotes the

 language

 The operator has the same precedence and associativity as the

 operator

 Two useful algebraic laws and

 relate the

 Kleene closure and positive closure



 Zero or one instance

 The unary postfix operator means zero_or one occurrence

 That is is equivalent to or put another_way



 The operator has the same precedence and associativity as

 and



 Character classes

 A regular_expression where the 's are each

 symbols of the alphabet can be replaced_by the shorthand



 More_importantly when form a logical sequence

 eg consecutive uppercase letters lowercase letters or digits we

 can replace them by that is just the first and last

 separated_by a hyphen

 Thus is shorthand for and

 is shorthand for



 enumerate



 ex

 re-short-ex

 Using these shorthands we can rewrite the regular definition of

 Example re-id-ex as



 center

 tabularr_c l

 letter



 digit



 id letter letter digit



 tabular

 center

 The regular definition of Example re-numbers-ex can also be

 simplified



 center

 tabularr_c l

 digit



 digits digit



 number digits ( digits)

 ( E - digits )



 tabular

 center

 ex



 exer

 Consult the language reference manuals to determine

 the sets of characters that form the

 input alphabet (excluding those that may only appear in character

 strings or comments) the lexical form of numerical constants and

 the lexical form of identifiers

 for each of the following languages

 (a) C

 (b) C

 (c) C

 (d) Fortran

 (e) Java

 (f) Lisp

 (g) SQL

 exer



 hexer

 re-list-exer

 Describe the languages denoted_by the following regular_expressions



 itemize



 a)





 b)





 c)





 d)





 e)







 itemize

 hexer



 exer

 In a string of length how many of the following are there



 itemize



 a) Prefixes

 b) Suffixes

 c) Proper prefixes

 d) Substrings

 e) Subsequences



 itemize

 exer



 sexer

 Most_languages are case sensitive so keywords can be written only

 one way and the regular_expressions describing their lexemes are very

 simple However some languages_like SQL are case insensitive so

 a keyword can be written either in lowercase or in uppercase or in

 any mixture of cases Thus the SQL keyword SELECT can

 also be written select Select or sElEcT for

 instance

 Show_how to write a regular_expression for a keyword in a

 case-insensitive language Illustrate the idea by writing the

 expression for select in SQL

 sexer



 hexer

 reg-def-exer

 Write regular definitions for the following languages



 itemize



 a)

 All strings of lowercase letters that contain the five vowels in order



 b)

 All strings of lowercase letters in which the letters are in ascending

 lexicographic order



 c)

 Comments consisting of a string surrounded_by and

 without an intervening unless it is inside double-quotes

 ()



 d)

 All strings of digits with no repeated digits

 Hint Try this problem first with a few digits such_as





 e)

 All strings of digits with at most one repeated digit



 f)

 All strings of 's and 's with an even number of 's and an odd

 number of 's



 g)

 The set of Chess moves in the informal notation such_as p-k4 or

 kbpqn



 h)

 All strings of 's and 's that do_not contain the substring



 i)

 All strings of 's and 's that do_not contain the subsequence





 itemize

 hexer



 exer

 Write character classes for the following sets of characters



 itemize



 a) The first ten letters (up to j) in either upper or lower

 case

 b) The lowercase consonants

 c) The digits in a hexadecimal number (choose either upper or

 lower case for the digits above 9)

 d) The characters that can appear at the end of a legitimate

 English sentence (eg exclamation point)



 itemize

 exer



 The following exercises up to and including

 Exercise lex-re-end-exer discuss the extended regular-expression

 notation from Lex (the lexical-analyzer generator that we_shall

 discuss extensively in Section lex-sect) The extended notation

 is listed in Fig lex-re-fig



 figurehtfb



 center

 tabularlll

 expression matches example



 the one non-operator character a



 character literally 92



 string literally



 any character but newline ab



 beginning of a line 94abc



 end of a line abc



 any one of the characters in string abc



 any one character not in string 94abc



 zero_or more strings matching a



 one or_more strings matching a



 zero_or one a



 between and occurrences of a15



 an followed_by an ab



 an or an ab



 same as (ab)



 when followed_by abc123



 tabular

 center



 Lex regular_expressions

 lex-re-fig



 figure



 sexer

 Note_that these regular_expressions give all of the following symbols

 (operator characters) a

 special meaning



 verbatim

































 94

























 rmnmn

 r

 a













 94

 94

















 94





 94





























 's's

 cc





 ss





























 cc

















 e

 e

 e







 Software Pipelining

 secsp



 As_discussed in the introduction of this_chapter numerical

 applications tend to have much parallelism In_particular they often

 have loops whose iterations are completely independent of one another

 These loops known_as do-all loops are

 particularly attractive from a parallelization perspective because

 their iterations can be executed in parallel to achieve a speed-up

 linear in the number of iterations in the loop Do-all loops with many

 iterations

 have enough parallelism to saturate all the resources on a processor

 It is up to the scheduler to take full advantage of the available

 parallelism This_section describes an algorithm known_as software_pipelining that schedules an entire loop at a time taking

 full advantage of the parallelism across iterations



 Introduction



 We_shall use the do-all loop in Example_exdoall throughout this

 section to explain software_pipelining We first show

 that scheduling across iterations is of great importance because

 there is relatively little parallelism among operations in a single

 iteration Next we show that loop unrolling improves performance by

 overlapping the computation of unrolled iterations However the

 boundary of the unrolled loop still poses as a barrier to code_motion

 and unrolling still leaves a lot of performance on the table The

 technique of software_pipelining on the other_hand overlaps a number

 of consecutive_iterations continually until it runs out of iterations

 This technique allows software_pipelining to produce highly_efficient and

 compact code



 ex

 exdoall

 Here is a typical do-all loop



 verbatim

 for_(i 0 i_n i)

 Di AiBi c

 verbatim

 Iterations in the above loop write to different memory_locations which

 are themselves distinct from any of the locations read Therefore

 there are no memory dependences_between the iterations and all

 iterations can proceed in parallel



 We adopt the following model as our target_machine throughout this

 section In this model



 itemize



 The machine can issue in a single clock one

 load one store one arithmetic operation and one branch operation



 The machine has a loop-back operation of the form



 verbatim

 BL R L

 verbatim

 which decrements register and unless the result is 0 branches to

 location



 Memory operations have an auto-increment addressing mode denoted_by

 after the register

 The register is automatically incremented to point to the next

 consecutive address after each access



 The arithmetic_operations are

 fully pipelined they can be initiated every clock but their results

 are not available until 2 clocks later All other instructions have a

 single-clock latency



 itemize



 If iterations are scheduled one at a time the best schedule we can

 get on our machine_model is shown in Fig figsp1

 Some assumptions about the layout of the data also indicated in

 that figure registers R1_R2 and R3 hold the

 addresses of the beginnings of arrays and register R4 holds the constant and register R10 holds the value

 which has_been computed outside the loop

 The computation is mostly serial taking a total of 7 clocks only the

 loop-back instruction is overlapped with the last operation in the

 iteration

 ex



 figurehtb

 verbatim

 R1_R2 R3 A B D

 R4 c

 R10 n-1



 L LD R5 0(R1)

 LD R6 0(R2)

 MUL R7 R5 R6

 nop

 ADD R8 R7 R4

 nop

 ST 0(R3) R8 BL R10 L

 verbatim



 Locally scheduled code for Example_exdoall

 figsp1

 figure



 In_general we get better hardware utilization by

 unrolling several iterations of a loop

 However doing_so also increases the code size

 which in turn can have a negative impact on overall performance

 Thus we have to compromise picking a number of times to unroll a loop

 that gets most of the performance improvement yet doesn't expand the

 code too_much

 The next example_illustrates the tradeoff



 ex

 exunroll

 While hardly any parallelism can be found in each iteration of the

 loop in Example_exdoall there is plenty of parallelism across

 the iterations Loop unrolling places several iterations of the loop

 in one large basic_block and a simple list-scheduling algorithm can be

 used to schedule the operations to execute in parallel If we unroll

 the loop in our example four times and apply_Algorithm alglist

 to the code we can get the schedule shown in Fig figunroll

 (For simplicity we ignore the details of register_allocation for

 now) The loop executes in 13 clocks or one iteration

 every 325 clocks



 figure



 center

 tabularl_l l_l l_l

 L LD



 LD



 LD



 MUL_LD



 MUL_LD



 ADD_LD



 ADD_LD



 ST MUL_LD



 ST MUL



 ADD



 ADD



 ST



 ST BL (L)



 tabular

 center



 Unrolled code for Example_exdoall

 figunroll



 center

 tabularr_l l_l l_l

 Clock



 1 LD



 2 LD



 3 MUL_LD



 4 LD



 5 MUL_LD



 6 ADD_LD



 7 MUL_LD



 8 ST_ADD LD



 9 MUL_LD



 10 ST_ADD LD



 11 MUL



 12 ST_ADD



 13



 14 ST_ADD



 15



 16 ST



 tabular

 center



 Five unrolled iterations of the code in Example_exdoall

 figsp-unroll

 figure



 A loop unrolled times takes at_least clocks achieving a

 throughput of one iteration every clocks Thus the

 more iterations we unroll the faster the loop runs As

 a fully unrolled loop can execute on average an

 iteration every two_clocks However the more iterations we unroll

 the larger the code gets We certainly cannot afford to unroll all

 the iterations in a loop Unrolling the loop 4 times produces code

 with 13 instructions or 163 of the

 optimum unrolling the loop 8 times produces code with 21

 instructions or 131 of the optimum Conversely if

 we_wish to operate at say only 110 of the optimum we need to

 unroll the loop 25 times which would result in code with 55

 instructions

 ex



 Software Pipelining of Loops



 Software_pipelining provides a convenient_way of getting optimal resource

 usage and compact code at the same time Let_us illustrate the idea

 with our_running example



 ex

 exsp

 In Fig figsp-unroll is the code from

 Example_exdoall unrolled five times (Again we leave

 out the consideration of register usage) Shown in row are

 all the operations issued at clock shown in column are all

 the operations from iteration Note_that every iteration has the

 same schedule relative to its beginning

 and also note_that every iteration is initiated two

 clocks after the preceding one It is easy to see that this schedule

 satisfies all the resource and data-dependence constraints



 We observe that the operations executed at clocks 7 and 8 are the same

 as those executed at clocks 9 and 10 Clocks 7 and 8 execute

 operations from the first four iterations in the original_program

 Clocks 9 and 10 also execute operations from four iterations this time

 from iterations 2 to 5 In_fact we can keep executing this same pair

 of multi-operation

 instructions to get the effect of retiring the oldest iteration and

 adding a new one until we run out of iterations



 Such dynamic

 behavior can be encoded succinctly with the code shown in

 Fig figspcode if we assume that the loop has at_least 4

 iterations Each row in the figure corresponds to one machine

 instruction Lines 7 and 8 form a 2-clock loop which is executed

 times where is the number of iterations in the original

 loop

 ex



 figurehtb



 center

 tabularr_l l_l l_l l

 1) LD



 2) LD



 3) MUL_LD



 4) LD



 5) MUL_LD



 6) ADD_LD



 7) L MUL_LD



 8) ST_ADD LD BL (L)



 9) MUL



 10) ST_ADD



 11)



 12) ST_ADD



 13)



 14) ST



 tabular

 center



 Software-pipelined code for Example_exdoall

 figspcode

 figure



 The technique described above is called software_pipelining because it

 is the software analog of a technique used for scheduling hardware

 pipelines We can think of the schedule executed by each iteration in

 this example as an 8-stage pipeline A new iteration can be started

 on the pipeline every 2 clocks At the beginning there is only one

 iteration in the pipeline As the first iteration proceeds to stage

 three the second iteration starts to execute in the first pipeline stage



 By clock 7 the pipeline is fully filled with the first four iterations

 In the steady_state four consecutive_iterations are executing at the

 same time A new iteration is started as the oldest iteration in the

 pipeline retires When we run out of iterations the pipeline drains

 and all the iterations in the pipeline run to completion The

 sequence of instructions used to fill the pipeline lines 1 through 6 in our

 example is called the prolog lines 7 and 8 are the steady_state and the sequence of instructions used to drain the

 pipeline lines 9 through 14 is called the epilog



 For this example we know that the loop cannot be run at a rate faster

 than 2 clocks per iteration since the machine can only issue one read

 every clock and there are two reads in each iteration The

 software-pipelined_loop above executes in clocks where

 is the number of iterations in the original loop As

 the throughput of the loop approaches the rate of one iteration

 every two_clocks Thus software scheduling unlike unrolling can

 potentially encode the optimal schedule with a very compact code

 sequence



 Note_that the schedule adopted for each individual iteration is not

 the shortest possible Comparison with the locally optimized schedule

 shown in Fig figsp1 shows that a delay is introduced before

 the ADD operation The delay is placed strategically so that

 the schedule can be initiated every two_clocks without resource

 conflicts Had we stuck with the locally compacted schedule the

 initiation_interval would have to be lengthened to 4 clocks to avoid

 resource_conflicts and the throughput rate would be halved This

 example_illustrates an important principle in pipeline scheduling the

 schedule must_be chosen carefully in order to optimize the throughput A

 locally compacted schedule while minimizing the time to complete an

 iteration may result in suboptimal throughput when pipelined



 Register_Allocation and Code_Generation



 Let_us begin by discussing

 register_allocation for the software-pipelined_loop in

 Example exsp



 ex

 exspreg

 In Example exsp the result of the multiply operation in the first

 iteration is produced at clock 3 and used at clock 6 Between these

 clock cycles

 a new result is generated_by the multiply operation in the

 second iteration at clock 5 this value is used at clock 8 The results

 from these two iterations must_be held in different registers to

 prevent them from interfering with each other Since interference

 occurs only between adjacent pairs of iterations it can be avoided

 with the use of two registers one for the odd iterations and one for

 the even iterations Since the code for odd iterations is

 different from that for the even iterations the size of the steady-state

 loop is

 doubled This code can be used

 to execute any loop that has an odd number of iterations greater_than

 or equal to 5



 figurehtfb



 verbatim

 if (N 5)

 N2 3 2 floor((N-3)2)

 else

 N2 0

 for_(i 0 i N2 i)

 Di Ai_Bi c

 for_(i N2 i_N i)

 Di Ai_Bi c

 verbatim



 Source-level unrolling of the loop from Example_exdoall

 do-all2-fig



 figure



 To handle loops that have fewer_than 5 iterations and loops with an

 even number of iterations we generate the code whose source-level

 equivalent is shown in

 Fig do-all2-fig

 The first loop is pipelined as seen in the machine-level equivalent of

 Fig figspmodular

 The second loop of Fig do-all2-fig

 need not be optimized since it can iterate at

 most four times

 ex



 figurehtb



 center

 tabularr_l l_l l_l

 1 LD R50(R1)



 2 LD R60(R2)



 3 LD R50(R1) MUL R7R5R6



 4 LD R60(R2)



 5 LD R50(R1) MUL R9R5R6



 6 LD R60(R2) ADD R8R7R4



 7 L LD R50(R1) MUL R7R5R6



 8 LD R60(R2) ADD R8R9R4 ST 0(R3)R8



 9 LD R50(R1) MUL R9R5R6



 10 LD R60(R2) ADD R8R7R4 ST 0(R3)R8

 BL R10L



 11 MUL R7R5R6



 12 ADD R8R9R4 ST 0(R3)R8



 13



 14 ADD R8R7R4 ST 0(R3)R8



 15



 16 ST 0(R3)R8



 tabular

 center



 Code after software_pipelining and register_allocation in

 Example exspreg

 figspmodular

 figure



 Do-Across Loops



 Software_pipelining can also be applied to loops whose iterations

 share data_dependences Such loops are known_as

 do-across loops



 ex

 exdoacross

 The code



 verbatim

 for_(i 0 i_n i)

 sum sum Ai

 Bi Ai b



 verbatim

 has a data_dependence between consecutive_iterations because the previous

 value of sum is added to to create a new

 value of sum It is possible to execute the summation in



 time if the machine can deliver sufficient parallelism

 but for the sake of this discussion we simply assume that all

 the sequential dependences must_be obeyed and that the additions must

 be performed in the original_sequential order Because our assumed

 machine_model takes two_clocks to complete an

 ADD the loop cannot execute faster_than one iteration every two

 clocks Giving the machine

 more adders or multipliers will not

 make this loop run any faster The throughput of do-across loops like

 this one is limited by the chain of dependences across iterations



 figurehtfb



 center

 tabularl_l l

 R1 A R2 B



 R3 sum



 R4 b



 R10 n-1







 L LD R5 0(R1)



 MUL R6 R5 R4



 ADD_R3 R3 R5



 ST R6 0(R2) BL R10 L



 tabular

 center



 center

 (a) The best locally compacted schedule

 center



 center

 tabularl_l l_l

 R1 A R2 B



 R3 sum



 R4 b



 R10 n-2







 LD R5 0(R1)



 MUL R6 R5 R4



 L ADD_R3 R3 R5 LD R5 0(R1)



 ST R6 0(R2) MUL R6 R5 R4 BL R10 L



 ADD_R3 R3 R5



 ST R6 0(R2)



 tabular

 center



 center

 (b) The software-pipelined version

 center



 Software-pipelining of a do-across loop

 add-mult-fig



 figure



 The best locally compacted schedule for each iteration is shown in

 Fig add-mult-fig(a) and the software-pipelined code is in

 Fig add-mult-fig(b)

 This software-pipelined_loop starts an iteration every two_clocks and

 thus operates at the optimal rate

 ex



 Goals and Constraints of Software Pipelining

 secspgoals



 The primary goal of software_pipelining is to maximize the throughput

 of a long-running loop A secondary goal is to keep the size of the

 code generated reasonably small In other_words the

 software-pipelined_loop should have a small steady_state of the

 pipeline We can

 achieve a small steady_state by requiring that the relative schedule

 of each iteration be the same and that the iterations be

 initiated at a constant interval Since the throughput of the loop is

 simply the inverse of the initiation_interval the objective of

 software_pipelining is to minimize this interval



 A software-pipeline schedule for a data-dependence_graph

 can be specified_by



 enumerate



 An initiation_interval and



 A relative schedule that

 specifies for each operation

 when that operation is executed relative to the start of the

 iteration to which it belongs



 enumerate

 Thus an operation in the th_iteration

 counting from 0 is executed at clock Like all the other

 scheduling problems software_pipelining has two kinds of constraints

 resources and data_dependences We discuss each kind in detail below



 Modular Resource Reservation



 Let a machine's resources be represented_by

 where is the number of units of the th

 kind of resource available If an iteration of a loop requires units of

 resource then the average initiation_interval of a pipelined loop

 is at_least clock cycles

 Software_pipelining requires that the initiation intervals between any

 pair of iterations have a constant value Thus the initiation

 interval must have at_least



 clocks If is less_than 1 it is useful to unroll the

 source code a small number of times



 ex

 Let_us return to our software-pipelined_loop shown in

 Fig figspcode Recall that the target_machine can issue one

 load one arithmetic operation one store and one loop-back branch per

 clock Since the loop has two loads two arithmetic_operations and

 one store operation the minimum_initiation interval based_on resource

 constraints is 2 clocks



 figurehtb

 fileuullmanalsuch10figssp-resourceeps

 Resource requirements of four consecutive_iterations from the code

 in Example exunroll

 figsp-resource

 figure



 Figure figsp-resource shows the resource requirements of

 four consecutive_iterations across time More resources are used as

 more iterations get initiated culminating in maximum resource

 commitment in the steady_state Let be the resource-reservation

 table representing the commitment of one iteration and let

 represent the commitment of the steady_state

 combines the commitment from four consecutive

 iterations started clocks apart The commitment of row 0

 in the table corresponds to the sum of the resources

 committed in and Similarly the commitment

 of row 1 in the table corresponds to the sum of the resources

 committed in and That is the resources

 committed in the th_row in the steady_state are given by



 RTsi t (t 2) i RTt



 We refer to the resource-reservation_table representing the steady

 state as the modular_resource-reservation table of the pipelined

 loop



 To check if the software-pipeline schedule has any resource_conflicts

 we can simply check the commitment of the modular_resource-reservation

 table Surely if the commitment in the steady_state can be satisfied

 so can the commitments in the prolog and epilog the portions of code

 before and after the steady-state loop

 ex



 In_general given an initiation_interval and a

 resource-reservation_table of an iteration the pipelined

 schedule has no resource_conflicts on a machine with resource vector

 if and only if

 for all



 Data-Dependence Constraints



 Data_dependences in software_pipelining are different from those we

 have encountered so_far because they can form cycles An operation

 may depend_on the result of the same operation from a previous

 iteration It is no_longer adequate to label a dependence edge by

 just the delay we also need to distinguish_between instances of the

 same operation in different iterations We label a

 dependence edge with label

 if operation in iteration must_be delayed by

 at_least clocks after the execution of operation in

 iteration Let a function from the nodes of the

 data-dependence_graph to integers be the software

 pipeline schedule and let be the initiation_interval target Then



 (T) S(n2) - S(n1) d



 The iteration difference must_be nonnegative Moreover

 given a cycle of data-dependence edges at_least one of the edges has

 a positive iteration difference



 ex

 exdoacross-dep

 Consider the following loop and suppose we do_not know the values of

 and



 verbatim

 for_(i 0 i_n i)

 (p) (q) c

 verbatim

 We must assume that any pair of (p) and

 (q) accesses may refer to the same memory_location Thus all the

 reads and writes must execute in the original_sequential order

 Assuming that the target_machine has the same characteristics as that

 described in Example_exdoall the data-dependence edges

 for this code are as shown in Fig figdoacross-dep

 Note_however that we ignore the loop-control instructions that would

 have to be present either computing and testing or doing the test

 based_on the value of R1 or R2

 ex





 LD R4 0(R1)

 ADD R5 R4 R3

 nop

 ST O(R2) R5



 figurehtb

 verbatim

 R1_R2 q p

 R3 c

 verbatim

 fileuullmanalsuch10figssp-depeps

 Data-dependence graph for Example exdoacross-dep

 figdoacross-dep

 figure



 The iteration difference_between related operations can be greater

 than one as shown in the following example



 verbatim

 for_(i 2 i_n i)

 Ai_Bi Ai-2

 verbatim

 Here the value written in iteration is used two iterations later

 The dependence edge between the store of and the load of

 thus has a difference of 2 iterations



 The presence of data-dependence cycles in a loop imposes yet another

 limit on its execution throughput For_example the data-dependence

 cycle in Fig figdoacross-dep imposes a delay of 4 clock

 ticks between load operations from consecutive_iterations That is

 loops cannot execute at a rate faster_than one iteration every 4

 clocks



 The initiation_interval of a pipelined loop is no smaller_than



 c a cycle in G

 e in c dee in c e



 clocks



 In summary

 the initiation_interval of each software-pipelined_loop is bounded

 by the resource usage in each iteration Namely the initiation

 interval must_be no smaller_than the ratio of units needed of each

 resource and the units available on the machine In_addition if the

 loops have data-dependence cycles then the initiation_interval is

 further constrained by the sum of the delays in the cycle divided by

 the sum of the iteration differences The largest of these quantities

 defines a lower_bound on the initiation_interval



 A Software-Pipelining Algorithm



 The goal of software_pipelining is to find a schedule with the

 smallest possible initiation_interval The problem is NP-complete

 and can be formulated as an integer-linear-programming problem We

 have shown that if we know what the minimum_initiation interval is

 the scheduling algorithm can avoid resource_conflicts by using the

 modular_resource-reservation table in placing each operation But we

 do_not know what the minimum_initiation interval is until we can find

 a schedule How do we resolve this circularity



 We know that the initiation_interval must_be greater_than the bound

 computed from a loop's resource requirement and dependence cycles as

 discussed_above If we can find a schedule meeting this bound we

 have found the optimal schedule If we fail to find such a schedule

 we can try again with larger initiation intervals until a schedule

 is found

 Note_that if heuristics rather_than exhaustive search

 are used this process may not

 find the optimal schedule



 Whether we can find a schedule near the lower_bound depends_on

 properties of the data-dependence_graph and the architecture of the

 target_machine We can easily find the optimal schedule if the

 dependence graph is acyclic and if every machine_instruction needs

 only one_unit of one resource It is also easy to find a schedule

 close to the lower_bound if there are more hardware resources than

 can be used by graphs with dependence cycles For such cases it is

 advisable to start with the lower_bound as the initial

 initiation-interval target then keep increasing the target by just

 one clock with each scheduling attempt Another possibility

 is to find the initiation_interval using a binary search We can use

 as an upper_bound on the initiation_interval the length of the schedule

 for one iteration produced_by list scheduling



 Scheduling Acyclic Data-Dependence Graphs



 For_simplicity we assume for now that the loop to be software

 pipelined contains only one basic_block This assumption will be

 relaxed in Section secspcond



 alg Software_pipelining an acyclic dependence graph

 algsp



 A machine-resource vector where

 is the number of units available of the th kind of resource and a

 data-dependence_graph Each operation in is

 labeled with its resource-reservation_table each edge

 in is labeled with

 indicating that must execute no earlier than clocks after

 node from the th preceding iteration



 A software-pipelined schedule and an

 initiation_interval



 Execute the program in Fig figspalg

 alg



 figurehtb

 center

 tabularl

 main()









 for ( until all nodes in are scheduled)



 an empty reservation table with rows



 for_(each in in prioritized topological order)







 for ()



 if (NodeScheduled()) break



 if_( cannot be scheduled in ) break



















 NodeScheduled()







 for_(each row in )







 if (for all )











 return true







 else_return false







 tabular

 center



 Software-pipelining algorithm for acyclic graphs

 figspalg

 figure



 Algorithm_algsp software pipelines acyclic data-dependence

 graphs The algorithm first finds a bound_on the initiation_interval

 based_on the resource requirements of the operations in the graph It

 then attempts to find a software-pipelined schedule starting_with

 as the target initiation_interval The algorithm repeats with

 increasingly larger initiation intervals if it fails to find a

 schedule



 The algorithm uses a list-scheduling approach in each attempt It

 uses a modular_resource-reservation to keep_track of the resource

 commitment in the steady_state Operations are scheduled in

 topological_order so that the data_dependences can always be satisfied

 by delaying operations To schedule an operation it first

 finds a lower_bound according to the data-dependence

 constraints It then invokes NodeScheduled to check for

 possible resource_conflicts in the steady_state If there is a

 resource conflict the algorithm tries to schedule the operation in

 the next clock If the operation is found to conflict for

 consecutive clocks because of the modular nature of resource-conflict

 detection further attempts are guaranteed to be futile At that

 point the algorithm considers the attempt a failure and another

 initiation_interval is tried



 The heuristics of scheduling operations as_soon as possible tends to

 minimize the length of the schedule for an iteration Scheduling an

 instruction as early as possible however can lengthen the

 lifetimes of some variables For_example loads of data tend to be

 scheduled early sometimes long before they are used One simple

 heuristic is to schedule the dependence graph backwards because

 there are usually more loads than stores



 Scheduling Cyclic Dependence Graphs

 cyc-dep-subsect



 Dependence cycles complicate software_pipelining significantly When

 scheduling operations in an acyclic_graph in topological_order data

 dependences with scheduled operations can impose only a lower_bound on

 the placement of each operation As a result it is always possible

 to satisfy the data-dependence constraints by delaying operations

 The concept of topological_order does_not apply to cyclic graphs

 In_fact given a pair of operations sharing a cycle placing one

 operation will impose both a lower and upper_bound on the placement of

 the second



 Let and be two operations in a dependence cycle

 be a software-pipeline schedule and be the

 initiation_interval for the schedule A dependence edge

 with label imposes the following

 constraint on and



 (1 T) S(n2) - S(n1) d1



 Similarly a dependence edge with label

 imposes constraint



 (2 T) S(n1) - S(n2) d2



 Thus



 S(n1) d1 - (1 T) S(n2) S(n1) - d2

 (2 T)





 A strongly_connected component (SCC) in a graph is a set of

 nodes where every node in the component can be reached by every other

 node in the component Scheduling one node in an SCC

 will bound the time of every other node in the component both from above

 and from below

 Transitively if there_exists a path leading_from

 to then



 equation

 cycle-eq

 S(n2) - S(n1) e in p (de - (e T))

 equation



 Observe that



 itemize



 Around any cycle the sum of the 's must_be

 positive If it were 0 or negative then it would say that an operation

 in the cycle either had to precede itself or be executed at the same

 clock for all iterations



 The schedule of

 operations within an iteration is the same for all iterations that requirement

 is essentially the meaning of a software pipeline

 As a result the sum of the delays (second components of edge labels in

 a data-dependence graph) around a cycle is a lower_bound on the initiation

 interval



 itemize

 From these two points if path is a cycle then for any feasible

 initiation_interval the value of the right_side of

 Equation (cycle-eq) is negative or zero

 As a result the strongest constraints on the placement

 of nodes is obtained from the simple paths - those paths that

 contain no cycles



 Thus for each feasible computing the transitive effect of data

 dependences on each pair of nodes is equivalent to finding the length of

 the longest

 simple path from the first node to the second

 Moreover since cycles cannot increase the length of a path we can use

 a simple dynamic-programming algorithm to find the longest paths without

 the simple-path requirement and be_sure that the resulting lengths

 will also be the lengths of the longest_simple paths (see

 Exercise simple-path-exer)



 figurehtb

 fileuullmanalsuch10figssp-cycleeps

 Dependence graph and resource requirement in Example_exspscc

 figsp-cycle

 figure



 ex

 exspscc

 Figure figsp-cycle shows a data-dependence_graph with

 four nodes Attached to each node

 is its resource-reservation_table attached to each edge is its

 iteration difference and delay Assume for this example

 that the target_machine has one_unit of each kind of resource Since

 there are three uses of the first resource and two of the second the

 initiation_interval must_be no less_than 3 clocks There_are two

 SCC's in this graph the first is a trivial

 component consisting of the node alone and the second consists of

 nodes and The longest cycle

 has a total delay of 3 clocks

 connecting nodes that are 1 iteration apart Thus the lower_bound on

 the

 initiation_interval provided by data-dependence cycle constraints is also

 3 clocks



 Placing any of or in a schedule constrains all the other

 nodes in the component Let be the initiation_interval

 Figure figsp-trans shows the transitive dependences

 Part (a) shows the delay and the iteration difference

 for each edge The delay is represented directly but is

 represented_by adding to the delay the value



 Figure figsp-trans(b) shows the length of the longest_simple

 path between two nodes when such a path exists its entries are the sums

 of the expressions given by Fig figsp-trans(a) for each edge

 along the path Then in (c) and (d) we see the expressions of (b)

 with the two relevant values of that is 3 and 4 substituted for



 The difference_between the schedule of two nodes must_be

 no less_than the value given in entry in each of the

 tables (c) or (d) depending_on the value of chosen



 For_instance consider the entry in Fig figsp-trans

 for the longest (simple) path from

 to which is The longest_simple path from to is

 The total delay is 2 along this path and the

 sum of the 's is 1 representing the fact that the iteration

 number must increase by 1 Since is the time by which each

 iteration follows the previous the clock at which must_be scheduled

 is at_least clocks after the clock at which is scheduled

 Since is at_least 3 we are really saying that may be scheduled

 clocks before or later than that clock but not earlier



 Notice_that considering nonsimple paths from to does_not produce

 a stronger constraint We can add to the path any

 number of iterations of the cycle involving and If we add

 such cycles we get a path length of since the total delay

 along the path is 3 and the sum of the 's is 1 Since

 this length can never exceed ie the strongest lower_bound on

 the clock of relative to the clock of is the bound we get by

 considering the longest_simple path



 figurehtb

 fileuullmanalsuch10figssp-transeps

 Transitive dependences in Example_exspscc

 figsp-trans

 figure



 For_example from entries and we see that



 arrayrl

 S(c)-S(b) 1



 S(b)-S(c) 2 - T



 array



 That is



 S(b) 1 S(c) S(b) - 2 T



 If



 S(b) 1 S(c) S(b) 1



 Put equivalently must_be scheduled one clock after

 If however



 S(b) 1 S(c) S(b) 2



 That is is scheduled one or two_clocks after



 Given the all-points longest path information we can easily compute

 the range where it is legal to place a node due to data_dependences

 We see that there is no slack in the case when and the slack

 increases as increases

 ex





 alg Software_pipelining

 algsp-cyclic



 A machine-resource vector where

 is the number of units available of the th kind of resource and a

 data-dependence_graph Each operation in is

 labeled with its resource-reservation_table each edge

 in is labeled with

 indicating that must execute no earlier than clocks after

 node from the th preceding iteration



 figure



 center

 tabularl

 main()





















 for ( or until all SCC's in are scheduled)



 an empty reservation table with rows







 for_(each SCC in in prioritized topological

 order)



 for (all in )









 first some such that is a minimum







 for (_)



 if (SccScheduled ()) break



 if_( cannot be scheduled in ) break



















 SccScheduled()







 if (not NodeScheduled ()) return false



 for_(each remaining in in prioritized



 topological_order of edges in )















 for (_)



 if NodeScheduled break



 if_( cannot be scheduled in )_return false











 return true







 tabular

 center



 A software-pipelining algorithm for cyclic dependence graphs

 figspalg2

 figure



 A software-pipelined schedule and an

 initiation_interval



 Execute the program in Fig figspalg2

 alg



 Algorithm_algsp-cyclic has a high-level structure similar to that

 of

 Algorithm_algsp which only handles acyclic graphs The

 minimum_initiation interval in this case is bounded not just by

 resource requirements but also by the data-dependence cycles in the

 graph The graph is scheduled one strongly_connected component at a

 time By treating each strongly_connected component as a unit edges

 between strongly_connected components necessarily form an acyclic

 graph While the top-level loop in Algorithm_algsp schedules

 nodes in the graph in topological_order the top-level loop in

 Algorithm_algsp-cyclic schedules strongly_connected

 components in topological_order As before if the algorithm_fails to

 schedule all the components then a larger initiation_interval is

 tried Note_that Algorithm_algsp-cyclic behaves exactly like

 Algorithm_algsp if given an acyclic data-dependence_graph



 Algorithm_algsp-cyclic computes two more sets of edges is

 the set of all edges whose iteration difference is 0 is the

 all-points longest-path edges

 That is for each pair of nodes there is an edge in

 whose associated distance is the length of the longest_simple path

 from to provided that there is at_least one path from to



 is computed for each value of

 the initiation-interval target It is also possible to perform

 this computation just once with a symbolic value of and then

 substitute for in each iteration as we did in

 Example_exspscc



 Algorithm_algsp-cyclic uses backtracking

 If it fails to schedule a SCC it

 tries to reschedule the entire SCC a clock later

 These scheduling attempts continue

 for up to clocks Backtracking is important because as shown in

 Example_exspscc the placement of the first node in an SCC can

 fully dictate the schedule of all other nodes If the schedule

 happens not to fit with the schedule created thus far the attempt

 fails



 To schedule a SCC the algorithm determines the earliest time each

 node in the component can be scheduled satisfying the transitive data

 dependences in It then picks the one with the earliest start

 time as the first node to schedule The algorithm then invokes

 SccScheduled to try to schedule the component at the earliest

 start time The algorithm makes at most attempts with successively

 greater start times If it fails then the algorithm tries another

 initiation_interval



 The SccScheduled algorithm resembles Algorithm_algsp but

 has three major differences

 enumerate

 The goal of SccScheduled is to schedule the strongly_connected

 component at the given time slot If the first node of the

 strongly_connected component cannot be scheduled at SccScheduled returns false The main function can invoke SccScheduled again with a later time slot if that is desired



 The nodes in the strongly_connected component are scheduled in

 topological_order based_on the edges in Because the iteration

 differences on all the edges in are 0 these edges do_not cross

 any iteration boundaries and cannot form cycles (Edges that cross

 iteration boundaries are known_as loop carried) Only

 loop-carried dependences place upper_bounds on where operations can be

 scheduled So this scheduling order along with the strategy of

 scheduling each operation as early as possible maximizes the ranges

 in which subsequent nodes can be scheduled



 For strongly_connected components dependences impose both a lower and

 upper_bound on the range in which a node can be scheduled SccScheduled computes these ranges and uses them to further limit the

 scheduling attempts

 enumerate



 ex

 exspscc2

 Let_us apply_Algorithm algsp-cyclic to the cyclic data-dependence

 graph in Example_exspscc

 The algorithm first computes that the bound_on the initiation_interval

 for this example is 3 clocks We note_that it is not possible to meet

 this lower_bound When the initiation_interval is 3 the

 transitive dependences in Fig figsp-trans dictate that

 Scheduling nodes and two_clocks apart will

 produce a conflict in a modular_resource-reservation table of length 3



 figurehtb



 arraycccccc

 Attempt

 arrayc Initiation

 Interval array

 Node Range Schedule

 arrayc Modular

 Resource



 Reservation

 array





 1 T 3

 arrayc

 a



 b



 c



 array



 arrayc

 (0)



 (2)



 (33)



 array



 arrayc

 0



 2



 -



 array

 fileuullmanalsuch10figssp-try1epsscale5



 2 T 3

 arrayc

 a



 b



 c



 d



 array



 arrayc

 (0)



 (2)



 (44)



 (55)



 array



 arrayc

 0



 3



 4



 -



 array

 fileuullmanalsuch10figssp-try2epsscale5



 3 T 3

 arrayc

 a



 b



 c



 d



 array



 arrayc

 (0)



 (2)



 (55)



 (66)



 array



 arrayc

 0



 4



 5



 -



 array

 fileuullmanalsuch10figssp-try2bepsscale5



 4 T 4

 arrayc

 a



 b



 c



 d



 array



 arrayc

 (0)



 (2)



 (34)



 (45)



 array



 arrayc

 0



 2



 3



 -



 array

 fileuullmanalsuch10figssp-try3epsscale5



 5 T 4

 arrayc

 a



 b



 c



 d



 array



 arrayc

 (0)



 (2)



 (45)



 (66)



 array



 arrayc

 0



 3



 5



 -



 array

 fileuullmanalsuch10figssp-try4epsscale5



 6 T 4

 arrayc

 a



 b



 c



 d



 array



 arrayc

 (0)



 (2)



 (56)



 (67)



 array



 arrayc

 0



 4



 5



 6



 array

 fileuullmanalsuch10figssp-try5epsscale5



 array



 Behavior of Algorithm_algsp-cyclic on Example_exspscc

 figbacktrack

 figure



 Figure figbacktrack shows_how Algorithm_algsp-cyclic

 behaves with this example It first tries to find a schedule with a

 3-clock initiation_interval The attempt starts by scheduling nodes

 and as early as possible However once node is placed in

 clock node can only be placed_at clock 3 which conflicts

 with the resource usage of node

 That is and both need the first resource at clocks that have a

 remainder of 0 modulo 3



 The algorithm backtracks and

 tries to schedule the strongly_connected component

 a clock later

 This time node is scheduled at clock 3 and node is scheduled

 successfully at clock 4 Node however cannot be scheduled in

 clock 5

 That is both and need the second resource at clocks that have a

 remainder of 0 modulo 3

 Note_that it is just a coincidence that the two conflicts discovered so

 far are at clocks with a remainder of 0 modulo 3 the conflict might

 have occurred at clocks with remainder 1 or 2 in another example



 The algorithm repeats by delaying the start of the SCC

 by one more clock But as discussed earlier this

 SCC can never be scheduled with an initiation

 interval of 3 clocks so the attempt is bound to fail At this point

 the algorithm gives up and tries to find a schedule with an initiation

 interval of 4 clocks The algorithm eventually finds the optimal

 schedule on its sixth attempt

 ex



 Improvements to the Pipelining Algorithms



 Algorithm_algsp-cyclic is a rather simple algorithm although it

 has

 been_found to work well on actual machine targets The important

 elements in this algorithm are



 enumerate



 The use of a modular_resource-reservation table to check for resource

 conflicts in the steady_state



 The need to compute the transitive dependence relations to find the

 legal range in which a node can be scheduled in the presence of

 dependence cycles



 Backtracking is useful and nodes on critical cycles (cycles that

 place the highest lower_bound on the initiation_interval ) must_be

 rescheduled together because there is no slack between them



 enumerate



 There_are many_ways to improve Algorithm_algsp-cyclic For

 instance the algorithm takes a while to realize that a

 3-clock initiation_interval is infeasible for the simple

 Example exspscc2 We

 can schedule the strongly_connected components independently first to

 determine if the initiation_interval is feasible for each component



 We can also modify the order in which the nodes are scheduled The

 order used in Algorithm_algsp-cyclic has a few disadvantages

 First because nontrivial

 SCC's are harder to schedule

 it is desirable to schedule them first Second some of the registers

 may have unnecessarily long lifetimes It is desirable to pull the

 definitions closer to the uses One possibility is to start with

 scheduling strongly_connected components with critical cycles first

 then extend the schedule on both ends





 Are There Alternatives to Heuristics

 We can

 formulate the problem of simultaneously finding an optimal software

 pipeline schedule and register assignment as an integer-linear-programming

 problem While many integer_linear programs can be_solved

 quickly some of them can take an exorbitant amount of time To use an

 integer-linear-programming solver in a compiler we must_be able to

 abort the procedure if it does_not complete within some preset

 limit



 Such an approach has_been tried on a target_machine (the SGI

 R8000) empirically and it was found that the solver could find the

 optimal solution for a large percentage of the programs in the

 experiment within a reasonable amount of time It turned out that the

 schedules produced using a heuristic approach were also close to

 optimal The results suggest that at_least for that machine it does

 not make sense to use the integer-linear-programming approach especially

 from a software engineering perspective Because the integer-linear

 solver may not finish it is still necessary to implement some kind of

 a heuristic scheduler in the compiler Once such a heuristic

 scheduler is in place there is little incentive to implement a

 scheduler based_on integer programming techniques as_well



 Modular Variable Expansion



 A scalar_variable is said to be privatizable in a loop if its

 live range falls within an iteration of the loop In other_words a

 privatizable variable must not be live upon either entry or exit of

 any iteration These variables are so named because different

 processors executing different iterations in a loop can have their own

 private copies and thus not interfere with one another



 Variable expansion refers to the transformation of converting a

 privatizable scalar_variable into an array and having the th

 iteration of the loop read and write the th_element This

 transformation eliminates the antidependence constraints between

 reads in one iteration and writes in the subsequent_iterations as

 well_as output dependences_between writes from different iterations

 If all loop-carried dependences can be_eliminated all the iterations

 in the loop can be executed in parallel



 Eliminating loop-carried dependences and thus eliminating cycles in the

 data-dependence_graph can greatly improve the effectiveness of

 software_pipelining As illustrated by Example exspreg we

 need not expand a privatizable variable fully by the number of

 iterations in the loop Only a small number of iterations can be

 executing at a time and privatizable variables may simultaneously be

 live in an even smaller number of iterations The same storage can

 thus be reused to hold variables with nonoverlapping lifetimes

 More_specifically if the lifetime of a register is clocks and the

 initiation_interval is then only

 values can be live at any one point We can

 allocate registers to the variable with the variable in the th

 iteration using the th register We refer to this

 transformation as modular variable expansion



 alg

 algsp-mod

 Software_pipelining with modular variable expansion



 A data-dependence_graph and a machine-resource description



 Two loops one software pipelined and one unpipelined



 enumerate



 Remove the loop-carried antidependences and output dependences

 associated_with privatizable variables from the data-dependence_graph



 Software-pipeline the resulting dependence graph using

 Algorithm_algsp-cyclic Let be the initiation_interval for

 which a schedule is found and be the length of the schedule for

 one iteration



 From the resulting schedule compute the minimum number of registers

 needed by each privatizable variable Let



 Generate two loops a software-pipelined_loop and an unpipelined loop

 The software-pipelined_loop has



 LT Q - 1



 copies of the iterations placed

 clocks apart It has a prolog with



 (LT - 1)T



 instructions a steady_state with instructions and an epilog

 of instructions Insert a loop-back instruction that branches

 from the bottom of the steady_state to the top of the steady_state



 The number of registers assigned to privatizable variable is



 q'v

 arrayll

 qv if Q qv 0



 Q otherwise

 array



 The variable in iteration

 uses the th register assigned



 Let be the variable representing the number of iterations in the

 source loop The software-pipelined_loop is executed if



 n LT Q - 1





 The number of times the loop-back branch is taken is



 n1 n - LT 1Q



 Thus the number of source iterations executed by

 the software-pipelined_loop is



 n2

 arraycl

 LT - 1 Qn1

 if n LT Q - 1



 0 otherwise

 array







 The number of iterations executed by the unpipelined loop is



 enumerate

 alg



 ex

 exmod

 For the software-pipelined_loop in Fig figspmodular

 and The software-pipelined_loop has 7 copies of the

 iterations with the prolog steady_state and epilog having 6

 4 and 6 instructions respectively Let be the number of

 iterations in the source loop The software-pipelined_loop is

 executed if in which case the loop-back branch is taken



 n - 32

 times and the software-pipelined_loop is responsible_for



 3 2 n - 32

 of the iterations in the source loop

 ex



 Modular expansion increases the size of the steady_state by a

 factor of Despite this increase the code generated_by

 Algorithm algsp-mod is still fairly compact In the worst

 case the software-pipelined_loop would take three times as many

 instructions as that of the schedule for one iteration Roughly

 together_with the extra loop generated to handle the left-over

 iterations the total code size is about four times the original This

 technique is usually applied to tight inner_loops so this increase is

 reasonable



 Algorithm algsp-mod minimizes code expansion at the expense of

 using more registers We can reduce register usage by generating more

 code We can use the minimum registers for each variable

 if we use a steady_state with



 T LCMv qv



 instructions

 Here represents the operation of taking

 the least common multiple of all the 's as ranges over all the

 privatizable variables (ie the smallest integer that is an_integer

 multiple of all the 's)

 Unfortunately the least common multiple can be quite

 large even for a few small 's



 Conditional Statements

 secspcond



 If predicated instructions are available we can convert

 control-dependent instructions into predicated ones Predicated

 instructions can be software-pipelined like any other operations

 However if

 there is a large amount of data-dependent control_flow within the loop

 body scheduling techniques described in Section secglsched may

 be more appropriate



 If a machine does_not have predicated instructions we can use the

 concept of hierarchical reduction described below

 to handle a small amount of

 data-dependent control_flow Like Algorithm algglsch in

 hierarchical reduction the

 control constructs in the loop are scheduled inside-out starting_with

 the most deeply nested structures As each construct is scheduled

 the entire construct is reduced to a single_node representing all the

 scheduling constraints of its components with_respect to the

 other parts of the program This

 node can then be scheduled as if it were a simple node within the

 surrounding control construct The scheduling process is complete

 when the entire program is reduced to a single_node



 In the case of a conditional_statement with then and else

 branches we schedule each of the branches independently Then



 enumerate



 The constraints of the entire conditional_statement are conservatively

 taken to be the union of the constraints from both branches



 Its resource usage is the maximum of the resources used in each branch



 Its precedence constraints are the union of those in each branch

 obtained_by

 pretending that both branches are executed



 enumerate

 This node can then be

 scheduled like any other node Two sets of code corresponding to

 the two branches are generated Any code scheduled in parallel with

 the conditional_statement is duplicated in both branches If multiple

 conditional_statements are overlapped separate code must_be generated

 for each combination of branches executed in parallel



 Hardware Support for Software Pipelining



 Specialized hardware support has_been proposed for minimizing the size

 of software-pipelined code The rotating register file in the

 Itanium architecture is one such example A rotating register file

 has a base register which is added to the register number

 specified in the code to derive the actual register accessed We can

 get different iterations in a loop to use different registers

 simply by changing the contents of the base register at the boundary of

 each iteration The Itanium architecture also has extensive

 predicated instruction support Not_only can predication be used to

 convert control dependence to data_dependence but it also can be used

 to avoid generating the prologs and epilogs The body of a

 software-pipelined_loop contains a superset of the instructions issued

 in the prolog and epilog We can simply generate the code for the

 steady_state and use predication appropriately

 to suppress the extra operations to

 get the effects of having a prolog and an epilog



 While Itanium's hardware support improves the density of

 software-pipelined code we must also realize that the support is not

 cheap Since software_pipelining is a technique intended for tight

 innermost_loops pipelined loops tend to be small anyway Specialized

 support for software_pipelining is warranted principally for machines

 that are

 intended to execute many software-pipelined loops and in situations

 where it is very

 important to minimize code size



 figurehtfb



 center

 tabularr_r l

 1) L LD_R1 a(R9)



 2) ST b(R9) R1



 3) LD_R2 c(R9)



 4) ADD_R3 R1_R2



 5) ST c(R9) R3



 6) SUB R4 R1_R2



 7) ST b(R9) R4



 8) BL R9 L

 tabular

 center



 Machine code for Exercise sched-loop-exer

 sched-loop-exer-fig



 figure



 exer

 In Example_exspscc we showed how to establish the bounds on the

 relative clocks at which and are scheduled Compute the bounds

 for each of five other pairs of nodes for general for

 for

 exer



 exer

 sched-loop-exer

 In Fig sched-loop-exer-fig is the body of a loop Addresses such

 as a(R9) are intended to be memory_locations where is a

 constant and R9 is the register that counts iterations through

 the loop You_may assume that each iteration of the loop accesses

 different locations because R9 has a different value

 Using the machine_model of Example_exdoall schedule the loop

 of Fig sched-loop-exer-fig in the following ways



 itemize

 a) Keeping each iteration as tight as possible (ie only

 introduce one nop after each arithmetic operation) unroll the

 loop twice Schedule the second iteration to commence at the earliest

 possible moment without violating the constraint that the machine can

 only do one load one store one arithmetic operation and one branch at

 any clock

 b) Repeat part (a) but unroll the loop three times Again

 start each iteration as_soon as you can subject to the machine

 constraints

 c) Construct fully pipelined code subject to the machine

 constraints In this part you can introduce extra nop's if

 needed but you must start a new iteration every two clock ticks

 itemize

 exer



 exer

 A certain loop requires 5 loads 7 stores and 8 arithmetic_operations

 What is the minimum_initiation interval for a software_pipelining of

 this loop on a machine that executes each operation in one clock_tick

 and has resources enough to do in one clock_tick



 itemize

 a) 3 loads 4 stores and 5 arithmetic_operations

 b) 3 loads 3 stores and 3 arithmetic_operations

 itemize



 exer



 hexer

 Using the machine_model of Example_exdoall find the minimum

 initiation_interval and a uniform schedule for the iterations for the

 following loop



 verbatim

 for_(i 1_i n_i)

 Ai Bi-1 1

 Bi Ai-1 2



 verbatim

 Remember that the counting of iterations is handled by auto-increment of

 registers and no operations are needed solely for the counting

 associated_with the for-loop

 hexer



 hexer

 Prove that Algorithm_algsp

 in the special_case where every operation requires only one

 unit of one resource can always find a

 software-pipeline schedule meeting the lower_bound

 hexer



 hexer

 Suppose we have a cyclic data-dependence_graph with nodes

 and There_are edges from to and from to with

 label and there are edges from to and from

 to with label There_are no other edges



 itemize

 a) Draw the cyclic dependence graph

 b) Compute the table of longest_simple paths among the nodes

 c)_Show the lengths of the longest_simple paths if the initiation

 interval is 2

 d) Repeat (c) if

 e) For what are the constraints on the relative times that

 each of the instructions represented_by and may be

 scheduled

 itemize



 hexer



 hexer

 simple-path-exer

 Give an algorithm to find the length of the longest_simple path

 in an -node graph on the assumption that no cycle has a positive

 length

 Hint

 Adapt Floyd's algorithm for shortest paths (see eg A V Aho and J_D

 Ullman Foundations of Computer_Science Computer_Science Press

 New_York 1992)

 hexer



 vhexer

 Suppose we have a machine with three instruction

 types which we'll call and All instructions require one

 clock_tick and the machine can execute one instruction of each type at

 each clock Suppose a loop consists of six instructions two of each

 type Then it is possible to execute the loop in a software

 pipeline with an initiation_interval

 of two However some sequences of the six

 instructions require insertion of one delay and some require insertion

 of two delays Of the 90 possible sequences of two 's two 's and

 two 's how many require no delay How many require one delay

 Hint There is symmetry among the three instruction types so two

 sequences that can be transformed into one another by permuting the

 names and must require the same number of delays For

 example must_be the same as

 vhexer

 Access to Nonlocal Data on the Stack

 stack-access-sect



 In this_section we consider how procedures access their data

 Especially important is the mechanism for finding data used within

 a procedure but that does_not belong to Access becomes

 more_complicated in languages where procedures can be declared

 inside other procedures We therefore begin_with the simple case

 of C functions and then introduce a language ML that permits

 both nested function declarations and functions as first-class

 objects that is functions can take functions as arguments and

 return functions as values This capability can be supported by

 modifying the implementation of the run-time_stack and we_shall

 consider several options for modifying the activation_records of

 Section stack-alloc-sect



 Data Access Without Nested Procedures

 st-nonest-subsect



 In the C family of languages all variables are defined either

 within a single function or outside any function (globally)

 Most importantly it is impossible to declare one procedure whose

 scope is entirely within another procedure Rather a global

 variable has a scope consisting of all the functions that

 follow the declaration of except where there is a local

 definition of the identifier Variables declared within a

 function have a scope consisting of that function only or part of

 it if the function has nested blocks as discussed in

 Section static-scope-subsect



 For languages that do_not allow nested procedure declarations

 allocation of storage for variables and access to those variables

 is simple



 enumerate



 Global variables are allocated static storage The

 locations of these variables remain fixed and are known at_compile

 time So to access any variable that is not local to the

 currently executing procedure we simply use the statically

 determined address



 Any other name must_be local to the activation at the top of the stack

 We may access these variables through the topsp pointer of the

 stack



 enumerate



 An_important benefit of static allocation for globals is that

 declared procedures may be passed as parameters or returned as

 results (in C a pointer to the function is passed) with no

 substantial change in the data-access strategy With the C

 static-scoping rule and without nested procedures any name

 nonlocal to one procedure is nonlocal to all procedures

 regardless of how they are activated Similarly if a procedure

 is returned as a result then any nonlocal name refers to the

 storage statically allocated for it



 Issues With Nested Procedures



 Access becomes far more_complicated when a language allows

 procedure declarations to be nested and also uses the normal

 static_scoping rule that is a procedure can access variables of

 the procedures whose declarations surround its_own declaration

 following the nested scoping rule described for blocks in

 Section static-scope-subsect The_reason is that knowing at

 compile_time that the declaration of is immediately nested

 within does_not tell_us the relative positions of their

 activation_records at_run time In_fact since either or

 or both may be recursive there may be several activation_records

 of andor on the stack



 Finding the declaration that applies to a nonlocal name in a

 nested procedure is a static decision it can be done by an

 extension of the static-scope rule for blocks Suppose is

 declared in the enclosing procedure Finding the relevant

 activation of from an activation of is a dynamic decision

 it requires additional run-time information_about activations One

 possible solution to this problem is to use access_links

 which we introduce in Section access-link-subsect



 A Language With Nested Procedure Declarations

 ml-intro-subsect



 The C family of languages and many other familiar languages do

 not support nested procedures so we introduce one that does The

 history of nested procedures in languages is long Algol_60 an

 ancestor of C had this capability as did its descendant Pascal

 a once-popular teaching language Of the later languages with

 nested procedures one of the most influential is ML and it is

 this language whose syntax and semantics we_shall borrow (see the

 box on More about ML for some of the interesting features of

 ML)



 itemize



 ML is a functional language meaning that variables

 once declared and initialized are not changed There_are only a

 few exceptions such_as the array whose elements can be changed

 by special function calls



 Variables are defined and have their unchangeable values

 initialized by a statement of the form

 center

 val name expression

 center



 Functions are defined using the syntax

 center

 fun name (

 arguments ) body

 center



 For function bodies we_shall use let-statements of the form

 center

 let list of definitions in

 statements end

 center

 The definitions are normally val or fun statements

 The scope of each such definition consists of all following

 definitions up to the in and all the statements up to the

 end Most importantly function definitions can be nested

 For_example the body of a function can contain a

 let-statement that includes the definition of another (nested)

 function Similarly can have function definitions within

 its_own body leading to arbitrarily deep nesting of functions



 itemize



 More About ML

 In_addition to being almost purely functional ML presents a number of

 other surprises to the programmer who is used to C and its family



 itemize



 ML supports higher-order functions That is a

 function can take functions as arguments and can construct and

 return other functions Those functions in turn can take

 functions as arguments to any level



 ML has essentially no iteration as in C's for- and

 while-statements for instance Rather the effect of iteration

 is achieved by recursion This_approach is essential in a

 functional language since we cannot change the value of an

 iteration variable like in for(i0i10i) of

 C Instead ML would make a function argument and the

 function would call itself with progressively higher values of

 until the limit was reached



 ML supports lists and labeled tree structures as primitive

 data types



 ML does_not require declaration of variable types Rather

 it deduces types at_compile time and treats it as an error if it

 cannot For_example val x 1 evidently makes have

 integer type and if we also see val y 2x then we know

 is also an_integer



 itemize





 Nesting Depth



 Let_us give nesting_depth 1 to procedures that are not nested

 within any other procedure

 For_example all C functions are at nesting_depth 1

 However if a procedure is defined immediately_within a procedure at

 nesting_depth then give the nesting_depth



 ex

 nesting-depth-ex Figure qs-ml-fig contains a sketch

 in ML of our_running quicksort example The only function at

 nesting_depth 1 is the outermost function sort which reads

 an array of 9 integers and sorts them using the quicksort

 algorithm Defined within sort at line (2) is the array

 itself Notice the form of the ML declaration The first

 argument of array says we_want the array to have 11

 elements all ML arrays are indexed by integers starting_with 0

 so this array is quite similar to the C array from

 Fig qs1-fig The second argument of array says_that

 initially all elements of the array hold the value 0 This

 choice of initial value lets the ML compiler deduce that is an

 integer array since 0 is an_integer so we never have to declare

 a type for



 figurehtfb

 center

 tabularl

 1) fun sort(inputFile outputFile)



 let



 2) val a array(110)



 3) fun readArray(inputFile)



 4) a



 5) fun exchange(ij)



 6) a



 7) fun quicksort(mn)



 let



 8) val v



 9) fun partition(yz)



 10) a v exchange



 in



 11) a v partition quicksort



 end



 in



 12) a readArray quicksort



 end



 tabular

 center

 A version of quicksort in ML style using nested

 functions qs-ml-fig



 figure





 Also declared within sort are several functions readArray exchange and quicksort On lines_(4) and

 (6) we suggest that readArray and exchange each access

 the array Note_that in ML array_accesses can violate the

 functional nature of the language and both these functions

 actually change values of 's elements as in the C version of

 quicksort Since each of these three functions is defined

 immediately_within a function at nesting_depth 1 their nesting

 depths are all 2



 Lines (7) through (11) show some of the detail of quicksort

 Local value the pivot for the partition is declared at

 line_(8) Function partition is defined at line_(9) In

 line (10) we suggest that partition accesses both the array

 and the pivot value and also calls the function exchange Since partition is defined immediately_within a

 function at nesting_depth 2 it is at depth 3 Line (11) suggests

 that quicksort accesses variables and the function

 partition and itself recursively



 Line (12) suggests that the outer function sort accesses

 and calls the two procedures readArray and quicksort

 ex



 Access Links

 access-link-subsect



 A direct implementation of the normal static_scope rule for nested

 functions is obtained_by adding a pointer called the access

 link to each activation_record If procedure is nested

 immediately_within procedure in the source code then the

 access_link in any activation of points to the most recent

 activation of Note_that the nesting_depth of must_be

 exactly one less_than the nesting_depth of Access links form

 a chain from the activation_record at the top of the stack to a

 sequence of activations at progressively lower nesting depths

 Along this chain are all the activations whose data and procedures

 are accessible to the currently executing procedure



 Suppose that the procedure at the top of the stack is at

 nesting_depth and needs to access which is an

 element defined within some procedure that surrounds and

 has nesting_depth Note_that with equality

 only if and are the same procedure To_find we start

 at the activation_record for at the top of the stack and

 follow the access_link times from activation_record to

 activation_record Finally we wind_up at an activation_record

 for and it will always be the most recent (highest)

 activation_record for that currently appears on the stack

 This activation_record contains the element that we_want

 Since the compiler knows the layout of activation_records

 will be found at some fixed offset from the position in 's

 activation_record that we can reach by following the last access

 link



 ex

 Figure access-link-fig shows a sequence of stacks that might

 result from execution of the function sort of

 Fig qs-ml-fig As before we represent function names by

 their first letters and we show some of the data that might

 appear in the various activation_records as_well as the access

 link for each activation In Fig access-link-fig(a) we see

 the situation after sort has called readArray to load

 input into the array and then called

 to sort the array The access_link from

 points to the activation_record for sort not because sort called quicksort but because sort is the most

 closely nested function surrounding quicksort in the program

 of Fig qs-ml-fig





 figurehtfb





 Access links for finding nonlocal data

 access-link-fig

 figure



 In successive steps of Fig access-link-fig we see a

 recursive call to followed_by a call to

 partition which calls exchange Notice_that

 's access_link points to sort for

 the same reason that 's does



 In Fig access-link-fig(d) the access_link for exchange bypasses the activation_records for quicksort and

 partition since exchange is nested immediately_within

 sort That arrangement is fine since exchange needs

 to access only the array and the two elements it must swap

 are indicated by its_own parameters and

 ex



 Manipulating Access Links

 manip-al-subsect



 How are access_links determined The simple case occurs_when a

 procedure call is to a particular procedure whose name is given

 explicitly in the procedure call The harder case is when the call

 is to a procedure-parameter in that case the particular

 procedure being called is not known until run_time and the

 nesting_depth of the called procedure may differ in different

 executions of the call Thus let_us first consider what should

 happen when a procedure_calls procedure explicitly There

 are two cases



 enumerate



 Procedure is at a higher nesting_depth than Then

 must_be defined immediately_within or the call by

 would not be at a position that is within the scope of the

 procedure name Thus the nesting_depth of is exactly one

 greater_than that of and the access_link from must lead

 to It is a simple matter for the calling_sequence to include

 a step that places in the access_link for a pointer to the

 activation_record of Examples include the call of quicksort by sort to set up Fig access-link-fig(a)

 and the call of partition by quicksort to create

 Fig access-link-fig(c)



 The nesting_depth of is less_than or equal to the nesting

 depth of In order for the call within to be in the

 scope of name procedure must_be nested_within some

 procedure while is a procedure defined immediately_within

 The top activation_record for can therefore be found by

 following the chain of access_links starting in the activation

 record for for hops Then the access_link for

 must go to this activation of

 This case includes recursive_calls where In that case

 the chain of access_links is followed for one hop and the access_links

 for and are the same

 An_example is the call of by

 to set up Fig access-link-fig(b)

 It also includes the case of mutually_recursive calls where two or_more

 procedures are defined within a common parent



 enumerate



 ex

 For an example of case (3) notice how we go from

 Fig access-link-fig(c) to Fig access-link-fig(d)

 The nesting_depth 2 of the called function exchange is one

 less_than the depth 3 of the calling function partition

 Thus we start at the activation_record for partition and

 follow access_links which takes us from partition's activation_record to that of

 to that of sort The access_link for exchange

 therefore goes to the activation_record for sort as we see

 in Fig access-link-fig(d)



 An_equivalent way to discover this access_link is simply to follow

 access_links for hops and copy the access_link found in

 that record In our example we would go one hop to the activation

 record for and copy its access_link to

 sort Notice_that this access_link is correct for exchange even_though exchange is not in the scope of quicksort these being sibling functions nested_within sort

 ex



 Access Links for Procedure Parameters



 When a procedure is passed to another procedure as a

 parameter and then calls its parameter (and_therefore calls

 in this activation of ) it is possible that does_not

 know the context in which appears in the program If so it

 is impossible for to know how to set the access_link for

 The solution to this problem is as_follows when procedures are

 used as parameters the caller needs to pass along with the name

 of the procedure-parameter the proper access_link for that

 parameter



 The caller always knows the link since if is passed by

 procedure as an actual_parameter then must_be a name

 accessible to and therefore can determine the access

 link for exactly as if were being called by directly

 That is we use the rules for constructing access_links given in

 Section manip-al-subsect



 ex

 In Fig param-ml-fig we see a sketch of an ML function

 that has functions and nested_within it Function has

 a function-valued parameter which it calls Function

 defines within it a function and then calls with

 actual_parameter



 figurehtfb

 center

 tabularl

 fun a(x)



 let



 fun b(f)



 f



 fun c(y)



 let



 fun d(z)



 in



 b(d)



 end



 in



 c(1)



 end

 tabular

 center

 Sketch of ML program that uses function-parameters

 param-ml-fig



 figure



 Let_us trace what_happens when is executed First

 calls so we place an activation_record for above that

 for on the stack The access_link for points to the

 record for since is defined immediately_within

 Then calls The calling_sequence sets up an

 activation_record for as shown in Fig fun-param-fig(a)



 figurehtfb





 Actual parameters carry their access_link with them

 fun-param-fig

 figure



 Within this activation_record is the actual_parameter and its

 access_link which together form the value of formal_parameter

 in the activation_record for Notice_that knows about

 since is defined within and therefore passes a

 pointer to its_own activation_record as the access_link No matter

 where was defined if is in the scope of that definition

 then one of the three rules of Section manip-al-subsect must

 apply and can provide the link



 Now let_us look_at what does We know that at some point it

 uses its parameter which has the effect of calling An

 activation_record for appears on the stack as shown in

 Fig fun-param-fig(b) The proper access_link to place in

 this activation_record is found in the value for parameter

 the link is to the activation_record for since

 immediately surrounds the definition of Notice_that is

 capable of setting up the proper link even_though is not in

 the scope of 's or 's definitions

 ex



 Displays



 One problem with the access-link approach to nonlocal data is that

 if the nesting_depth gets large we may have to follow long chains

 of links to reach the data we need A more_efficient

 implementation uses an auxiliary array called the display which consists of one pointer for each nesting_depth

 We arrange that at all times is a pointer to the highest

 activation_record on the stack for any procedure at nesting_depth

 Examples of a display are shown in Fig display-fig

 For_instance in Fig display-fig(d) we see the display

 with holding a pointer to the activation_record for

 sort the highest (and only) activation_record for a

 function at nesting_depth 1 Also holds a pointer to the

 activation_record for exchange the highest record at depth

 2 and points to partition the highest record at

 depth 3



 figure





 Maintaining the display display-fig

 figure



 The advantage of using a display is that if procedure is

 executing and it needs to access element belonging to some

 procedure we need to look only in where is the

 nesting_depth of we follow the pointer to the

 activation_record for wherein is found at a known offset

 The compiler knows what is so it can generate code to access

 using and the offset of from the top of the

 activation_record for Thus the code never needs to follow a

 long chain of access_links



 In order to maintain the display correctly we need to save

 previous values of display entries in new activation_records If

 procedure at depth is called and its activation_record

 is not the first on the stack for a procedure at depth then

 the activation_record for needs to hold the previous value of

 while itself is set to point to this activation

 of When returns and its activation_record is removed

 from the stack we restore to have its value prior to the

 call of



 ex

 Several steps of manipulating the display are illustrated in

 Fig display-fig In Fig display-fig(a) sort

 at depth 1 has called at depth 2 The

 activation_record for quicksort has a place to store the old

 value of indicated as although in

 this case since there was no prior activation_record at depth 2

 this pointer is null



 In Fig display-fig(b) calls

 Since the activation_records for both

 calls are at depth 2 we must store the pointer to

 which was in in the record for

 Then is made to point to





 Next partition is called This function is at depth 3 so

 we use the slot in the display for the first time and make

 it point to the activation_record for partition The record

 for partition has a slot for a former value of but

 in this case there is none so the pointer remains null The

 display and stack at this time are shown in

 Fig display-fig(c)



 Then partition calls exchange That function is at

 depth 2 so its activation_record stores the old pointer

 which goes to the activation_record for

 Notice_that the display pointers cross that is points

 further down the stack than does However that is a proper

 situation exchange can only access its_own data and that of

 sort via

 ex



 figurehtfb



 verbatim

 fun main ()

 let

 fun fib0(n)

 let

 fun fib1(n)

 let

 fun fib2(n) fib1(n-1) fib1(n-2)

 in

 if n 4 then fib2(n)

 else fib0(n-1) fib0(n-2)

 end

 in

 if n 2 then fib1(n)

 else 1

 end

 in

 fib0(4)

 end

 verbatim



 Nested functions computing Fibonacci numbers

 access-link-exer-fig



 figure



 sexer

 In Fig access-link-exer-fig is a ML function main

 that computes Fibonacci numbers in a nonstandard way Function

 fib0 will compute the th Fibonacci number for any

 Nested within it is fib1 which computes the th

 Fibonacci number on the assumption and nested_within fib1 is fib2 which assumes Note_that neither

 fib1 nor fib2 need to check for the basis cases Show

 the stack of activation_records that result from a call to main up until the time that the first call (to fib0(1)) is

 about to return Show the access_link in each of the activation

 records on the stack

 sexer



 exer

 Suppose that we implement the functions of

 Fig access-link-exer-fig using a display Show the display

 at the moment the first call to fib0(1) is about to return

 Also indicate the saved display entry in each of the activation

 records on the stack at that time

 exer

 Intermediate_Code for Statements

 stmt-java-sect



 Each statement construct is implemented_by a subclass of Stmt The fields for the components of a construct are in the

 relevant subclass for example class While has fields for a

 test expression and a substatement as we_shall see



 Lines 3-4 in the following code for class_Stmt deal_with

 syntax-tree construction The constructor Stmt() does

 nothing since the work is done in the subclasses The static

 object StmtNull (line_4) represents an empty sequence of

 statements



 footnotesize

 flushleft

 1)_package inter_File Stmtjava



 2)_public class_Stmt extends Node



 3)_public Stmt()



 4)_public static Stmt Null new Stmt()



 5)_public void_gen(int b int_a) called with labels begin and after



 6) int after 0 saves label after



 7) public_static Stmt Enclosing StmtNull used for break stmts



 8)



 flushleft

 footnotesize



 Lines 5-7 deal_with the generation of three-address_code The

 method gen is called with two labels b and a

 where b marks the beginning of the code for this statement

 and a marks the first instruction after the code for this

 statement Method gen (line_5) is a placeholder for the

 gen methods in the subclasses The subclasses While

 and Do save their label a in the field after

 (line 6) so it can be used by any enclosed break statement to jump

 out of its enclosing construct The object StmtEnclosing is

 used during_parsing to keep_track of the enclosing construct (For

 a source_language with continue statements we can use the same

 approach to keep_track of the enclosing construct for a continue

 statement)



 The constructor for class If builds a node for a statement

 Fields expr and stmt

 hold the nodes for and respectively Note_that expr in lower-case letters names a field of class Expr

 similarly stmt names a field of class_Stmt



 footnotesize

 flushleft

 1)_package inter_File Ifjava



 2)_import symbols



 3)_public class If extends_Stmt



 4)_Expr expr Stmt_stmt



 5)_public If(Expr x Stmt s)



 6) expr x stmt s



 7) if(_exprtype TypeBool_) exprerror(boolean_required in if)



 8)



 9)_public void_gen(int b int_a)



 10) int label newlabel() label for the code for stmt



 11) exprjumping(0 a) fall_through on true goto a on false



 12) emitlabel(label) stmtgen(label a)



 13)



 14)



 flushleft

 footnotesize



 The code for an If object consists of jumping_code for expr followed_by the code for stmt As_discussed in

 Section jumping-java-sect the call exprjumping(0a)

 on line 11 specifies that control must fall_through the code for

 expr if expr evaluates to true and must flow to label

 a otherwise



 The implementation of class Else which handles conditionals

 with else parts is analogous to that of class If



 footnotesize

 flushleft

 1)_package inter_File Elsejava



 2)_import symbols



 3)_public class Else extends_Stmt



 4)_Expr expr Stmt stmt1 stmt2



 5)_public Else(Expr x Stmt s1 Stmt s2)



 6) expr x stmt1 s1 stmt2 s2



 7) if(_exprtype TypeBool_) exprerror(boolean_required in if)



 8)



 9)_public void_gen(int b int_a)



 10) int label1 newlabel() label1 for stmt1



 11) int label2 newlabel() label2 for stmt2



 12) exprjumping(0label2) fall_through to stmt1 on true



 13) emitlabel(label1) stmt1gen(label1 a) emit(goto_L a)



 14) emitlabel(label2) stmt2gen(label2 a)



 15)



 16)



 flushleft

 footnotesize



 The construction of a While object is split between the

 constructor While() which creates a node with null children

 (line_5) and an initialization function init(xs) which

 sets child expr to x and child stmt to s

 (lines 6-9) Function gen(ba) for generating three-address

 code (lines 10-16) is in the spirit of the corresponding function

 gen() in class If The difference is that label a is saved in field after (line 11) and that the code for

 stmt is followed_by a jump to b (line 15) for the next

 iteration of the while loop



 footnotesize

 flushleft

 1)_package inter_File Whilejava



 2)_import symbols



 3)_public class While extends_Stmt



 4)_Expr expr Stmt_stmt



 5)_public While() expr null stmt null



 6)_public void init(Expr x Stmt s)



 7) expr x stmt s



 8) if(_exprtype TypeBool_) exprerror(boolean_required in while)



 9)



 10)_public void_gen(int b int_a)



 11) after a save label a



 12) exprjumping(0 a)



 13) int label newlabel() label for stmt



 14) emitlabel(label) stmtgen(label b)



 15) emit(goto_L b)



 16)



 17)



 flushleft

 footnotesize



 Class Do is very similar to class While



 footnotesize

 flushleft

 1)_package inter_File Dojava



 2)_import symbols



 3)_public class Do extends_Stmt



 4)_Expr expr Stmt_stmt



 5)_public Do() expr null stmt null



 6)_public void init(Stmt s Expr x)



 7) expr x stmt s



 8) if(_exprtype TypeBool_) exprerror(boolean_required in do)



 9)



 10)_public void_gen(int b int_a)



 11) after a



 12) int label newlabel() label for expr



 13) stmtgen(blabel)



 14) emitlabel(label)



 15) exprjumping(b0)



 16)



 17)



 flushleft

 footnotesize



 Class Set implements assignments with an_identifier on the

 left_side and an expression on the right Most of the code in

 class Set is for constructing a node and checking types

 (lines 5-13) Function gen emits a three-address

 instruction (lines 14-16)



 footnotesize

 flushleft

 1)_package inter_File Setjava



 2)_import lexer_import symbols



 3)_public class Set extends_Stmt



 4)_public Id id public_Expr expr



 5)_public Set(Id i Expr x)



 6) id i expr x



 7) if_( check(idtype exprtype) null_) error(type_error)



 8)



 9)_public Type_check(Type p1 Type p2)



 10) if_( Typenumeric(p1) Typenumeric(p2) )_return p2



 11) else if_( p1 TypeBool p2 TypeBool_) return p2



 12) else_return null



 13)



 14) public_void gen(int_b int_a)



 15) emit( idtoString() exprgen()toString() )



 16)



 17)



 flushleft

 footnotesize



 Class SetElem implements assignments to an array_element



 footnotesize

 flushleft

 1)_package inter_File SetElemjava



 2)_import lexer_import symbols



 3)_public class SetElem extends_Stmt



 4)_public Id array public_Expr index public_Expr expr



 5)_public SetElem(Access x Expr y)



 6) array xarray index xindex expr y



 7) if_( check(xtype exprtype) null_) error(type_error)



 8)



 9)_public Type_check(Type p1 Type p2)



 10) if_( p1 instanceof_Array p2 instanceof_Array )_return null



 11) else if_( p1 p2 )_return p2



 12) else if_( Typenumeric(p1) Typenumeric(p2) )_return p2



 13) else_return null



 14)



 15)_public void_gen(int b int_a)



 16) String s1 indexreduce()toString()



 17) String s2 exprreduce()toString()



 18) emit(arraytoString() s1 s2)



 19)



 20)



 flushleft

 footnotesize



 Class Seq implements a sequence of statements The tests for

 null statements on lines 6-7 are for avoiding labels Note_that

 no code is generated for the null statement StmtNull

 since method gen in class_Stmt does nothing



 footnotesize

 flushleft

 1)_package inter_File Seqjava



 2)_public class Seq extends_Stmt



 3) Stmt stmt1 Stmt stmt2



 4)_public Seq(Stmt s1 Stmt s2) stmt1 s1 stmt2 s2



 5)_public void_gen(int b int_a)



 6) if_( stmt1 StmtNull ) stmt2gen(b a)



 7) else if_( stmt2 StmtNull ) stmt1gen(b a)



 8) else



 9) int label newlabel()



 10) stmt1gen(blabel)



 11) emitlabel(label)



 12) stmt2gen(labela)



 13)



 14)



 15)



 flushleft

 footnotesize



 A break statement sends control out of an enclosing_loop or switch

 statement Class Break uses field stmt to save the

 enclosing statement construct (the parser ensures that StmtEnclosing denotes the syntax-tree_node for the enclosing

 construct) The code for a Break object is a jump to the

 label stmtafter which marks the instruction immediately

 after the code for stmt



 footnotesize

 flushleft

 1)_package inter_File Breakjava



 2)_public class Break extends_Stmt



 3) Stmt_stmt



 4)_public Break()



 5) if( StmtEnclosing StmtNull ) error(unenclosed break)



 6) stmt StmtEnclosing



 7)



 8)_public void_gen(int b int_a)



 9) emit(_goto L stmtafter)



 10)



 11)



 flushleft

 footnotesize

 Storage Organization

 stor-org-sect



 From the perspective of the compiler_writer the executing

 target program_runs in its_own logical address space in which

 each program value has a location

 The management and organization of this logical address space

 is shared between the compiler operating_system and target_machine

 The operating_system maps the logical addresses into

 physical addresses which are usually spread throughout memory



 The run-time representation of an object program in the logical

 address space consists of data and program areas

 as shown in Fig rt-mem-fig

 A compiler for a language like C on an operating

 system like Linux might subdivide memory in this way



 figurehtfb





 Typical subdivision of run-time memory into code and data

 areas rt-mem-fig

 figure



 Throughout this_book we assume the run-time storage comes in

 blocks of contiguous bytes where a byte is the smallest unit of

 addressable memory A byte is eight bits and four_bytes form a

 machine word Multibyte objects are stored in consecutive bytes

 and given the address of the first byte



 As_discussed in Chapter_inter-ch the amount of storage

 needed for a name is determined from its type An elementary data

 type such_as a character integer or float can be stored in an

 integral number of bytes Storage for an aggregate type such_as

 an array or structure must_be large_enough to hold all its

 components



 The storage layout for data objects is strongly influenced by the

 addressing constraints of the target_machine On many machines

 instructions to add integers may expect integers to be aligned that is placed_at an address divisible by 4 Although

 a character array (as in C) of length 10 needs only enough bytes to hold ten

 characters a compiler may allocate 12 bytes to get the proper

 alignment leaving 2 bytes unused Space left unused due to

 alignment considerations is referred to as padding When

 space is at a premium a compiler may pack data so that no

 padding is left additional instructions may then need to be

 executed at_run time to position packed data so that it can be

 operated on as if it were properly aligned



 The size of the generated target code is fixed at_compile time so

 the compiler can place the executable target code in a statically

 determined area Code usually in the low end of memory

 Similarly the size of some program data objects such_as global

 constants and data generated_by the compiler such_as information

 to support garbage_collection may be known at_compile time and

 these data objects can be placed in another statically_determined area

 called Static One reason for statically allocating as many data

 objects as possible is that the addresses of these objects can be

 compiled into the target code In early versions of Fortran all

 data objects could be allocated statically



 To maximize the utilization of space at_run time the other two

 areas Stack and Heap are at the opposite ends of the

 remainder of the address space These areas are dynamic their

 size can change as the program executes These areas grow towards

 each other as needed The stack is used to store

 data_structures called activation

 records that get generated during procedure_calls



 In_practice the stack grows towards lower addresses the heap towards higher

 However throughout this_chapter and the next we_shall assume that

 the stack grows towards higher addresses so that we can use positive

 offsets for notational_convenience in all our examples



 As we_shall see in the next section an activation_record is

 used to store information_about the status of the machine such_as

 the value of the program counter and machine registers when a

 procedure call occurs When control returns from the call the

 activation of the calling_procedure can be restarted after

 restoring the values of relevant registers and setting the program

 counter to the point_immediately after the call Data objects

 whose lifetimes are contained in that of an activation can be

 allocated on the stack along with other information associated

 with the activation



 Many programming_languages allow the programmer to allocate and

 deallocate data under program control For_example C has the

 functions malloc and free that can be used to obtain

 and give back arbitrary chunks of storage The heap is used to

 manage this kind of long-lived data

 Section heap-sect will

 discuss various memory-management algorithms that can be used to

 maintain the heap







 Static Versus Dynamic Storage Allocation



 The layout and allocation of data to memory_locations in the run-time

 environment are key issues in storage management

 These issues are tricky because the same name in

 a program text can refer to multiple locations at_run time

 The two adjectives static and dynamic distinguish

 between compile_time and run_time respectively We_say that a

 storage-allocation decision is static if it can be made by

 the compiler looking only at the text

 of the program not at what the program does when it executes

 Conversely a decision is dynamic if it can be decided only

 while the program is running

 Many compilers use some combination of the following two strategies for

 dynamic storage allocation



 enumerate



 Stack storage Names local to a procedure are

 allocated space on a stack We discuss the run-time

 stack starting in Section stack-alloc-sect The stack

 supports the normal callreturn policy for procedures



 Heap storage Data that may outlive the call to the

 procedure that created it is usually allocated on a heap of

 reusable storage We discuss heap management starting in

 Section heap-sect The heap is an area of virtual_memory

 that allows objects or other data elements to obtain storage when

 they are created and to return that storage when they are

 invalidated



 enumerate



 To support heap management garbage_collection enables the

 run-time system to detect useless data elements and reuse their

 storage even if the programmer does_not return their space

 explicitly Automatic garbage_collection is an essential feature

 of many modern languages despite it being a difficult

 operation to do efficiently it may not even be possible for

 some languages



 The Structure of a Compiler

 structure-sect



 Up to this point we have treated a compiler as a single box that

 maps a source_program into a semantically equivalent target

 program If we open up this box a little we see that there are

 two_parts to this mapping analysis and synthesis



 The analysis part breaks up the source_program into

 constituent pieces and imposes a grammatical_structure on them It

 then uses this structure to create an intermediate_representation

 of the source_program If the analysis part detects that the

 source_program is either syntactically ill formed or semantically

 unsound then it must provide informative messages so the user

 can take corrective action The analysis part also collects

 information_about the source_program and stores it in a data

 structure called a symbol_table which is passed along with

 the intermediate_representation to the synthesis part



 The synthesis part constructs the desired target program

 from the intermediate_representation and the information in the

 symbol_table The analysis part is often called the front

 end of the compiler the synthesis part is the back end



 figurehtfb





 Phases of a compiler phases-fig

 figure



 If we examine the compilation process in more_detail we see that

 it operates as a sequence of phases each of which

 transforms one representation of the source_program to another A

 typical decomposition of a compiler into phases is shown in

 Fig phases-fig In_practice several phases may be grouped

 together and the intermediate_representations between the grouped

 phases need not be constructed explicitly The symbol_table which

 stores information_about the entire source_program is used by all

 phases of the compiler



 Some compilers have a machine-independent optimization_phase

 between the front_end and the back end The purpose of this

 optimization_phase is to perform transformations on the

 intermediate_representation so that the back end can produce a

 better target program than it would have otherwise produced from

 an unoptimized intermediate_representation Since optimization is

 optional one or the other of the two optimization phases shown in

 Fig phases-fig may be missing



 Lexical Analysis



 The first phase of a compiler is called lexical analysis or

 scanning The lexical_analyzer reads the stream of

 characters making up the source_program and groups the characters

 into meaningful sequences called lexemes For each lexeme

 the lexical_analyzer produces as output a token of the form



 center



 center

 that it passes on to the subsequent phase syntax analysis In the

 token the first component token-name is an abstract symbol

 that is used during syntax analysis and the second_component attribute-value points to an entry in the symbol_table for this

 token Information from the symbol-table_entry is needed for

 semantic analysis and code_generation



 For_example suppose a source_program contains the assignment

 statement

 equation

 position initial rate 60 assign-eq

 equation

 The characters in this assignment could be grouped

 into the following lexemes and mapped_into

 the following tokens passed on to the syntax analyzer



 itemize



 1 position is a lexeme that would be mapped_into a

 token where id is an abstract symbol standing for

 identifier and points to the symbol-table_entry for position The symbol-table_entry for an_identifier holds

 information_about the identifier such_as its name and type



 2 The assignment symbol is a lexeme that is mapped

 into the token Since this token needs no

 attribute-value we have omitted the second_component We could

 have used any abstract symbol such_as assign for the

 token-name but for notational_convenience we have chosen to use

 the lexeme itself as the name of the abstract symbol



 3 initial is a lexeme that is mapped_into the token

 where points to the symbol-table_entry for initial



 4 is a lexeme that is mapped_into the token





 5 rate is a lexeme that is mapped_into the token

 where points to the symbol-table_entry for rate



 6 is a lexeme that is mapped_into the token





 7 60 is a lexeme that is mapped_into the token

 (Technically speaking for the lexeme 60

 we should make up a token like where

 points to the symbol_table for the internal representation of

 integer but we_shall defer the discussion of tokens for

 numbers until Chapter simple-ch Chapter lexan-ch

 discusses techniques for building lexical analyzers)

 itemize

 Blanks separating the lexemes would be discarded by

 the lexical_analyzer



 Figure trans-assign-fig shows the representation of the assignment

 statement (assign-eq) after lexical analysis as the

 sequence of tokens

 equation

 1_2 3 60

 lexout-eq

 equation

 In this representation the token names and are

 abstract symbols for the assignment addition and multiplication

 operators respectively



 figure





 Translation of an assignment_statement

 trans-assign-fig

 figure



 Syntax Analysis



 The second phase of the compiler is syntax analysis or parsing The parser uses the first components of the tokens

 produced_by the lexical_analyzer to create a tree-like

 intermediate_representation that depicts the grammatical_structure

 of the token_stream A_typical representation is a syntax

 tree in which each interior_node represents an operation and the

 children of the node represent the arguments of the operation A

 syntax_tree for the token_stream (lexout-eq) is shown as the

 output of the syntactic analyzer in Fig trans-assign-fig



 This tree shows the order in which the operations in the

 assignment

 center

 position initial rate 60

 center

 are to be performed The tree has an interior_node labeled

 with as its left_child and the integer as its right

 child The node represents the identifier rate The

 node_labeled makes it explicit that we must first multiply the

 value of rate by The node_labeled indicates that

 we must add the result of this multiplication to the value of initial The root of the tree labeled indicates that we

 must store the result of this addition into the location for the

 identifier position This ordering of operations is

 consistent_with the usual conventions of arithmetic which tell_us

 that multiplication has higher_precedence than addition and hence

 that the multiplication is to be performed before the addition



 The subsequent phases of the compiler use the grammatical

 structure to help analyze the source_program and generate the

 target program In Chapter_parse-ch we_shall use

 context-free_grammars to specify the grammatical_structure of

 programming_languages and discuss algorithms for constructing

 efficient syntax analyzers automatically from certain classes of

 grammars In Chapters simple-ch and sdt-ch we_shall

 see that syntax-directed_definitions can help specify the

 translation of programming_language constructs



 Semantic Analysis



 The semantic analyzer uses the syntax_tree and the

 information in the symbol_table to check the source_program for

 semantic consistency with the language definition It also gathers

 type information and saves it in either the syntax

 tree or the symbol_table for subsequent use during

 intermediate-code_generation



 An_important part of semantic analysis is type_checking

 where the compiler checks that each operator has matching

 operands For_example many programming_language definitions

 require an array index to be an_integer the compiler must report

 an error if a floating-point_number is used to index an array



 The language specification may permit some type_conversions called

 coercions For_example a binary arithmetic operator may be

 applied to either a pair of integers or to a pair of

 floating-point_numbers If the operator is applied to a

 floating-point_number and an_integer the compiler may convert or

 coerce the integer into a floating-point_number



 Such a coercion appears in Fig trans-assign-fig Suppose

 that position initial and rate have_been

 declared to be floating-point_numbers and that the lexeme 60 by itself forms an_integer The type checker in the semantic

 analyzer in Fig trans-assign-fig discovers that the

 operator is applied to a floating-point_number rate and

 an_integer In this case the integer may be converted into a

 floating-point_number In Fig trans-assign-fig notice that

 the output of the semantic analyzer has an extra node for the

 operator inttofloat which explicitly converts its integer

 argument into a floating-point_number Type_checking and semantic

 analysis are discussed in Chapter_inter-ch



 Intermediate_Code Generation



 In the process of translating a source_program into target code a

 compiler may construct one or_more intermediate_representations

 which can have a variety of forms Syntax trees are a form of

 intermediate_representation they are commonly_used during syntax

 and semantic analysis



 After syntax and semantic analysis of the source_program many

 compilers generate an explicit low-level or machine-like

 intermediate_representation which we can think of as a program

 for an abstract machine This intermediate_representation should

 have two important properties it should be easy to produce and it

 should be easy to translate into the target_machine



 In Chapter_inter-ch we consider an intermediate form called

 three-address_code which consists of a sequence of

 assembly-like instructions with three operands per instruction

 Each operand can act like a register The output of the

 intermediate_code generator in Fig trans-assign-fig

 consists of the three-address_code sequence



 equation

 arrayl

 t1 inttofloat(60)



 t2 id3 t1



 t3 id2 t2



 id1 t3

 array

 tac-eq

 equation



 There_are several points worth noting about three-address

 instructions First each three-address assignment instruction has

 at most one operator on the right_side Thus these instructions

 fix the order in which operations are to be done the

 multiplication precedes the addition in the source

 program (assign-eq) Second the compiler must generate a

 temporary_name to hold the value computed by a three-address

 instruction Third some three-address_instructions like the

 first and last in the sequence (tac-eq) above have fewer

 than three operands



 In Chapter_inter-ch we cover the principal intermediate

 representations used in compilers Chapter_sdt-ch

 introduces techniques for syntax-directed_translation that are

 applied in Chapter_inter-ch to type_checking and

 intermediate-code_generation for typical programming_language

 constructs such_as expressions flow-of-control constructs and

 procedure_calls





 Code Optimization



 The machine-independent code-optimization phase attempts to

 improve the intermediate_code so that better target code will

 result Usually better means faster but other objectives may be

 desired such_as shorter code or target code that consumes less

 power For_example a straightforward algorithm generates the

 intermediate_code (tac-eq) using an instruction for each

 operator in the tree representation that comes_from the semantic

 analyzer



 A simple intermediate_code generation algorithm followed_by code

 optimization is a reasonable way to generate good target code The

 optimizer can deduce that the conversion of 60 from integer to

 floating_point can be done once and for all at_compile time so

 the inttofloat operation can be_eliminated by_replacing the

 integer 60 by the floating-point_number 600 Moreover t3

 is used only once to transmit its value to id1 so the

 optimizer can transform (tac-eq) into the shorter sequence



 equation

 arrayl

 t1 id3 600



 id1 id2 t1

 array

 taco-eq

 equation



 There is a great variation in the amount of code_optimization

 different compilers perform

 In those that do the most the so-called optimizing compilers

 a significant amount of time is spent on this phase

 There_are simple optimizations that significantly improve the

 running_time of the target program without slowing down compilation

 too_much

 The chapters from 8 on discuss machine-independent and

 machine-dependent optimizations in detail



 Code_Generation



 The code_generator takes as input an intermediate_representation

 of the source_program and maps it into the target language If the

 target language is machine code registers or memory_locations are

 selected for each of the variables used by the program Then the

 intermediate instructions are translated_into sequences of machine

 instructions that perform the same task A crucial aspect of code

 generation is the judicious assignment of registers to hold

 variables



 For_example using registers R1 and R2 the intermediate

 code in (taco-eq) might get translated_into the machine code



 equation

 arrayl

 LDF R2 id3



 MULF R2 R2 600



 LDF R1 id2



 ADDF R1_R1 R2



 STF id1 R1

 array

 mc-eq

 equation



 The first operand of each instruction specifies a destination The

 F in each instruction tells_us that it deals_with

 floating-point_numbers The code in (mc-eq) loads the

 contents of address id3 into register R2 then

 multiplies it with floating-point constant 600 The

 signifies that 600 is to be treated_as an immediate constant The

 third instruction moves id2 into register_R1 and the

 fourth adds to it the value previously_computed in register R2 Finally the value in register_R1 is stored into the

 address of id1 so the code correctly implements the

 assignment_statement (assign-eq) Chapter codegen-ch

 covers code_generation



 This discussion of code_generation has ignored the important issue

 of storage allocation for the identifiers in the source_program

 As we_shall see in Chapter_run-time-ch the organization of

 storage at run-time depends_on the language being compiled

 Storage-allocation decisions are made either during intermediate

 code_generation or during code_generation



 Symbol-Table Management



 An essential function of a compiler is to record the variable

 names used in the source_program and collect information_about

 various attributes of each name These attributes may provide

 information_about the storage allocated for a name its type its

 scope (where in the program its value may be used) and in the

 case of procedure names such things as the number and types of

 its arguments the method of passing each argument (for example

 by value or by reference) and the type returned



 The symbol_table is a data_structure containing a record for each

 variable name with fields for the attributes of the name The

 data_structure should be designed to allow the compiler to find

 the record for each name quickly and to store or retrieve data

 from that record quickly Symbol_tables are discussed in

 Chapter simple-ch



 The Grouping of Phases into Passes



 The discussion of phases deals_with the logical organization of a

 compiler In an implementation activities from several phases may

 be grouped together into a pass that reads an input file and

 writes an output_file For_example the front-end phases of

 lexical analysis syntax analysis semantic analysis and

 intermediate_code generation might be grouped together into one

 pass Code optimization might be an_optional pass Then there

 could be a back-end pass consisting of code_generation for a

 particular target_machine



 Some compiler collections have_been created around carefully

 designed intermediate_representations that allow the front_end for

 a particular language to interface with the back end for a certain

 target_machine With these collections we can produce compilers

 for different source languages for one target_machine by combining

 different front_ends with the back end for that target_machine

 Similarly we can produce compilers for different target machines

 by combining a front_end with back ends for different target

 machines



 Compiler-Construction Tools



 The compiler_writer like any software developer can profitably

 use modern software development environments containing tools such

 as language editors debuggers version managers profilers test

 harnesses and so on In_addition to these general

 software-development tools other more specialized tools have_been

 created to help implement various phases of a compiler



 These tools use specialized languages for specifying and

 implementing specific components and many use quite sophisticated

 algorithms The most successful tools are those that hide the

 details of the generation algorithm and produce components that

 can be easily integrated into the remainder of the compiler Some

 commonly_used compiler-construction tools include



 enumerate



 Parser generators that automatically produce syntax analyzers

 from a grammatical description of a programming_language



 Scanner generators that produce lexical_analyzers from a

 regular-expression description of the tokens of a language



 Syntax-directed_translation engines that produce collections

 of routines for walking a parse_tree and generating intermediate_code



 Code-generator generators that produce a code

 generator from a collection of rules for translating each

 operation of the intermediate language into the machine language

 for a target_machine



 Data-flow_analysis engines that facilitate the

 gathering of information_about how values are transmitted from one

 part of a program to each other part Data-flow_analysis is a key

 part of code_optimization



 Compiler-construction toolkits that provide an integrated

 set of routines for constructing various phases of a compiler



 enumerate



 We_shall describe many of these tools throughout this_book

 The Science of Building a Compiler

 science-sect



 Compiler design is full of beautiful examples where complicated

 real-world problems are solved by abstracting the essence of the

 problem mathematically These serve as excellent illustrations of

 how abstractions can be used to solve problems take a problem

 formulate a mathematical abstraction that captures the key

 characteristics and solve it using mathematical techniques The

 problem formulation must_be grounded in a solid understanding of

 the characteristics of computer programs and the solution must_be

 validated and refined empirically



 A compiler must accept all source programs that conform to the

 specification of the language the set of source programs is

 infinite and any program can be very_large consisting of possibly

 millions of lines of code Any transformation performed by the

 compiler while translating a source_program must preserve the

 meaning of the program being compiled Compiler writers thus have

 influence over not just the compilers they create but all the

 programs that their compilers compile This leverage makes

 writing compilers particularly rewarding however it also makes

 compiler development challenging



 Modeling in Compiler Design and Implementation



 The study of compilers is mainly a study of how we design the

 right mathematical models and choose the right algorithms while

 balancing the need for generality and power against simplicity and

 efficiency



 Some of most fundamental models are finite-state machines and

 regular_expressions which we_shall meet in

 Chapter lexan-ch These models are useful for describing

 the lexical units of programs (keywords identifiers and such)

 and for describing the algorithms used by the compiler to

 recognize those units Also among the most fundamental models are

 context-free_grammars used to describe the syntactic_structure of

 programming_languages such_as the nesting of parentheses or

 control constructs We_shall study grammars in

 Chapter_parse-ch Similarly trees are an important model

 for representing the structure of programs and their translation

 into object code as we_shall see in Chapter_sdt-ch



 The Science of Code Optimization

 study-comp-opt-subsect



 The term optimization in compiler design refers to the attempts that

 a compiler makes to produce code that is more_efficient than the obvious

 code Optimization is thus a misnomer since there is no way that

 the code produced_by a compiler can be guaranteed to be as fast or

 faster_than any other code that performs the same task



 In modern times the optimization of code that a compiler performs

 has become both more important and more_complex It is more

 complex because processor architectures have become more_complex

 yielding more opportunities to improve the way code executes It

 is more important because massively parallel computers require

 substantial optimization or their performance suffers by orders

 of magnitude With the likely prevalence of multicore machines

 (computers with chips that have large_numbers of processors on

 them) all compilers will have to face the problem of taking

 advantage of multiprocessor machines



 It is hard if not impossible to build a robust compiler out of

 hacks Thus an extensive and useful theory has_been built up

 around the problem of optimizing code The use of a rigorous

 mathematical foundation allows_us to show that an optimization is

 correct and that it produces the desirable effect for all possible

 inputs We_shall see starting in Chapter_code-op-ch how models

 such_as graphs matrices and linear programs are necessary if the

 compiler is to produce well optimized code



 On the other_hand pure theory alone is insufficient Like

 many real-world problems there are no perfect answers In_fact most

 of the questions that we ask in compiler optimization are

 undecidable One of the most_important skills in compiler design is

 the ability to formulate the right problem to solve We need a good

 understanding of the behavior of programs to start with and thorough

 experimentation and evaluation to validate our intuitions



 Compiler optimizations must meet the following design objectives

 itemize

 The optimization must_be correct that is preserve the

 meaning of the compiled program



 The optimization must

 improve the performance of many programs



 The compilation time must_be kept reasonable and



 The engineering effort required must_be manageable

 itemize



 It is impossible to overemphasize the importance of correctness

 It is trivial to write a compiler that generates fast code if the

 generated code need not be correct Optimizing compilers are so

 difficult to get right that we dare say that no optimizing

 compiler is completely error-free Thus the most_important

 objective in writing a compiler is that it is correct



 The second goal is that the compiler must_be effective in improving

 the performance of many input programs Normally performance means

 the speed of the program execution Especially in embedded

 applications we may also wish to minimize the size of the generated

 code And in the case of mobile devices it is also desirable that

 the code minimizes power consumption Typically the same

 optimizations that speed_up execution time also conserve power

 Besides performance usability aspects such_as error reporting and

 debugging are also important



 Third we need to keep the compilation time short to support a rapid

 development and debugging cycle This requirement has become easier

 to meet as machines get faster Often a program is first

 developed and debugged without program optimizations Not_only is the

 compilation time reduced but more_importantly unoptimized programs are

 easier to debug because the optimizations introduced by a compiler

 often obscure the relationship_between the source code and the

 object code Turning on optimizations in the compiler

 sometimes exposes new

 problems in the source_program thus testing must again be performed

 on the optimized code The need for additional testing sometimes

 deters the use of optimizations in applications especially if their

 performance is not critical



 Finally a compiler is a complex system we must keep the system

 simple to assure that the engineering and maintenance costs

 of the compiler are manageable There is

 an_infinite number of program optimizations that we could implement and

 it takes a nontrivial amount of effort to create a correct and

 effective optimization We must prioritize the optimizations

 implementing only those that lead to the greatest benefits on

 source programs encountered in practice



 Thus in studying compilers we learn not only how to build a

 compiler but also the general methodology of solving complex and

 open-ended problems The approach used in compiler development

 involves both theory and experimentation We normally start by

 formulating the problem based_on our intuitions on what the important

 issues are

 itemize





 Architectural Issues

 Optimized code scheduling takes advantage of features of modern computer

 architectures Such machines often allow pipelined execution where

 several instructions are in different stages of execution at the same

 time Some machines also allow several instructions to begin execution

 at the same time





 Data Dependences

 When scheduling instructions we must_be aware of the effect

 instructions have on each memory_location and register True data

 dependences occur when one instruction must read a location after another has

 written it Antidependences occur when there is a write after a read

 and output dependences occur when there are two writes to the same_location





 Eliminating Dependences

 By using additional locations to store data antidependences and output

 dependences can be_eliminated Only true dependences cannot be

 eliminated and must surely be respected when the code is scheduled





 Data-Dependence Graphs for Basic_Blocks

 These graphs represent the timing_constraints among the statements of a

 basic_block Nodes correspond to the statements An edge from to

 labeled says_that the instruction must start at_least

 clock cycles after instruction starts





 Prioritized Topological Orders

 The data-dependence_graph for a basic_block is always acyclic and there

 usually are many topological orders consistent_with the graph One of

 several heuristics can be used to select a preferred topological_order

 for a given graph eg choose nodes with the longest critical path

 first





 List Scheduling

 Given a prioritized topological_order for a data-dependence_graph we

 may consider the nodes in that order Schedule each node at the

 earliest clock cycle that is consistent_with the timing_constraints

 implied by the graph edges the schedules of all previously scheduled

 nodes and the resource constraints of the machine





 Interblock Code_Motion

 Under some circumstances it is possible to move statements from the

 block in which they appear to a predecessor or successor block The

 advantage is that there may be opportunities to execute instructions in

 parallel at the new location that do_not exist at the original location

 If there is not a dominance relation between the old and new locations

 it may be necessary to insert compensation_code along certain paths in

 order to make_sure that exactly the same sequence of instructions is

 executed regardless of the flow of control





 Do-All Loops

 A do-all loop has no dependences across iterations so any iterations

 may be executed in parallel





 Software Pipelining of Do-All Loops

 Software_pipelining is a technique for

 exploiting the ability of a machine to execute several instructions at

 once We schedule iterations of the loop to begin at small intervals

 perhaps placing no-op instructions in the iterations to avoid conflicts

 between iterations for the machine's resources The result is that the

 loop can be executed quickly with a preamble a coda and (usually) a

 tiny inner_loop





 Do-Across Loops

 Most loops have data_dependences from each iteration to later

 iterations These are called do-across loops





 Data-Dependence Graphs for Do-Across Loops

 To represent the dependences among instructions of a do-across loop

 requires that the edges be labeled by a pair of values the required

 delay (as for graphs representing basic blocks) and the number of

 iterations that elapse between the two instructions that have a

 dependence





 List Scheduling of Loops

 To schedule a loop we must choose the one schedule for all the iterations

 and also choose the initiation_interval at which successive iterations

 commence The algorithm involves deriving the constraints on the

 relative schedules of the various instructions in the loop by finding

 the length of the longest acyclic_paths between the two nodes These

 lengths have the initiation_interval as a parameter and thus put a

 lower_bound on the initiation_interval



 itemize

 itemize





 Parallelism and Locality from Arrays

 The most_important opportunities for both parallelism and locality-based

 optimizations come from loops that access arrays These loops tend to

 have limited dependences among accesses to array_elements and tend to

 access arrays in a regular pattern allowing efficient use of the cache

 for good locality





 Affine Accesses

 Almost all theory and techniques for parallelism and locality

 optimization assume accesses to arrays are affine the expressions for

 the array indexes are linear functions of the loop_indexes





 Iteration Spaces

 A loop_nest with nested_loops defines a -dimensional iteration

 space The points in the space are the -tuples of values that the

 loop_indexes can assume during the execution of the loop_nest In the

 affine case the limits on each loop_index are linear functions of the

 outer_loop indexes so the iteration_space is a polyhedron





 Fourier-Motzkin Elimination

 A key manipulation of iteration_spaces is to reorder the loops that

 define the iteration_space Doing_so requires that a polyhedral

 iteration_space be projected onto a subset of its dimensions The

 Fourier-Motzkin algorithm replaces the upper and lower limits on a given

 variable by inequalities between the limits themselves





 Data Dependences and Array Accesses

 A central problem we must solve in order to manipulate loops for

 parallelism and locality optimizations is whether two array_accesses

 have a data_dependence (can

 touch the same array element) When the accesses and loop_bounds are

 affine the problem can be_expressed as whether there are solutions to a

 matrix-vector equation within the polyhedron that defines the iteration

 space





 Matrix Rank and Data Reuse

 The matrix that describes an array_access can tell_us several important

 things about that access If the rank of the matrix is as large as

 possible (minimum of the number of rows and number of columns) then the

 access never touches the same element twice as the loops iterate If

 the array is stored in row- (column-)major form then the rank of the matrix

 with the last (first) row deleted tells_us whether the access has good

 locality ie elements in a single cache_line are accessed at about

 the same time





 Data_Dependence and Diophantine Equations

 Just because two accesses to the same array touch the same region of the

 array does_not mean that they actually access any element in common

 The_reason is that each may skip some elements eg one accesses even

 elements and the other accesses odd elements In order to be_sure that

 there is a data_dependence we must solve a Diophantine (integer

 solutions only) equation





 Solving Diophantine Linear Equations

 The key technique is to compute the greatest common divisor (GCD) of the

 coefficients of the variables Only if that GCD divides the constant

 term will there be integer_solutions





 Space-Partition Constraints

 To parallelize the execution of a loop_nest we need to map the

 iterations of the loop to a space of processors which can have one or

 more dimensions The space-partition_constraints say that if two

 accesses in two different iterations share a data_dependence (ie they

 access the same array element) then they must map to the same

 processor As_long as the mapping of iterations to processors is

 affine we can formulate the problem in matrix-vector terms





 Primitive Code Transformations

 The transformations used to parallelize programs with affine array

 accesses are combinations of seven primitives loop fusion loop

 fission re-indexing (adding a constant to loop indexes) scaling

 (multiplying loop_indexes by a constant) reversal (of a loop index)

 permutation (of the order of loops) and skewing (rewriting loops so the

 line of passage through the iteration_space is no_longer along one of

 the axes)





 Synchronization of Parallel Operations

 Sometimes more parallelism can be obtained if we insert synchronization

 operations between steps of a program For_example consecutive loop

 nests may have data_dependences but synchronizations between the loops

 can allow the loops to be_parallelized separately





 Pipelining

 This parallelization technique allows processors to share data by

 synchronously passing certain data (typically array elements) from one

 processor to an adjacent processor in the processor space The method

 can improve the locality of the data_accessed by each processor





 Time-Partition_Constraints

 To discover opportunities for pipelining we need to discover solutions

 to the time-partition_constraints These say that whenever two array

 accesses can touch the same array_element then the access in the

 iteration that occurs first must_be assigned to a stage in the

 pipeline that occurs no later than the stage to which the second access

 is assigned





 Solving Time-Partition_Constraints

 Farkas'_Lemma provides a powerful technique for finding all the affine

 time-partition_mappings that are allowed by a given

 loop_nest with array_accesses The technique is essentially to replace

 the primal formulation of the linear_inequalities that express the

 time-partition_constraints by their dual





 Blocking

 This technique breaks each of several loops in

 a loop_nest into two loops each The advantage is that doing_so may

 allow_us to work on small sections (blocks) of a multidimensional array one

 block at a time That in turn improves the locality of the program

 letting all the needed data reside in the cache while working on a

 single block





 Stripmining

 Similar to blocking this technique breaks only a subset of the loops of

 a loop_nest into two loops each A possible advantage is that a

 multidimensional array is accessed a strip at a time which may lead

 to the best possible cache utilization



 itemize

 itemize





 Interprocedural_Analysis

 A data-flow_analysis that tracks information across procedure boundaries

 is said to be interprocedural Many analyses such_as points-to

 analysis can only be done in a meaningful way if they are

 interprocedural





 Call Sites

 Programs call procedures at certain points referred to as call_sites

 The procedure called at a site may be obvious or it may be ambiguous

 should the call be indirect through a pointer or a call of a virtual

 method that has several implementations





 Call Graphs

 A call_graph for a program is a bipartite graph with nodes for call

 sites and nodes for procedures An edge goes from a call-site node to a

 procedure node if that procedure may be called at the site





 Inlining As_long as there is no recursion in a program we can

 in principle replace all procedure_calls by copies of their code and

 use intraprocedural analysis on the resulting program This analysis is

 in effect interprocedural





 Flow Sensitivity and Context-Sensitivity

 A data-flow_analysis that produces facts that depend_on location in the

 program is said to be flow-sensitive If the analysis produces facts

 that depend_on the history of procedure_calls is said to be

 context-sensitive A data-flow_analysis can be either flow- or

 context-sensitive both or neither





 Cloning-Based Context-Sensitive_Analysis

 In principle once we establish the different_contexts in which a

 procedure can be called we can imagine that there is a clone of each

 procedure for each context In that way a context-insensitive_analysis

 serves as a context-sensitive_analysis





 Summary-Based Context-Sensitive_Analysis

 Another_approach to interprocedural_analysis extends the region-based

 analysis technique that was described for intraprocedural analysis

 Each procedure has a transfer_function and is treated_as a region at

 each place where that procedure is called





 Applications of Interprocedural_Analysis

 An_important application requiring interprocedural_analysis is the

 detection of software vulnerabilities These are often characterized by

 having data read from an untrusted input source by one procedure and

 used in an exploitable way by another procedure





 Datalog

 The language Datalog is a simple notation for if-then rules that can be

 used to describe data-flow_analyses at a high_level Collections of

 Datalog_rules or Datalog_programs can be evaluated using one of

 several standard algorithms





 Datalog Rules

 A Datalog rule consists of a body (antecedent) and head (consequent)

 The body is one or_more atoms and the head is an atom Atoms are

 predicates applied to arguments that are variables or constants

 The atoms of the body are connected by logical AND and an atom in the

 body may be negated





 IDB and EDB Predicates

 EDB_predicates in a Datalog_program have their true facts given

 a-priori In a data-flow_analysis these predicates correspond to the

 facts that can be obtained from the code being analyzed IDB_predicates

 are defined by the rules themselves and correspond in a data-flow

 analysis to the information we are trying to extract from the code being

 analyzed





 Evaluation of Datalog_programs

 We apply rules by substituting constants for variables that make the

 body true Whenever we do so we infer that the head with the same

 substitution for variables is also true This operation is repeated

 until_no more facts can be inferred





 Incremental Evaluation of Datalog Programs

 An efficiency improvement is

 obtained_by doing incremental evaluation We perform a series of

 rounds In one round we consider only substitutions of constants for

 variables that make at_least one atom of the body be a fact that was

 just discovered on the previous round





 Java Pointer_Analysis

 We can model pointer_analysis in Java by a framework in which there are

 reference_variables that point to heap_objects which may have fields

 that point to other heap_objects An insensitive pointer_analysis can

 be written as a Datalog_program that infers two kinds of facts a

 variable can point to a heap_object or a field of a heap_object can

 point to another heap_object





 Type Information to Improve Pointer_Analysis

 We can get more_precise pointer_analysis if we take_advantage of the

 fact that reference_variables can only point to heap_objects that are of

 the same type as the variable or a subtype





 Interprocedural Pointer_Analysis

 To make the analysis interprocedural we must add rules that reflect how

 parameters are passed and return values assigned to variables These

 rules are essentially the same as the rules for copying one reference

 variable to another





 Call-Graph Discovery

 Since Java has virtual methods

 interprocedural_analysis requires that we first limit what procedures

 can be called at a given call_site The principal way to discover

 limits on what can be called where is to analyze the types of objects

 and take_advantage of the fact that the actual method referred to by a

 virtual_method call must belong to an appropriate class





 Context-Sensitive_Analysis

 When procedures are recursive we must condense the information

 contained in call strings into a finite number of contexts An

 effective way to do so is to drop from the call string any call_site

 where a procedure_calls another procedure (perhaps itself) with which it

 is mutually_recursive Using this representation we can modify the

 rules for intraprocedural pointer_analysis so the context is carried

 along in predicates this approach simulates cloning-based analysis





 Binary Decision Diagrams

 BDD's are a succinct representation of boolean_functions by rooted DAG's

 The interior_nodes correspond to boolean variables and have two children

 low (representing truth value 0) and high (representing 1) There_are

 two leaves labeled 0 and 1

 A truth_assignment makes the represented

 function true if and only if the path from the root in which we go to

 the low child if the variable at a node is 0 and to the high child

 otherwise leads to the 1 leaf





 BDD's and Relations

 A BDD can serve as a succinct representation of one of the predicates in

 a Datalog_program Constants are encoded as truth assignments to a

 collection of boolean variables and the function represented_by the BDD

 is true if an only if the boolean variables represent a true fact for

 that predicate





 Implementing Data-Flow_Analysis by BDD's

 Any data-flow_analysis that can be_expressed as Datalog_rules can be

 implemented_by manipulations on the BDD's that represent the predicates

 involved in those rules Often this representation leads to a more

 efficient_implementation of the data-flow_analysis than any other known

 approach



 itemize

 itemize



 Language Processors An integrated software development

 environment includes many different_kinds of language processors

 such_as compilers interpreters assemblers linkers loaders

 debuggers and profilers



 Compiler Phases A compiler operates as a

 sequence of phases each of which transforms the source_program

 from one intermediate_representation to another



 Machine and Assembly Languages Machine languages

 were the first-generation programming_languages followed

 by assembly languages Programming in these languages was time

 consuming and error prone



 Modeling in Compiler Design Compiler design is one of

 the places_where theory has had the most impact on practice Models that

 have_been found useful include automata grammars regular_expressions

 trees and many others



 Code Optimization Although code cannot truly be

 optimized the science of improving the efficiency of code is both

 complex and very important It is a major portion of the study of

 compilation



 Higher-Level Languages As time goes on programming

 languages take on progressively more of the tasks that formerly were left to

 the programmer such_as memory_management type-consistency

 checking or parallel_execution of code



 Compilers and Computer Architecture Compiler

 technology influences computer architecture as_well as being influenced by

 the advances in architecture Many modern innovations in architecture

 depend_on compilers being able to extract from source programs the

 opportunities to use the hardware capabilities effectively



 Software Productivity and Software Security The same

 technology that allows compilers to optimize code can be used for a variety

 of program-analysis tasks ranging_from detecting common program bugs to

 discovering that a program is vulnerable to one of the many kinds of

 intrusions that hackers have discovered



 Scope Rules The scope of a declaration

 of is the context in which uses of refer to this

 declaration A language uses static_scope or lexical

 scope if it is possible to determine the scope of a declaration

 by_looking only at the program Otherwise the language uses dynamic_scope



 Environments The association of names with

 locations in memory and then with values can be described in terms

 of environments which map names to locations in store and

 states which map locations to their values



 Block Structure Languages that allow blocks

 to be nested are said to have block structure A name

 in a nested block is in the scope of a declaration of

 in an enclosing block if there is no other declaration of in

 an intervening block



 Parameter Passing Parameters are passed from a

 calling_procedure to the callee either by value or by

 reference When large objects are passed by value the values passed

 are really references to the objects themselves resulting in an

 effective call-by-reference



 Aliasing When parameters are (effectively) passed

 by reference two formal_parameters can refer to the same object This

 possibility allows a change in one variable to change another



 itemize





 The syntax-directed techniques in this_chapter can be used to

 construct compiler_front ends such_as those illustrated in

 Fig sum-fig

 figurehtfb

 Two

 possible translations of a statement sum-fig

 figure



 itemize



 The starting point for a syntax-directed translator

 is a grammar for the source_language A grammar describes

 the hierarchical_structure of programs It is defined in terms

 of elementary symbols called terminals and variable symbols

 called nonterminals These symbols represent language

 constructs The rules or productions of a grammar consist of

 a nonterminal called the head or left_side of a

 production and a sequence of terminals and nonterminals called the

 body or right_side of the production One nonterminal

 is designated as the start_symbol



 In specifying a translator it is helpful to attach

 attributes to programming_constructs where an attribute is

 any quantity associated_with a construct Since constructs are

 represented_by grammar_symbols the concept of attributes extends

 to grammar_symbols Examples of attributes include an_integer

 value associated_with a terminal num representing numbers

 and a string associated_with a terminal id representing

 identifiers



 A lexical_analyzer reads the input one

 character at a time and produces as output a stream of tokens where a

 token consists of a terminal symbol along with additional

 information in the form of attribute values In

 Fig sum-fig tokens are written as tuples enclosed between

 The token

 consists of the terminal id

 and a pointer to the symbol-table_entry containing

 the string peek The translator uses the

 table to keep_track of reserved_words and identifiers that

 have_already been seen



 Parsing is the problem of figuring out how a

 string of terminals can be derived_from the start_symbol of the

 grammar by repeatedly replacing a nonterminal by the body of one

 of its productions Conceptually a parser builds a parse_tree in

 which the root is labeled with the start_symbol each nonleaf

 corresponds to a production and each leaf is labeled with a

 terminal or the empty_string The parse_tree derives

 the string of terminals at the leaves read from left to right



 Efficient parsers can be built by hand using a

 top-down (from the root to the leaves of a parse tree) method

 called predictive_parsing A predictive_parser has a

 procedure for each nonterminal procedure bodies mimic the

 productions for nonterminals and the flow of control through the

 procedure bodies can be determined unambiguously by_looking one

 symbol ahead in the input stream See Chapter_parse-ch for

 other approaches to parsing



 Syntax-directed_translation is done by_attaching

 either rules or program_fragments to productions in a grammar In

 this_chapter we have considered only synthesized

 attributes - the value of a synthesized_attribute at any node

 can depend only on attributes at the children of if any

 A syntax-directed_definition attaches rules to productions

 the rules compute attribute values A translation_scheme

 embeds program_fragments called semantic_actions in

 production_bodies The actions are executed in the order that

 productions are used during syntax analysis



 The result of syntax analysis is a representation of

 the source_program called intermediate_code Two primary

 forms of intermediate_code are illustrated in Fig sum-fig

 An abstract_syntax tree has nodes for programming

 constructs the children of a node give the meaningful

 subconstructs Alternatively three-address_code is a

 sequence of instructions in which each instruction carries out a

 single operation



 Symbol_tables are data_structures that hold

 information_about identifiers Information is put into the symbol

 table when the declaration of an_identifier is analyzed A

 semantic_action gets information from the symbol_table when the

 identifier is subsequently used for example as a factor in an

 expression



 itemize

 itemize





 Tokens

 The lexical_analyzer scans the source_program and produces

 as output a sequence of

 tokens which are normally passed one at a time to the parser

 Some tokens may consist only of a token_name

 while others may also have an associated lexical value that

 gives information_about the

 particular instance of the token that has_been found on the input





 Lexemes

 Each time the lexical_analyzer returns a token to the parser it has an

 associated lexeme - the sequence of input_characters that the token

 represents





 Buffering

 Because it is often necessary to scan ahead on the input in order to see

 where the next lexeme ends it is usually necessary for the lexical

 analyzer to buffer its input Using a pair of buffers cyclicly and

 ending each buffer's contents with a sentinel that warns of its end are

 two techniques that accelerate the process of scanning the input





 Patterns

 Each token has a pattern that describes which sequences of characters

 can form the lexemes corresponding to that token

 The set of words or strings of characters that match a given pattern

 is called a language





 Regular Expressions

 These expressions

 are commonly_used to describe patterns Regular_expressions are built

 from single characters using union concatenation and

 the Kleene closure or any-number-of operator





 Regular Definitions

 Complex collections of languages such_as the patterns that describe the

 tokens of a programming_language are often defined by a regular

 definition which is a sequence of statements that each define one

 variable to stand_for some regular_expression The regular_expression

 for one variable can use previously defined variables in its regular

 expression





 Extended Regular-Expression Notation

 A number of additional operators may appear

 as shorthands in regular_expressions to make it easier to

 express patterns

 Examples include the operator (one-or-more-of) (zero-or-one-of)

 and character classes (the union of the strings each consisting of one

 of the characters)





 Transition Diagrams

 The behavior of a lexical_analyzer can often be described by a

 transition_diagram These diagrams have states each of which

 represents something about the history of the characters seen during the

 current search for a lexeme that matches one of the possible patterns

 There_are arrows or transitions from one state

 to another each of which indicates

 the possible next_input characters that cause the lexical_analyzer to

 make that change of state





 Finite_Automata

 These are a formalization of transition_diagrams that include a

 designation of a start_state and one or_more accepting_states as_well

 as the set of states input_characters and transitions among states

 Accepting states indicate that the lexeme for some token has_been

 found

 Unlike transition_diagrams

 finite_automata can make transitions on empty input as

 well_as on input_characters





 Deterministic Finite_Automata

 A DFA is a special kind of finite_automaton that has exactly one

 transition out of each state for each input_symbol Also transitions on

 empty input are disallowed The DFA is easily simulated and makes a

 good implementation of a lexical_analyzer similar to a transition

 diagram





 Nondeterministic Finite_Automata

 Automata that are not DFA's are called nondeterministic NFA's

 often are easier to design than are DFA's

 Another possible architecture for a lexical_analyzer is to tabulate all

 the states that NFA's for each of the possible patterns can be in as we

 scan the input_characters





 Conversion Among Pattern Representations

 It is possible to convert any regular_expression into an NFA of about

 the same size recognizing the same language as the regular_expression

 defines

 Further any NFA can be converted to a DFA for the same pattern although

 in the worst_case (never encountered in common programming languages)

 the size of the automaton can grow exponentially

 It is also possible to convert any nondeterministic or deterministic

 finite_automaton into a regular_expression that defines the same

 language recognized by the finite_automaton



 Lex

 There is a family of software systems including Lex and Flex that are lexical-analyzer generators The user specifies the

 patterns for tokens using an extended regular-expression notation

 Lex converts these expressions into a lexical_analyzer that is

 essentially a deterministic_finite

 automaton that recognizes any of the patterns





 Minimization of Finite_Automata

 For every DFA there is a minimum-state_DFA accepting the same language

 Moreover the minimum-state_DFA for a given language is unique except

 for the names given to the various states



 itemize

 itemize





 Parsers

 A parser takes as input tokens from the lexical_analyzer and treats

 the token names as

 terminal_symbols of a context-free_grammar The parser then constructs

 a parse_tree for its input sequence of tokens the parse_tree may be

 constructed figuratively (by going_through the corresponding derivation

 steps) or literally





 Context-Free Grammars

 A grammar specifies a set of terminal_symbols (inputs) another set of

 nonterminals (symbols representing syntactic constructs) and a set of

 productions each of which gives a way in which strings represented_by

 one nonterminal can be constructed from terminal_symbols and

 strings represented_by certain other

 nonterminals A production consists of a head (the nonterminal to be

 replaced) and a body (the replacing string of grammar symbols)





 Derivations

 The process of starting_with the start-nonterminal of a grammar and

 successively replacing it by the body of one of its productions is

 called a derivation If the leftmost (or rightmost) nonterminal is

 always replaced then the derivation is called leftmost (respectively

 rightmost)





 Parse Trees

 A parse_tree is a picture of a derivation in which there is a node

 for each nonterminal that appears in the derivation The children of a

 node are the symbols by which that nonterminal is replaced in the

 derivation There is a one-to-one correspondence between parse_trees

 leftmost_derivations and rightmost_derivations of the same terminal

 string





 Ambiguity

 A grammar for which some terminal string has two or_more different parse

 trees or equivalently two or_more leftmost_derivations or two or_more

 rightmost_derivations is said to be ambiguous In most cases of

 practical interest it is possible to redesign an ambiguous_grammar so

 it becomes an unambiguous_grammar for the same language However

 ambiguous_grammars with certain tricks applied sometimes lead to more

 efficient parsers





 Top-Down and Bottom-Up Parsing

 Parsers are generally distinguished by whether they work top-down (start

 with the grammar's start_symbol and construct the parse_tree from the

 top) or bottom-up (start with the terminal_symbols that form the leaves

 of the parse_tree and build the tree from the bottom) Top-down parsers

 include recursive-descent and LL parsers while the most_common forms of

 bottom-up parsers are LR_parsers





 Design of Grammars

 Grammars suitable for top-down_parsing often are harder to design than

 those used by bottom-up parsers It is necessary to eliminate

 left-recursion a situation where one nonterminal derives a string that

 begins_with the same nonterminal We also must left-factor - group

 productions for the same nonterminal that have a common prefix in the

 body





 Recursive-Descent Parsers

 These parsers use a procedure for each nonterminal The procedure looks

 at its input and decides which production to apply for its nonterminal

 Terminals in the body of the production are matched to the input at the

 appropriate time while nonterminals in the body result in calls to

 their procedure Backtracking in the case when the wrong production

 was chosen is a possibility





 LL(1) Parsers

 A grammar such that it is possible to choose the correct production with

 which to expand a given nonterminal looking only at the next_input

 symbol is called LL(1) These grammars allow_us to construct a

 predictive_parsing table that gives for each nonterminal and each

 lookahead_symbol the correct choice of production Error correction

 can be facilitated by placing error_routines in some or all of the table

 entries that have no legitimate production





 Shift-Reduce Parsing

 Bottom-up parsers generally operate by choosing on the basis of the

 next_input symbol (lookahead symbol)

 and the contents of the stack whether to shift the

 next_input onto the stack or to reduce some symbols at the top of the

 stack A reduce step takes a production_body at the top of the stack

 and replaces it by the head of the production





 Viable Prefixes

 In shift-reduce_parsing the stack_contents are always a viable

 prefix - that is a prefix of some right-sentential_form that ends no

 further right than the end of the

 handle of that right-sentential_form The handle

 is the substring that was_introduced in the last step of the rightmost

 derivation of that sentential_form





 Valid Items

 An item is a production with a dot somewhere in the body An item is

 valid for a viable_prefix if the production of that item is used to

 generate the handle and the viable_prefix includes all those symbols to

 the left of the dot but not those to the right





 LR Parsers

 Each of the several kinds of LR_parsers operate by first constructing

 the sets of valid_items (called LR states)

 for all possible viable_prefixes and keeping

 track of the state for each prefix on the stack

 The set of valid_items guide the shift-reduce_parsing decision We

 prefer to reduce if there is a valid item with the dot at the right

 end of the body and we prefer to shift the lookahead_symbol onto the stack

 if that symbol appears immediately to the right of the dot in some valid

 item





 Simple LR Parsers

 In an SLR_parser we perform a reduction implied by a valid item with a

 dot at the right_end provided the lookahead_symbol can follow the head

 of that production in some sentential_form The grammar is SLR and

 this method can be applied if there are no parsing-action

 conflicts that is for no

 set of items and for no lookahead_symbol are there two productions to

 reduce by nor is there the option to reduce or to shift





 Canonical-LR Parsers

 This more_complex form of LR_parser uses items that are augmented by

 the set of lookahead symbols that can follow the use of the underlying

 production Reductions are only chosen when there is a valid item with

 the dot at the right_end and the current lookahead_symbol is one of

 those allowed for this item A canonical-LR parser can avoid some of

 the parsing-action_conflicts that are present in SLR parsers

 but often has many more

 states than the SLR_parser for the same grammar





 Lookahead-LR Parsers

 LALR_parsers offer many of the advantages of SLR and Canonical-LR

 parsers by combining the states that have the same kernels (sets of

 items ignoring the associated lookahead sets) Thus the number of

 states is the same as that of the SLR_parser but some

 parsing-action_conflicts present

 in the SLR_parser may be removed in the LALR_parser LALR_parsers

 have become the method of choice in practice





 Bottom-Up Parsing of Ambiguous Grammars

 In many important situations such_as parsing arithmetic_expressions we

 can use an ambiguous_grammar and exploit side information such_as the

 precedence of operators to resolve conflicts between shifting and

 reducing or between reduction by two different productions Thus LR

 parsing techniques extend to many ambiguous_grammars





 Yacc

 The parser-generator Yacc takes a (possibly) ambiguous_grammar and

 conflict-resolution information and constructs the LALR states It then

 produces a function that uses these states to perform a bottom-up_parse

 and call an associated function each time a reduction is performed



 itemize

 itemize



 Inherited and Synthesized Attributes

 Syntax-directed definitions may use two kinds of attributes A

 synthesized_attribute at a parse-tree_node is computed from

 attributes at its_children An inherited_attribute at a node is

 computed from attributes at its parent andor siblings



 Dependency Graphs Given a parse_tree and an

 we draw edges among the attribute instances associated

 with each parse-tree_node to denote that the value of the

 attribute at the head of the edge is computed in terms of the

 value of the attribute at the tail of the edge



 Cyclic Definitions In problematic 's

 we find that there are some parse_trees for which it is impossible

 to find an order in which we can compute all the attributes at all

 nodes These parse_trees have cycles in their associated

 dependency_graphs It is intractable to decide_whether an

 has such circular dependency_graphs



 S-Attributed Definitions In an S-attributed

 all attributes are synthesized



 L-Attributed Definitions In an L-attributed

 attributes may be inherited or synthesized However

 inherited_attributes at a parse-tree_node may depend only on

 inherited_attributes of its parent and on (any) attributes of

 siblings to its left



 Syntax_Trees Each node in a syntax_tree

 represents a construct the children of the node represent the

 meaningful_components of the construct



 Implementing S-Attributed 's An

 S-attributed definition can be_implemented by an in which

 all actions are at the end of the production (a postfix

 ) The actions compute the synthesized_attributes of the

 production head in terms of synthesized_attributes of the symbols

 in the body If the underlying_grammar is LR then this

 can be_implemented on the LR_parser stack



 Eliminating Left Recursion From 's If

 an has only side-effects (no attributes are computed) then

 the standard left-recursion-elimination algorithm for grammars

 allows_us to carry the actions along as if they_were terminals

 When attributes are computed we can still eliminate_left

 recursion if the is a postfix



 Implementing L-attributed 's by

 Recursive-Descent Parsing If we have an L-attributed_definition on a

 top-down parsable grammar we can build a recursive-descent_parser

 with no backtracking to implement the translation Inherited

 attributes become arguments of the functions for their

 nonterminals and synthesized_attributes are returned by that

 function



 Implementing L-Attributed 's on an LL

 Grammar Every L-attributed_definition with an underlying

 LL grammar can be_implemented along with the parse Records to

 hold the synthesized_attributes for a nonterminal are placed below

 that nonterminal on the stack while inherited_attributes for a

 nonterminal are stored with that nonterminal on the stack Action

 records are also placed on the stack to compute attributes at the

 appropriate time



 Implementing L-Attributed 's on an LL

 Grammar Bottom-Up An L-attributed_definition with an

 underlying LL grammar can be converted to a translation on an LR

 grammar and the translation performed in connection_with a

 bottom-up_parse The grammar transformation introduces marker

 nonterminals that appear on the bottom-up parser's stack and hold

 inherited_attributes of the nonterminal above it on the stack

 Synthesized attributes are kept with their nonterminal on the

 stack



 itemize

 The techniques in this_chapter can be_combined to build a simple

 compiler_front end like the one in Appendix together-sect

 The front_end can be built incrementally



 itemize



 Pick an intermediate_representation An

 intermediate_representation is typically some combination of a

 graphical notation and three-address_code As in syntax_trees a

 node in a graphical notation represents a construct the children

 of a node represent its subconstructs Three address code takes

 its name from instructions of the form with

 at most one operator per instruction There_are additional

 instructions for control_flow



 Translate expressions Expressions with

 built-up operations can be unwound into a sequence of individual

 operations by_attaching actions to each production of the form

 The action either creates a node

 for with the nodes for and as children or it

 generates a three-address_instruction that applies op to the

 addresses for and and puts the result into a new

 temporary_name which becomes the address for



 Check types The type of an expression

 is determined by the operator op and the

 types of and A coercion is an implicit type

 conversion such_as from integer to float Intermediate

 code contains explicit type_conversions to ensure an exact match

 between operand types and the types expected by an operator



 Use a symbol_table to implement

 declarations A declaration specifies the type of a name The

 width of a type is the amount of storage needed for a name with

 that type Using widths the relative_address of a name at_run

 time can be computed as an offset from the start of a data area

 The type and relative_address of a name are put into the symbol

 table due to a declaration so the translator can subsequently get

 them when the name appears in an expression



 Flatten arrays For quick access

 array_elements are stored in consecutive locations Arrays of

 arrays are flattened so they can be treated_as a one-dimensional

 array of individual elements The type of an array is used to

 calculate the address of an array_element relative to the base of

 the array



 Generate jumping_code for boolean

 expressions In short-circuit or jumping_code the value of a

 boolean_expression is implicit in the position reached in the

 code Jumping code is useful because a boolean_expression is

 typically used for control_flow as in Boolean

 values can be computed by jumping to ttrue or tfalse as appropriate where t is a temporary

 name Using labels for jumps a boolean_expression can be

 translated by inheriting labels corresponding to its true and

 false_exits The constants true and false translate

 into a jump to the true and false_exits respectively



 Implement statements using control_flow

 Statements can be translated by inheriting a label next

 where next marks the first instruction after the code for

 this statement The conditional can

 be translated by_attaching a new_label marking the beginning of

 the code for and passing the new_label and

 for the true and false_exits respectively of



 Alternatively use backpatching

 Backpatching is a technique for generating code for boolean

 expressions and statements in one pass The idea is to maintain

 lists of incomplete jumps where all the jump instructions on a

 list have the same target When the target becomes known all the

 instructions on its list are completed by filling in the target



 Implement records Field names in a record

 or class can be treated_as a sequence of declarations A record

 type encodes the types and relative_addresses of the fields A

 symbol_table object can be used for this purpose



 itemize

 itemize



 Run-Time Organization To implement the

 abstractions embodied in the source_language a compiler

 creates and manages a run-time environment in concert with

 the operating_system and the target_machine

 The run-time environment has static data areas for the

 object code and the static data objects_created at_compile time

 It also has dynamic stack and heap areas for managing objects_created

 and destroyed as the target program executes



 Control Stack Procedure calls and returns

 are usually managed by a run-time_stack called the control

 stack We can use a stack because procedure_calls or activations nest in time that is if calls then this

 activation of is nested_within this activation of



 Stack Allocation Storage for local_variables

 can be allocated on a run-time_stack for languages that allow or

 require local_variables to become inaccessible when their

 procedures end For such languages each live activation has an

 activation_record (or frame) on the control stack

 with the root of the activation tree at the bottom and the entire

 sequence of activation_records on the stack corresponding to the

 path in the activation tree to the activation where control

 currently resides The latter activation has its record at the top

 of the stack



 Access to Nonlocal Data on the Stack For

 languages_like C that do_not allow nested procedure declarations

 the location for a variable is either global or found in the

 activation_record on top of the run-time_stack For languages with

 nested procedures we can access nonlocal data on the stack

 through access_links which are pointers added to each

 activation_record The desired nonlocal data is found by following

 a chain of access_links to the appropriate activation_record A

 display is an auxiliary array used in conjunction with

 access_links that provides an efficient short-cut alternative to

 a chain of access_links



 Heap Management The heap is the portion

 of the store that is used for data that can live indefinitely or

 until the program deletes it explicitly The memory_manager

 allocates and deallocates space within the heap Garbage

 collection finds spaces within the heap that are no_longer in use

 and can therefore be reallocated to house other data items For

 languages that require it the garbage_collector is an important

 subsystem of the memory_manager



 Exploiting Locality By making good use of the

 memory hierarchy memory managers can influence the run_time of a

 program The time taken to access different parts of memory can

 vary from nanoseconds to milliseconds Fortunately most programs

 spend_most of their time executing a relatively_small fraction of

 the code and touching only a small_fraction of the data A program

 has temporal_locality if it is likely to access the same

 memory_locations again soon it has spatial_locality if it

 is likely to access nearby memory_locations soon



 Reducing Fragmentation As the program

 allocates and deallocates memory the heap may get fragmented or broken into large_numbers of small noncontiguous

 free spaces or holes The best fit strategy -

 allocate the smallest available hole that satisfies a request -

 has_been found empirically to work well While best fit tends to

 improve space utilization it may not be best for spatial

 locality Fragmentation can be reduced by combining or coalescing adjacent holes



 Manual Deallocation Manual memory_management

 has two common failings not deleting data that can not be

 referenced is a memory-leak error and referencing deleted

 data is a dangling-pointer-dereference error



 Reachability Garbage is data that

 cannot be referenced or reached There_are two basic ways

 of finding unreachable_objects either catch the transition as a

 reachable object turns unreachable or periodically locate all

 reachable_objects and infer that all remaining objects are

 unreachable



 Reference-Counting Collectors maintain a count

 of the references to an object when the count transitions to

 zero the object becomes_unreachable Such collectors introduce

 the overhead of maintaining references and can fail to find

 cyclic_garbage which consists of unreachable_objects that

 reference each other perhaps through a chain of references



 Trace-Based Garbage Collectors iteratively

 examine or trace all references to find reachable_objects

 starting_with the root_set consisting of objects that can be

 accessed directly without_having to dereference any pointers



 Mark-and-Sweep Collectors visit and mark all

 reachable_objects in a first tracing step and then sweep the heap

 to free up unreachable_objects



 Mark-and-Compact Collectors improve upon

 mark-and-sweep they relocate reachable_objects in the heap

 to eliminate memory fragmentation



 Copying Collectors break the dependency

 between tracing and finding free_space They partition the memory

 into two semispaces and Allocation requests are

 satisfied from one semispace say until it fills up at which

 point the garbage_collector takes over copies the reachable

 objects to the other space say and reverses the roles of the

 semispaces



 Incremental Collectors Simple trace-based

 collectors stop the user program while garbage is collected Incremental collectors interleave the actions of the garbage

 collector and the mutator or user program The mutator can

 interfere with incremental reachability analysis since it can

 change the references within previously scanned objects

 Incremental collectors therefore play it safe by overestimating

 the set of reachable_objects any floating garbage can be

 picked up in the next round of collection



 Partial Collectors also reduce pauses they

 collect a subset of the garbage at a time The best known of

 partial-collection algorithms generational_garbage

 collection partitions objects according to how long they have

 been allocated and collects the newly_created objects more often

 because they tend to have shorter lifetimes An alternative

 algorithm the train_algorithm uses fixed length

 partitions called cars that are

 collected into trains Each

 collection step is applied to the first remaining car of the first

 remaining train When a car is collected reachable_objects are

 moved out to other cars so this car is left with garbage and can

 be removed_from the train These two algorithms can be used

 together to create a partial collector that applies the

 generational algorithm to younger objects and the train_algorithm

 to more mature_objects



 itemize

 itemize





 Code generation is the final phase of a compiler

 The code_generator maps the intermediate_representation

 produced_by the front_end or if there is a code_optimization

 phase by the code optimizer into the target program





 Instruction selection is the process of choosing

 target-language instructions for each IR statement





 Register_allocation is the process of deciding

 which IR values to keep in registers Graph coloring

 is an effective technique for doing register_allocation

 in compilers





 Register assignment is the process of deciding

 which register should hold a given IR value





 A retargetable compiler is one that can generate

 code for multiple instruction sets





 A virtual machine is an interpreter for a

 bytecode intermediate language produced for languages

 such_as Java and C



 A machine is typically a two-address

 machine with relatively few registers several register classes

 and variable-length instructions with complex addressing_modes



 A machine is typically a three-address

 machine with many registers in which operations are done in

 registers





 A basic_block is a maximal sequence of consecutive

 three-address_statements in which flow of control can only enter

 at the first statement of the block and leave at the

 last statement without halting or branching except possibly

 at the last statement in the basic_block





 A flow_graph is a graphical representation of a program

 in which the nodes of the graph are basic_blocks and the edges

 of the graph show_how control can flow among the blocks





 A loop in a flow_graph is a strongly_connected region

 with a single entry point called the loop entry



 A representation of a basic_block is a

 directed acyclic_graph in which the nodes of the represent

 the statements within the block and each child of a node

 corresponds to the statement that is the last definition of an

 operand used in the statement





 Peephole optimizations are local code-improving

 transformations that can be applied to a program

 usually through a sliding window





 Instruction selection can be done by a tree-rewriting

 process in which tree patterns corresponding to machine

 instructions are used to tile a syntax_tree

 We can associate costs with the tree-rewriting_rules

 and apply dynamic_programming to obtain an optimal

 tiling for useful classes of machines and expressions



 An Ershov number tells how many registers are

 needed to evaluate an expression without storing any temporaries





 Spill code is an instruction sequence that

 stores a value in a register into memory in order

 to make room to hold another value in that register



 itemize

 itemize





 Global_Common Subexpressions

 An_important optimization is finding computations of the same expression

 in two different basic_blocks If one precedes the other we can store

 the result the first time it is computed and use the stored result on

 subsequent occurrences





 Copy Propagation

 A copy_statement assigns one variable to another In some

 circumstances we can replace all uses of by thus eliminating

 both the assignment and





 Code_Motion

 Another optimization is to move a computation outside the loop in which

 it appears This change is only correct if the computation produces the

 same value each time around the loop





 Induction_Variables

 Many loops have induction_variables

 variables that take on a linear sequence of

 values each time around the loop Some of these are used only to count

 iterations and they often can be_eliminated thus reducing the time it

 takes to go_around the loop





 Data-Flow_Analysis

 A data-flow_analysis schema defines a value at each point in the

 program Statements of the program have associated transfer_functions

 that relate the value before the statement to the value after

 Statements with more_than one_predecessor must have their value defined

 by combining the values at the predecessors using a meet (or

 confluence) operator





 Data-Flow_Analysis on Basic_Blocks

 Because the propagation of data-flow_values within a block is usually

 quite_simple data-flow_equations are generally set up to have two

 variables for each block called IN and OUT that represent the

 data-flow_values at the beginning and end of the block respectively

 The transfer_functions for the statements in a block are composed to get

 the transfer_function for the block as a whole





 Reaching_Definitions

 The reaching-definitions data-flow_framework

 has values that are sets of statements in the

 program that define values for one or_more variables The transfer

 function for a block kills definitions of variables that are definitely

 redefined in the block and adds (generates) those definitions of

 variables that occur within the block The confluence operator is

 union since definitions reach a point if they reach any predecessor of

 that point





 Live Variables

 Another important data-flow_framework computes the variables that are

 live (will be used before redefinition) at each point The framework is

 similar to reaching_definitions except that the transfer_function runs

 backward A variable is live at the beginning of a block if it is

 either used before definition in the block or is live at the end of the

 block and not redefined in the block





 Available_Expressions

 To discover global_common subexpressions we determine the available

 expressions at each point - expressions that have_been computed and

 neither of the expression's arguments were redefined after the last

 computation The data-flow_framework is similar to reaching

 definitions but the confluence operator is intersection rather_than

 union





 Abstraction of Data-Flow Problems

 Common data-flow_problems such_as those already mentioned

 can be_expressed in a common mathematical

 structure The values are members of a semilattice whose meet is the

 confluence operator Transfer_functions map lattice elements to lattice

 elements The set of allowed transfer_functions must_be closed_under

 composition and include the identity_function





 Monotone Frameworks

 A semilattice has a relation defined by if and only if

 Monotone frameworks have the property that each transfer

 function preserves the relationship that is implies

 for all lattice elements and and transfer

 function





 Distributive Frameworks

 These frameworks satisfy the condition that

 for all lattice elements and and transfer_function

 It can be shown that the distributive condition implies the monotone

 condition





 Iterative Solution to Abstract Frameworks

 All monotone data-flow_frameworks can be_solved by an iterative

 algorithm in which the IN and OUT values for each block are initialized

 appropriately (depending on the framework) and new values for these

 variables are repeatedly computed by_applying the transfer and

 confluence operations This solution is always safe (optimizations

 that it suggests will not change_what the program does) but the

 solution is certain to be the best possible only if the framework is

 distributive





 The Constant Propagation Framework

 While the basic frameworks such_as reaching_definitions are

 distributive there are interesting monotone-but-not-distributive

 frameworks as_well One involves propagating constants by using a

 semilattice whose elements are mappings from the program variables to

 constants plus two special values that represent no information and

 definitely not a constant





 Partial-Redundancy Elimination

 Many useful optimizations such_as code_motion and global

 common-subexpression_elimination can be generalized to a single problem

 called partial-redundancy_elimination Expressions that are needed but

 are available

 along only some of the paths to a point are computed only along the

 paths where they are not available The correct application of this

 idea requires the solution to a sequence of four different data-flow_problems

 plus other operations





 Dominators

 A node in a flow_graph dominates another if every_path to the latter

 must go_through the former A proper dominator is a dominator other

 than the node itself Each node except the entry_node has an

 immediate_dominator - that one of its proper dominators

 that is dominated by all the other proper dominators





 Depth-First Ordering of Flow_Graphs

 If we perform a depth-first_search of a flow_graph starting_at its

 entry we produce a depth-first_spanning tree The depth-first_order of

 the nodes is the reverse of a postorder_traversal of this tree





 Classification of Edges

 When we construct a depth-first_spanning tree

 all the edges of the flow_graph can be divided_into three groups

 advancing_edges (those that go from ancestor to proper descendant)

 retreating_edges (those from descendant to ancestor)

 and cross_edges (others) An_important property is that all the cross

 edges go from right to left in the tree Another important_property is

 that of these edges only the retreating_edges have a head lower than

 its_tail in the depth-first_order (reverse postorder)





 Back_Edges

 A back_edge is one whose head_dominates its_tail Every back_edge is a

 retreating_edge regardless of which depth-first_spanning tree for its

 flow_graph is chosen





 Reducible Flow_Graphs

 If every retreating_edge is a back_edge regardless of which depth-first

 spanning_tree is chosen then the flow_graph is said to be reducible

 The vast majority of flow_graphs are reducible those whose only

 control-flow statements are the usual loop-forming and branching

 statements are certainly reducible





 Natural Loops

 A natural_loop is a set of nodes with a header node that dominates all

 the nodes in the set and has at_least one back_edge entering that node

 Given any back_edge we can construct its natural_loop by_taking the

 head of the edge plus all nodes that can reach the tail of the edge

 without_going through the head Two natural_loops with different

 headers are either_disjoint or one is completely contained in the other

 this fact lets_us talk_about a hierarchy of nested_loops as_long as

 loops are taken to be natural_loops





 Depth-First Order Makes the Iterative_Algorithm Efficient

 The iterative_algorithm requires few passes as_long as propagation of

 information along acyclic_paths is sufficient ie cycles add

 nothing

 If we visit nodes in depth-first_order any data-flow_framework that propagates information forward eg reaching_definitions will converge in no more

 than 2 plus the largest_number of retreating_edges on any_acyclic path

 The same holds for backward-propagating frameworks like live_variables

 if we visit in the reverse of depth-first_order (ie in postorder)





 Regions

 Regions are sets of nodes and edges with a header that dominates all

 nodes in the region The predecessors of any node other_than in the

 region must also be in the region The edges of the region are all that

 go between nodes of the region with the possible exception of some or

 all that enter the header





 Regions and Reducible Flow_Graphs

 Reducible flow_graphs can be_parsed into a hierarchy of regions These

 regions are either loop regions which include all the edges into the

 header or body regions that have no edges into the header





 Region-Based Data-Flow_Analysis

 An alternative to the iterative_approach to data-flow_analysis is to

 work up and down the region_hierarchy computing transfer_functions from

 the header of each region to each node in that region





 Region-Based Induction Variable Detection

 An_important application of region-based_analysis is in a data-flow

 framework that tries to compute

 formulas for each variable in a loop

 region whose value is an affine (linear) function of the number of times

 around the loop



 itemize

 Switch-Statements



 The switch or case statement is available in a variety of

 languages Our switch-statement syntax is shown in

 Fig switch-syn-fig

 There is a selector expression which is to be evaluated

 followed_by constant values that the

 expression might take perhaps including a default

 value which always matches the expression if no other value

 does



 figurehtfb



 center

 tabularl

 switch (_) ''



 case



 case







 case



 default



 ''

 tabular

 center



 Switch-statement syntax

 switch-syn-fig



 figure



 Translation of Switch-Statements



 The intended translation of a switch is code to



 enumerate

 Evaluate the expression



 Find the value in the list of cases that is the same

 as the value of the expression Recall that the default value

 matches the expression if none of the values explicitly mentioned

 in cases does



 Execute the statement associated_with the value found

 enumerate



 Step (2) is an -way branch which can be_implemented in one of

 several ways If the number of cases is small say 10 at most

 then it is reasonable to use a sequence of conditional jumps each

 of which tests for an individual value and transfers to the code

 for the corresponding statement



 A compact way to implement this sequence of conditional jumps is

 to create a table of pairs each pair consisting of a value and a

 label for the corresponding statement's code The value of the

 expression itself paired with the label for the default statement

 is placed_at the end of the table at_run time A simple loop

 generated_by the compiler compares the value of the expression

 with each value in the table being assured that if no other match

 is found the last (default) entry is sure to match



 If the number of values exceeds 10 or so it is more_efficient to

 construct a hash_table for the values with the labels of the

 various statements as entries If no entry for the value possessed

 by the switch expression is found a jump to the default statement

 is generated



 There is a common special_case that can be_implemented even more

 efficiently than by an -way branch If the values all lie in

 some small range say min to max and the number of

 different values is a reasonable fraction of

 then we can construct an array of

 buckets where bucket contains

 the label of the statement with value any bucket that would

 otherwise remain unfilled contains the default label



 To perform the switch evaluate the expression to obtain the value

 check that it is in the range min to max and

 transfer indirectly to the table entry at offset

 For_example if the expression is of type character a table of

 say 128 entries (depending on the character set) may be created

 and transferred through with no range testing



 Syntax-Directed Translation of Switch-Statements



 The intermediate_code in Fig switch-trans1-fig is a

 convenient translation of the switch-statement in

 Fig switch-syn-fig The tests all appear at the end so that

 a simple code_generator can recognize the multiway branch and

 generate efficient code for it using the most appropriate

 implementation suggested at the beginning of this_section



 figurehtfb

 center

 tabularl_l

 code to evaluate into t



 goto test



 code for



 goto next



 code for



 goto next







 code for



 goto next



 code for



 goto next



 test if t goto



 if t goto







 if t goto



 goto



 next

 tabular

 Translation of a switch-statementswitch-trans1-fig

 center

 figure



 The more straightforward sequence shown in

 Fig switch-trans2-fig would_require the compiler to do

 extensive analysis to find the most efficient_implementation Note

 that it is inconvenient in a one-pass compiler to place the

 branching statements at the beginning because the compiler could

 not then emit code for each of the statements as it saw

 them

 figurehtfb

 center

 tabularl_l

 code to evaluate into t



 if t goto



 code for



 goto next



 if t goto



 code for



 goto next











 if t goto



 code for



 goto next



 code for



 next

 tabular

 Another translation of a switch

 statement

 switch-trans2-fig

 center

 figure



 To translate into the form of Fig switch-trans1-fig when

 we see the keyword switch we generate two new labels test and next and a new_temporary Then as we

 parse the expression we generate code to evaluate into

 After processing we generate the jump





 Then as we see each case keyword we create a new_label

 and enter it into the symbol_table We place in a

 queue used only to store cases a value-label pair consisting of

 the value of the case constant and (or a

 pointer to the symbol-table_entry for ) We process

 each statement case by emitting the label

 attached to the code for followed_by the jump





 When the end of the switch is found we are ready to generate the

 code for the -way branch Reading the queue of value-label

 pairs we can generate a sequence of three-address_statements of

 the form shown in Fig switch-trans3-fig

 There is the temporary holding the value of the selector

 expression and is the label for the default

 statement



 figurehtfb



 center

 tabularl

 case t



 case t







 case t



 case t t



 next

 tabular

 center



 Case three-address-code instructions used to translate a

 switch-statement

 switch-trans3-fig



 figure



 The

 instruction is a synonym for

 in

 Fig switch-trans1-fig but the case instruction is easier for the

 final code_generator to detect as a candidate for special

 treatment At the code-generation phase these sequences of case statements can be translated_into an -way branch of the

 most efficient type depending_on how many there are and whether

 the values fall into a small range



 hexer

 In order to translate a switch-statement into a sequence of

 case-statements as in Fig switch-trans3-fig

 the translator needs to

 create the list of value-label pairs as it processes the source code for

 the switch We can do so using an additional translation that accumulates

 just the pairs Sketch a syntax-directed_definition that produces the

 list of pairs while also emitting code for the statements that are

 the actions for each case

 hexer

 Symbolic Analysis



 We_shall use symbolic analysis in this_section to illustrate the use

 of region-based_analysis In this analysis we track the values of

 variables in programs symbolically as expressions of input variables

 and other variables which we call reference_variables

 Expressing variables in terms of the same set of reference

 variables draws out their relationships

 Symbolic analysis can be used for

 a range of purposes such_as optimization parallelization and analyses

 for program understanding













 An_example program motivating symbolic analysis







 Consider the simple example program in Fig

 Here we use as the sole reference variable

 Symbolic analysis will find that has the value and has

 the value after their_respective assignment statements in lines

 (2) and (3) This information is useful for example in determining

 that the two assignments in lines_(4) and (5) write to different

 memory_locations and can thus be executed in parallel Furthermore

 we can tell that the condition is never true

 thus allowing the optimizer to remove the conditional_statement in

 lines (6) and (7) all together



 Affine Expressions of Reference Variables



 Since we cannot create succinct and closed-form symbolic expressions

 for all values computed we choose an abstract domain and approximate

 the computations with the most_precise expressions within the domain

 We have_already seen an example of this strategy before constant_propagation

 In constant_propagation our abstract domain consists of the

 constants an symbol if we have not_yet determined if the value

 is a constant and a special symbol that is used whenever a variable

 is found not to be a constant



 The symbolic analysis we present here expresses values as affine

 expressions of reference_variables whenever possible An expression

 is affine with_respect to variables if it can

 be_expressed as where

 are constants Such expressions are informally known_as

 linear expressions Strictly speaking an affine_expression is linear

 only if is zero We are_interested in affine_expressions

 because they are often used to index arrays in loops-such information

 is useful for optimizations and parallelization Much more will be said

 about this topic in Chapter



 Induction_Variables



 Instead of using program variables as reference

 variables an affine_expression can also be

 written in terms of the count of iterations through the loop

 Variables whose values can be_expressed

 as where is the count of iterations through the

 closest enclosing_loop are

 known_as induction_variables





 Consider the code_fragment





 Suppose we introduce for the loop a variable say

 representing the number of iterations executed The value is 0

 in the first iteration of the loop 1 in the second and so

 on We can express variable as an affine

 expression of namely Variable which is

 takes on values during successive iterations of the

 loop

 Thus has the affine_expression

 We_conclude that both and are induction_variables of this loop



 Expressing variables as affine_expressions of loop_indexes makes the

 series of values being computed explicit and enables several

 transformations The series of values taken on by an induction_variable

 can be computed with additions rather_than

 multiplications This transformation is known_as strength

 reduction and was_introduced in Sections and For

 instance we can eliminate the multiplication xm3 from the loop of

 Example by

 rewriting the loop as







 In_addition notice

 that the locations assigned 0 in that loop



 are also affine_expressions of the loop_index In_fact this series of

 integers

 is the only one that needs to be computed

 We do_not need both and

 for instance the code above can be

 replaced_by







 Besides speeding up the computation symbolic analysis

 is also useful for parallelization When the array indexes in a loop

 are affine_expressions of loop_indexes we can reason_about relations of

 data_accessed across the iterations For_example we can tell that

 the locations written are different in each iteration and therefore

 all the iterations in the loop can be executed in parallel on

 different_processors Such information is used

 in Chapters and to extract parallelism from sequential programs



 Other Reference Variables



 If a variable is not a linear function of the reference

 variables already chosen

 we have the option of treating its

 value as reference for future operations

 For_example consider the code_fragment





 While the value held by after the function call cannot itself be

 expressed_as a linear function of any reference_variables it can be used

 as reference for subsequent statements For_example using as a

 reference variable we can discover that

 is one larger_than at the end of the program













 Source code for Example







 Our running_example for this_section is based_on the source code

 shown in Fig

 The inner and outer loops are easy to understand since and are

 not modified except as required by the for-loops It is thus possible

 to replace and by reference

 variables and that count the number of

 iterations of the outer and inner_loops respectively That is we can

 let and and substitute for and throughout

 When translating to intermediate_code we can take_advantage of the fact

 that each loop iterates at_least once and so postpone the test for

 and to the ends of the loops

 Figure shows the flow_graph for the code of

 Fig after introducing and and treating the

 for-loops as if they_were repeat-loops







 fileuullmanalsuch9figsiv-regioneps



 Flow_graph and its region_hierarchy for Example





 It_turns out that and are all induction_variables

 The sequences of values

 assigned to the variables in each line of the code are shown in

 Figure

 As we_shall see it is possible to discover the affine_expressions for

 these variables in terms of the reference_variables and That

 is at line_(3) at line_(7) and at line_(8)





































 Sequence of values seen in program points in Example



 Data-Flow Problem Formulation



 This analysis finds affine_expressions of reference_variables

 introduced (1) to count the number of iterations executed in each

 loop and (2) to hold values at the entry of regions where necessary

 This analysis also finds

 induction_variables loop invariants as_well as constants

 as degenerate affine_expressions Note_that

 this analysis cannot find all constants because it only tracks affine

 expressions of reference_variables



 Data-Flow Values Symbolic Maps



 The domain of data-flow_values for this analysis is symbolic_maps

 which are functions that map each variable in the program to a value

 The value is either an affine function of reference values or the

 special symbol to represent a non-affine expression If there

 is only one variable the bottom value of the semilattice is a map

 that sends the variable to The semilattice for variables

 is simply the product of the individual semilattices We use

 to denote the bottom of the semilattice which maps all variables to

 We can define the symbolic_map that sends all variables to an

 unknown value to be the top data-flow value as we did for constant

 propagation However we do_not need top values in region-based

 analysis







































 Symbolic maps of the program in Example





 The symbolic_maps associated_with each block for the code in

 Example are shown in Figure

 We_shall see later how these maps are discovered they are the result of

 doing region-based data-flow_analysis on the flow_graph of

 Fig



 The symbolic_map associated_with the entry of the program is At

 the exit of the value of is set to 0 Upon entry to

 block has value 0 in the first iteration and increments by

 one in each subsequent iteration of the outer

 loop Thus has value at the

 entry of the th_iteration and value at the end The symbolic

 map at the entry of maps variables to because the

 variables have unknown values on entry to the outer_loop Their values

 depend_on the number of iterations of the outer_loop so_far

 The symbolic_map on exit from reflects the assignment

 statements to and in that block The rest of the symbolic

 maps can be deduced in a similar_manner

 Once we have established the validity of the maps in

 Fig we can replace each of the assignments to

 and in Fig by the appropriate affine

 expressions That is we can replace Fig by the code in

 Fig













 The code of Fig with assignments replaced_by

 affine_expressions of the reference_variables and





 Transfer Function of a Statement



 The transfer_functions in this data-flow_problem send symbolic_maps to

 symbolic_maps To_compute the transfer_function of an assignment

 statement we interpret the semantics of the statement and determine

 if the assigned variable can be_expressed as an affine_expression

 of the values on the right of the

 assignment The values of all other variables remain

 unchanged



 Cautions Regarding Transfer_Functions on Value Maps A

 subtlety in the way we define the transfer_functions on symbolic_maps

 is that we have options regarding how the effects of a computation are

 expressed When is the map for the input of a transfer_function

 is really just whatever value variable happens to have on

 entry We try very_hard to express the result of the transfer

 function as an affine_expression of reference_variables used by the

 input map



 You_should observe the proper interpretation of expressions like

 where is a transfer_function a map and a

 variable As is conventional in mathematics we apply functions from

 the left meaning that we first compute which is a map Since

 a map is a function we may then apply it to variable to produce a

 value







 The transfer_function of statement denoted is defined as

 follows





 If is not an assignment_statement then is the

 identity_function



 If is an assignment_statement to variable then



































 The expression is intended to

 represent all the possible forms of expressions_involving arbitrary

 variables

 and that may appear on the

 right_side of an assignment to and that give a value

 that is an affine transformation on prior values of variables

 These expressions are





 and



 Note_that in many_cases one or_more of and are 0





 If the assignment is xyz then and

 If the assignment is xy5 then and



 Composition of Transfer_Functions

 To_compute where and are defined in terms

 of input map we substitute the value of in the

 definition of with the definition of We replace

 all operations on values with That is



 If then



 If then

































 The transfer_functions of the blocks in Example can

 be computed by_composing the transfer_functions of their constituent

 statements These transfer_functions are defined in Fig































 Transfer_Functions of Example



 Solution to Data-Flow Problem

 We use the

 notation and

 to refer to the input and output

 data-flow_values of block in iteration of the inner_loop and

 iteration of the outer_loop

 For the other blocks we use and to refer

 to these values in the th_iteration of the outer_loop

 Also

 we can see that the symbolic_maps shown in Fig satisfy the constraints_imposed by the transfer_functions listed in

 Fig





























 Constraints satisfied on each iteration of the nested_loops





 The first constraint says_that the output map of a basic_block is

 obtained_by applying the block's transfer_function to the input map

 The rest of the constraints say that the output map of a basic_block

 must_be greater_than or equal to the input map of a successor

 block in the execution



 Note_that our iterative_data-flow algorithm cannot produce the above

 solution because it lacks the concept of expressing data-flow_values

 in terms of the number of iterations executed Region-based_analysis

 can be used to find such solutions

 as we_shall see in the next section



 Region-Based Symbolic Analysis



 We can extend the region-based_analysis described in

 Section to find expressions of variables in the

 th_iteration of a loop A region-based symbolic analysis has a

 bottom-up_pass and a top-down_pass like other region-based

 algorithms The bottom-up_pass summarizes the effect of a region with

 a transfer_function that sends a symbolic_map at the entry to an output

 symbolic_map at the exit In the top-down_pass values of symbolic

 maps are propagated down to the inner regions



 The difference lies in how we handle loops In

 Section the effect of a loop is summarized with a

 closure_operator Given a loop with body its closure is

 defined as an_infinite meet of all possible numbers of applications of

 However to find induction_variables we need to determine if a

 value of a variable is an affine function of the number of iterations

 executed so_far The symbolic_map must_be parameterized by the number of

 the iteration being executed Furthermore whenever we know

 the total_number of

 iterations executed in a loop we can use that number to find

 the values of induction_variables after the loop For_instance in

 Example we claimed that has the value of after

 executing the th_iteration Since the loop has 100 iterations the

 value of must_be 100 at the end of the loop



 In what_follows we first define the primitive operators meet and

 composition of transfer_functions for symbolic analysis

 Then show_how we

 use them to perform region-based_analysis of induction_variables



 Meet of Transfer_Functions



 When computing the meet of

 two functions the value of a variable is unless

 the two functions map the variable to the same

 value and the value is not

 Thus





















 Parameterized Function Compositions



 To express a variable as an affine function of a loop_index we need

 to compute the effect of composing a function some given number of

 times If the effect of one iteration is summarized by transfer

 function then the effect of executing iterations for some

 is

 denoted Note_that when the

 identify function



 Variables in the program are divided_into four categories





 If where is a constant then

 for every value of We_say that is a basic

 induction_variable of the loop whose body is represented_by the transfer

 function



 If then for

 all The variable is not modified and it remains

 unchanged at the end of any number of iterations through the loop with

 transfer_function We_say that is a symbolic_constant in the

 loop



 If

 where each

 is either a basic_induction variable or a symbolic_constant then

 for







 We_say that is also an induction_variable though not a basic one

 Note_that the formula above does_not apply if



 In all other cases





 To_find the effect of executing a fixed_number of iterations we

 simply replace above by that number In the case_where the number

 of iterations is unknown the value at the start of the last iteration

 is given by In this case the only variables whose

 values can still be_expressed in the affine form are the

 loop-invariant variables























 For the innermost_loop in Example

 the effect of executing iterations is summarized by



 From the definition of we see that and are symbolic

 constants is a basic_induction variable as it is incremented by

 one every iteration is an induction_variable because it is an

 affine function of the symbolic_constant and basic_induction

 variable

 Thus

























 If we could not tell how many_times the loop of block iterated

 then we could not use and would have to use to express the

 conditions at the end of the loop In this case we would have

























 A Region-Based Algorithm





 Region-based symbolic analysis



 A reducible_flow graph



 Symbolic maps for each block of



 We make the following modifications to Algorithm



 We change how we construct the transfer_function for a loop region

 In the original algorithm we use the transfer_function

 to map the symbolic_map at the entry of loop region to a symbolic

 map at the entry of loop body after executing an unknown number of

 iterations It is defined to be the closure of the transfer_function

 representing all paths_leading back to the entry of the loop as shown

 in Fig (b) Here we define to

 represent the effect of execution from the start of the loop region to

 the entry of the th_iteration Thus











 If the number of iterations of a region is known the summary of the

 region is computed by_replacing with the actual count



 In the top-down_pass we compute to find the

 symbolic_map associated_with the entry of the th_iteration of a

 loop



 In the case_where the input value of a variable is used on the

 right-hand-side of a symbolic_map in region and

 upon_entry to the region we introduce a new reference variable

 add assignment t v to the beginning of region and all

 references of are replaced_by

 If we

 did_not introduce a reference variable at this point the

 value held by would penetrate into inner_loops







 Transfer_function relations in the bottom-up_pass for

 Example





 For Example we show_how the transfer_functions for the

 program are

 computed in the bottom-up_pass in Fig

 Region is the inner_loop with body

 The transfer

 function representing the path from the entry of region to the

 beginning of the th_iteration is The

 transfer_function representing the path to the end of the th

 iteration is



 Region consists of blocks and with loop region

 in the middle The

 transfer_functions from the entry of and can be computed

 in the same way as in the original algorithm Transfer_function

 represents the composition of block and

 the entire execution of the inner_loop since is the identity

 function Since the inner_loop is known

 to iterate 10 times we can replace by 10 to summarize the

 effect of the inner_loop precisely The rest of the transfer

 functions can be computed in a similar_manner The actual transfer

 functions computed are shown in Fig





















































 Transfer_functions computed in the bottom-up_pass for

 Example



 The symbolic_map at the entry of the program is simply We

 use the top-down_pass to compute the symbolic_map to the entry to

 successively nested regions until we find all the symbolic_maps for

 every basic_block We start by computing the data-flow_values for

 block in region



 Descending down to regions and we get



 Finally in region we get



 Not surprisingly these equations produce the results we showed in

 Fig



 Example shows a simple program where every variable used

 in the symbolic_map has an affine_expression We use

 Example to illustrate why and how we introduce

 reference_variables in Algorithm













 (a) A loop where fluctuates









 (b) A reference variable makes an induction_variable



 The need to introduce reference_variables







 Consider the simple example in Fig (a)

 Let be the transfer_function summarizing the effect of executing

 iterations of the inner_loop Even_though the value of may

 fluctuate during the execution of the loop we see that is an

 induction_variable based_on the value of on entry of the loop that

 is Because is assigned an input value the

 symbolic_map upon_entry to the inner_loop maps to We

 introduce a new reference variable to save the value of upon

 entry and perform the substitutions as in Fig (b)





 For the flow_graph of Fig (see the exercises for

 Section ) give the transfer_functions for







 a) Block

 b) Block

 c) Block







 Consider the inner_loop of Fig consisting of blocks

 and If represents the number of times_around the loop

 and is the transfer_function for the loop body (ie excluding the edge

 from to ) from the entry of the loop (ie

 the beginning of ) to the exit from then what is

 Remember that takes as argument a map and assigns a value

 to each of variables and We denote these values

 and so on although we do_not know their values





 Now_consider the outer_loop of Fig consisting of blocks

 and Let be the transfer_function for

 the loop body from

 the entry of the loop at to its exit at Let measure

 the number of iterations of the inner_loop of and (which count

 of iterations we

 cannot know) and let measure the number of iterations of the outer

 loop (which we also cannot know) What is

 Symbol Tables and Types

 symbols-java-sect



 Package symbols implements symbol_tables and types



 Class Env is essentially unchanged from

 Fig symtab-java-fig Whereas class_Lexer maps strings

 to words class_Env maps word tokens to objects of class

 Id which is defined in package_inter along with the

 classes for expressions and statements



 footnotesize

 flushleft

 1)_package symbols File Envjava



 2)_import javautil import_lexer import inter



 3)_public class_Env



 4) private Hashtable table



 5) protected Env prev



 6)_public Env(Env n) table new_Hashtable() prev n



 7) public_void put(Token w Id i) tableput(w i)



 8)_public Id get(Token w)



 9) for( Env e this e null e eprev )



 10) Id found (Id)(etableget(w))



 11) if( found null_) return found



 12)



 13) return_null



 14)



 15)



 flushleft

 footnotesize



 We define class Type to be a subclass of Word since

 basic type names like int are simply reserved_words to be

 mapped from lexemes to appropriate objects by the lexical

 analyzer The objects for the basic types are TypeInt TypeFloat TypeChar and TypeBool (lines 7-10)

 All of them have inherited field tag set to TagBASIC

 so the parser treats them all alike



 footnotesize

 flushleft

 1)_package symbols File Typejava



 2)_import lexer



 3)_public class Type extends Word



 4)_public int width 0 width is used for storage allocation



 5)_public Type(String s int tag int w) super(s tag) width w



 6)_public static final Type



 7) Int new Type( int TagBASIC 4 )



 8) Float new Type( float TagBASIC 8 )



 9) Char new Type( char TagBASIC 1 )



 10) Bool new Type( bool TagBASIC 1 )



 flushleft

 footnotesize



 Functions numeric (lines 11-14) and max (lines 15-20)

 are useful for type_conversions



 footnotesize

 flushleft

 11) public_static boolean numeric(Type p)



 12) if_(p TypeChar p TypeInt p TypeFloat) return true



 13) else_return false



 14)



 15)_public static Type max(Type p1 Type p2)



 16) if_( numeric(p1) numeric(p2) )_return null



 17) else if_( p1 TypeFloat p2 TypeFloat )_return TypeFloat



 18) else if_( p1 TypeInt p2 TypeInt )_return TypeInt



 19) else_return TypeChar



 20)



 21)



 flushleft

 footnotesize



 Conversions are allowed between the numeric types

 TypeChar TypeInt and TypeFloat When an

 arithmetic operator is applied to two numeric types the result has

 the max of the two types



 Arrays are the only constructed type in the source_language The

 call to super on line 7 sets field width which is

 essential for address_calculations It also sets lexeme and

 tok to default values that are not used



 footnotesize

 flushleft

 1)_package symbols File Arrayjava



 2)_import lexer



 3)_public class Array extends Type



 4)_public Type of array of type



 5)_public int size 1 number of elements



 6)_public Array(int sz Type p)



 7) super( TagINDEX szpwidth) size sz of p



 8)



 9)_public String_toString() return size oftoString()



 10)



 flushleft

 footnotesize

 Symbol Tables

 symtab-sect



 Symbol_tables are data_structures that hold information

 about identifiers The information is collected incrementally by

 the analysis phases of a compiler and used by the synthesis phases

 to generate the target code Entries in the symbol_table contain

 information_about an_identifier such_as its character string or

 lexeme its type its position in storage and any other

 relevant information



 Symbol_tables typically need to support multiple declarations of

 the same identifier within a program Conceptually scopes can be

 implemented_by setting up a separate symbol_table for each scope

 A program block with declarations(In C for instance program

 blocks are either functions or sections of functions that are separated

 by curly_braces and that have one or_more declarations within them)

 then has its_own symbol_table with an

 entry for each declaration in the block This_approach also works

 for other constructs that set up scopes for example a class will

 have its_own symbol_table with an entry for each of its fields

 and methods



 This_section contains a symbol-table module suitable for the Java

 translator in this_chapter The module will be used as is when we

 put together the translator in Appendix together-sect

 Meanwhile for simplicity the main example of this_section is a

 stripped-down language with just the key constructs that touch

 symbol_tables namely blocks declarations and factors All of

 the other statement and expression constructs are omitted so we

 can focus_on the symbol-table operations A program consists of

 blocks with optional declarations and statements consisting of

 single identifiers Each such statement represents a use of the

 identifier An_example of a program in our simple language is



 center

 int x char y bool y x_y x_y

 center



 The task we_shall perform is

 to print a revised program in which the declarations have_been removed

 and each statement has its identifier followed_by a colon and its

 type



 ex

 On the above input the goal is to

 produce



 center

 xint ybool xint ychar

 center

 That is we first see the inner block with its uses of x and y

 Since x refers to the declaration of x in the outer block

 it is followed_by int the type of that declaration The

 y in the inner block refers to the declaration of y in that

 block and therefore has boolean type

 We also see the uses of x and y in the outer block with

 their types as given by declarations of the outer block integer and

 character respectively

 ex



 Who Creates Symbol-Table Entries Symbol-table

 entries are created during the analysis phase by either the

 lexical_analyzer or the parser In this_chapter the parser

 creates entries

 With its knowledge of the syntactic_structure of a program a

 parser is often in a better position than the lexical_analyzer to

 distinguish_among different declarations of an_identifier



 In some

 cases a lexical_analyzer can retrieve the relevant symbol-table

 entry as_soon as it_sees the characters that make up a lexeme

 More often the lexical_analyzer must return to the parser a

 token say id along with the lexeme or_more likely a

 reference to the lexeme The parser uses the token and lexeme to

 decide_whether to use an existing symbol-table_entry or to create

 a new one for the identifier



 Symbol Table Per Scope



 The same identifier can be declared for different purposes in

 different parts of a program For_example is an_integer

 parameter on line 7 in Fig scan-java1-fig and an object of

 class_Token on line 39 in Fig scan-java2-fig As

 another example subclasses can redeclare a method name to

 override a method in a superclass



 The scope of a declaration is the portion of a program to

 which the declaration applies Thus the term scope of

 identifier x really refers to the scope of a particular declaration

 of x The term scope by itself refers to

 a portion of a program that is the scope of one or_more

 declarations



 Several declarations of the same identifier

 can be found in a single block

 The syntax



 center

 block '' decls_stmts

 ''

 center

 results in nested blocks when stmts can generate a block

 As we_shall see in Fig symtab-scheme-fig the complete grammar

 for our language decls generates an_optional sequence of declarations

 and stmts generates an_optional sequence of statements

 Moreover a statement can be a block so we have in our language the

 possibility of multiple declarations of the same identifier



 The most-closely nested rule for blocks is that an

 identifier is in the scope of the most-closely nested

 declaration of that is the declaration of found by

 examining blocks inside-out starting_with the block in which

 appears



 ex

 scope-ex The following pseudocode uses subscripts to

 distinguish_among distinct declarations of the same identifier

 center

 tabularr_l

 1) int int



 2) bool int



 3)



 4)



 5)



 6)

 tabular

 center

 The subscript is not part of an_identifier it is in fact the line

 number of the declaration that applies to the identifier Thus

 all occurrences of are within the scope of the declaration on

 line 1 The occurrence of on line 3 is in the scope of the

 declaration of on line 2 since is redeclared within the

 inner block However the occurrence of on line 5 is within

 the scope of the declaration of on line 1 since the nested

 declaration applies only to the nested block The occurrence of

 on line 5 is presumably within the scope of a declaration of

 outside this program_fragment so its subscript is for

 global or external to this block

 ex



 The most-closely nested rule for blocks can be_implemented by

 chaining symbol_tables

 That is the table for a nested block

 points to the table for its enclosing block



 ex

 Fig blocks-fig shows symbol_tables for the pseudocode in

 Example scope-ex is for the block starting on line 1

 and is for the block starting_at line 2 At the top of the

 figure is an additional symbol_table for any global or default

 declarations provided by the language

 During the time that we are analyzing lines 2 through 4 the environment

 is represented_by a reference to the lowest symbol_table - the one for



 When we move to line 5 the symbol_table for becomes invisible

 and the environment would instead refer to the symbol_table for

 from which we can reach the global symbol_table but not the table for



 ex



 figurehtfb

 Chained

 symbol_tables for Example scope-ex blocks-fig

 figure



 Optimization of Symbol Tables for Blocks

 Implementations of symbol_tables for blocks can take_advantage of

 the most-closely nested rule Nesting ensures that the chain of

 applicable symbol_tables forms a stack At the top of the stack is

 the table for the current block Below it in the stack are the

 tables for the enclosing blocks Thus symbol_tables can be

 allocated and deallocated in a stack-like fashion



 Some compilers maintain a single hash_table of accessible entries

 that is of entries that are not hidden by a declaration in a

 nested block Such a hash_table supports essentially constant-time

 lookups at the expense of inserting and deleting entries on block

 entry and exit Upon exit from a block the compiler must undo

 any changes to the hash_table due to declarations in block It

 can do so by using an auxiliary stack to keep_track of changes to

 the hash_table while block is processed



 The Java implementation of chained symbol_tables in

 Fig symtab-java-fig defines a class_Env short for

 environment(Environment is another term for the collection

 of symbol_tables that are relevant at a point in the program)

 Class Env supports three operations



 figurehtfb



 center

 tabularr_l

 1)_package symbols File Envjava



 2)_import javautil



 3)_public class_Env



 4) private Hashtable table



 5) protected Env next







 6)_public Env(Env n)



 7) table new_Hashtable() next n



 8)







 9)_public void put(String s Symbol sym)



 10) tableput(s sym)



 11)







 12) public Symbol get(String s)



 13) for ( Env e this e null e enext )



 14) Symbol found (Symbol)(etableget(s))



 15) if_( found null_) return found



 16)



 17) return_null



 18)



 19)

 tabular

 center

 Class Env implements chained symbol_tables

 symtab-java-fig

 figure



 itemize

 Create a new symbol_table The constructor Env(n) on lines 6 through 8 of Fig symtab-java-fig

 creates an Env object with a hash_table

 named table The object is chained to the environment-valued

 parameter n

 by setting field next to n Although it is the Env objects that form a chain it is convenient to talk of the

 tables being chained



 Put a new entry in the current table The hash_table

 holds key-value pairs where

 itemize

 The key

 is a string or rather a reference to a string We could

 alternatively have used references to token objects for

 identifiers as keys

 The value is an entry of class Symbol The code on lines 9 through 11 does_not need to know the

 structure of an entry that is the code is independent of the

 fields and methods in class Symbol

 itemize



 Get an entry for an_identifier by searching the chain

 of tables starting_with the table for the current block The code

 for this operation on lines 12 through 18 returns either a symbol-table

 entry or null

 itemize



 Chaining of symbol_tables results in a tree structure since more

 than one block can be nested inside an enclosing block The dotted

 lines in Fig blocks-fig are a reminder that chained symbol

 tables can form a tree



 The Use of Symbol Tables

 symtab-use-subsect



 The translation_scheme in Fig symtab-scheme-fig illustrates

 the use of symbol_tables in the example language of

 this_section

 Notice_that the bodies of the productions have_been aligned so all the

 grammar_symbols appear in one column and all the actions in a second

 column

 As a result components of the body are often spread over several

 lines

 The underlying_grammar

 generates programs consisting of blocks with optional declarations

 and statements which are simply identifiers eg



 figurehtfb

 center

 tabularr_c l_l

 program top null



 block







 block saved top



 top new Env(top)



 print('')



 decls_stmts top saved



 print('')







 decls decls decl











 decl type id s new Symbol



 type typelexeme



 topput(idlexeme s)







 stmts stmts stmt











 stmt block



 factor







 factor id s topget(idlexeme)



 print(type)



 print



 print(idlexeme)



 tabular

 center

 The use of symbol_tables for translating a language with

 blocks symtab-scheme-fig

 figure

 center

 int x char y bool y x_y x_y

 center

 The translation_scheme strips the declarations and prints the

 statement identifiers and their types

 center

 xint ybool xint ychar

 center



 The approach in Fig symtab-scheme-fig is to create and

 discard symbol_tables upon block entry and exit respectively

 Declarations result in entries put into the table Factors

 (identifiers playing the role of statements)

 result in get operations to retrieve the type of an_identifier

 from the symbol_table



 Now_consider the semantic_actions Variable top denotes the

 top table at the head of a chain of tables The first production

 of the underlying_grammar is

 The semantic_action before block initializes top to null with no entries



 The second production

 has actions

 upon block entry and exit On block entry before decls a

 semantic_action saves a reference to the current table using

 variable saved

 Note_that saved is a local variable belonging to this use of the

 production and must_be

 stored in a place associated_with this use of the production not to be

 confused with the variable saved associated_with any other use of

 this production For_example the value of saved could be a

 translation of block placed_at a parse-tree_node with that label

 The situation is similar to the way local_variables of a recursive

 function must_be treated a matter we take_up in detail in

 Section stack-alloc-sect

 The code



 center

 top new Env(top)

 center

 sets variable top to a newly_created new table that is

 chained to the previous value of top just_before block

 entry Variable top is an object of class_Env the

 code for the constructor Env appears in

 Fig symtab-java-fig



 On block exit after a semantic_action restores top to its value saved on block entry In effect the tables form

 a stack restoring top to its saved value pops the effect of

 the declarations in the block Thus the declarations in the block

 are not visible outside the block



 A declaration

 results in a new entry for the declared identifier

 We assume that tokens type and id each have an associated

 lexeme which is the type and name respectively of the declared

 identifier

 We_shall not go into all the fields of a symbol object but we

 assume that there is a field type that gives the type of the

 symbol

 We create a new symbol object and assign its type properly by

 type typelexeme

 The complete entry is put into the

 top symbol_table by topput(idlexeme

 s)



 The semantic_action in the production uses

 the symbol_table to get the entry for the identifier The get operation searches for the first entry in the chain of

 tables starting_with top The retrieved entry contains any

 information needed about the identifier such_as the type

 of the identifier

 Parallelism with Synchronization

 ch11sync



 In this_section we_shall relax the assumption that no synchronization

 between processors is permitted We first consider some simple

 transformations that allow a finite number of synchronization steps

 ie a constant number of synchronizations independent of the size of

 the arrays or loop_bounds

 We then look_at pipelining where data is passed in a regular

 pattern through an array of processors in order to achieve more

 parallelism in our code

 We first discuss_how

 we pipeline iterations in a loop across multiprocessors Next we

 discuss_how pipelining can be found by generalizing the concept of

 affine_partitioning Finally we discuss_how to find all the degrees

 of parallelism in a program



 Allowing Constant Synchronizations



 Not all programs can be_parallelized without synchronizations

 Just introducing a constant number of synchronizations can

 expose degrees of parallelism not previously available For_instance a

 sequence of two

 loops can be_parallelized if we

 introduce a synchronization between the parallel_execution of the first

 loop and the parallel_execution of the second

 Even operations that originally reside in the same loop can benefit

 from a constant number of synchronizations if they can be pulled apart

 and put in different loops separated_by a single synchronization

 These two cases are illustrated by Examples exadi

 and exscc2



 figurehtfb



 verbatim

 for_(i 1_i n_i)

 for_(j 0 j_n j)

 Aij f(Aij Ai-1j)

 for_(i 0 i_n i)

 for_(j 1_j n_j)

 Aij g(Aij Aij-1)

 verbatim



 Two sequential loop_nests

 adi-fig



 figure



 ex

 exadi

 In Fig adi-fig

 is a code representative of an ADI (Alternating Direction Implicit)

 integration algorithm

 There is no_synchronization-free parallelism because dependences in

 the first loop_nest

 force operations on the same column of array to be mapped

 onto the same processor and dependences in

 the second loop_nest force operations on

 the same row of to

 be mapped to the same processor One_way to parallelize

 the code is to have different_processors work on different rows of the

 matrix in the first loop then they synchronize and wait for all

 processors to finish then operate_on the individual columns This

 approach parallelizes the computation with the introduction of one

 synchronization operation but incurs a significant amount of data

 traffic (We shall_discuss alternative better parallelization schemes in

 Section pipeline-subsect)

 ex



 ex

 exscc2

 Consider the following loop



 verbatim

 for (i1 in i)

 Ai_Bi Ci (s1)

 DPi Ai (s2)



 verbatim

 Without knowledge of the values in the matrix we must

 assume that the access in statement may read any of the elements

 of

 Thus in any parallelization

 the iterations of must_be executed sequentially in the

 order in which they are executed by the original code above



 The affine_partition algorithm will assign all the computation to the

 same processor whereas clearly at_least instances of statement

 can be executed in parallel We can parallelize this code by

 having each processor perform an instance of statement

 Then in a separate sequential loop one processor numbered 0

 executes as in Fig one-synch-fig



 figurehtfb



 verbatim

 Ap Bp Cp (s1)

 synchronization_barrier

 if_(p 0)

 for (i1 in i)

 DPi Ai (s2)

 verbatim



 Fission of a loop using a synchronization_barrier is the

 processor number

 one-synch-fig



 figure



 This example_illustrates that we sometimes

 need to break up the computation in

 a single loop This transformation also known_as fission or

 distribution is the opposite of the fusion transform introduced

 in Fig figtransforms

 ex



 Any operation executed before a processor enters a

 synchronization_barrier is guaranteed to be completed before any other

 processors are allowed to leave the barrier and execute operations

 that come after the barrier Two data-dependent operations can be

 assigned to different_processors if and only if one is scheduled to

 execute before some synchronization_barrier and the other after



 Two statements that have mutually dependent instances cannot be executed

 in parallel The best way to introduce a finite number of

 synchronization_barriers

 is to group sets of mutually dependent statements on a

 single processor and separate their execution from other parts of the

 program via barriers Formally

 a program dependence graph for a program is a graph whose nodes

 are the statements of the program There is an arc from statement

 to statement if there is an access in say and an access

 in say such that depends_on (must follow)



 If there is a cycle of one or_more nodes in the dependence

 graph then all the statements on that cycle must_be assigned to the

 same processor and executed as written in the original_program

 Moreover synchronization_barriers must separate the execution of these

 statements from any other statements that are their predecessors or

 successors in the program dependence graph



 ex

 For the program of Example_exscc2 there are no dependences among

 the instances of statement However the th instance of

 statement must follow the th instance of statement

 Worse since the reference DPi may write any element of array

 the th instance of depends_on all previous instances of

 That is statement depends_on itself



 The dependence graph for the program of Example_exscc2 is shown

 in Fig scc2-fig Note_that there is one loop containing

 only Thus instances of must_be isolated by synchronization

 barriers Recall that in Fig one-synch-fig we placed a barrier before

 to separate from its predecessor

 There is no need for a barrier following since is

 the last step of the program

 ex



 figurehtfb



 fileuullmanalsuch11figsscc2eps

 Program dependence graph for the program of

 Example_exscc2

 scc2-fig

 figure



 alg

 alg1sync

 Maximize the degree of parallelism allowed by

 synchronizations



 A program with array_accesses



 SPMD code with a constant number of synchronization_barriers



 Do the following



 enumerate



 Construct the program dependence graph and partition the statements

 into strongly_connected components (SCC's) Recall from

 Section cyc-dep-subsect that a strongly_connected

 component is a subgraph of the original whose every node in the

 subgraph can reach every other node Note_that a single_node is an SCC

 regardless of whether or not it has a loop However an SCC consisting

 of a single_node without a loop (ie a statement of the program that

 is not dependent on itself and not mutually dependent with any other

 statement) need not be synchronized call such an SCC trivial

 and other SCC's nontrivial



 When we collapse each nontrivial SCC to a single_node we get an acyclic

 graph Pick a topological_order for this reduced graph

 Replace each node by the code it represents grouping code for different

 statements into the same loop_nest if possible



 Introduce synchronization_barriers around the code for

 each of the nodes that represents a nontrivial SCC



 Apply_Algorithm algnosync to the portion of the code between each

 synchronization_barrier to find all of its

 synchronization-free_parallelism



 enumerate

 alg



 While this simple algorithm finds all degrees of parallelism with

 synchronizations it has a weakness By optimizing

 every nontrivial

 strongly_connected component individually it may require a lot

 of data to be transferred among processors with each synchronization

 In many instances the cost of communication makes the parallelism too

 expensive and we are better off with a simple sequential algorithm on

 a uniprocessor We_shall next take_up ways to increase the spatial

 locality and thus control the amount of

 communication



 Pipelining

 pipeline-subsect



 In pipelining a task is decomposed into a number of stages to be

 performed on different_processors For_example a task computed using

 a loop of iterations can be structured as a pipeline of

 stages Each stage is assigned to a different processor when one

 processor is finished with its stage the results are passed as input

 to the next processor in the pipeline



 ex

 pipeline-loop-ex

 Consider the loop



 verbatim

 for_(i 1_i n_i)

 Ai Ai-1 1

 verbatim

 Each stage (a stage is one iteration of the loop in this

 example) depends upon the value computed in the previous

 stage which suggests that we need to perform the loop at only one

 processor However a different approach - pipelining - has each

 stage computed by a different processor As soon_as the th processor

 performs the th_stage it passes the value of to the next

 processor which treats that value as and performs its

 assignment

 ex



 Whether or not the strategy of Example_pipeline-loop-ex

 makes_sense as a way to execute this loop depends_on what use can be made

 of the processors that are not performing an iteration at any

 particular moment

 The advantage of pipelining comes when the loop of this example is just

 one of many similar tasks each of which can be executed by the

 same pipeline of processors If organized properly all or most of

 the processors are busy at any given time and the data that must_be

 communicated from one to the other flows uniformly and at a modest

 rate



 If each task is executed

 sequentially in the pipeline we can overlap the executions of different

 tasks We can initiate a new task as_soon as the first processor is

 done with the first stage of the previous task

 At the beginning the pipeline is empty and only the first processor

 is executing the first stage After it completes the results are

 passed to the second processor while the first processor starts on the second

 task and so on In this way the pipeline gradually fills until all

 the processors are busy When the first

 processor_finishes with the last_task the pipeline starts to drain

 with more and more processors becoming idle until the last processor

 finishes the last_task In the steady_state tasks can be

 executed concurrently in a pipeline of processors



 It is interesting to contrast pipelining with simple parallelism

 where different_processors execute different tasks



 itemize



 Pipelining

 can only be applied to nests of depth at_least two We

 can treat each iteration of the outer_loop as a task and the

 iterations in the inner_loop as stages of that task



 Tasks

 executed on a pipeline may share a dependence Information pertaining

 to the same stage of each task is held on the same processor thus

 results generated_by the th_stage of a task can be used by the

 th_stage of subsequent tasks without any interprocessor

 communication Similarly if there is a lot of code or data used by

 a stage it can be held locally by the processor for that stage



 If the

 tasks are independent then pipelining is not useful

 We can simply assign each task to a separate

 processor as we have done in the Section ch11affine Such a

 scheme is preferable because all processors can execute all at once

 without_having to pay for the overhead of filling and draining the

 pipeline



 itemize



 ex

 expipeline

 The code of Fig_pipeline-nest-fig

 represents a data-access pattern found in many

 programs Here the new value of an element in the array depends_on the

 values of elements in its neighborhood In some_cases such an

 operation is performed repeatedly_until some convergence criterion is

 met



 figurehtfb



 verbatim

 for_(i 1_i n_i)

 for_(j 1_j n_j)

 Aij 05 (Ai-1j Aij-1)

 verbatim



 A loop_nest that can profit from pipelining

 pipeline-nest-fig



 figure



 Shown in Fig figpipeline is a picture of the data

 dependences Since dependences exist across both the rows and the

 columns in the iteration_space we cannot parallelize this code unless

 synchronization is introduced From the figure we can see that

 iterations along the diagonal share no dependences These sets of

 independent iterations are known_as wavefronts and can be

 executed in parallel as_long as synchronizations occur before each

 wavefront The amount of communication incurred between these

 wavefronts depends_on the actual assignment statement(s) being executed



 figurehtfb

 fileuullmanalsuch11figspipelineeps

 The dependences of the pipelining example (Example expipeline)

 figpipeline

 figure



 We can execute Fig_pipeline-nest-fig as a pipeline There

 are two ways to pipeline the code First we can create a pipeline by

 making the th processor be responsible_for the th_row of that

 is those elements of the form

 The execution starts by having processor 1 execute

 iteration (11) It then signals processor 2 to execute iteration

 (21) ie to compute while processor 1

 goes on to execute iteration (21) and so on The

 SPMD code that implements this pipelining scheme is shown in

 Fig_signal-fig(a)



 figurehtfb



 verbatim

 1 p n

 for_(j 1_j n_j)

 if_(p 1)_wait (p-1)

 Apj 05 (Ap-1j Apj-1)

 if_(p n-1) signal_(p1)



 verbatim



 center

 (a) Processors_assigned to rows

 center



 verbatim

 1 p n

 for_(i 1_i n_i)

 if_(p 1)_wait (p-1)

 Aip 05 (Ai-1p Aip-1)

 if_(p n-1) signal_(p1)



 verbatim



 center

 (b) Processors_assigned to columns

 center



 Two implementations of pipelining for the code

 of Fig_pipeline-nest-fig

 signal-fig



 figure



 Similarly we can create a pipeline by having processor be

 responsible_for the th column - the elements

 This code is shown in

 Fig signal-fig(b)

 Notice_that

 instead of a barrier to synchronize along each wavefront either scheme

 requires that a processor synchronize with and communicate with only two

 other processors The iterations being performed at any one time form

 a relaxed wavefront through the computation that is the rows or

 columns may progress through the pipeline at slightly_different rates

 ex



 It is not a coincidence that we find more_than one way to pipeline the

 code in Example_expipeline In_fact pipelining along the rows

 and along the columns are just two special_cases of the many_ways one

 can pipeline this code Any partition-mapping function of the form

 where and are positive constants

 will create a pipeline that can execute

 the iterations in time



 ex

 exnsync1

 Our next example_illustrates that pipelining can be_combined with

 constant-synchronization or

 synchronization-free techniques that we saw earlier

 Figure synch-pipe-fig is a more_complex version of the problem we

 saw in Example_exscc2



 figurehtfb



 verbatim

 for_(i 0 i_100 i)

 for_(j 0 j_100 j)

 Aj Aj Bij (s1)

 Ci APi (s2)



 verbatim



 A partially parallelizable loop_nest

 synch-pipe-fig



 figure



 Without knowledge of the values in the matrix we must assume

 that the access in statement may read from any of the elements

 of As a consequence all instances of

 in iteration must

 complete before beginning in iteration which in turn must

 precede operations of any subsequent_iterations for index Thus the

 composition of the outermost_loop cannot be changed all operations in

 one iteration must_be completed before the second and thus there is

 no opportunity for pipelining



 The body of the outer_loop however can be_parallelized with the SCC

 parallelization algorithm (Algorithm alg1sync) The algorithm

 will determine that the inner_loop can be_parallelized if we insert a

 synchronization_barrier before the execution of statement

 The question remaining is what form of parallelism should be applied to

 the inner_loop and that is the subject we address next

 ex



 This discussion thus suggests the following strategy If we do_not

 find enough parallelism that requires at most a constant number of

 synchronizations we try to find_pipelined parallelism in each

 strongly_connected component of the program-dependence graph All

 computation in a strongly_connected component is nested_within the

 same outermost_loop Thus we try to find different_ways in which a

 computation can be divided_into iterations of the outermost_loop

 There is guaranteed to be at_least one legal decomposition which is the

 original one If we can find different orders in which the iterations

 of the outer_loop can be executed then we have found pipelined

 parallelism Otherwise the computation of each iteration of the

 outer_loop must complete before the next So we apply the same

 procedure to the body of the outermost_loop to find finer

 granularities of parallelism This process is repeated until all the

 degrees of parallelism are found



 Time-Partition_Constraints

 time-part-subsect



 In this_section

 we assume that we have_already partitioned the code into strongly

 connected_components and thus need to only parallelize computation

 that shares at_least one common outermost_loop We also assume that

 the loop has_been found not to contain any synchronization-free

 parallelism



 To study the options for parallelizing a loop_nest we need to think

 about the constraints on the time at which each statement instance is

 executed Dependences among statement instances provide a partial_order

 on these instances Our task is thus to assign instances to time units

 so that the dependences are respected and yet provide as much

 parallelism as possible The form of parallelism might involve

 pipelining or another division of tasks to processors However our

 focus in this_section will be on time rather_than on space (ie

 the processor on which a task is performed)



 It is normal to think of time to be one-dimensional However when

 dealing with the times at which components of a program can be executed

 it often helps to think of time as multidimensional for at_least two

 reasons



 enumerate



 The granularity of time can vary in effect turning time into a

 multidimensional value akin to (hours minutes seconds) We_shall

 typically assign iterations of the outer_loop of a loop_nest to time

 steps that you might think of as hours Subtasks within one iteration

 of the outermost_loop eg iterations of the second outermost_loop

 might themselves need to be scheduled into smaller

 time slots like minutes However the process is the same as for

 the outer_loop so we_shall discuss only the process of

 time-partitioning the iterations of the outer_loop



 Sometimes the constraints on the times at which different iterations of

 a statement can execute are best described as a position in a

 multidimensional grid For_instance we_shall discuss a 2-deep_loop

 nest with indexes and where the iteration of the body for

 is only constrained to follow the iterations for and

 In that case we should think of time as a position in a

 time space with coordinates corresponding to and



 enumerate



 Let_us first present a basic constraint that must_be satisfied by any

 transformation that preserves the semantics of the

 original_sequential program The

 time-partition_constraints simply say that two operations sharing a

 data_dependence must_be executed in the original relative sequential ordering

 To_begin we must define precisely the sequential ordering of two

 instances of statements within a loop_nest



 Let be an instance of statement and

 be an instance of statement Both

 statements are within the same loop_nest of depth and we should

 interpret and as the indexes of the outermost_loop

 and as indexes of the second loop and so on

 We_say that if either



 enumerate



 i comes lexicographically before j that is for some

 with

 we have while or



 and statement appears before statement in

 the code as written



 enumerate



 An affine-partition mapping of a program is a legal time partition if and

 only if for every two (not_necessarily distinct)_accesses sharing a dependence

 say



 in statement_nested in loops

 and





 in statement_nested in loops

 the partition_mappings and

 for statements and

 respectively satisfy the time-partition_constraints



 itemize



 For all index vectors

 in and in such that

 itemize

 a)



 b)



 c)





 d)





 itemize

 it is the case that







 itemize



 This constraint illustrated in Fig figtime looks

 remarkably similar to the space-partition_constraints It is a

 relaxation of the space-partition_constraints in that if two iterations

 refer to the same_location they do_not necessarily have to be mapped

 to the same partition we only require that the original relative

 execution order between the two iterations is preserved

 That is the conclusion here has where the space-partition

 constraints have



 figurehtfb

 fileuullmanalsuch11figstimeeps

 Time-Partition_Constraints

 figtime

 figure



 ex

 exnsync1p2

 Let_us consider Example_exnsync1 and in particular the data

 dependences of references to array in statements and

 Because the access is not affine in statement we approximate

 the code by modeling matrix simply as a scalar_variable Let

 be the index value of a dynamic_instance of and let be

 the index value of a dynamic_instance of Let the computation

 mappings of statements and be

 and respectively There_are

 two sets of time-partition_constraints first from statement to

 then from to



 First suppose Then the th_iteration of must

 precede the th_iteration of that is





 arrayrr

 C11_C12



 array





 arrayr

 i



 j



 array





 c1

 C21_i' c2



 Expanding



 C11 i C12j c1_C21 i'_c2



 Since can be arbitrarily_large independent of and it must

 be that

 Thus one possible solution to the constraints is



 C11_C21 1 and C12 c1_c2 0





 Similar arguments about the data_dependence from to

 will yield a similar answer In this particular solution the th

 iteration of the outer_loop which consists of the instance of

 and all instances of are all assigned to timestep

 Other legal choices of and yield

 similar assignments although there might be timesteps at which nothing

 happens

 That is all ways to schedule the outer_loop

 require the iterations to execute in the same order as in the original code

 This statement holds whether all 100 iterations are executed on the same

 processor on 100 different_processors or anything in-between

 ex



 ex

 expipelinesoln

 In Example_expipeline the write_reference Aij

 shares a dependence with itself and the two read references in the

 code

 Let_us first consider the constraint due to the dependence from

 Aij to Ai-1j Let and be two

 data-dependent instances of Aij and Ai-1j respectively

 Instance can depend_on only if

 that is either or and But for the

 instances to access the same array_element it must_be that and



 Therefore the time-partition_constraint reduces

 to





 arrayrr

 C11_C12

 array





 arrayr

 i



 j



 array







 arrayr

 c1



 array





 arrayrr

 C11_C12

 array





 arrayr

 i'



 j'



 array







 arrayr

 c1



 array







 If we substitute for and for the above inequality

 simplifies to

 Thus there are two independent_solutions in the two dimensional index

 space and these form the basis_vectors for the solution space





 arrayr

 1



 0



 array





 arrayr

 0



 1



 array





 If we consider the the dependence from Aij to Aij-1 we draw

 the same conclusion Thus one solution based_on the first vector

 is to assign the th_iteration of the outer_loop to the th

 timestep Of_course we still need to schedule the iterations of the

 inner_loop on within these coarse timesteps



 Another solution is based_on the vector Here we assign all

 iterations of the inner_loop that have index to the th timestep

 regardless of That is we break up each iteration of the outer

 loop or equivalently we swap the order of the two loops in the nest



 A third solution among the infinite set of possibilities is to use the

 sum of the two basis_vectors for the solution space

 That is we assign the iteration with index values and to the

 timestep In this solution at each timestep we execute all the

 iterations along a diagonal wavefront as in Example_expipeline

 This choice corresponds to the pipelined solution outlined in that

 example

 ex



 Solving Time-Partition_Constraints by Farkas'Lemma



 Since the time-partition_constraints are similar to the

 space-partition_constraints can we use a similar algorithm to solve

 them

 Unfortunately the slight difference_between the two problems

 translates_into a big technical difference_between the two solution

 methods

 Algorithm_algnosync simply solves for

 and such that for all in

 and in if



 F1_i1 f1_F2 i2_f2

 then



 C1i1_c1 C2i2_c2



 The linear_inequalities due to the loop_bounds are only used in

 determining if two references share a data_dependence and not used

 otherwise



 To_find solutions to the time-partition_constraints we cannot ignore

 the linear_inequalities ignoring them

 often would allow only the trivial_solution of placing all iterations in the

 same partition Thus the algorithm to find solutions to the

 time-partition_constraints must handle both equalities and

 inequalities



 The general problem we_wish to solve is given a matrix A

 find a vector c such that for all vectors x such that

 it is the case that



 In other_words we are seeking c such that the inner product of

 c and any coordinates in the polyhedron defined by the

 inequalities always yields a

 non-negative answer



 This problem is addressed by Farkas'_Lemma

 Let A be an matrix of reals

 and let c be a real nonzero

 -vector Farkas'_lemma says_that either the primal_system of

 inequalities



 Ax_0 cT x 0



 has a real-valued_solution x or the dual_system



 AT_y c y 0



 has a real-valued_solution y but never both



 The dual_system can be handled by using Fourier-Motzkin_elimination to

 project_away the variables of y For each c that has a

 solution in the dual_system the lemma guarantees that there are no

 solutions to the primal_system Put_another way we can prove the

 negation of the primal_system ie we can prove that



 for all x such that by finding a

 solution y to the dual_system

 and



 About Farkas'_Lemma

 The proof of

 the lemma can be found in many standard texts on linear_programming

 Farkas'_Lemma originally proved in 1901

 is one of the theorems of the alternative

 These

 theorems are all equivalent but despite attempts over the years a

 simple intuitive proof for this lemma or any of its equivalents has

 not been_found



 alg

 algsync

 Finding a set of legal maximally independent affine_time-partition mappings

 for an outer sequential loop



 A loop_nest with array_accesses



 A maximal_set of linearly_independent time-partition_mappings



 The following steps constitute the algorithm



 enumerate



 Find all data-dependent pairs of accesses in a program



 For each pair of data_dependent accesses



 in statement_nested in loops

 and



 in statement_nested in loops

 let and be the

 (unknown) time-partition

 mappings of statements and respectively

 The time-partition_constraint states that for all

 in and in if

 itemize

 )



 )



 )



 and

 )



 itemize

 it must_be the case that





 Since

 is a disjunctive union of a

 number of clauses We create a system of constraints for each clause

 and solve each of them separately as_follows

 enumerate

 Similarly to step (trick-item) in Algorithm_algnosync apply

 Gaussian_elimination to

 the equations



 F1_i1 f1_F2 i2_f2



 to reduce the vector





 arrayc

 i1



 i2



 1



 array





 to some vector of unknowns x

 Let c be all the unknowns in the partition_mappings

 Express the linear inequality constraints_due to the partition

 mappings as



 cTDx 0



 for some matrix D

 Express the precedence constraints on the loop_index variables and

 the loop_bounds as



 Ax_0



 for some matrix A

 Apply Farkas'_Lemma finding x to satisfy the two constraints

 above is equivalent to finding y such that



 AT_y DTc and y 0



 Note_that here is in the

 statement of Farkas'_Lemma and we are using the negated form of the

 lemma

 In this form apply Fourier-Motzkin_elimination to project_away

 the variables and express the constraints on the

 coefficients c as

 Let

 be the system without the constant terms

 enumerate



 Find a maximal_set of linearly_independent solutions to

 using Algorithm figtime-math in

 Appendix time-math-ch

 The approach of that complex algorithm

 is to keep_track of the current set of solutions for each

 of the statements then incrementally look for more independent

 solutions by inserting constraints that force the solution to be

 linearly_independent for at_least one statement



 From each solution of found derive one affine

 time-partition mapping The constant terms are

 derived using

 enumerate

 alg



 ex

 The constraints for Example_exnsync1p2 can be written as





 arrayrrrr

 -C11_-C12 C21 (c2-c1)



 array





 arrayc

 i



 j



 i'



 1



 array



 0









 arrayrrrr

 -1_0 1_0

 array





 arrayc

 i



 j



 i'



 1



 array



 0





 Farkas'_lemma says_that these constraints are equivalent to





 arrayr

 -1



 0



 1



 0



 array





 arrayr

 z



 array







 arrayr

 -C11



 -C12



 C21



 c2-c1



 array

 and

 z 0



 Solving this system we get



 C11_C21 0 and

 C12 c2-c1 0



 Notice_that these constraints are satisfied by the particular

 solution we obtained

 in Example_exnsync1p2

 ex



 Fully_Permutable Loop Nests

 ch11fpn



 If only one solution is found to the time-partition_constraints that

 one solution is necessarily the same as the original code execution

 order In other_words the order of the iterations of the

 outermost_loop are fixed To_find parallelism in the program we can

 re-apply our parallelization algorithm to the body of the outermost

 loop However if we do

 at_least O(n) synchronizations are necessary where is

 the number of iterations in the outermost_loop to

 assure that the iterations of the outermost_loop are executed in

 sequence



 If however more_than one solution is found then the outermost_loop

 or loops are

 amenable to a number of different transformations

 We_shall consider the most_important options below



 Permuted Sequential Loops



 Suppose is the number of

 independent_solutions to the time-partition_constraints

 Then we can rewrite the loop_nest so the outer loops are

 fully_permutable meaning that reordering the nesting of these loops

 arbitrarily does_not

 change the semantics of the original_program execution provided the

 loop_bounds are properly converted



 ex

 perm-loops-ex

 Recall the loop_nest of Example_expipeline which involves the

 nest



 verbatim

 for_(i 1_i n_i)

 for_(j 1_j n_j)

 Aij 05 (Ai-1j Aij-1)

 verbatim

 We saw in Example expipelinesoln that there were two independent

 solutions to the time-dependence constraints which we represented_by

 the basis_vectors and These vectors correspond to

 variation in the direction and the direction respectively

 That is to say the time constraints tell_us to order the instances of

 the assignment_statement having the same value of according to their

 values (lowest first) and order instances having the same value of

 according to their values However those are the only

 constraints we could for example order the instances and

 arbitrarily



 In this case the fully_permutable loops are the two loops that

 correspond to the indexes and themselves The fact that they

 are permutable says_that we could just as_well execute the loop_nest as



 verbatim

 for_(j 1_i n_j)

 for_(i 1_i n_i)

 Aij 05 (Ai-1j Aij-1)

 verbatim



 Observe also that we chose the basis_vectors and

 arbitrarily although they are in some sense the obvious choice We

 could just as_well have chosen another pair of independent vectors like

 and That choice says_that one direction in the time

 space is and the other is There_are still two fully

 permutable_loops but now one has index and the other has index

 In this case the loops look_like



 verbatim

 for_(i 1_i n_i)

 for (p i1 p in p)

 Aip-i 05 (Ai-1p-i Aip-i-1)

 verbatim

 These loops also can be reordered they become



 verbatim

 for (p 2 p 2n p)

 for_(i max(1p-n) i min(p-1n) i)

 Aip-i 05 (Ai-1p-i Aip-i-1)

 verbatim

 Finally observe that in this example the dimension of the time space

 2 equals the total depth of the loop_nest It is however quite

 possible that there would be more nested_loops than the dimension of the

 time space That would be the case for instance if the assignment

 statement in our_running example were replaced_by one or_more loops that

 had the same effect on the dependences Aij had to be preceded by

 Ai-1j and Aij-1

 ex



 We can use

 Algorithm_algenumerate to create a sequential program that

 serially iterates through the -dimensions of partitions

 lexicographically and executes the body of each partition in the

 original_sequential order It should be clear that the program generated

 thusly is

 legal The dependences spanning iterations of the outermost_loops

 are honored since the time-partition_constraints are satisfied

 The ordering within the

 body of the th loop is legal because it is simply the original

 sequential_execution order The first loops of the code generated

 are fully_permutable because the independent affine_partition mappings

 can be arbitrarily ordered



 Pipelining



 A -deep_fully permutable_loop nest has degrees of pipeline

 parallelism The iteration with index values

 can be executed

 without violating data_dependence constraints provided iterations



 (p1-1 p2 pk)

 (p1 p2-1 p3 pk)

 (p1 pk-1 pk-1)



 have_been

 executed

 We can assign the partitions to processors with each

 processor executing iterations with fixed values of

 the first indexes

 Each_processor executes its portion of the iterations in sequential

 order It can execute iteration (ie its task with the th loop

 index equal to ) as_long as it receives a signal

 from processors



 (p1-1 p2 pk-1) (p1

 pk-2 pk-1-1)



 that they have executed their iterations with the th loop_index equal

 to A relaxed

 version of a -dimensional wavefront is being executed at any one

 time



 Blocking



 A -deep_fully permutable_loop nest can be blocked in

 -dimensions Instead of manipulating the individual partitions we

 can aggregate blocks of iterations into one_unit Blocking is useful

 for enhancing data_locality as_well as for minimizing the overhead of

 pipelining



 figure



 verbatim

 for_(i0 in i)

 for (j1 jn_j)

 S



 verbatim



 center

 (a) A simple loop_nest

 center



 verbatim

 for (ii 0 iin ib)

 for (jj 0 jjn jjb)

 for_(i iib i min(iib-1 n) i)

 for_(j iib j min(jjb-1 n) j)

 S



 verbatim



 center

 (b) A blocked_version of this loop_nest

 center



 A 2-dimensional loop_nest and its blocked_version

 blocked-code-fig



 fileuullmanalsuch11figstileeps

 Execution order before and after blocking a 2-deep_loop nest

 figtile



 figure



 Suppose we have a two-dimensional fully_permutable loop_nest as in

 Fig blocked-code-fig(a) and we

 wish to break the computation into blocks The

 execution order of the blocked code is shown in

 Fig figtile and the equivalent code is in

 Fig blocked-code-fig(b)



 We can coarsen the granularity of pipelining by assigning a column

 of blocks to one processor Notice_that

 each processor synchronizes with its

 predecessors and successors only at block boundaries Thus another

 advantage of blocking is that programs

 only need to communicate data

 accessed at the boundaries of the block with their neighbor blocks

 Values that are interior to a block are managed by only one processor



 figure



 verbatim

 for_(i 1_i N_i)

 for_(j 1_j i-1 j)

 for (k 1 k j-1 k)

 Aij_Aij - Aik Ajk

 Aij_Aij Ajj



 for (m 1 m i-1 m)

 Aii Aii - Aim Aim

 Aii sqrt(Aii)



 verbatim



 Cholesky_decomposition

 cholesky-fig



 verbatim

 for (i2 1 i2 N i2)

 for (j2 1 j2 i2 j2)

 beginning of code for processor_(i2j2)

 for (k2 1 k2 i2 k2)



 Mapping_i2 i_j2 j_k2 k

 if (j2i2 k2j2)

 Ai2j2 Ai2j2 - Ai2k2 Aj2k2



 Mapping_i2 i_j2 j_k2 j

 if (j2k2 j2i2)

 Ai2j2 Ai2j2 Aj2j2



 Mapping_i2 i_j2 i_k2 m

 if_(i2j2 k2i2)

 Ai2i2 Ai2i2 - Ai2k2 Ai2k2



 Mapping_i2 i_j2 i_k2 i

 if_(i2j2 j2k2)

 Ak2k2 sqrt(Ak2k2)



 ending of code for processor_(i2j2)



 verbatim



 Figure cholesky-fig written as a fully_permutable loop

 nest

 cholesky-nest-fig



 figure



 ex

 We_now use a real numerical algorithm -

 Cholesky_decomposition - to illustrate_how Algorithm_algsync handles

 single loop_nests with only pipelining parallelism

 The code shown in Fig cholesky-fig implements

 an algorithm operating on a 2-dimensional data array The

 executed iteration_space is a triangular pyramid since only

 iterates up to the value of the outer_loop index and

 only_iterates to the value of The loop has four statements all

 nested in different loops



 Applying Algorithm_algsync to this program finds three

 legitimate time dimensions It nests all the operations some of

 which were_originally nested in 1- and 2-deep_loop nests into a

 3-dimensional fully_permutable loop_nest The code together_with the

 mappings is shown in Fig cholesky-nest-fig



 The code_generation routine guards the execution of the operations

 with the original loop_bounds to ensure_that the new programs execute

 only operations that are in the original code We can pipeline this

 code by mapping the 3 dimensional structure to a 2-dimensional

 processor space Iterations are assigned to the processor

 with ID Each_processor executes the innermost_loop the loop

 with the index

 Before it

 executes the th_iteration the processor_waits for signals from

 the processors with ID's and After it executes its

 iteration it signals processors and

 ex



 Putting it All_Together

 ch11par



 We have described two powerful parallelization algorithms in the last

 two sections We can combine these algorithms recursively to find

 more degrees of parallelism at the cost of higher synchronization

 Thus in Example_exnsync1 we found that

 applying_Algorithm algsync determines that there is

 only one legal outer_loop which is the one in the original code of

 Fig synch-pipe-fig

 Then we can apply_Algorithm alg1sync to the time partition to

 parallelize the inner_loop We can treat the code within a partition

 like a whole program the only_difference being that the partition

 number is treated_like a symbolic_constant



 alg

 algpar

 Find all the degrees of parallelism in a program with all the

 parallelism being as coarse-grained as possible



 A program to be_parallelized



 A parallelized version of the same program



 Do the following



 enumerate



 Find the maximum_degree of parallelism_requiring no synchronization

 Apply_Algorithm algnosync to the program



 Find the maximum_degree of parallelism that requires

 synchronizations_Apply Algorithm_alg1sync to each of the space

 partitions_found in step 1



 Find the maximum_degree of parallelism that requires

 synchronizations_Apply Algorithm_algsync to each of the

 partitions_found in step 2 Then apply_Algorithm alg1sync to

 each of the partitions assigned to each processor



 Find the maximum_degree of parallelism with successively greater

 degrees of synchronizations Recursively apply step 3 to computation

 belonging to each of the space partitions generated_by the previous

 step until all parallelism is found



 enumerate

 alg



 The above algorithm does_not address the full

 parallelization problem Remember that while synchronization is important

 communication can also impose a significant overhead If we simply

 minimize synchronization we can increase communication unnecessarily

 Here is a concrete example



 ex

 As_discussed in Example_exadi optimizing the first and second

 loop_nests independently finds parallelism in each of the nests but

 requires that the matrix be transposed between the

 loops incurring data_traffic

 If we use Algorithm_algsync to find_pipelined parallelism we

 find that we can turn the entire program into a fully_permutable loop

 nest as in Fig another-nest-fig

 We then can apply_blocking to reduce the communication overhead This

 scheme would incur synchronizations but would_require much less

 communication

 ex



 figurehtfb



 verbatim

 for_(j 0 j_n j)

 for_(i 1_i n1 i)

 if (i n) Aij f(Aij Ai-1j)

 if (j 0) Ai-1j g(Ai-1jAi-1j-1)



 verbatim



 A fully_permutable loop_nest for the code of

 Example_exadi

 another-nest-fig



 figure



 Additionally it sometimes helps to

 distinguish_between global and neighboring communication The cost of

 transposing a matrix is significantly higher_than having neighboring

 processors share boundary data When trying to find parallelism

 across strongly_connected components we can minimize_communication by

 requiring that accesses to the same data be mapped to neighboring

 processors Constraints on the partition_mapping in the

 space-partition_constraints are changed from



 Cs1i1 cs1 - Cs2i2

 cs2 0



 to



 Cs1i1 cs1 - Cs2i2

 cs2

 where is a small constant

 By design programming_languages have a syntactic_structure that

 offers significant benefits for language description and compiler

 writing These benefits flow from a precise notation for

 specifying syntax called context-free_grammars or BNF introduced

 in Chapter 2 A C grammar for example specifies that a program

 is made up of functions a function out of declarations and

 statements a statement out of expressions and so on This

 grammatical_structure can be used to understand a program or to

 translate it



 itemize





 A grammar gives a precise yet easy-to-understand syntactic

 specification of a programming_language





 From certain classes of grammars we can automatically construct

 an efficient parser that determines the syntactic_structure of a

 source_program As a side benefit the parser construction

 process can reveal syntactic ambiguities and trouble spots that

 might have slipped through the initial design phase of a language





 The structure imparted on a language by a properly designed

 grammar is useful for translating source programs into correct

 object code and for detecting errors





 Languages evolve over time acquiring new constructs to perform

 additional tasks These new constructs can be integrated into a

 language more easily when there is an existing implementation

 based_on a grammatical description of the language

 itemize



 The benefits of grammars are available not only for programming

 languages but for specialized or little languages for

 applications ranging_from document layout to circuit design Tools

 are readily available for converting grammar-based descriptions

 into working programs



 The bulk of this_chapter is devoted to parsing methods that are

 typically used in compilers We first present the basic concepts

 then techniques suitable for hand implementation and finally

 algorithms that have_been used in automated tools Since programs

 may contain syntactic_errors we discuss extensions of the parsing

 methods for recovery from common errors



 Introduction

 syntax-intro-sect

 The Role of the Parser

 In our compiler model the parser obtains a string of tokens from

 the lexical_analyzer as shown in Fig parser-role-fig and

 verifies that the string can be generated_by the grammar for the

 source_language We expect the parser to report any syntax errors

 in an intelligible fashion and to recover from commonly occurring

 errors to continue processing the remainder of the program

 Conceptually for well formed programs the parser constructs a

 parse_tree and passes it onto the rest of the program for further

 processing In_fact the parse_tree need not be constructed

 explicitly since checking and translation actions can be

 interspersed with parsing as we_shall see



 figurehtfb

 center



 Position of parser in compiler model

 parser-role-fig

 center

 figure



 There_are three general types of parsers for grammars Universal

 parsing methods such_as the Cocke-Younger-Kasami algorithm and

 Earley's algorithm can parse any grammar (see the bibliographic

 notes) These general methods are however too inefficient to

 use in production compilers



 The methods commonly_used in compilers can be classified as being

 either top-down or bottom-up As implied by their names top-down

 methods build parse_trees from the top (root) to the bottom

 (leaves) while bottom-up methods start from the leaves and work

 their way up to the root In either case the input to the parser

 is scanned from left to right one symbol at a time



 The most efficient top-down and bottom-up methods work only for

 subclasses of grammars but several of these classes

 particularly LL and LR_grammars are expressive enough to

 describe most syntactic_constructs Parsers implemented_by hand

 often work with LL_grammars eg the approach of Section OLD24

 constructs parsers for LL_grammars Parsers for the larger class

 of LR_grammars are usually constructed using automated tools



 In this_chapter we assume that the output of the parser is some

 representation of the parse_tree for the stream of tokens from the

 lexical_analyzer In_practice there are a number of tasks that

 might be conducted during_parsing such_as collecting information

 about various tokens into the symbol_table performing type

 checking and other kinds of semantic analysis and generating

 intermediate_code We have lumped all of these activities into

 the rest of the front_end box in Fig parser-role-fig

 These activities will be covered in detail in subsequent chapters

 Representative Grammars

 Grammars for simple arithmetic_expressions are sufficient for

 illustrating the essence of parsing since parsing techniques for

 expressions carry over to most programming_constructs Constructs

 that begin_with keywords_like while or int are

 relatively_easy to parse because the keyword guides the choice of

 the grammar production that must_be applied to match the input

 Expressions present more of challenge because of the associativity

 and precedence of operators



 Some of the grammars that will be examined in this_chapter are

 presented here for ease of reference Associativity and precedence

 are captured in the following grammar which is similar to ones

 used in Chapter 2 for describing expressions terms and factors

 E represents expressions consisting of terms separated_by

 signs T represents terms consisting of factors

 separated_by signs and F represents factors that

 can either be parenthesized_expressions or identifiers



 displayexpr-gram-display

 317ptstabularl_c l

 E_E T_T



 T_T F_F



 F ( E ) id



 tabular (expr-gram-display)

 display

 Expression grammar_(expr-gram-display) belongs to the class

 of LR_grammars that are suitable for bottom-up_parsing This

 grammar can be adapted to handle additional operators and

 additional levels of precedence It cannot be used however for

 top-down_parsing because it is left_recursive



 The following non-left-recursive variant of the expression

 grammar_(expr-gram-display) will be used for top-down

 parsing



 displaytopdown-expr-display

 317ptstabularl_c l

 E_T E



 E_T E



 T_F T



 T_F T



 F ( E

 ) id

 tabular

 (topdown-expr-display)

 display

 The following grammar treats and alike so it is

 useful for illustrating techniques for handling ambiguities during

 parsing



 displayexpr-ambig-display

 E_E E

 E_E ( E

 ) id(expr-ambig-display)

 display

 Here E represents expressions This grammar

 (expr-ambig-display) permits more_than one parse_tree for

 expressions like abc



 Syntax Error Handling



 The remainder of this_section considers the nature of syntactic

 errors and general strategies for error_recovery Two of these

 strategies called panic-mode and phrase-level recovery are

 discussed in more_detail together_with the individual parsing

 methods The implementation of each strategy calls upon the

 compiler writer's experience and judgement however we_shall give

 some hints regarding approach



 If a compiler had to process only correct programs its design and

 implementation would be greatly simplified A good compiler is

 expected to assist the programmer in locating and tracking down

 errors that inevitably creep into programs despite the

 programmer's best efforts Strikingly few languages have_been

 designed with error_handling in mind even_though errors are so

 commonplace Our civilization would be radically different if

 spoken languages had the same requirements for syntactic accuracy

 as computer languages Most programming_language specifications

 do_not describe_how a compiler should respond to errors error

 handling is left to the compiler_designer Planning the error

 handling right from the start can both simplify the structure of a

 compiler and improve its handling of errors



 Common programming_errors can occur at many different_levels

 itemize





 Lexical errors_include misspellings of identifiers

 keywords or operators as in the use of an_identifier

 elipsesize instead of ellipsesize and missing quotes

 around text intended as a string







 Syntactic errors_include misplaced semicolons or extra or

 missing braces that is 123 or

 125 In C and Java the appearance of a case

 statement without an enclosing switch is a syntactic error

 (Although syntactic this constraint is usually checked during

 processing after parsing)





 Semantic errors_include type mismatches between operators

 and operands An_example is a return statement in a Java

 method with result type void Unlike in C it is a

 semantic error in Java for a value of type int to appear

 where a boolean is expected





 Logical errors can be anything from incorrect reasoning on

 the part of the programmer to the use in a C program of the

 assignment operator instead of the comparison operator

 The program containing may be well formed

 however it may not reflect the programmers intent

 itemize



 This_chapter deals_with error_recovery during syntax analysis

 One reason for emphasizing error_recovery during_parsing is that

 many errors are syntactic in nature and are exposed when the

 stream of tokens from the lexical_analyzer disobeys the

 grammatical rules defining the programming_language Another

 reason is that the precision of parsing methods allows syntactic

 errors to be detected very efficiently Certain semantic errors

 such_as type mismatches can also be detected efficiently

 however accurate detection of semantic and logical errors at

 compile_time is in general a difficult task



 The error handler in a parser has goals that are simple to state

 but challenging to realize



 itemize





 Report the presence of errors clearly and accurately





 Recover from each error quickly enough to detect subsequent

 errors





 Add minimal overhead to the processing of correct programs

 itemize



 Fortunately common errors are simple ones and a relatively

 straightforward error-handling mechanism often suffices



 Several parsing methods such_as the LL and LR methods detect an

 error as_soon as possible More_precisely they have the

 viable-prefix property meaning that they detect that an error

 has_occurred as_soon as they see prefix of the input that is not a

 prefix of any string in the language



 How should an error handler report the presence of an error At

 the very least it must report the place in the source_program

 where an error is detected because there is a good chance that the

 actual error occurred within the previous few tokens A common

 strategy is to print the offending line with a pointer to the

 position at which an error is detected



 Error-Recovery Strategies



 Once an error is detected how should the parser recover Although

 no strategy has proven itself universally acceptable a few

 methods have broad applicability In most cases it is not

 adequate for the parser to quit after detecting the first error

 because its recovery attempts may provide hints to the programmer

 about the location of the actual error and may also reveal

 further errors Usually there is some form of error_recovery in

 which the parser attempts to restore itself to a state where

 processing of the input can continue with reasonable hopes that

 the further processing will provide meaningful diagnostic

 information If errors pile up it is better for the compiler to

 give up after exceeding some threshold than it is to produce an

 annoying avalanche of spurious errors



 Here we introduce the following recovery strategies panic mode

 phrase level error productions and global correction





 Panic-mode recovery This is the simplest method to

 implement On discovering an error the parser discards input

 symbols one at a time until one of a designated set of

 synchronizing_tokens is found The synchronizing_tokens are

 usually delimiters such_as semicolon or 125 whose

 role in the source_program is clear The compiler_designer must

 select the synchronizing_tokens appropriate for the source

 language of course While panic-mode correction often skips a

 considerable amount of input without checking for it for

 additional errors it has the advantage of simplicity and unlike

 some methods to be considered later is guaranteed not to go into

 an_infinite loop In situations_where multiple scans of the same

 statement are quite rare this method may be adequate





 Phrase-level recovery On discovering an error a

 parser may perform local_correction on the remaining_input that

 is it may replace a prefix of the remaining_input by some string

 that allows the parser to continue A_typical local_correction is

 to replace a comma by a semicolon delete an extraneous semicolon

 or insert a missing semicolon The choice of the local_correction

 is left to the compiler_designer Of_course we must_be careful

 to choose replacements that do_not lead to infinite loops as

 would be the case for example if we always inserted something on

 the input ahead of the current_input symbol



 Phrase-level replacement has_been used in several error-repairing

 compilers it can correct any input_string The method was first

 used with top-down_parsing Its major drawback is the difficulty

 it has in coping with situations in which the actual error has

 occurred before the point of detection





 Error productions By anticipating common errors that might

 be encountered we can augment the grammar for the language at

 hand with productions that generate the erroneous constructs A

 parser constructed from a grammar augmented by these error

 productions detects the anticipated errors when an error

 production is used during_parsing The parser can then generate

 appropriate error diagnostics about the erroneous construct that

 has_been recognized in the input





 Global correction Ideally we would like a compiler to

 make as few changes as possible in processing an incorrect input

 string There_are algorithms for choosing a minimal sequence of

 changes to obtain a globally least-cost correction Given an

 incorrect input_string and grammar these algorithms will

 find a parse_tree for a related string such that the number

 of insertions deletions and changes of tokens required to

 transform into is as small as possible Unfortunately

 these methods are in general too costly to implement in terms of

 time and space so these techniques are currently only of

 theoretical interest



 Do note_that a closest correct program may not be what the

 programmer had in mind Nevertheless the notion of least-cost

 correction provides a yardstick for evaluating error-recovery

 techniques and has_been used for finding optimal replacement

 strings for phrase-level recovery

 Introduction

 syntax-intro-sect



 In this_section we examine the way the parser fits into a

 typical compiler We then look_at typical grammars for

 arithmetic_expressions Grammars for expressions suffice for

 illustrating the essence of parsing since parsing techniques for

 expressions carry over to most programming_constructs This

 section ends with a discussion of error_handling since the parser

 must respond gracefully to finding that its input cannot be

 generated_by its grammar



 The Role of the Parser

 parser-role-subsect



 In our compiler model the parser obtains a string of tokens from

 the lexical_analyzer as shown in Fig parser-role-fig and

 verifies that the string of token names can be generated_by the

 grammar for the source_language

 We expect the parser to report any syntax errors

 in an intelligible fashion and to recover from commonly occurring

 errors to continue processing the remainder of the program

 Conceptually for well-formed programs the parser constructs a

 parse_tree and passes it to the rest of the compiler for further

 processing In_fact the parse_tree need not be constructed

 explicitly since checking and translation actions can be

 interspersed with parsing as we_shall see Thus the parser and

 the rest of the front_end could well be_implemented by a single

 module



 figurehtfb

 center



 Position of parser in compiler model

 parser-role-fig

 center

 figure



 There_are three general types of parsers for grammars universal

 top-down and bottom-up Universal

 parsing methods such_as the Cocke-Younger-Kasami algorithm and

 Earley's algorithm can parse any grammar (see the bibliographic

 notes) These general methods are however too inefficient to

 use in production compilers



 The methods commonly_used in compilers can be classified as being

 either top-down or bottom-up As implied by their names top-down

 methods build parse_trees from the top (root) to the bottom

 (leaves) while bottom-up methods start from the leaves and work

 their way up to the root In either case the input to the parser

 is scanned from left to right one symbol at a time



 The most efficient top-down and bottom-up methods work only for

 subclasses of grammars but several of these classes

 particularly LL and LR_grammars are expressive enough to

 describe most of the syntactic_constructs

 in modern programming_languages Parsers implemented_by hand

 often use LL_grammars for example the predictive-parsing

 approach of Section predictive-sect works for LL_grammars

 Parsers for the larger class of LR_grammars are usually

 constructed using automated tools



 In this_chapter we assume that the output of the parser is some

 representation of the parse_tree for the stream of tokens that

 comes_from the

 lexical_analyzer In_practice there are a number of tasks that

 might be conducted during_parsing such_as collecting information

 about various tokens into the symbol_table performing type

 checking and other kinds of semantic analysis and generating

 intermediate_code We have lumped all of these activities into

 the rest of the front_end box in Fig parser-role-fig

 These activities will be covered in detail in subsequent chapters



 Representative Grammars

 rep-grammars-subsect



 Some of the grammars that will be examined in this_chapter are

 presented here for ease of reference Constructs that begin_with

 keywords_like while or int are relatively_easy to

 parse because the keyword guides the choice of the grammar

 production that must_be applied to match the input We therefore

 concentrate_on expressions which present more of challenge

 because of the associativity and precedence of operators



 Associativity and precedence are captured in the following

 grammar which is similar to ones used in Chapter 2 for describing

 expressions terms and factors E represents expressions

 consisting of terms separated_by signs T represents

 terms consisting of factors separated_by signs and F represents factors that can be either parenthesized_expressions

 or identifiers



 displayexpr-gram-display

 317ptstabularl l_l













 tabular (expr-gram-display)

 display

 Expression grammar_(expr-gram-display) belongs to the class

 of LR_grammars that are suitable for bottom-up_parsing This

 grammar can be adapted to handle additional operators and

 additional levels of precedence However it cannot be used for

 top-down_parsing because it is left_recursive



 The following non-left-recursive variant of the expression

 grammar_(expr-gram-display) will be used for top-down

 parsing



 displaytopdown-expr-display

 317ptstabularl l_l





















 tabular

 (topdown-expr-display)

 display

 The following grammar treats and alike so it is useful

 for illustrating techniques for handling ambiguities during

 parsing



 displayexpr-ambig-display

 317ptstabularl l_l





 tabular

 (expr-ambig-display)

 display

 Here E represents expressions of all types Grammar

 (expr-ambig-display) permits more_than one parse_tree for

 expressions like



 Syntax Error Handling

 syn-err-handle-subsect



 The remainder of this_section considers the nature of syntactic

 errors and general strategies for error_recovery Two of these

 strategies called panic-mode and phrase-level recovery are

 discussed in more_detail in connection_with specific parsing

 methods



 If a compiler had to process only correct programs its design and

 implementation would be simplified greatly However a compiler is

 expected to assist the programmer in locating and tracking down

 errors that inevitably creep into programs despite the

 programmer's best efforts Strikingly few languages have_been

 designed with error_handling in mind even_though errors are so

 commonplace Our civilization would be radically different if

 spoken languages had the same requirements for syntactic accuracy

 as computer languages Most programming_language specifications

 do_not describe_how a compiler should respond to errors error

 handling is left to the compiler_designer Planning the error

 handling right from the start can both simplify the structure of a

 compiler and improve its handling of errors



 Common programming_errors can occur at many different_levels

 itemize



 Lexical errors_include misspellings of identifiers

 keywords or operators - eg the use of an_identifier elipseSize instead of ellipseSize - and missing quotes

 around text intended as a string





 Syntactic errors_include misplaced semicolons or extra or

 missing braces that is 123 or 125 As_another example in

 C or Java the appearance of a case

 statement without an enclosing switch is a syntactic error

 (however this situation is usually allowed by the parser and caught

 later in the processing as the compiler attempts to generate code)



 Semantic errors_include type mismatches between

 operators and operands eg the return of a value in

 a Java method with result type void



 Logical errors can be anything from incorrect reasoning on

 the part of the programmer to the use in a C program of the

 assignment operator instead of the comparison operator

 The program containing may be well formed

 however it may not reflect the programmer's intent

 itemize



 The precision of parsing methods allows syntactic_errors to be

 detected very efficiently Several parsing methods such_as the LL

 and LR methods detect an error as_soon as possible that is when

 the stream of tokens from the lexical_analyzer cannot be_parsed

 further according to the grammar for the language More_precisely

 they have the viable-prefix property meaning that they

 detect that an error has_occurred as_soon as they see a prefix of

 the input that cannot be completed to form a string in the

 language



 Another reason for emphasizing error_recovery during_parsing is

 that many errors appear syntactic whatever their cause and are

 exposed when parsing cannot continue A few semantic errors such

 as type mismatches can also be detected efficiently however

 accurate detection of semantic and logical errors at_compile time

 is in general a difficult task



 The error handler in a parser has goals that are simple to state

 but challenging to realize



 itemize



 Report the presence of errors clearly and accurately



 Recover from each error quickly enough to detect subsequent

 errors



 Add minimal overhead to the processing of correct programs



 itemize



 Fortunately common errors are simple ones and a relatively

 straightforward error-handling mechanism often suffices



 How should an error handler report the presence of an error At

 the very least it must report the place in the source_program

 where an error is detected because there is a good chance that the

 actual error occurred within the previous few tokens A common

 strategy is to print the offending line with a pointer to the

 position at which an error is detected



 Error-Recovery Strategies

 syn-err-recover-subsect



 Once an error is detected how should the parser recover Although

 no strategy has proven itself universally acceptable a few

 methods have broad applicability The simplest approach is for

 the parser to quit with an informative error message when it

 detects the first error Additional errors are often uncovered if

 the parser can restore itself to a state where processing of the

 input can continue with reasonable hopes that the further

 processing will provide meaningful diagnostic information If

 errors pile up it is better for the compiler to give up after

 exceeding some error limit than to produce an annoying avalanche

 of spurious errors



 The balance of this_section is devoted to the following recovery

 strategies panic-mode phrase-level error-productions and

 global-correction



 Panic-Mode Recovery



 With this method on discovering an error the parser discards

 input symbols one at a time until one of a designated set of synchronizing_tokens is found The synchronizing_tokens are

 usually delimiters such_as semicolon or 125 whose

 role in the source_program is clear and unambiguous The compiler

 designer must select the synchronizing_tokens appropriate for the

 source_language While panic-mode correction often skips a

 considerable amount of input without checking it for additional

 errors it has the advantage of simplicity and unlike some

 methods to be considered later is guaranteed not to go into an

 infinite loop









 Phrase-Level Recovery



 On discovering an error a

 parser may perform local_correction on the remaining_input that

 is it may replace a prefix of the remaining_input by some string

 that allows the parser to continue A_typical local_correction is

 to replace a comma by a semicolon delete an extraneous semicolon

 or insert a missing semicolon The choice of the local_correction

 is left to the compiler_designer Of_course we must_be careful

 to choose replacements that do_not lead to infinite loops as

 would be the case for example if we always inserted something on

 the input ahead of the current_input symbol



 Phrase-level replacement has_been used in several error-repairing

 compilers as it can correct any input_string

 Its major drawback is the difficulty

 it has in coping with situations in which the actual error has

 occurred before the point of detection



 Error Productions



 By anticipating common errors that might

 be encountered we can augment the grammar for the language at

 hand with productions that generate the erroneous constructs A

 parser constructed from a grammar augmented by these error

 productions detects the anticipated errors when an error

 production is used during_parsing The parser can then generate

 appropriate error diagnostics about the erroneous construct that

 has_been recognized in the input



 Global Correction



 Ideally we would like a compiler to

 make as few changes as possible in processing an incorrect input

 string There_are algorithms for choosing a minimal sequence of

 changes to obtain a globally least-cost correction Given an

 incorrect input_string and grammar these algorithms will

 find a parse_tree for a related string such that the number

 of insertions deletions and changes of tokens required to

 transform into is as small as possible Unfortunately

 these methods are in general too costly to implement in terms of

 time and space so these techniques are currently only of

 theoretical interest



 Do note_that a closest correct program may not be what the

 programmer had in mind Nevertheless the notion of least-cost

 correction provides a yardstick for evaluating error-recovery

 techniques and has_been used for finding optimal replacement

 strings for phrase-level recovery

 Syntax Definition

 syntax-sect



 In this_section we introduce a notation - the context-free

 grammar or grammar for short -

 that is used to specify the syntax of a

 language Grammars will be used_throughout this_book to organize

 compiler_front ends



 A grammar naturally describes the hierarchical_structure of most

 programming_language constructs For_example an if-else statement

 in Java can have the form

 center

 if_( expression ) statement else statement

 center



 That is an if-else statement is the concatenation of the keyword

 if an opening parenthesis an expression a closing

 parenthesis a statement the keyword else and another

 statement Using the

 variable expr to denote an expression and the variable stmt to denote a statement this structuring rule can be

 expressed_as

 center

 stmt if_( expr ) stmt_else stmt

 center

 in which the arrow may be read as can have the form Such a

 rule is called a production In a production lexical

 elements like the keyword if and the parentheses are called

 tokens or terminals Variables like expr and stmt represent

 sequences of tokens and are called nonterminals



 Tokens Versus Terminals

 In grammars the elementary symbols may be referred to either as tokens or as

 terminals

 The difference is a matter of viewpoint

 In a real compiler the characters of the source_program are processed by a lexical

 analyzer (see_Section lexan-sect) that groups characters into units called

 tokens

 The tokens form the elementary units of the grammar for the programming_language

 When we speak of parsing or processing the grammar independent of how the tokens

 are constructed we usually refer to tokens as terminals in contrast to the

 variables or nonterminals which are the other symbols that appear in grammars



 Definition of Grammars

 gram-defn-subsect



 A context-free_grammar has four components



 enumerate



 A set of terminal_symbols sometimes referred to as tokens

 The terminals are elementary symbols of the language defined by the

 grammar



 A set of nonterminals sometimes_called variables

 Each nonterminal represents a set of strings of terminals in a manner

 we_shall describe



 A set of productions where each production consists of a

 nonterminal called the head or left_side of the

 production an arrow and a sequence of terminals andor

 nonterminals called the body or right_side of the

 production

 The intuitive intent of a production is to say that one way to form one

 of the strings represented_by the

 nonterminal of the head is to use each of the grammar_symbols of the

 body in order from the left We use a nonterminal of the body by

 replacing it by one of the strings it represents



 A designation of one of the nonterminals as the start

 symbol



 enumerate



 We specify grammars by listing their

 productions with the productions for the start_symbol listed

 first We assume that digits signs such_as and boldface

 strings such_as while are terminals An italicized name is a

 nonterminal and any nonitalicized name or symbol may be assumed to

 be a terminalfootnoteIndividual italic letters will be used

 for additional purposes when grammars are studied in detail in

 Chapter_parse-ch For_example we_shall use and

 to talk_about a symbol that is either a terminal or a

 nonterminal However any italicized name containing two or_more

 characters will continue to represent a

 nonterminalfootnote For notational_convenience

 productions with the same nonterminal as the head can have their

 bodies grouped with the alternative bodies separated_by the

 symbol which we read as or



 ex

 digits-ex Several examples in this_chapter use expressions

 consisting of digits and plus and minus_signs eg strings such_as

 9-52

 3-1 or 7 Since a plus or minus_sign must appear

 between two digits we refer to such expressions as lists of

 digits separated_by plus or minus_signs The following grammar

 describes the syntax of these expressions The productions are



 eqnarray

 list list digit eqlist-plus



 list list digit eqlist-minus



 list digit eqlist-digit



 digit

 0_1 2_3 4_5 6_7 8 9eqdigits

 eqnarray



 The bodies of the three productions with nonterminal list as

 head equivalently can be grouped



 center

 list list digit list digit digit

 center



 According to our conventions the tokens or terminals of the grammar are the

 symbols

 center

 - 0_1 2_3 4_5 6_7 8_9

 center

 The nonterminals are the italicized names list and digit with list being the start_symbol because its

 productions are given first

 ex



 We_say a production is for a nonterminal if the nonterminal

 is the head of the production A string of terminals is a sequence of

 zero_or more terminals The string of zero terminals written as

 is called the empty stringfootnoteTechnically

 can be a string of zero symbols from any alphabet (collection

 of symbols)footnote



 Derivations

 deriv-subsect



 A grammar derives strings by beginning with the start_symbol and

 repeatedly replacing a nonterminal by the body of a production for

 that nonterminal The terminal strings that can be derived_from the

 start_symbol form the language defined by the grammar



 ex

 deriv-ex

 The language defined by the grammar of

 Example_digits-ex consists of lists of digits separated_by

 plus and minus_signs

 The ten productions for the nonterminal digit allow it to

 stand_for any of the terminals

 From production

 (eqlist-digit) a single digit by itself is a list

 Productions (eqlist-plus) and (eqlist-minus) express

 the rule that any list followed_by a plus or minus_sign and then

 another digit makes up a new list



 It_turns out that productions (eqlist-plus) to

 (eqdigits) are all we need to define the desired language

 For_example we can deduce that 9-52 is a list as

 follows



 enumerate



 a)

 9 is a list by production (eqlist-digit) since

 9 is a digit



 b)

 9-5 is a list by production (eqlist-minus)

 since 9 is a list and 5 is a digit



 c)

 9-52 is a list by production (eqlist-plus)

 since 9-5 is a list and 2 is a digit

 enumerate

 ex



 ex

 params-ex A somewhat different sort of list is the list of

 parameters in a function call In Java the parameters are

 enclosed within parentheses as in the call 'max(xy)' of

 function max with parameters x and y One nuance

 of such lists is that an empty list of parameters may be found

 between the terminals '(' and ')' We may start to

 develop a grammar for such sequences by including the productions

 center

 tabularr_c l

 call id '(' optparams ')'



 optparams params



 params params param param

 tabular

 center

 Note_that the second possible body for optparams (optional

 parameter list) is which stands_for the empty_string

 of symbols That is optparams can be replaced_by the empty

 string so a call can consist of a function name followed_by

 the two-terminal string () Notice_that the productions for params are analogous to those for list in

 Example_digits-ex with comma in place of the arithmetic

 operator or and param in place of digit We have not

 shown the productions for param since parameters are really

 arbitrary expressions Shortly we_shall discuss

 the appropriate productions for the various language_constructs

 such_as expressions statements and so on

 ex



 The problem of taking a string of tokens such_as which in

 practice is a source_program with multicharacter tokens grouped by a

 lexical_analyzer and figuring out what grouping of tokens into

 nonterminals makes it

 derivable_from the start_symbol of the grammar for this programming

 languge is one of the most fundamental in all of compiling This

 parsing problem is solved by a variety of means we_shall discuss

 the dominant approaches in Chapter_parse-ch



 Tree Terminology

 Tree data_structures figure prominently in compiling We_shall use a

 fairly standard notation for trees



 itemize



 A tree consists of one or_more nodes

 Nodes may have labels which in this_book typically will be

 grammar_symbols

 When we draw a tree we often represent the nodes by these labels only



 Exactly one node is the root All nodes except the root have a

 unique parent the root has no parent

 When we draw trees we place the parent of a node above that node and

 draw an edge between them



 If node is the parent of node then is a child of

 The children of one node are called siblings

 They have an order from the left and

 when we draw trees we order the childen of a given node in this

 manner



 A node with no children is called a leaf Other nodes - those

 with one or_more children - are interior_nodes



 A descendant of a node is either itself a child of a

 child of a child of and so on for any number of levels We_say

 node is an_ancestor of node if is a descendant of





 itemize





 Parse Trees

 parse-tree-subsect



 A parse_tree pictorially shows_how the start_symbol of a grammar

 derives a string in the language If nonterminal has a

 production then a parse_tree may have an interior

 node_labeled with three children_labeled and

 from left to right

 center



 center



 Formally given a context-free_grammar a parse_tree is a

 tree with the following properties



 enumerate



 The root is labeled by the start_symbol



 Each leaf is labeled by a terminal or by



 Each interior_node is labeled by a nonterminal



 If is the nonterminal labeling some interior_node and

 are the labels of the children of that

 node from left to right then there must_be a production

 Here each

 stand_for a symbol that is either a terminal or a nonterminal As

 a special_case if is a production

 then a node_labeled

 may have a single child_labeled

 enumerate



 ex

 The derivation of 9-52 in Example deriv-ex is

 illustrated by the tree in Fig list-tree-fig Each node in

 the tree is labeled by a grammar symbol An interior_node and its

 children correspond to a production the interior_node corresponds

 to the head of the production the children to the body



 figurehtfb



 Parse_tree for 9-52 according to the grammar in

 Example_digits-ex list-tree-fig

 figure



 In Fig list-tree-fig the root is labeled list the

 start_symbol of the grammar in Example_digits-ex The

 children of the root are labeled from left to right list

 and digit Note_that



 center

 list list digit

 center

 is a production in the grammar of Example_digits-ex The

 left_child of the root is similar to the root with a child

 labeled instead of The three nodes labeled digit each have one child that is labeled by a digit

 ex



 From left to right the leaves of a parse_tree form the yield of the tree which is the string generated or derived_from the nonterminal at the root of the parse_tree In

 Fig list-tree-fig the yield is 9-52 for

 convenience all the leaves are shown at the bottom level

 Henceforth we_shall not_necessarily line up the leaves in this

 way Any tree imparts a natural left-to-right_order to its leaves

 based_on the idea that if and are two children with the

 same parent and is to the left of then all descendants

 of are to the left of descendants of



 Another definition of the language generated_by a grammar is as

 the set of strings that can be generated_by some parse_tree The

 process of finding a parse_tree for a given string of terminals is

 called parsing that string



 Ambiguity

 ambig-subsect



 We have to be careful in talking_about the structure of a

 string according to a grammar

 A grammar can

 have more_than one parse_tree generating a given string of terminals

 Such a grammar is said to be ambiguous To show that a

 grammar is ambiguous all we need to do is find a terminal string

 that is the yield of more_than one parse_tree Since a string with more_than

 one parse_tree usually has more_than one meaning we need to

 design unambiguous_grammars for compiling applications or to use

 ambiguous_grammars with additional rules to resolve the

 ambiguities



 ex

 ambig-ex

 Suppose we did_not distinguish_between digits and

 lists as in Example_digits-ex We could have written the

 grammar

 center

 string string string string string 0_1 2_3 4_5 6_7 8_9

 center

 Merging the notion of digit and list into the

 nonterminal string makes superficial sense because a single

 digit is a special_case of a list



 However Fig ambig-list-fig shows that an expression like

 9-52 now has more_than one parse_tree The two trees for

 9-52 correspond to the two ways of parenthesizing the

 expression (9-5)2 and 9-(52) This second

 parenthesization gives the expression the unexpected value 2

 rather_than the customary value 6 The grammar of

 Example_digits-ex does_not permit this interpretation

 ex

 figurehtfb

 Two

 parse_trees for ambig-list-fig

 figure



 Associativity of Operators

 assoc-subsect



 By convention 952 is equivalent to (95)2 and 9-5-2 is equivalent to (9-5)-2 When an operand like 5 has operators to its left and right conventions are needed for

 deciding which operator applies to that operand We_say that the

 operator associates to the left because an operand

 with plus signs on both_sides of it belongs to the operator to

 its left In most programming_languages the four arithmetic

 operators addition subtraction multiplication and division are

 left-associative



 Some common operators such_as exponentiation are right-associative

 As_another example the assignment operator

 in Java is right-associative ie the expression abc is

 treated in the same way as the expression a(bc)



 Strings like abc with a right-associative operator are

 generated_by the following grammar

 center

 tabularr_c l

 right letter right letter



 letter



 tabular

 center



 The contrast between a parse_tree for a left-associative operator

 like - and a parse_tree for a right-associative operator

 like is shown by Fig assoc-fig Note_that the parse

 tree for 9-5-2 grows down towards the left whereas the

 parse_tree for abc grows down towards the right



 figurehtfb

 Parse

 trees for left- and right-associative grammars assoc-fig

 figure



 Precedence of Operators

 prec-subsect



 Consider the expression 952 There_are two possible

 interpretations of this expression (95)2 or 9(52) The associativity rules for and apply to

 occurrences of the same operator so they do_not resolve this

 ambiguity Rules defining the relative precedence of operators are

 needed when more_than one kind of operator is present



 We_say that has higher_precedence than if

 takes its operands before does In ordinary

 arithmetic multiplication and division have higher_precedence

 than addition and subtraction Therefore 5 is taken by in both 952 and 952 ie the expressions are

 equivalent to 9(52) and (95)2 respectively



 ex

 expr-gram-ex A grammar for arithmetic_expressions can be

 constructed from a table showing the associativity and precedence

 of operators We start with the four common arithmetic operators

 and a precedence table showing the operators in order of

 increasing precedence Operators on the same line have the same

 associativity and precedence



 center

 tabularl_l

 left-associative -



 left-associative

 tabular

 center



 We create two nonterminals expr and term for the two

 levels of precedence and an extra nonterminal factor for

 generating basic units in expressions The basic units in

 expressions are presently digits and parenthesized_expressions



 center

 tabularr_c l

 factor digit (_expr )

 tabular

 center



 Now_consider the binary operators and that have

 the highest precedence Since these operators associate to the

 left the productions are similar to those for lists that

 associate to the left



 center

 tabularr_c l

 term_term factor



 term_factor



 factor

 tabular

 center

 Similarly expr generates lists of terms separated_by the

 additive operators



 center

 tabularr_c l

 expr_expr term



 expr - term



 term

 tabular

 center

 The resulting grammar is therefore



 center

 tabularr_c l

 expr_expr term expr - term_term



 term_term factor term_factor factor



 factor digit (_expr )

 tabular

 center

 This grammar says_that an expression is a list of terms separated_by

 either or - signs and a term is a list of factors

 separated_by or signs Notice_that any

 parenthesized expression is a factor so with parentheses we can

 develop expressions that have arbitrarily deep nesting (and

 arbitrarily deep trees)

 ex



 Generalizing the Expression Grammar of

 Example expr-gram-ex

 We can think of a factor as an expression that cannot be torn_apart

 by any operator By torn_apart we mean that placing an operator

 next to any factor on either side does_not cause any piece of the

 factor other_than the whole to become an operand of that operator

 If the factor is a parenthesized expression the parentheses protect

 against such tearing while if the factor is a single operand it

 cannot be torn_apart



 A term (that is not also a factor)

 is an expression that can be torn_apart by operators of the highest

 precedence and but not by the lower-precedence

 operators An expression (that is not a term or factor) can be torn

 apart by any operator



 We can generalize this idea to any number of precedence levels We

 need nonterminals

 The first like factor in Example expr-gram-ex can never be

 torn_apart

 Typically the production_bodies for this nonterminal are only

 single operands and parenthesized_expressions

 Then for each precedence_level there is one nonterminal

 representing expressions that can be torn_apart only by operators at

 that level or higher

 Typically the productions for this nonterminal have bodies representing

 uses of the operators at that level plus one body that is just the

 nonterminal for the next higher level



 ex

 stmt-gram-ex Keywords allow_us to recognize statements

 since most statement begin_with a keyword or a special character

 Exceptions to this rule include assignments and procedure_calls

 The statements defined by the (ambiguous) grammar in

 Fig java-stat-fig are

 legal in Java



 figurehtfb



 center

 tabularr_c l

 stmt id expression



 if_( expression ) stmt



 if_( expression ) stmt_else stmt



 while_( expression ) stmt



 do stmt while_( expression )



 '' stmts ''







 stmts stmts stmt





 tabular

 center



 A grammar for a subset of Java statements

 java-stat-fig



 figure



 In the first production for stmt the

 token_id represents

 any identifier The assignment statements specified_by the first

 production are a subset of assignments in Java since Java treats

 as an assignment operator Thus Java allows abc

 but this grammar does_not The productions for expression

 are not shown



 The nonterminal stmts generates a possibly empty list of

 statements The second production for stmts generates the

 empty list The first production generates a possibly

 empty list of statements followed_by a statement



 There is a subtletly involving the placement of semicolons which appear

 at the end of every body that does_not end in stmt The_reason is

 that a semicolon can end any number of nested statements

 As a result any statement that consists of a statement_nested within it

 at the end such_as an if- or while-statement does_not need an ending

 semicolon That semicolon will be generated_by its substatement at the

 end

 ex

 1

 1

 1

 Intermediate_Code Generation

 syntree-sect



 The front_end of a compiler constructs an intermediate

 representation of the source_program from which the back end

 generates the target program In this_section we consider

 intermediate_representations for expressions and statements and learn

 by example how to produce such representations



 Two Kinds of Intermediate Representations



 As was suggested in Section overview-sect and especially

 Fig do-trans-fig the two most_important intermediate

 representations are



 enumerate



 Trees including parse_trees and (abstract) syntax_trees



 Linear representations especially three-address_code



 enumerate



 Abstract-syntax trees or simply syntax_trees

 were_introduced informally in

 Section absyn-subsect and in Section syn-tree-sect they

 will be reexamined more formally

 During parsing syntax-tree nodes

 are created to represent important programming_constructs As analysis

 proceeds information is added to the nodes in the form of

 attributes associated_with the nodes The choice of attributes

 depends_on the translation to be performed



 Three-address_code on the other_hand is a sequence of elementary

 program steps such_as the addition of two values

 Unlike the tree there is no

 hierarchical_structure As we_shall see in Chapter_code-op-ch we

 need this representation if we are to do any significant optimization of

 code In that case we break the long sequence of three-address

 statements that form a program into basic_blocks which are sequences

 of statements that are always executed one-after-the-other with no

 branching



 In_addition to creating an intermediate_representation a compiler

 front_end checks that the source_program follows the syntactic and

 semantic_rules of the source_language This checking is called

 static checking in general static means done by the

 compiler(Its opposite dynamic means while the program is

 running

 Many languages also make certain dynamic checks For_instance

 a typical object-oriented language like Java sometimes must check during program

 execution that a method applied to an object is appropriate for the

 particular subclass to which the object belongs)

 Static

 checking assures that certain kinds of programming_errors

 including type mismatches are detected and reported during

 compilation



 It is possible that a compiler will construct a syntax_tree at the same

 time it emits steps of three-address_code However it is common for

 compilers to emit the three-address_code while the parser goes_through

 the motions of constructing a syntax_tree without actually

 constructing the complete tree data_structure Rather the

 compiler stores nodes and

 their attributes needed for semantic checking or other purposes in the

 same data_structure used for the parser

 By so doing those parts of the syntax_tree that are needed to construct

 the three-address_code are available when needed but disappear when

 no_longer needed We take_up the details of this process in

 Chapter_sdt-ch



 Construction of Syntax_Trees

 ast-create-subsect



 We_shall first give a translation_scheme that constructs syntax_trees

 and later in Section 3code-subsect show_how the scheme can be

 modified to emit three-address_code along with or instead of the

 construction of the syntax_tree



 As we recall from Section absyn-subsect the syntax_tree



 center



 center

 represents an expression formed_by applying the operator op

 to the subexpressions represented_by and

 Syntax trees can be created for any construct not just

 expressions Each construct is represented_by a node with

 children for the semantically meaningful_components of the

 construct For_example the semantically meaningful_components of

 a C while-statement



 center

 while_( expr ) stmt

 center

 are the expression expr and the statement stmt(The

 right_parenthesis serves only to separate the expression from the

 statement The left_parenthesis actually has no meaning it is there

 only to please the eye since without it C would allow unbalanced

 parentheses)

 The

 syntax-tree_node for such a while-statement has an operator which we

 call while and two children-the syntax_trees for the expr

 and the stmt



 The translation_scheme in Fig_syntree-scheme-fig constructs

 syntax_trees for a representative but very limited

 language of expressions and

 statements All the nonterminals in the translation_scheme have an

 attribute n which is a node of the syntax

 tree Nodes are implemented as

 objects of class Node



 Class Node has two immediate subclasses Expr for all kinds

 of expressions and Stmt for all kinds of statements

 Each type of statement has a corresponding subclass of Stmt

 for example operator while corresponds to subclass

 While A syntax-tree_node for operator while with

 children and is created by the pseudocode



 center

 new

 center

 Objects of class While are created by calling constructor

 function While with the same name as the class Just as

 constructors correspond to operators constructor parameters

 correspond to operands in the abstract_syntax



 When we

 study the detailed code in Appendix together-sect we_shall see_how

 methods are placed where they belong in this hierarchy of classes

 In this_section we_shall discuss only a few of the methods

 informally



 figurehtfb



 center

 tabularr_c l_l

 program block print blockn







 block '' stmts '' blockn stmtsn







 new



 null







 new



 2lif (_)



 new



 2ldo while_( )



 new



 block blockn







 rel new



 rel reln







 rel add new



 add new



 add reln addn







 add new











 new











 (_)



 num new



 tabular

 center



 Construction of syntax_trees for expressions and

 statements

 syntree-scheme-fig



 figure



 We_shall consider each of the productions and rules of

 Fig_syntree-scheme-fig in turn

 First the productions defining different types of statements are

 explained followed_by the productions that define our limited types of

 expressions



 Syntax_Trees for Statements



 For each statement construct we define an operator in the

 abstract_syntax For constructs that begin_with a keyword we_shall

 use the keyword for the operator Thus there is an operator

 while for while-statements and an operator do for

 do-while statements Conditionals can be handled by defining two

 operators ifelse and if for conditionals with and

 without an else part respectively

 In our simple example language we do_not use else and so have only

 an if-statement

 Adding else presents some tricky parsing issues which we discuss

 in Section dang-else-subsect



 Each statement operator has a corresponding class of the same name with a

 capital first letter eg class If corresponds to if

 In_addition we define the subclass Seq which represents a

 sequence of statements This subclass corresponds to the nonterminal

 of the grammar

 Each of these classes are subclasses of Stmt which in turn is a

 subclass of Node



 The translation_scheme in Fig_syntree-scheme-fig

 illustrates the construction of syntax-tree nodes A_typical rule

 is the one for if-statements



 center

 tabularl_l

 if_( )

 new

 tabular

 center

 The meaningful_components of the if-statement are and

 The semantic_action defines the node as a new

 object of subclass If The code for the constructor If

 is not shown It creates a new node_labeled if with the

 nodes and as children



 Expression statements do_not begin_with a keyword so we define a

 new operator eval and class Eval which is a subclass of

 Stmt to represent expressions that are statements

 The relevant rule is



 center

 tabularl_l

 new



 tabular

 center



 Representing Blocks in Syntax_Trees



 The remaining statement construct in Fig_syntree-scheme-fig

 is the block consisting of a sequence of statements Consider the

 rules



 center

 tabularr_c l_l

 stmt block stmtn blockn



 block '' stmts ''

 blockn stmtsn

 tabular

 center

 The first says_that

 when a statement is a block it has the same syntax_tree as the

 block

 The second rule_says that

 the syntax_tree for nonterminal block is simply the syntax

 tree for the sequence of statements in the block



 For_simplicity

 the language in Fig_syntree-scheme-fig does_not include

 declarations Even when declarations are included in

 Appendix together-sect we_shall see that the syntax_tree for

 a block is still the syntax_tree for the statements in the block

 Since information from declarations is incorporated into the

 symbol_table they are not needed in the syntax_tree Blocks with

 or without declarations therefore appear to be just another

 statement construct in intermediate_code



 A sequence of statements is represented_by using a leaf null

 for an empty statement and a operator seq for a sequence of

 statements as in



 center

 tabularl_l

 new



 tabular

 center



 ex

 In Fig stmts-tree-fig we see part of a syntax_tree representing a block or

 statement list There_are two statements in the list the first an if-statement and

 the second a while-statement

 We do_not show the portion of the tree above this statement list and we show only

 as a triangle each of the necessary subtrees two expression trees for the

 conditions of the if- and while-statements and two statement trees for their

 substatements

 ex



 figurehtfb

 fileuullmanalsuch2figsstmts-treeeps

 Part of a syntax_tree for a statement list consisting of an if-statement and a

 while-statement

 stmts-tree-fig

 figure



 Syntax_Trees for Expressions



 Previously we handled the higher_precedence of

 over by using three nonterminals expr_term

 and factor The number of nonterminals increases with the

 the number of levels of precedence in expressions as we suggested in

 Section prec-subsect

 Here we have two comparison_operators and at one

 precedence_level as_well as the usual and operators so

 we need one additional nonterminal which we called add in

 Fig_syntree-scheme-fig



 Abstract syntax allows_us to group similar operators to reduce

 the number of cases and subclasses of nodes in an implementation of

 expressions In this_chapter we take similar to mean that the

 type-checking and code-generation rules for the operators are

 similar For_example typically the operators and can be

 grouped since they can be handled in the same way - their requirements

 regarding the types of operands

 are the same and they each result in a single three-address_instruction

 that applies one operator to two values In_general

 the grouping of operators in the abstract_syntax is based_on the

 needs of the later phases of the compiler

 The table in Fig op-syntax-fig specifies the correspondence

 between the concrete and abstract_syntax for several of the

 operators of Java



 figurehtfb

 center

 tabularc_c

 concrete syntax abstract_syntax



 assign



 cond



 cond



 rel



 rel



 - op



 op



 not



 minus



 access

 tabular

 center

 Concrete and abstract_syntax for expressions

 op-syntax-fig

 figure



 In the concrete syntax all operators are left_associative except

 the assignment operator which is right associative The

 operators on a line have the same precedence eg and

 have the same precedence The lines are in order of

 increasing precedence eg has higher_precedence than

 the operators and The subscript unary in

 is solely to distinguish a

 leading unary_minus sign as in -2 from a binary minus

 sign as in 2-a The operator represents array

 access as in ai



 The abstract-syntax column specifies the grouping of operators

 The assignment operator is in a group by itself The group

 cond contains the conditional boolean operators

 and The group rel contains the relational

 comparison_operators on the lines for and The

 group op contains the arithmetic operators like and

 Unary minus boolean negation and array_access are in

 groups by themselves



 The mapping between concrete and abstract_syntax in

 Fig op-syntax-fig can be_implemented by writing a

 translation_scheme The productions for nonterminals expr

 rel add term and factor in

 Fig_syntree-scheme-fig specify the concrete syntax for a

 representative subset of the operators in

 Fig op-syntax-fig The semantic_actions in these

 productions create syntax-tree nodes For_example the rule

 center

 tabularl_l



 new

 tabular

 center

 creates a node of class Op which implements the operators

 grouped under op in Fig op-syntax-fig The

 constructor Op has a parameter to identify the

 actual operator in addition to the nodes and

 for the subexpressions



 Static Checking

 checking-subsect



 Static checks are consistency checks that are done during

 compilation They not only assure that a program can be compiled

 successfully they have the potential for catching programming

 errors early before a program is run Static checking includes



 itemize

 Syntactic Checking There is more to syntax than

 grammars For_example we can tell by_looking at a program that an

 identifier is declared twice in a scope or that a break statement

 has no enclosing_loop or switch statement Such constraints are

 syntactic although they are not encoded in or enforced by a grammar used for

 parsing



 Type Checking The type rules of a language assure

 that an operator or function is applied to the right number and

 type of operands

 If conversion between types is necessary eg when an_integer is added

 to a real then the type-checker can insert an operator into the syntax

 tree to represent that conversion

 We discuss type conversion using the common term coercion

 below



 itemize



 We_now consider some simple static checks that can be done during

 the construction of a syntax_tree for a source_program In

 general complex static checks may need to be done by first

 constructing an intermediate_representation and then analyzing it



 L-values and R-values



 There is a distinction between the meaning of identifiers on the

 left and right_sides of an assignment In each of the assignments



 center

 tabularl

 i 5



 i i 1

 tabular

 center

 the right_side specifies an_integer value while the left_side

 specifies where the value is to be stored

 The terms l-value and r-value refer to values that are

 appropriate on the left and right_sides of an assignment

 respectively That is r-values are what we usually think of

 as values while l-values are locations



 Static checking must assure that the left_side of an assignment

 denotes an l-value An identifier like i has an l-value as does an array_access like a2 But a constant

 like 2 is not appropriate on the left_side of an assignment

 since it has an r-value but not an l-value



 Type Checking



 Type_checking assures that the type of a construct matches that

 expected by its context For_example in the conditional

 center

 if_( expr ) stmt

 center

 the expression expr is expected to have type boolean



 Type_checking rules follow the operatoroperand structure of the

 abstract_syntax For_example consider operator rel

 representing operators such

 as The type rule for operator group rel is

 that its two operands must have the same type and the result

 has type boolean Using attribute type for the type of an

 expression let consist of a rel applied to and

 The type of can be checked when its node is

 constructed by executing code like the following



 center

 tabularl

 if_( type type ) type boolean



 else error



 tabular

 center



 The idea of matching actual with expected types continues to

 apply even in the following situations



 itemize



 Coercions A coercion occurs if the type of an

 operand is automatically converted to the type expected by the

 operator In an expression like 2314 the usual

 transformation is to convert the integer 2 into an equivalent real

 number 20 and then perform a real operation on the resulting pair of

 real operands The language definition specifies the allowable

 coercions

 For_example the actual rule for rel discussed_above might be that

 type and type are convertible to the same type

 In that case it would be legal to compare say an_integer with a

 real



 Overloading The operator in Java represents

 addition when applied to integers it means concatenation when

 applied to strings A symbol is said to be overloaded if it

 has different meanings depending_on its context Thus is

 overloaded in Java The meaning of an overloaded operator

 is determined by considering the known types of its operands and

 results

 For_example we know that the in zxy is concatenation if

 we know that any of x_y or z is of type string

 However if we also know that another one of these is of type integer

 then we have a type error and there is no meaning to this use of





 itemize



 Three-Address Code

 3code-subsect



 Once syntax_trees are constructed further analysis and synthesis

 can be done by evaluating attributes and executing code fragments

 at nodes in the tree We illustrate the possibilities

 by using syntax_trees to

 generate three-address_code

 Specifically we show_how to write functions that process the syntax

 tree and as a side-effect emit the necessary three-address_code



 An Instruction Set

 Three-address_code is a sequence of instructions of the form



 center

 x_y op z

 center

 where x_y and z are names constants or

 compiler-generated temporaries and op stands_for an

 operator



 Arrays will be handled by using the following two variants of

 instructions



 center

 tabularl

 x_y z



 x_y z

 tabular

 center

 The first puts the value of in the location and the second

 puts the value of in the location



 Three-address instructions are executed in numerical sequence

 unless forced to do otherwise by a conditional or unconditional

 jump We choose the following instructions for control_flow



 center

 tabularl_l

 ifFalse_x goto_L if x is false next execute the instruction labeled L



 ifTrue x goto_L if x is true next execute the instruction labeled L



 goto_L next execute the instruction labeled L

 tabular

 center

 A label L can be attached to any instruction by a prefix

 L An instruction can have more_than one label



 Finally we need instructions that copy a value



 center

 x_y

 center

 copies the value of y into x



 Translation of Statements



 Statements are translated_into three-address_code by using jump

 instructions to implement the flow of control through the

 statement The layout in Fig if-layout-fig illustrates the

 translation of if then The jump

 instruction in the layout



 center

 ifFalse goto after

 center

 jumps over the translation of if evaluates to

 false

 Other statement constructs are similarly translated using

 appropriate jumps around the code for their components



 figurehtfb



 fileuullmanalsuch2figsif-layouteps

 Code

 layout for conditional_statements

 if-layout-fig



 figure



 For concreteness we show the pseudocode for class If in

 Fig if-pseudo-fig Class If is a subclass of Stmt as are the classes for the other statement constructs Each

 subclass of Stmt has a constructor - If in this case -

 and a function gen

 that is called to generate three-address_code for this kind of

 statement



 figurehtfb



 center

 tabularl

 class If extends_Stmt



 Expr Stmt



 public If(Expr Stmt ) after



 public_void gen()



 Expr rvalue()



 emit( ifFalse toString() goto after)



 gen()



 emit(after )









 tabular

 center

 Function gen in class If generates

 three-address_code

 if-pseudo-fig



 figure



 The constructor If in Fig if-pseudo-fig creates

 syntax-tree nodes for conditionals It is called with two

 parameters an expression node and a statement node which

 it saves as attributes and The constructor also assigns

 attribute after a unique new_label by calling function

 The label will be used according to the

 layout in Fig if-layout-fig



 Once the entire syntax_tree for a source_program is constructed

 the function gen is called at the root of the syntax_tree

 Since a program is a block in our simple language the root of the syntax

 tree represents the sequence of statements in the block All

 statement classes contain a function gen



 The pseudocode for function gen of class If in

 Fig if-pseudo-fig is representative It calls

 to translate the expression (the boolean-valued expression that is

 part of the if-statements) and saves the result

 node returned by Translation of expressions will be discussed

 shortly Function gen then emits a conditional_jump and

 calls gen() to translate the substatement



 Translation of Expressions



 To_begin we_shall take a simple approach of generating one

 three-address_instruction for each operator node in the syntax

 tree for an expression Thus the expression ii1 with the

 two operators and will be translated_into two

 instructions



 center

 tabularl

 t1 i 1



 i t1

 tabular

 center

 Here t1 is a compiler-generated_temporary name

 Compilers usually try very_hard to generate code that is as good or

 better than hand-written assembly code

 Thus we_shall see later in this_section

 how to generate the single instruction ii1 instead

 The simple approach uses the two functions lvalue and rvalue which appear in Fig lvalue-fig and rvalue-fig

 respectively

 We describe function lvalue first



 figurehtfb

 center

 tabularl





 if_( is an Id node )_return x



 else if_( is an node and is an Id node )



 return







 else error







 tabular

 center

 Pseudocode for function lvalue lvalue-fig

 figure



 When applied to

 a node function lvalue simply returns if it is the

 node for an_identifier (ie if is of class Id)

 The only other case in our simple language where

 an expression has an -value

 is if represents an array_access such_as ai(Our

 simple language supports only one-dimensional arrays Thus aan is supported but amn is not Note_that aan has the form a where is an)

 In this case will be of class Access a subclass of Expr

 Here the name of the array accessed and the offset (index) of the

 chosen element in that array are attributes of nodes in the class Access

 If node is of class Access then the condition



 center

 is an node and is an Id node

 center

 in Fig lvalue-fig will be satisfied

 The result returned which is an Access node has the -value of

 (the array element's offset) evaluated and retains the name for

 the array itself



 ex

 When node represents the array_access a2k the call

 generates an instruction



 center

 t 2 k

 center

 and returns a new node representing the l-value at where t is a new_temporary name



 In detail the code

 fragment



 center

 return

 center

 is reached with being the node for a and being the

 node for expression

 2k The call generates code for

 the expression 2k (ie the three-address statement t 2 k) and returns the new node representing

 the temporary_name t That node becomes the value of the

 second field in the new Access node created

 ex



 figurehtfb

 center

 tabularl





 if_( is an Id or a Constant node )_return x



 else if_( is an or a node )



 new_temporary



 emit string for



 return a new node for







 else if_( is an node )



 new_temporary



 call which returns



 emit string for



 return a new node for







 else if_( is an node )







 emit string for



 return









 tabular

 center

 Pseudocode for function rvalue rvalue-fig

 figure



 Function rvalue in Fig rvalue-fig generates

 instructions and returns a possibly new node When represents

 an_identifier or a constant rvalue returns itself In

 all other cases it returns an Id node for a new_temporary

 The cases are as_follows



 itemize



 When represents then the code first

 computes and It

 creates a new_temporary and generates an instruction

 (more precisely an instruction

 formed from the string representations of op and

 ) It returns a node for identifier



 When represents an array_access

 we can reuse function lvalue The call

 returns an access where represents an

 identifier holding the offset for the array_access

 The code creates a new_temporary and generates an

 instruction based_on It returns a

 node for



 When represents then the code first

 computes It generates an instruction

 based_on and returns the node

 itemize



 ex

 When applied to the syntax_tree for

 center

 ai 2aj-k

 center

 function rvalue generates

 center

 tabularl

 t1 j - k



 t2 a t1



 t3 2 t2



 a i t3

 tabular

 center



 That is the root is an Assign node with first argument ai

 and second argument 2aj-k Thus the third case applies and

 function rvalue evaluates the latter (second) argument

 The root of its subtree is the Op node for which causes

 the left_operand 2 to be evaluated and then the right operand The

 constant 2 generates no three-address_code and its -value is returned

 as a Constant node with value 2



 The right operand aj-k is an Access node which causes

 function lvalue to be called on this node Recursively rvalue is called on the expression j-k As a side-effect

 of this call the

 three-address statement t1 j - k is generated after the new

 temporary t1 is created

 Then returning to the call of lvalue on aj-k a new

 temporary t2 is created and it is assigned the -value of the

 entire access-expression that is t2 a t1



 Now we return to the call of rvalue on the Op node 2aj-k A new_temporary t3 is created and a three-address

 statement t3 2 t2 is generated as a side-effect to evaluate

 this multiplication-expression Last the call to rvalue on the

 whole expression completes by calling lvalue on the left_side ai and then generating a three-address_instruction a i t3 in which the right_side of the assignment is assigned to

 the left_side

 ex



 Better Code for Expressions



 We can improve on function rvalue in Fig rvalue-fig

 and generate fewer three-address_instructions in several

 ways



 itemize



 Reduce the number of move instructions in a subsequent

 optimization_phase For_example the pair of instructions t i1 and i t can be_combined

 into i_i1 if there are no_subsequent uses of

 t



 Generate fewer instructions in the first place by_taking

 context into_account For_example if the left_side of a three-address

 assignment is an array_access at then the right_side must_be a

 name a constant or a temporary all of which use just one

 address But if the left_side is a name x then the right

 side can be an operation y op z that uses two

 addresses



 itemize



 The rest of this_section outlines an approach to generating fewer

 instructions The key idea is that the intermediate_code for the

 right_side of an assignment depends_on whether the left_side is an

 identifier or an array_access The generation of three-address

 code from syntax_trees is a preview of machine code_generation

 where a single complex instruction can cover not just one

 node but a portion of a syntax_tree see Chapter oldCh9



 The generation of three-address_code using function rvalue

 can be thought of as tree rewriting Suppose node represents

 From Fig rvalue-fig the call

 proceeds as_follows



 center

 tabularl

 new_temporary



 emit string for



 return a new node for

 tabular

 center

 This code can be thought of as rewriting the subtree at to a

 leaf representing the temporary as in

 Fig prune-fig



 figurehtfb



 fileuullmanalsuch2figspruneeps

 Cases

 during the generation of three-address_code

 prune-fig



 figure



 Now_consider a function prune also illustrated in

 Fig prune-fig which rewrites the operands in

 to leaves and returns a tree with a single

 operator With again representing the call

 proceeds as_follows

 center

 tabularl





 return_new

 tabular

 center



 The translation of assignments uses both prune and rvalue Suppose an assignment node has children and

 Then three-address_instructions can be generated using the

 following pseudocode

 center

 tabularl

 the left_side of the three-address assignment implements



 if_( node is for an_identifier )



 the right_side implements



 else if_( node is for an array_access )



 the right_side implements

 tabular

 center



 The translation of the program_fragment in

 Fig quick-src-fig into the three-address_code in

 Fig quick-quad-fig was done using functions like prune and rvalue

 Variants of Syntax_Trees

 syn-tree-sect



 Nodes in a syntax_tree represent constructs in the source_program

 the children of a node represent the meaningful_components of a

 construct A directed acyclic_graph (hereafter called a DAG) for an expression identifies the common

 subexpressions (subexpressions that occur more_than once) of the

 expression As we_shall see in this_section DAG's can be

 constructed by using the same techniques that construct syntax

 trees



 Directed Acyclic Graphs for Expressions

 dag-subsect



 Like the syntax_tree for an expression a DAG has leaves

 corresponding to atomic operands and interior_nodes corresponding

 to operators The difference is that a node in a DAG has

 more_than one parent if represents a common_subexpression in

 a syntax_tree the tree for the common_subexpression would be

 replicated as many_times as the subexpression appears in the original

 expression Thus a DAG not only represents expressions more

 succinctly it gives the compiler important clues regarding the

 generation of efficient code to evaluate the expressions



 ex

 exp-dag-ex Figure exp-dag-fig shows the DAG for

 the expression



 center

 a a (b - c) (b - c) d

 center

 The leaf for a has two parents because a appears

 twice in the expression More interestingly the two_occurrences

 of the common_subexpression b-c are represented_by one node

 the node_labeled That node has two parents representing its

 two uses in the subexpressions a(b-c) and (b-c)d

 Even_though b and c appear twice in the complete

 expression their nodes each have one parent since both uses are

 in the common_subexpression b-c

 ex



 figurehtfb



 DAG

 for the expression aa(b-c)(b-c)d exp-dag-fig

 figure



 An like that of Fig expr-ast-fig can construct either syntax

 trees or DAG's A similar was used to construct syntax_trees in

 Example syn-tree-ex where functions Leaf and Node created a fresh node each time they_were called It will

 construct a DAG if before creating a new node these functions

 first check_whether an identical node already exists If a

 previously created identical node exists the existing node is

 returned For_instance before constructing a new node

 we check

 whether there is already a node with label op and children

 left and right in that order If so Node

 returns the existing node otherwise it creates a new node



 height 9pt depth 4pt width 0pt

 figurehtfb

 center

 tabularl_ll

 Production_Semantic Rules



 1)





 2)





 3)





 4)



 5)



 6)





 7)





 tabular

 center

 Syntax-directed definition to produce syntax_trees or

 DAG'sexpr-ast-fig

 figure





 ex

 exp-dag-steps-ex The sequence of steps shown in

 Fig exp-dag-steps-fig constructs the DAG in

 Fig exp-dag-fig provided Node and Leaf

 return an existing node if possible as discussed_above We

 assume that entry-a points to the symbol-table_entry for

 a and similarly for the other identifiers



 figurehtfb



 center

 tabularr_l

 1)



 2)



 3)



 4)



 5)



 6)



 7)



 8)



 9)



 10)



 11)



 12)



 13)



 tabular

 center



 Steps for constructing the DAG of

 Fig exp-dag-fig exp-dag-steps-fig



 figure



 When the call to

 is repeated at step 2 the node created by the previous

 call is returned so Similarly the nodes returned at

 steps 8 and 9 are the same as those returned at steps 3 and 4

 (ie and ) Hence the node returned at

 step 10 must_be the same at that returned at step 5 ie



 ex





 The Value-Number Method for Constructing DAG's

 val-num-subsect



 Often the nodes of a syntax_tree or DAG are stored in an array

 of records as suggested by Fig val-num-fig Each row of

 the array represents one record and therefore one node In each

 record the first field is an operation code indicating the label

 of the node In Fig val-num-fig(b) leaves have one

 additional field which holds the lexical value (either a

 symbol-table pointer or a constant in this case) and interior

 nodes have two additional fields indicating the left and right

 children



 figurehtfb



 Nodes

 of a DAG for allocated in an array

 val-num-fig

 figure



 In this array we refer to nodes by giving the integer index

 of the record for that node within the array This integer

 historically has_been called the value number for the node

 or for the expression represented_by the node For_instance in

 Fig val-num-fig the node_labeled has value number 3

 and its left and right children have value numbers 1 and 2

 respectively In_practice we could use pointers to records or

 references to objects instead of integer indexes but we_shall

 still refer to the reference to a node as its value number If

 stored in an appropriate data_structure value numbers help us

 construct expression DAG's efficiently the next algorithm shows

 how



 Suppose that nodes are stored in an array as in

 Fig val-num-fig and each node is referred to by its value

 number Let the signature of an interior_node be the triple

 where is the label

 its left child's value number and its right child's value

 number A unary operator may be assumed to have



 alg

 val-num-alg The value-number method for constructing

 the nodes of a DAG



 Label node and node



 The value number of a node

 in the array with signature



 Search the array for a node with label

 left_child and right_child If there is such a node

 return the value number of If not create

 in the array a new node

 with label left_child and right_child

 and return its value number

 alg



 While Algorithm val-num-alg

 yields the desired output searching the entire array

 every time we are asked to locate one node is expensive

 especially if the array holds expressions from an entire program

 A more_efficient approach is to use a hash_table in which the

 nodes are put into buckets each of which typically will

 have only a few nodes The hash_table is one of several data

 structures that support dictionaries

 efficientlyfootnoteSee Aho_A V J E Hopcroft and

 J_D Ullman Data Structures and Algorithms Addison-Wesley

 1983 for a discussion of data_structures supporting

 dictionariesfootnote A dictionary

 is an abstract data type that allows_us to insert and

 delete elements of a set and to determine_whether a given element is

 currently in the set A good data_structure for dictionaries such

 as a hash_table performs each of these operations in time that is

 constant or close to constant independent of the size of the set



 To construct a hash_table for the nodes of a DAG

 we need a hash function

 that computes the index of the bucket for a signature

 in a way that distributes the

 signatures across buckets so that it is unlikely that any one

 bucket will get much more_than a fair share of the nodes The

 bucket index is computed deterministically

 from and so that we may repeat the

 calculation and always get to the same bucket index for node





 The buckets can be_implemented as linked_lists as in

 Fig val-num-hash-fig An array indexed by hash value

 holds the bucket headers each of which points to the first

 cell of a list Within the linked_list for a bucket each cell

 holds the value number of one of the nodes that hash to that

 bucket That is node can be

 found on the list whose header is at index of

 the array



 figurehtfb





 Data structure for searching buckets

 val-num-hash-fig

 figure



 Thus given the input node and we compute

 the bucket index and search the list

 of cells in this bucket for the given input node

 Typically there are enough buckets so that no list has more_than

 a few cells We may need to look_at all the cells within

 a bucket however and

 for each value number found in a cell we must check_whether

 the signature of the input node

 matches the node

 with value number in the list of cells (as in

 Fig val-num-hash-fig) If we find a match we return

 If we find no match

 we know no such node can exist in any other bucket so we

 create a new cell add it to the list of cells for bucket

 index and return the

 value number in that new cell



 exer

 Construct the DAG for the expression



 exer



 exer

 Construct the DAG and identify the value numbers for the

 subexpressions of the following expressions assuming associates

 from the left



 itemize



 a)



 b)



 c)



 itemize

 exer

 The Target Language

 target-sect



 Familiarity with the target_machine and its instruction set is a

 prerequisite for designing a good code_generator Unfortunately

 in a general discussion of code_generation it is not possible to

 describe any target_machine in sufficient detail to generate good

 code for a complete language on that machine In this_chapter we

 shall use as a target language assembly code for a simple computer

 that is representative of many register machines However the

 code-generation techniques presented in this_chapter can be used

 on many other classes of machines as_well



 A Simple Target Machine Model



 Our target computer models a three-address machine with load and

 store operations computation operations jump operations and

 conditional jumps The underlying computer is a byte-addressable

 machine with general-purpose registers

 A full-fledged assembly

 language would have scores of instructions To_avoid hiding the

 concepts in a myriad of details we_shall use a very limited set

 of instructions and assume that all operands are integers Most

 instructions consists of an operator followed_by a target

 followed_by a list of source operands A label may precede an

 instruction We assume the following kinds of instructions are

 available



 itemize



 Load operations The instruction_LD dst

 addr loads the value in location addr into location dst This instruction denotes the assignment

 The most_common form of this instruction is LD which loads the value in location into register

 An instruction of the form LD is a register-to-register copy in which the contents of register

 are copied_into register



 Store operations The instruction ST

 stores the value in register into the location This

 instruction denotes the assignment



 Computation operations of the form

 where

 is a operator like ADD or SUB and

 and are locations

 not_necessarily distinct The effect of this machine_instruction

 is to apply the operation represented_by to the values

 in locations and and place the

 result of this operation in location For_example

 computes Any value

 formerly stored in is lost but if is or

 the old value is read first Unary operators that take only one

 operand do_not have a



 Unconditional jumps The instruction BR

 causes control to branch to the machine_instruction with label

 (BR stands_for branch) Conditional

 jumps of the form where is a

 register is a label and stands_for any of the

 common tests on values in the register For_example

 causes a jump to label if the value in

 register is less_than zero and allows control to pass to the

 next machine_instruction if not



 itemize



 We assume our target_machine has a variety of addressing_modes



 itemize



 In instructions a location can be a variable name

 referring to the memory_location that is reserved for (that

 is the -value of )



 A location can also be an indexed address of the form

 where is a variable and is a register The memory

 location denoted_by is computed by_taking the -value of

 and adding to it the value in register For_example the

 instruction_LD R1a(R2) has the effect of setting

 where

 denotes the contents of the register or

 memory_location represented_by This addressing mode is useful

 for accessing arrays where is the base_address of the array

 (that is the address of the first element) and holds the

 number of bytes past that address we_wish to go to reach one of

 the elements of array



 A memory_location can be an_integer indexed by a register

 For_example LD R1100(R2) has the effect of setting

 that is of

 loading into the value in the memory_location obtained

 by_adding 100 to the contents of register This

 feature is useful for following pointers as we_shall see in the

 example below



 We also allow two indirect addressing_modes

 means the memory_location found in the location represented_by the

 contents of register and means the

 memory_location found in the location obtained_by adding 100 to

 the contents of For_example LD R1100(R2) has the

 effect of setting

 that is of loading into the

 value in the memory_location stored in the memory_location

 obtained_by adding 100 to the contents of register



 Finally we allow an immediate constant addressing mode The

 constant is prefixed by The instruction_LD

 R1100 loads the integer 100 into register_R1 and ADD R1R1100 adds the integer 100 into register_R1



 itemize



 Comments at the end of instructions are preceded by



 ex

 The three-address statement x y-z can be

 implemented_by the machine_instructions



 verbatim

 LD_R1 y R1 y

 LD_R2 z R2 z

 SUB R1_R1 R2_R1 R1 - R2

 ST x R1 x R1

 verbatim

 We can do better perhaps One of the goals of a good

 code-generation algorithm is to avoid using all four of these

 instructions whenever possible For_example y andor z may have_been computed in a register and if so we can avoid

 the LD step(s) Likewise we might be_able to avoid ever

 storing x if its value is used within the register set and

 is not_subsequently needed



 Suppose a is an array whose elements are 8-byte values

 perhaps real numbers Also assume elements of a are indexed

 starting_at 0 We may execute the three-address_instruction b ai by the machine_instructions



 verbatim

 LD_R1 i R1 i

 MUL R1_R1 8 R1_R1 8

 LD_R2 a(R1) R2 contents(a contents(R1))

 ST b R2 b R2

 verbatim

 That is the second step computes and the third_step places

 in register R2 the value in the th_element of a -

 the one found in the location that is bytes past the base

 address of the array a



 Similarly the assignment into the array a represented_by

 three-address_instruction aj c is implemented_by



 verbatim

 LD_R1 c R1 c

 LD_R2 j R2 j

 MUL R2 R2 8 R2 R2 8

 ST a(R2) R1 contents(a contents(R2)) R1

 verbatim



 To implement a simple pointer indirection such_as the three-address

 statement x p we can use machine_instructions like



 verbatim

 LD_R1 p R1 p

 LD_R2 0(R1) R2 contents(0 contents(R1))

 ST x R2 x R2

 verbatim

 The assignment through a pointer p y is similarly implemented

 in machine code by



 verbatim

 LD_R1 p R1 p

 LD_R2 y R2 y

 ST 0(R1) R2 contents(0 contents(R1)) R2

 verbatim



 Finally consider a conditional-jump three-address_instruction like



 verbatim

 if x_y goto_L

 verbatim

 The machine-code equivalent would be something_like

 verbatim

 LD_R1 x R1 x

 LD_R2 y R2 y

 SUB R1_R1 R2_R1 R1 - R2

 BLTZ R1 M if R1 0 jump to M

 verbatim

 Here M is the label that represents the first machine

 instruction generated from the three-address_instruction that has

 label L As for any three-address_instruction we hope that

 we can save some of these machine_instructions because the needed

 operands are already in registers or because the result need never

 be stored

 ex



 Program and Instruction Costs



 We often associate a cost with compiling and running a program

 Depending on what aspect of a program we are_interested in optimizing

 some common cost measures are the length of compilation time and

 the size running_time and power consumption of the target program



 Determining the actual cost of compiling and running a program is

 a complex problem Finding an optimal target program for a given

 source_program is an undecidable problem in general and many of

 the subproblems involved are NP-hard As we have indicated in

 code_generation we must often be content with heuristic techniques

 that produce good but not_necessarily optimal target programs



 For the remainder of this_chapter we_shall assume each

 target-language instruction has an associated cost For

 simplicity we take the cost of an instruction to be one plus the

 costs associated_with the addressing_modes of the operands This

 cost corresponds to the length in words of the instruction

 Addressing modes involving registers have zero additional cost

 while those involving a memory_location or constant in them have

 an additional cost of one because such operands have to be stored

 in the words following the instruction Some examples



 itemize



 The instruction_LD R0_R1 copies the contents of

 register_R1 into register R0 This instruction has a

 cost of one because no additional memory words are required



 The instruction_LD R0 M loads the contents of memory

 location M into register R0 The cost is two since the

 address of memory_location M is in the word following the

 instruction The instruction_LD R1 100(R2) loads

 into register_R1 the value given by



 The cost is two because the

 constant 100 is stored in the word following the instruction



 itemize



 In this_chapter we assume the cost of a target-language program on

 a given input is the sum of costs of the individual instructions

 executed when the program is run on that input Good

 code-generation algorithms seek to minimize the sum of the costs

 of the instructions executed by the generated target program on

 typical inputs We_shall see that in some situations we can

 actually generate_optimal code for expressions on certain classes

 of register machines



 exer

 Generate_code for the following three-address_statements

 assuming all variables are stored in memory_locations



 itemize



 a) x 1

 b) x a

 c) x a 1

 d) x a b

 e) The two statements



 verbatim

 x b_c

 y a x

 verbatim

 itemize

 exer



 exer

 Generate_code for the following three-address_statements

 assuming and are arrays whose elements are

 4-byte values



 itemize



 a) The four-statement sequence



 verbatim

 x_ai

 y bj

 ai_y

 bj x

 verbatim



 b) The three-statement sequence



 verbatim

 x_ai

 y bi

 z x_y

 verbatim



 c) The three-statement sequence



 verbatim

 x_ai

 y bx

 ai_y

 verbatim

 itemize

 exer



 exer

 Generate_code for the following three-address sequence assuming

 that p and q are in memory_locations



 verbatim

 y q

 q q 4

 p y

 p_p 4

 verbatim



 exer





 exer

 Generate_code for the following sequence assuming that x

 y and z are in memory_locations

 verbatim

 if x_y goto L1

 z 0

 goto_L2

 L1 z 1

 verbatim

 exer



 exer

 Generate_code for the following sequence assuming that n is

 in a memory_location

 verbatim

 s 0

 i 0

 L1 if i_n goto_L2

 s s i

 i i 1

 goto L1

 L2

 verbatim

 exer



 exer

 Determine the costs of the following instruction sequences



 itemize



 a)

 verbatim

 LD_R0 y

 LD_R1 z

 ADD R0_R0 R1

 ST x R0

 verbatim



 b)

 verbatim

 LD_R0 i

 MUL R0_R0 8

 LD_R1 a(R0)

 ST b R1

 verbatim



 c)

 verbatim

 LD_R0 c

 LD_R1 i

 MUL R1_R1 8

 ST a(R1) R0

 verbatim



 d)

 verbatim

 LD_R0 p

 LD_R1 0(R0)

 ST x R1

 verbatim



 e)

 verbatim

 LD_R0 p

 LD_R1 x

 ST 0(R0) R1

 verbatim



 f)

 verbatim

 LD_R0 x

 LD_R1 y

 SUB R0_R0 R1

 BLTZ R3 R0

 verbatim

 itemize

 exer

 Top-Down Parsing

 top-down-sect



 Top-down parsing can be_viewed as the problem of constructing a

 parse_tree for the input_string starting from the root and

 creating the nodes of the parse_tree in preorder (depth-first as

 discussed in Section dfs-subsect) Equivalently top-down

 parsing can be_viewed as finding a leftmost_derivation for an

 input_string



 figurehtfb

 center



 Top-down parse for

 td-seq-fig

 center

 figure



 ex

 td-seq-ex The sequence of parse_trees in

 Fig td-seq-fig for the input

 is a top-down parse according to grammar

 (topdown-expr-display) repeated_here

 displaytd-expr-gram

 317ptstabularl l_l





















 tabular

 (td-expr-gram)

 display

 This sequence of trees corresponds to a leftmost_derivation of the

 input

 ex



 At each step of a top-down parse the key problem is that of

 determining the production to be applied for a nonterminal say

 Once an -production is chosen the rest of the parsing

 process consists of matching the terminal_symbols in the

 production_body with the input_string



 The section begins_with a general form of top-down_parsing called

 recursive-descent_parsing which may require backtracking to find

 the correct -production to be applied

 Section predictive-sect introduced predictive_parsing a

 special_case of recursive-descent_parsing where no backtracking

 is required Predictive parsing chooses the correct -production

 by_looking ahead at the input a fixed_number of symbols typically

 we may look only at one (that is the next_input symbol)



 For_example consider the top-down parse in Fig td-seq-fig

 which constructs a tree with two nodes labeled At the first

 node (in preorder) the production is

 chosen at the second node the production

 is chosen A predictive_parser can choose between

 -productions by_looking at the next_input symbol



 The class of grammars for which we can construct predictive

 parsers looking symbols ahead in the input is sometimes_called

 the class We discuss the LL(1) class in

 Section ll-1-subsect but introduce certain computations

 called and in a preliminary

 Section first-follow-subsect From the and

 sets for a grammar we_shall construct predictive_parsing

 tables which make explicit the choice of production during

 top-down_parsing These sets are also useful during bottom-up

 parsing as we_shall see



 In Section nonrec-pp-subsect we give a nonrecursive parsing

 algorithm that maintains a stack explicitly rather_than

 implicitly via recursive_calls Finally in

 Section ll-err-subsect we discuss error_recovery during

 top-down_parsing



 Recursive-Descent Parsing

 rec-desc-subsect



 figurehtfb

 center

 tabularl_l

 void



 1) Choose an -production



 2) for ( to )



 3) if_( is a nonterminal )



 4) call procedure



 5) else if_( equals the current_input symbol )



 6) advance the input to the next symbol



 7) else an error has_occurred











 tabular

 center

 A_typical procedure for a nonterminal in a top-down

 parser td-parse-pseudo-fig

 figure



 A recursive-descent_parsing program consists of a set of

 procedures one for each nonterminal Execution begins_with the

 procedure for the start_symbol which halts and announces success

 if its procedure body scans the entire input_string Pseudocode

 for a typical nonterminal appears in

 Fig td-parse-pseudo-fig Note_that this pseudocode is

 nondeterministic since it begins by choosing the -production

 to apply in a manner that is not specified



 General recursive-descent may require backtracking that is it

 may require repeated scans over the input However backtracking

 is rarely needed to parse programming_language constructs so

 backtracking parsers are not seen frequently Even for situations

 like natural language parsing backtracking is not very efficient

 and tabular methods such_as the dynamic_programming algorithm of

 Exercise cyk-exer or the method of Earley (see the bibliographic

 notes) are preferred



 To allow backtracking the code of Fig td-parse-pseudo-fig

 needs to be modified First we cannot choose a unique

 -production at line_(1) so we must try each of several

 productions in some order Then failure at line_(7) is not

 ultimate failure but suggests only that we need to return to

 line_(1) and try another -production Only if there are no

 more -productions to try do we declare that an input error has

 been_found In order to try another -production we need to be

 able to reset the input pointer to where it was when we first

 reached line_(1) Thus a local variable is needed to store this

 input pointer for future use



 ex

 Consider the grammar

 center

 tabularl_l l









 tabular

 center

 To construct a parse_tree top-down for the input_string

 begin_with a tree consisting of a single_node labeled and the

 input pointer pointing to the first symbol of has

 only one production so we use it to expand and obtain the

 tree of Fig backtrack-fig(a) The leftmost leaf_labeled

 matches the first symbol of input so we advance the input

 pointer to the second symbol of and consider the next

 leaf_labeled



 figurehtfb

 center



 Steps in a top-down parse backtrack-fig

 center

 figure



 Now we expand using the first alternative to

 obtain the tree of Fig backtrack-fig(b) We have a match

 for the second input_symbol so we advance the input pointer to

 the third input_symbol and compare against the next

 leaf_labeled Since does_not match we report

 failure and go back to to see whether there is another

 alternative for that has not been tried but that might

 produce a match



 In going back to we must reset the input pointer to position

 2 the position it had when we first came to which means that

 the procedure for must store the input pointer in a local

 variable



 The second alternative for produces the tree of

 Fig backtrack-fig(c) The leaf matches the second

 symbol of and the leaf matches the third symbol Since we

 have produced a parse_tree for we halt and announce

 successful_completion of parsing

 ex



 A left-recursive grammar can cause a recursive-descent_parser

 even one with backtracking to go into an_infinite loop That is

 when we try to expand a nonterminal we may eventually find

 ourselves again trying to expand without_having consumed any

 input



 FIRST and FOLLOW

 first-follow-subsect



 The construction of both top-down and bottom-up parsers is aided

 by two functions and associated_with a grammar

 During top-down_parsing and allow_us to

 choose which production to apply based_on the next_input symbol

 During panic-mode error_recovery sets of tokens produced_by

 can be used as synchronizing_tokens



 Define () where is any string of

 grammar_symbols to be the set of terminals that begin strings

 derived_from If then

 is also in () For_example in

 Fig first-follow-fig so is

 in ()



 figurehtfb

 center



 Terminal is in and is in

 first-follow-fig

 center

 figure



 For a preview of how can be used during predictive

 parsing consider two -productions

 where () and () are disjoint

 sets We can then choose between these -productions by_looking

 at the next_input symbol since can be in at most one of

 () and () not both For_instance if

 is in () choose the production

 This idea will be explored when LL(1)_grammars are

 defined in Section ll-1-subsect



 Define () for nonterminal to be the set of

 terminals that can appear immediately to the right of in

 some sentential_form that is the set of terminals such that

 there_exists a derivation of the form

 for some and as in Fig first-follow-fig

 Note_that there may have_been symbols between and at some

 time during the derivation but if so they derived and

 disappeared In_addition if can be the rightmost symbol in

 some sentential_form then is in () recall that

 is a special endmarker symbol that is assumed not to be a

 symbol of any grammar



 To_compute () for all grammar_symbols apply the

 following rules until_no more terminals or can be added

 to any set



 enumerate



 If is a terminal then



 If is a nonterminal and

 is a production for some then place in () if

 for some is in () and is in all of

 that is

 If is in () for all

 then add to () For

 example everything in () is surely in () If

 does_not derive then we add nothing more to

 () but if then we add

 () and so on



 If is a production then add

 to ()



 enumerate



 Now we can compute for any string

 as_follows Add to () all non-

 symbols of () Also add the non-_symbols of

 () if is in () the

 non-_symbols of () if is in

 () and () and so on Finally add

 to () if for all

 is in ()



 To_compute () for all nonterminals apply the

 following rules until nothing can be added to any set

 enumerate

 Place in () where is the start_symbol and is

 the input right_endmarker

 If there is a production then

 everything in () except is in

 ()

 If there is a production or a production

 where () contains

 then everything in () is in ()

 enumerate



 ex

 first-follow-ex Consider_again the non-left-recursive

 grammar (td-expr-gram) Then



 enumerate





 To_see why note_that the two productions for have bodies that

 start with these two terminal_symbols and the left

 parenthesis has only one production and its body starts with

 Since does_not derive must_be the

 same as The same argument covers



 The_reason is

 that one of the two productions for has a body that begins

 with terminal and the other's body is Whenever a

 nonterminal derives we place in for

 that nonterminal



 The reasoning

 is analogous to that for



 Since

 is the start_symbol must contain The production

 body explains why the right_parenthesis is in

 For note_that this nonterminal appears only at

 the ends of bodies of -productions Thus must_be

 the same as



 Notice

 that appears in bodies only followed_by Thus everything

 except that is in must_be in

 that explains the symbol However since contains

 (ie ) and is the entire

 string following in the bodies of the -productions

 everything in must also be in That

 explains the symbols and the right_parenthesis As for

 since it appears only at the ends of the -productions it must

 be that



 The reasoning is

 analogous to that for in point_(5)



 enumerate

 ex



 LL(1) Grammars

 ll-1-subsect





 Transition Diagrams for Predictive Parsers

 Transition_diagrams are useful for visualizing predictive_parsers

 For_example the transition_diagrams for nonterminals and

 of grammar (td-expr-gram) appear in

 Fig trans-dia-fig(a) To construct the transition_diagram

 from a grammar first eliminate_left recursion and then left

 factor the grammar Then for each nonterminal

 enumerate



 Create an initial and final (return) state



 For each production create a

 path from the initial to the final state with edges labeled

 If the path is an

 edge labeled

 enumerate



 Transition_diagrams for predictive_parsers differ from those for

 lexical_analyzers Parsers have one diagram for each nonterminal

 The labels of edges can be tokens or nonterminals A transition

 on a token (terminal) means that we take that transition if that

 token is the next_input symbol A transition on a nonterminal

 is a call of the procedure for



 With an LL(1)_grammar the ambiguity of whether or not to take an

 -edge can be_resolved by making -transitions

 the default choice



 Transition_diagrams can be simplified provided the sequence of

 grammar_symbols along paths is preserved We may also substitute

 the diagram for a nonterminal in place of an edge labeled

 The diagrams in Fig trans-dia-fig(a) and (b) are

 equivalent if we trace paths from to an accepting_state and

 substitute for then in both sets of diagrams the grammar

 symbols along the paths make up strings of the form

 The diagram in (b) can be obtained from (a) by

 transformations akin to those in

 Section leftrec-elim-subsect where we used tail-recursion

 removal and substitution of procedure bodies to optimize the

 procedure for a nonterminal



 figurehtfb

 center



 Transition_diagrams for nonterminals and of

 grammar td-expr-gram trans-dia-fig

 center

 figure





 Predictive parsers that is recursive-descent parsers needing

 no backtracking can be constructed for a class of grammars called

 LL(1) The first L in LL(1) stands_for scanning the input from

 left to right the second L for producing a leftmost

 derivation and the 1 for using one input_symbol of lookahead

 at each step to make parsing_action decisions



 The class of LL(1)_grammars is rich enough to cover most

 programming_constructs although care is needed in writing a

 suitable grammar for the source_language For_example no

 left-recursive or ambiguous_grammar can be LL(1)



 A grammar is LL(1) if and only if whenever

 are two distinct productions of the following

 conditions hold



 enumerate



 For no terminal do both and derive strings

 beginning with



 At most one of and can derive the empty_string



 If then does_not derive

 any string beginning with a terminal in ()

 Likewise if then does_not derive any

 string beginning with a terminal in



 enumerate

 The first two conditions are equivalent to the statement that

 () and () are disjoint_sets The third

 condition is equivalent to stating that if is in

 () then () and () are

 disjoint_sets and likewise if is in



 Predictive parsers can be constructed for LL(1)_grammars since the

 proper production to apply for a nonterminal can be selected by

 looking only at the current_input symbol Flow-of-control

 constructs with their distinguishing keywords generally satisfy

 the LL(1) constraints For_instance if we have the productions



 center

 tabularl_c l

 stmt if_( expr ) stmt_else stmt



 while_( expr ) stmt



 123 stmtlist 125

 tabular

 center

 then the keywords if while and the symbol 123 tell_us which alternative is the only one that could

 possibly succeed if we are to find a statement



 The next algorithm collects the information from and

 sets into a predictive_parsing table a

 two-dimensional_array where is a nonterminal and is a

 terminal or the symbol the input endmarker The algorithm is

 based_on the following idea the production is

 chosen if the next_input symbol is in The

 only complication occurs_when or_more

 generally In this case we should again

 choose if the current_input symbol is in

 or if the on the input has_been reached and

 is in



 algpred-table-alg Construction of a predictive

 parsing_table



 Grammar



 Parsing_table



 For each production of the grammar do

 the following



 enumerate



 For each terminal in add

 to



 If is in then for each terminal

 in add to If

 is in and is in add

 to as_well



 enumerate

 If after performing the above there is no production at all in

 then set to error (which we normally

 represent by an empty entry in the table)

 alg



 exexpr-pred-parsing-ex

 For the expression grammar (td-expr-gram)

 Algorithm pred-table-alg produces the parsing_table in

 Fig expr-pred-parsing-fig Blanks are error_entries

 nonblanks indicate a production with which to expand a

 nonterminal



 figurehtfb

 center

 small

 tabularcc_c c_c c_c

 NON- 6c0pt6ptINPUT SYMBOL



 2-7

 TERMINAL 0pt6 ptid























 tabular

 small

 center

 Parsing_table for Example

 expr-pred-parsing-exexpr-pred-parsing-fig

 figure



 Consider production Since

 center



 center

 this production is added to and Production

 is added to since

 Since production

 is added to and

 ex



 Algorithm pred-table-alg can be applied to any grammar

 to produce a parsing_table For every LL(1)_grammar each

 parsing-table entry uniquely identifies a production or signals an

 error For some grammars however may have some entries that

 are multiply defined For_example if is left-recursive or

 ambiguous then will have at_least one multiply defined entry

 Although left-recursion elimination and left_factoring are easy to

 do there are some grammars for which no amount of alteration will

 produce an LL(1)_grammar



 The language in the following example has no LL(1)_grammar at all



 ex

 ambig-parsing-table-ex The following grammar which

 abstracts the dangling-else problem is repeated_here from

 Example left-factoring-ex

 disp

 tabularl_c l











 tabular

 disp

 The parsing_table for this grammar appears in

 Fig ambig-parsing-table-fig The entry for

 contains both and



 figurehtfb

 center

 tabularccccccc

 -3ptnon- 6cInputSymbol



 2-7

 TERMINAL -2pt



 -2pt



 0pt0pt0pt







 -2pt



 tabular

 center

 Parsing_table for Example

 ambig-parsing-table-exambig-parsing-table-fig

 figure



 The grammar is ambiguous and the ambiguity is manifested by a

 choice in what production to use when an (else) is seen

 We can resolve this ambiguity by choosing

 This choice corresponds to associating an else with the

 closest previous then Note_that the choice

 would prevent from ever being put on the stack or

 removed_from the input and is surely wrong

 ex



 Nonrecursive Predictive_Parsing

 nonrec-pp-subsect



 A nonrecursive predictive_parser can be built by maintaining a

 stack explicitly rather_than implicitly via recursive_calls The

 parser mimics a leftmost_derivation If is the input that has

 been matched so_far then the stack_holds a sequence of grammar

 symbols such that

 center



 center



 The table-driven parser in Fig nonrec-model-fig has an

 input_buffer a stack containing a sequence of grammar_symbols a

 parsing_table constructed by Algorithm pred-table-alg and

 an output stream The input_buffer contains the string to be

 parsed followed_by the endmarker We reuse the symbol to

 mark the bottom of the stack which initially contains the start

 symbol of the grammar on top of



 figurehtfb



 center



 center

 Model of a table-driven predictive_parser

 nonrec-model-fig

 figure



 The parser is controlled by a program that considers the

 symbol on top of the stack and the current_input symbol If

 is a nonterminal the parser chooses an -production by

 consulting entry of the parsing_table (Additional

 code could be executed here for example code to construct a node

 in a parse tree) Otherwise it checks for a match between the

 terminal and current_input symbol



 The behavior of the parser can be described in terms of its configurations which give the stack_contents and the remaining

 input The next algorithm describes how configurations are

 manipulated



 alg

 predictive-alg Table-driven predictive_parsing



 A string and a parsing_table for grammar



 If is in a leftmost_derivation of

 otherwise an error indication



 Initially the parser is in a configuration with in

 the input_buffer and the start_symbol of on top of the

 stack above The program in Fig predictive-fig uses

 the predictive_parsing table to produce a predictive parse for

 the input

 alg



 figurehtfb

 center

 tabularl

 let be the first symbol of



 let be the top stack symbol



 while_( ) stack is not empty



 if_( ) pop the stack and let be

 the next symbol of



 else if_( is a terminal ) error()



 else if_( is an error entry ) error()



 else if_(

 )



 output the production





 pop the stack



 push_onto the stack with

 on top







 let be the top stack symbol





 tabular

 center



 Predictive parsing algorithm predictive-fig



 figure



 ex

 pred-parsing-ex Consider grammar (td-expr-gram) we

 have_already seen its parsing_table in

 Fig expr-pred-parsing-fig On input

 the nonrecursive predictive_parser of

 Algorithm predictive-alg makes the sequence of moves in

 Fig pp-configs-fig These moves correspond to a leftmost

 derivation (see Fig td-seq-fig for the full derivation)



 figurehtfb

 center

 tabularl r r_l

 matched stack 1cinput action



 height 12 pt depth 0 pt width 0pt



 output



 output



 output



 match



 output



 output



 match



 output



 output



 match



 output



 match



 output



 match



 output



 output



 tabular

 center

 Moves made by a predictive_parser on input

 pp-configs-fig

 figure



 center





 center

 Note_that the sentential_forms in this derivation correspond to

 the input that has already_been matched (in column MATCHED) followed_by the stack_contents The matched input is

 shown only to highlight the correspondence For the same reason

 the top of the stack is to the left when we consider bottom-up

 parsing it will be more natural to show the top of the stack to

 the right The input pointer points to the leftmost symbol of the

 string in the INPUT column

 ex



 Error_Recovery in Predictive_Parsing

 ll-err-subsect



 This discussion of error_recovery refers to the stack of a

 table-driven predictive_parser since it makes explicit the

 terminals and nonterminals that the parser hopes to match with the

 remainder of the input the techniques can also be used with

 recursive-descent_parsing



 An error is detected during predictive_parsing when the terminal

 on top of the stack does_not match the next_input symbol or when

 nonterminal is on top of the stack is the next_input

 symbol and is error (ie the parsing-table entry

 is empty)



 Panic Mode



 Panic-mode error_recovery is based_on the idea of skipping over symbols

 on the input until a token in a selected set of synchronizing

 tokens appears Its effectiveness depends_on the choice of

 synchronizing_set The sets should be chosen so that the parser

 recovers quickly from errors that are likely to occur in practice

 Some heuristics are as_follows



 enumerate



 As a starting point place all symbols in () into the

 synchronizing_set for nonterminal If we skip tokens until an

 element of () is seen and pop from the stack it is

 likely that parsing can continue



 It is not enough to use () as the synchronizing

 set for For_example if semicolons terminate statements as

 in C then keywords that begin statements may not appear in the

 set of the nonterminal representing expressions A

 missing semicolon after an assignment may therefore result in the

 keyword beginning the next statement being skipped Often there

 is a hierarchical_structure on constructs in a language for

 example expressions appear within statements which appear within

 blocks and so on We can add to the synchronizing_set of a

 lower-level construct the symbols that begin higher-level

 constructs For_example we might add keywords that begin

 statements to the synchronizing sets for the nonterminals

 generating expressions



 If we add symbols in () to the synchronizing_set for

 nonterminal then it may be possible to resume parsing

 according to if a symbol in () appears in the input



 If a nonterminal can generate the empty_string then the

 production deriving can be used as a default Doing_so

 may postpone some error_detection but cannot cause an error to be

 missed This_approach reduces the number of nonterminals that have

 to be considered during error_recovery



 If a terminal on top of the stack cannot be matched a simple idea

 is to pop the terminal issue a message saying that the terminal

 was inserted and continue parsing In effect this approach takes

 the synchronizing_set of a token to consist of all other tokens

 enumerate



 ex

 synch-ex Using and symbols as

 synchronizing_tokens works reasonably well when expressions are

 parsed according to the usual grammar (td-expr-gram) The

 parsing_table for this grammar in Fig expr-pred-parsing-fig

 is repeated in Fig topdown-recovery-table-fig with

 synch indicating synchronizing_tokens obtained from the

 set of the nonterminal in question The

 sets for the nonterminals are obtained from

 Example first-follow-ex



 figurehtfb

 center

 small

 tabularcc_c c_c c_c

 NON- 6c0pt6ptINPUT SYMBOL



 2-7

 TERMINAL 0pt6 ptid



 synch_synch







 synch_synch synch







 synch_synch synch_synch



 tabular

 small

 Synchronizing tokens added to the parsing_table of

 Fig expr-pred-parsing-figtopdown-recovery-table-fig

 center

 figure



 The table in Fig topdown-recovery-table-fig is to be used

 as_follows If the parser looks up entry and finds that

 it is blank then the input_symbol is skipped If the entry is

 synch then the nonterminal on top of the stack is popped in an

 attempt to resume parsing If a token on top of the stack does_not

 match the input_symbol then we pop the token from the stack as

 mentioned above



 On the erroneous_input the parser and

 error_recovery mechanism of Fig topdown-recovery-table-fig

 behave as in Fig pred-recovery-moves-fig

 ex



 figurehtfb

 center

 tabularr_r l

 STACK 1cINPUT REMARK



 error skip



 is in























 error synch



 has_been popped



































 tabular

 center

 Parsing and error_recovery moves made by a predictive

 parserpred-recovery-moves-fig

 figure



 The above discussion of panic-mode recovery does_not address the

 important issue of error_messages The compiler_designer must

 supply informative error_messages that not only describe the

 error they must draw attention to where the error was discovered



 Phrase-level Recovery



 Phrase-level error_recovery is implemented

 by filling in the blank entries in the predictive_parsing table

 with pointers to error_routines These routines may change

 insert or delete symbols on the input and issue appropriate error

 messages They may also pop from the stack Alteration of stack

 symbols or the pushing of new symbols onto the stack is

 questionable for several reasons First the steps carried

 out by the parser might then not correspond to the derivation of

 any word in the language at all Second we must ensure_that there

 is no possibility of an_infinite loop Checking that any recovery

 action eventually results in an input_symbol being consumed (or

 the stack being shortened if the end of the input has_been

 reached) is a good way to protect against such loops



 exer

 For each of the following grammars devise predictive_parsers and show

 the parsing_tables

 You_may left-factor andor eliminate left-recursion from your grammars

 first



 itemize



 a) The grammar of Exercise more-cfgs-exer(a)

 b) The grammar of Exercise more-cfgs-exer(b)

 c) The grammar of Exercise more-cfgs-exer(c)

 d) The grammar of Exercise more-cfgs-exer(d)

 e) The grammar of Exercise more-cfgs-exer(e)

 f) The grammar of Exercise more-cfgs-exer(g)



 itemize

 exer



 vhexer

 Is it possible by modifying the grammar in any way to construct a

 predictive_parser for the language of Exercise_cfg-exer (postfix

 expressions with operand )

 vhexer



 sexer

 Compute and for the grammar of Exercise_cfg-exer

 sexer



 exer

 Compute and for each of the grammars of

 Exercise more-cfgs-exer

 exer



 exer

 birman-exer

 The grammar generates all even-length strings of

 's We can devise a recursive-descent_parser with backtrack for this

 grammar If we choose to expand by production first

 then we_shall only recognize the string Thus any reasonable

 recursive-descent_parser will try first



 itemize



 a)

 Show that this recursive-descent_parser recognizes inputs and

 but not



 b)

 What language does this recursive-descent_parser recognize



 itemize

 exer



 The following exercises are useful steps in the construction of a

 Chomsky Normal Form grammar from arbitrary grammars as defined in

 Exercise chomsky-exer



 hexer

 ep-free-exer

 A grammar is -free if no production_body is

 (called an -production)



 itemize



 a)_Give an algorithm to convert any grammar into an

 -free grammar that generates the same language

 (with the possible exception of the empty_string - no -free

 grammar can

 generate )

 Hint First find all the nonterminals

 that are nullable meaning that they generate perhaps

 by a long derivation



 b) Apply your algorithm to the grammar





 itemize

 hexer



 hexer

 no-cycles-exer

 A single production is a production whose body is a single

 nonterminal ie a production of the form





 itemize



 a)_Give an algorithm to convert any grammar into an

 -free

 grammar with

 no single productions that generates the same language

 (with the possible exception of the empty string)

 Hint First eliminate -productions and then find for

 which pairs of nonterminals and does by a sequence of

 single productions



 b) Apply your algorithm to the grammar_(expr-gram-display)

 in Section rep-grammars-subsect



 c)_Show that as a consequence of part (a) we can convert a

 grammar into an equivalent grammar that has no cycles (derivations

 of one or_more steps in which for some nonterminal )



 itemize

 hexer



 vhexer

 chomsky-exer

 A grammar is said to be in Chomsky Normal Form (CNF) if every

 production is either of the form or of the form

 where and are nonterminals and is a terminal

 Show_how to convert any grammar into a CNF grammar for the same language

 (with the possible exception of the empty_string - no CNF grammar can

 generate )

 vhexer



 hexer

 cyk-exer

 Every language that has a context-free_grammar can be recognized in at

 most time for strings of length A simple way to do so

 called the Cocke-Younger-Kasami (or CYK) algorithm is based_on

 dynamic_programming That is given a string we

 construct an -by- table such that is the set of

 nonterminals that generate the substring If the

 underlying_grammar is in CNF (see Exercise chomsky-exer) then one

 table entry can be filled in in time provided we fill the entries

 in the proper order lowest value of first

 Write an algorithm that correctly fills in the entries of the table and

 show that your algorithm takes time

 Having filled in the table how do you determine_whether

 is in the language

 hexer



 hexer

 Show_how having filled in the table as in Exercise cyk-exer we

 can in time recover a parse_tree for

 Hint modify the table so it records for each nonterminal in

 each table entry

 some pair of nonterminals in other table entries that justified putting

 in

 hexer



 hexer

 Modify your algorithm of Exercise cyk-exer so that it will

 find for any string the smallest number of insert delete and mutate

 errors (each error a single character) needed to turn the string into a

 string in the language of the underlying_grammar

 hexer



 figurehtfb



 centertabularl c_l

 stmt if e then_stmt stmtTail



 while e do stmt



 begin list end







 stmtTail else_stmt







 list stmt listTail



 listTail list







 tabularcenter



 A grammar for certain kinds of statements

 td-ec-gram-fig



 figure



 hexer

 td-ec-gram-exer

 In Fig td-ec-gram-fig is a grammar for certain statements

 You_may take and to be terminals standing for conditional

 expressions and other statements respectively

 If we resolve the conflict regarding expansion of the optional else

 (nonterminal stmtTail) by preferring to consume an else from

 the input whenever we see one we can build a predictive_parser for this

 grammar

 Using the idea of

 synchronizing symbols described in Section ll-err-subsect



 itemize



 a)

 Build an error-correcting predictive_parsing table for the grammar



 b)

 Show the behavior of your parser on the following inputs



 center

 tabularl_l

 if then if then end



 while do begin if then end



 tabular

 center



 itemize

 hexer

 Instruction Selection by Tree Rewriting

 tile-sect



 Instruction selection can be a large combinatorial task

 especially for machines that are rich in addressing_modes such_as

 machines or on machines with special-purpose

 instructions say for signal processing Even_if we assume that

 the order of evaluation is given and that registers are allocated

 by a separate mechanism instruction_selection - the problem of

 selecting target-language instructions to implement the operators

 in the intermediate_representation - remains a large

 combinatorial task



 In this_section we treat instruction_selection as a

 tree-rewriting problem Tree representations of target

 instructions have_been used effectively in code-generator

 generators which automatically construct the

 instruction-selection phase of a code_generator from a

 high-level specification of the target_machine Better code might

 be obtained for some machines by using 's rather_than trees

 but matching is more_complex than tree matching



 Tree-Translation Schemes



 Throughout this_section the input to the code-generation process

 will be a sequence of trees at the semantic level of the target

 machine The trees are what we might get after inserting run-time

 addresses into the intermediate_representation as described in

 Section rt-storage-sect

 In_addition the leaves of the trees contain information_about the

 storage types of their labels



 ex

 tile-tree-ex Figure tile-tree-fig contains a tree

 for the assignment_statement aib1 where the

 array a is stored on the run-time_stack and the variable

 b is a global in memory_location The run-time

 addresses of locals a and i are given as constant

 offsets and from SP the register containing the

 pointer to the beginning of the current activation_record



 The assignment to ai is an indirect assignment in which

 the r-value of the location for ai is set to the

 r-value of the expression b1 The addresses of

 array a and variable i are given by_adding the values

 of the constant and respectively to the contents of

 register SP We simplify array-address calculations by

 assuming that all values are one-byte characters (Some

 instruction sets make special provisions for multiplications by

 constants such_as 2_4 and 8 during address calculations)



 figurehtfb



 Intermediate-code tree for aib1tile-tree-fig

 figure



 In the tree the ind operator treats its argument as a

 memory address As the left_child of an assignment operator the

 ind node gives the location into which the r-value on

 the right_side of the assignment operator is to be stored If an

 argument of a or ind operator is a memory_location or a

 register then the contents of that memory_location or register

 are taken as the value The leaves in the tree are labeled with

 attributes a subscript indicates the value of the attribute

 ex



 The target code is generated_by applying a sequence of

 tree-rewriting_rules to reduce the input tree to a single_node

 Each tree-rewriting rule has the form

 center

 replacement template action

 center

 where replacement is a single_node template is a

 tree and action is a code_fragment as in a syntax-directed

 translation_scheme



 A set of tree-rewriting_rules is called a tree-translation

 scheme



 Each tree-rewriting rule represents the translation of a portion

 of the tree given by the template The translation consists of a

 possibly empty sequence of machine_instructions that is emitted by

 the action_associated with the template The leaves of the

 template are attributes with subscripts as in the input tree

 Sometimes certain restrictions apply to the values of the

 subscripts in the templates these restrictions are specified as

 semantic predicates that must_be satisfied before the template is

 said to match For_example a predicate might specify that the

 value of a constant fall in a certain range



 A tree-translation_scheme is a convenient_way to represent the

 instruction-selection phase of a code_generator As an example of

 a tree-rewriting rule consider the rule for the

 register-to-register add instruction





 This rule is used as_follows If the input tree contains

 a subtree that matches this tree template that is a subtree

 whose root is labeled by the operator and whose left and right

 children are quantities in registers and then we can

 replace that subtree by a single_node labeled and emit the

 instruction ADD RRR as output

 We call this replacement a tiling of the subtree

 More than one

 template may match a subtree at a given time we_shall describe

 shortly some mechanisms for deciding which rule to apply in cases

 of conflict



 ex

 tile-rules-ex Figure tile-rules-fig contains

 tree-rewriting_rules for a few instructions of our target_machine

 These rules will be used in a running_example throughout this

 section The first two rules correspond to load instructions the

 next two to store_instructions and the remainder to indexed loads

 and additions Note_that rule (8) requires the value of the

 constant to be This condition would be specified_by a

 semantic predicate

 ex



 figure



 Tree-rewriting rules for some target-machine

 instructions tile-rules-fig

 figure



 Code_Generation by Tiling an Input Tree



 A tree-translation_scheme works as_follows Given an input tree

 the templates in the tree-rewriting_rules are applied to tile its

 subtrees If a template matches the matching subtree in the input

 tree is replaced with the replacement node of the rule and the

 action_associated with the rule is done If the action contains a

 sequence of machine_instructions the instructions are emitted

 This process is repeated until the tree is reduced to a single

 node or until_no more templates match The sequence of machine

 instructions generated as the input tree is reduced to a single

 node constitutes the output of the tree-translation_scheme on the

 given input tree



 The process of specifying a code_generator becomes similar to that

 of using a syntax-directed_translation scheme to specify a

 translator We write a tree-translation_scheme to describe the

 instruction set of a target_machine In_practice we would like to

 find a scheme that causes a minimal-cost instruction sequence to

 be generated for each input tree Several tools are available to

 help build a code_generator automatically from a tree-translation

 scheme



 ex

 tiling-ex Let_us use the tree-translation_scheme in

 Fig_tile-rules-fig to generate code for the input tree in

 Fig tile-tree-fig Suppose that the first rule is applied

 to load the constant into register R0





 The label of the leftmost leaf then changes from

 to and the instruction_LD R0a is generated The

 seventh rule now matches the leftmost subtree with root labeled







 Using this rule we rewrite this subtree as a single

 node_labeled and generate the instruction ADD R0R0SP Now the tree looks like





 At this point we could apply rule (5) to reduce the

 subtree





 to a single_node labeled say We could also use

 rule (6) to reduce the larger subtree





 to a single_node labeled and generate the

 instruction ADD R0R0i(SP) Assuming that it is more

 efficient to use a single instruction to compute the larger

 subtree rather_than the smaller one we choose rule (6) to get





 In the right subtree rule_(2) applies to the leaf

 It generates an instruction to load b into register

 R1 say Now using rule (8) we can match the subtree





 and generate the increment instruction INC R1 At

 this point the input tree has_been reduced to





 This remaining tree is matched by rule (4) which

 reduces the tree to a single_node and generates the instruction

 ST R0R1

 We generate the following code sequence



 center

 tabularl

 'LD R0 a'



 'ADD R0_R0 SP'



 'ADD R0_R0 i(SP)'



 'LD_R1 b'



 'INC R1'



 'ST R0 R1'



 tabular

 center

 in the process of reducing the tree to a single_node

 ex



 In order to implement the tree-reduction process in

 Example tile-tree-ex we must address some issues related to

 tree-pattern_matching



 itemize



 How is tree-pattern_matching to be done The efficiency of

 the code-generation process (at compile time) depends_on the

 efficiency of the tree-matching algorithm



 What do we do if more_than one template matches at a given

 time The efficiency of the generated code (at run time) may

 depend_on the order in which templates are matched since

 different match sequences will in general lead to different

 target-machine code sequences some more_efficient than_others

 itemize



 If no template matches then the code-generation process blocks

 At the other extreme we need to guard against the possibility of

 a single_node being rewritten indefinitely generating an_infinite

 sequence of register move instructions or an_infinite sequence of

 loads and stores



 To prevent blocking we assume that each operator in the

 intermediate_code can be_implemented by one or_more target-machine

 instructions We further assume that there are enough registers to

 compute each tree node by itself Then no_matter how the tree

 matching proceeds the remaining tree can always be translated

 into target-machine_instructions



 Pattern Matching by Parsing

 lr-tree-parse-subsect



 Before considering general tree matching we consider a

 specialized approach that uses an LR_parser to do the pattern

 matching The input tree can be treated_as a string by using its

 prefix representation For_example the prefix representation for

 the tree in Fig tile-tree-fig is



 center



 center



 The tree-translation_scheme can be converted into a syntax-directed

 translation_scheme by_replacing the tree-rewriting_rules with the

 productions of a context-free_grammar in which the right_sides are

 prefix representations of the instruction templates



 ex

 tile-parse-ex The syntax-directed_translation scheme in

 Fig tile-parse-fig is based_on the tree-translation_scheme

 in Fig_tile-rules-fig



 figurehtfb



 center

 tabularr_r c_l l

 1) LD R



 2) LD R



 3) ST R



 4) ST R_R



 5) LD R (R)



 6) ADD R_R (R)



 7) ADD R_R R



 8) INC R



 9) sp



 10) m

 tabular

 center



 Syntax-directed_translation scheme constructed from

 Fig_tile-rules-fig tile-parse-fig

 figure



 The nonterminals of the underlying_grammar are and The

 terminal m represents a specific memory_location such_as

 the location for the global variable b in

 Example tile-tree-ex The production in

 Rule (10) can be thought of as matching with m prior to

 using one of the templates involving Similarly we introduce

 a terminal sp for register SP and add the production

 Finally terminal c represents

 constants



 Using these terminals the string for the input tree in

 Fig tile-tree-fig is



 center



 center

 ex



 From the productions of the translation_scheme we build an LR

 parser using one of the LR-parser construction techniques of

 Chapter_parse-ch The target code is generated_by emitting

 the machine_instruction corresponding to each reduction



 A code-generation grammar is usually highly ambiguous and some

 care needs to be given to how the parsing-action_conflicts are

 resolved when the parser is constructed In the absence of cost

 information a general rule is to favor larger reductions over

 smaller ones This means that in a reduce-reduce conflict the

 longer reduction is favored in a shift-reduce conflict the shift

 move is chosen This maximal munch approach causes a larger

 number of operations to be performed with a single machine

 instruction



 There_are some benefits to using LR_parsing in code_generation

 First the parsing method is efficient and well understood so

 reliable and efficient code generators can be produced using the

 algorithms described in Chapter_parse-ch Second it is

 relatively_easy to retarget the resulting code_generator a code

 selector for a new machine can be constructed by writing a grammar

 to describe the instructions of the new machine Third the

 the code generated can be made more_efficient by_adding

 special-case productions to take_advantage of machine idioms



 However there are some challenges as_well A left-to-right_order

 of evaluation is fixed by the parsing method Also for some

 machines with large_numbers of addressing_modes the

 machine-description grammar and resulting parser can become

 inordinately large As a consequence specialized techniques are

 necessary to encode and process the machine-description grammars

 We must also be careful that the resulting parser does_not block

 (has no next move) while parsing an expression tree either

 because the grammar does_not handle some operator patterns or

 because the parser has made the wrong resolution of some

 parsing-action conflict We must also make_sure the parser does

 not get into an_infinite loop of reductions of productions with

 single symbols on the right_side The looping problem can be

 solved_using a state-splitting technique at the time the parser

 tables are generated



 Routines for Semantic Checking



 In a code-generation translation_scheme the same attributes

 appear as in an input tree but often with restrictions on what

 values the subscripts can have For_example a machine_instruction

 may require that an attribute value fall in a certain range or

 that the values of two attributes be related



 These restrictions on attribute values can be specified as

 predicates that are invoked before a reduction is made In_fact

 the general use of semantic_actions and predicates can provide

 greater flexibility and ease of description than a purely

 grammatical specification of a code_generator Generic templates

 can be used to represent classes of instructions and the semantic

 actions can then be used to pick instructions for specific cases

 For_example two forms of the addition instruction can be

 represented with one template





 Parsing-action conflicts can be_resolved by disambiguating

 predicates that can allow different selection strategies to be

 used in different_contexts A smaller description of a target

 machine is possible because certain aspects of the machine

 architecture such_as addressing_modes can be factored into the

 attributes The complication in this approach is that it may

 become difficult to verify the accuracy of the translation_scheme

 as a faithful description of the target_machine although this

 problem is shared to some degree by all code generators





 General Tree Matching



 The LR-parsing approach to pattern_matching based_on prefix

 representations favors the left_operand of a binary_operator In a

 prefix representation the limited-lookahead

 LR_parsing decisions must_be made on the basis of some prefix of

 since can be arbitrarily long Thus pattern_matching

 can miss nuances of the target-instruction set that are due to

 right operands



 Instead prefix representation we could use a postfix

 representation But then an LR-parsing approach to pattern

 matching would favor the right operand



 For a hand-written code_generator we can use tree templates as

 in Fig_tile-rules-fig as a guide and write an ad-hoc

 matcher For_example if the root of the input tree is labeled

 ind then the only pattern that could match is for rule (5)

 otherwise if the root is labeled then the patterns that

 could match are for rules (6-8)



 For a code-generator generator we need a general tree-matching

 algorithm An efficient top-down algorithm can be developed by

 extending the string-pattern-matching techniques of

 Chapter lexan-ch The idea is to represent each template as

 a set of strings where a string corresponds to a path from the

 root to a leaf in the template We treat all operands equally by

 including the position number of a child from left to right in

 the strings



 extree-match-ex

 In building the set of strings for an instruction set we_shall

 drop the subscripts since pattern_matching is based_on the

 attributes alone not on their values



 The templates in Fig tree-match-fig have the following set

 of strings from the root to a leaf

 center

 tabularl





















 tabular

 center

 The string represents the template with at the

 root The string represents the and its left

 operand in the two templates that have at the root

 ex



 figurehtfb

 An

 instruction set for tree matchingtree-match-fig

 figure



 Using sets of strings as in Example tree-match-ex a

 tree-pattern matcher can be constructed by using techniques for

 efficiently matching multiple strings in parallel



 In_practice the tree-rewriting process can be_implemented by

 running the tree-pattern matcher during a depth-first_traversal of

 the input tree and performing the reductions as the nodes are

 visited for the last time



 Instruction costs can be taken into_account by associating with

 each tree-rewriting rule the cost of the sequence of machine

 instructions generated if that rule is applied In

 Section dyn-prog-sect we discuss a dynamic_programming

 algorithm that can be used in conjunction with tree-pattern

 matching



 By running the dynamic_programming algorithm concurrently we can

 select an optimal sequence of matches using the cost information

 associated_with each rule We may need to defer deciding upon a

 match until the cost of all alternatives is known Using this

 approach a small efficient code_generator can be constructed

 quickly from a tree-rewriting scheme Moreover the dynamic

 programming algorithm frees the code-generator designer from

 having to resolve conflicting matches or decide upon an order for

 the evaluation



 exer tile-exer

 Construct syntax_trees for each of the following statements

 assuming all nonconstant operands are in memory_locations



 itemize

 a) x a b_c d

 b) xi yj zk

 c) x x 1

 itemize

 Use the tree-rewriting scheme in Fig_tile-rules-fig

 to generate code for each statement

 exer



 exer

 Repeat_Exercise tile-exer above using the syntax-directed

 translation_scheme in Fig tile-parse-fig in place

 of the tree-rewriting scheme

 exer



 hexer

 Extend the tree-rewriting scheme in Fig_tile-rules-fig

 to apply to while-statements

 hexer



 hexer

 How_would you extend tree rewriting to apply to 's

 hexer



 Appendix

 alg

 figtime-math

 Finds a maximal_set of linearly_independent solutions for

 and expresses them as rows of matrix

 A a matrix

 B a matrix



 The algorithm is shown in Fig figtime-alg in the Appendix

 alg



 figure

 tabbing

 1234123412341234123412341234Let by be the size of matrix and

 denotes the th component of







 a matrix



 an by identity matrix







 While true do







 1 Make into a diagonal

 matrix with positive diagonal entries



 and are solutions







 While st do



 Move pivot to by row and column interchange



 Interchange row with row in



 If then







 end if



 For to do



 If then















 end if



 end for







 end while







 2 Find solution besides It must_be a non-negative

 combination of







 Find st





 If a non-trivial solution say then









 else are the only solutions







 end if







 tabbing

 figure

 figure

 tabbing

 1234123412341234123412341234 3 Make



 If then

 Move solutions to



 For to do



 Interchange rows and in and



 end for







 else Use row addition to find more solutions







 For to do



 If st then



 If st then



 For to do



 If then













 end if



 end for



 else



 For to step -1 do



 If then







 Interchange with

 Interchange with



 end if



 end for



 end if



 end if



 end for



 end if







 4 Make



 For to do



 For 1 to do



 If then



 Pick an st











 end if



 end for



 end for







 5 If necessary repeat with rows



 If () then



 Remove row to row from Return



 else







 For to 1 step -1 do



 If st then







 Interchange column with in



 end if



 end for







 end if







 end while

 tabbing

 Finding a maximal_set of linearly_independent solutions to

 where has_rank



 figtime-alg

 figure









 theorem

 Algorithm alglegal finds a maximal_set of linearly_independent

 solutions to where has_rank



 thmlegal

 theorem

 proof



 Let be a linear_combination of the rows of matrix





 As_discussed in Algorithm algfp

 is a solution to

 if and only if are non-negative



 The rows in are linearly_independent Since elementary row

 operations do_not change the number of linearly_independent rows in a

 matrix transforming to contain as many rows with non-negative

 as possible we find a maximal_set of linearly

 independent_solutions

 of the rows of matrix

 say is a solution to

 if and only if are non-negative

 proof



 3in1

 Putting the Techniques Together

 together-sect



 The syntax-directed techniques in this_chapter are applied in this

 section to implement a compiler_front end in Java The front_end

 consists of Java packages that are based_on the program_fragments

 in Sections postfix-sect-syntree-sect



 We begin_with a grammar for the source_language it consists

 primarily of statements and expressions We then consider the

 lexical_analyzer the symbol-table package and the classes for

 the language_constructs in the abstract_syntax The parser we save

 for last since its code interacts with the rest of the packages



 Grammar for the Source Language

 source-gram-subsect



 A program in the language consists of a block with optional

 declarations and statements Token basic represents basic

 types The grammar permits arrays of arrays however the front

 end handles only one-dimensional arrays



 tabularp6in c_l

 program block



 block 123 decls_stmts 125



 decls decls decl



 decl type id



 type type num basic



 stmts stmts stmt



 stmt expr



 if_( expr ) stmt



 if_( expr ) stmt_else stmt



 while_( expr ) stmt



 do stmt while_( expr )



 for ( optexpr_optexpr optexpr ) stmt



 break



 continue



 block



 optexpr expr

 tabular

 The productions for expressions handle associativity and

 precedence of operators They use a nonterminal for each level of

 precedence and a nonterminal factor for parenthesized

 expressions identifiers and constants



 tabularp6in c_l

 expr condition expr condition



 condition condition equals condition equals equals



 equals equals relation equals relation relation



 relation relation add relation add



 relation add relation add add



 add add term add - term_term



 term_term unary term unary term unary_unary



 unary_unary - unary access



 access access expr factor



 factor (_expr ) id num real true false

 tabular



 Lexical_Analyzer

 lexer-java-subsect



 Package lexer is an extension of the code for the lexical

 analyzer in Section_scan-java-subsect Class Tag

 defines constants for tokens



 footnotesize

 flushleft

 1)_package lexer_File Tagjava



 2)_public class_Tag



 3)_public final static_int



 4) AND 256 BASIC 257 BREAK 258 CONTINUE 259



 5) DO 260 ELSE 261 EQ 262 FALSE 263 FOR 264



 6) GE 265 ID 266 IF 267 INDEX 268 LE 269



 7) MINUS 270 NE 271 NUM 272 OR 273 REAL 274



 8) TEMP 275 TRUE 276 WHILE 277



 9)

 flushleft

 footnotesize



 Three of the constants INDEX MINUS and

 TEMP are not lexical tokens they will be used to generate

 intermediate_code



 Classes Token and Num are as in

 Section_scan-java-subsect with an additional method toString



 footnotesize

 flushleft

 1)_package lexer_File Tokenjava



 2)_public class_Token



 3)_public final_int tok



 4)_public Token(int t) tok t



 5)_public String_toString() return CharactertoString((char)tok)



 6)

 flushleft

 footnotesize



 footnotesize

 flushleft

 1)_package lexer_File Numjava



 2)_public class Num_extends Token



 3)_public final_int value



 4)_public Num(int v) super(TagNUM) value v



 5)_public String_toString() return IntegertoString(value)



 6)

 flushleft

 footnotesize



 Class Word is used for reserved_words identifiers and to

 specify the intermediate_code for specific tokens The

 intermediate form is usually the same as the lexical spelling

 exceptions include unary_minus (-2 in the source_program is

 written as minus 2 in the intermediate code)



 footnotesize

 flushleft

 1)_package lexer_File Wordjava



 2)_public class_Word extends_Token



 3)_public final String lexeme



 4)_public Word(String s int t) super(t) lexeme_s



 5)_public String_toString() return lexeme



 6)_public static final Word



 7) and new_Word( TagAND) or new_Word( TagOR)



 8) eq new_Word( TagEQ) ne new_Word( TagNE)



 9) le new_Word( TagLE) ge new_Word( TagGE)



 10) Minus new Word(minus TagMINUS)



 11) Index new_Word( TagINDEX)



 12) Temp new Word(t TagTEMP)



 13)



 flushleft

 footnotesize



 Class Real is for floating_point numbers



 footnotesize

 flushleft

 1)_package lexer_File Realjava



 2)_public class Real extends_Token



 3)_public final float value



 4)_public Real(float v) super(TagREAL) value v



 5)_public String_toString() return FloattoString(value)



 6)



 flushleft

 footnotesize



 Function scan is the main method in class_Lexer In

 addition to recognizing numbers identifiers and reserved_words

 as in Section_scan-java-subsect scan recognizes

 composite tokens like (lines 34-53) and floating_point

 numbers (lines 60-67) It uses a function readch() (line 21)

 to read the next_input character into variable peek In

 addition with a character as an parameter as in readch('') the same function name is overloaded or reused

 (lines 22-27) to check_whether the parameter matches the next

 input_character For_example readch('') reads the next

 input_character into peek and checks_whether it equals '' If so readch('') sets peek to a blank and

 returns false otherwise it returns false



 The basic types int char bool and float

 are represented_by objects that are reserved on lines 18-19 Class

 Type extends class_Word it's code appears in

 Section symbols-java-subsect



 footnotesize

 flushleft

 1)_package lexer_File Lexerjava



 2)_import javaio_import javautil import_symbols



 3)_public class_Lexer



 4)_public static_int line 1



 5) char peek_' '



 6) Hashtable_words new_Hashtable()



 7) void reserve(Word k) wordsput(klexeme k)



 8)_public Lexer()



 9) reserve(_new Word(true TagTRUE) )



 10) reserve(_new Word(false TagFALSE) )



 11) reserve(_new Word(if TagIF) )



 12) reserve(_new Word(else TagELSE) )



 13) reserve(_new Word(while TagWHILE) )



 14) reserve(_new Word(do TagDO) )



 15) reserve(_new Word(for TagFOR) )



 16) reserve(_new Word(break TagBREAK) )



 17) reserve(_new Word(continue TagCONTINUE) )



 18) reserve( TypeInt ) reserve( TypeChar )



 19) reserve( TypeBool_) reserve( TypeFloat )



 20)



 21) void readch() throws_IOException peek_(char)Systeminread()



 22) boolean readch(char c) throws_IOException



 23) readch()



 24) if(_peek c )_return false



 25) peek_' '



 26) return true



 27)



 28) public Token_scan() throws_IOException



 29) for( readch() )



 30) if(_peek '_' peek_') continue



 31) else_if( peek_') line line 1



 32) else_break



 33)



 34) switch( peek )



 35) case_''



 36) if(_readch('') )_return Wordand



 37) else_return new_Token('')



 38) case_''



 39) if(_readch('') )_return Wordor



 40) else_return new_Token('')



 41) case_''



 42) if(_readch('') )_return Wordeq



 43) else_return new_Token('')



 44) case_''



 45) if(_readch('') )_return Wordne



 46) else_return new_Token('')



 47) case_''



 48) if(_readch('') )_return Wordle



 49) else_return new_Token('')



 50) case_''



 51) if(_readch('') )_return Wordge



 52) else_return new_Token('')



 53)



 54) if(_CharacterisDigit(peek) )



 55) int_v 0



 56) do



 57) v 10v Characterdigit(peek_10) readch()



 58) while( CharacterisDigit(peek)_)



 59) if(_peek ''_) return_new Num(v)



 60) float_x v float d 10



 61) for()



 62) readch()



 63) if(_CharacterisDigit(peek) ) break



 64) x x Characterdigit(peek_10) d d d10



 65)



 66) return_new Real(x)



 67)



 68) if( CharacterisLetter(peek) )



 69) StringBuffer b new StringBuffer()



 70) do



 71) bappend(peek) readch()



 72) while( CharacterisLetterOrDigit(peek) )



 73) String_s btoString()



 74) Word w (Word)wordsget(s)



 75) if( w null_) return_w



 76) w new Word(s TagID)



 77) wordsput(s w)



 78) return_w



 79)



 80) Token_t new Token(peek) peek_' '



 81) return t



 82)



 83)



 flushleft

 footnotesize



 Symbol Tables and Types

 symbols-java-subsect



 Class Env is essentially unchanged from

 Fig symtab-java-fig Instead of objects of class Symbol it holds objects of class Id



 footnotesize

 flushleft

 1)_package symbols File Envjava



 2)_import javautil import syntax



 3)_public class_Env



 4) private Hashtable table



 5) protected Env next



 6)_public Env(Env n) table new_Hashtable() next n



 7) public_void put(String s Id i) tableput(s i)



 8)_public Id get(String s)



 9) for( Env e this e null e enext )



 10) Id found (Id)(etableget(s))



 11) if( found null_) return found



 12)



 13) return_null



 14)



 15)



 flushleft

 footnotesize



 Since identifiers and their types are kept in the symbol_table we

 put the classes for types in package_symbols Basic type

 names like int are reserved_words so class Type is

 defined to be a subclass of class_Word The code for class

 Type defines four static objects denoted_by TypeInt

 TypeChar TypeBool and TypeFloat (lines

 5-7) These objects are tokens with inherited field_tok set

 to the constant TagBASIC Function numeric (lines

 8-11) checks_whether a type object is one of TypeChar TypeInt and TypeFloat since characters can be converted

 to integers and integers can be converted to floating_point

 numbers Function max (lines 12-17) is used for type

 conversions when an arithmetic operator is applied to two numeric

 types



 footnotesize

 flushleft

 1)_package symbols File Typejava



 2)_import lexer



 3)_public class Type extends Word



 4)_public Type(String s int t) super(s t)



 5)_public static final Type



 6) Int new Type(intTagBASIC) Float new Type(floatTagBASIC)



 7) Char new Type(charTagBASIC) Bool new Type(boolTagBASIC)



 8)_public static boolean numeric(Type t)



 9) if( t TypeChar t TypeInt t TypeFloat )_return true



 10) else_return false



 11)



 12) public_static Type max(Type t1_Type t2 )



 13) if_( numeric(t1) numeric(t2) )_return null



 14) else if_( t1 TypeFloat t2 TypeFloat )_return TypeFloat



 15) else if_( t1 TypeInt t2 TypeInt )_return TypeInt



 16) else_return TypeChar



 17)



 18)



 flushleft

 footnotesize



 Arrays are the only constructed type in the source_language

 represented_by objects of class Array



 footnotesize

 flushleft

 1)_package symbols File Arrayjava



 2)_import lexer



 3)_public class Array extends Type



 4)_public int size public_Type of



 5)_public Array(int s Type t) super(TagINDEX) size s of t



 6)_public String_toString() return size oftoString()



 7)



 flushleft

 footnotesize



 Intermediate_Code for Statements

 stmt-java-subsect



 The parser constructs syntax_trees using the approach of

 Section ast-create-subsect Nodes in the syntax_tree are

 implemented as objects of class Node For error reporting

 field lexline (lines 4-5) holds the source-line number of

 the construct represented_by a node Lines 6-9 are used during the

 generation of three-address_code Field emitted counts the

 number of labels that have_been emitted so_far



 footnotesize

 flushleft

 1)_package syntax_File Nodejava



 2)_import lexer



 3)_public class Node



 4) int lexline 0



 5) Node() lexline Lexerline



 6)_public static_int emitted 0



 7) public_int newlabel() return emitted



 8)_public void emitlabel(int i) Systemoutprint(L i )



 9)_public void emit(String s) Systemoutprintln( s)



 10) void error(String s)



 11) String msg new String(near line lexline s)



 12) throw_new Error(msg)



 13)



 14)

 flushleft

 footnotesize



 The two subclasses of Node are Stmt for statement

 nodes and Expr for expression nodes Each statement

 construct is implemented_by a subclass of Stmt with fields

 for the components of the construct For_example class While has fields for a test expression a substatement as we

 shall_see



 Using an object-oriented approach all the code for a construct is

 collected in the class for the construct Thus the subclasses of

 Stmt have fields and methods to deal_with the following



 itemize

 Construct a syntax-tree_node



 Check types



 Generate three-address intermediate_code

 itemize

 Alternatively with a phase-oriented approach the code is grouped

 by phase so the code for checking types has a case for each

 construct the code for generating three-address_code has a case

 for each construct and so on



 Lines 3-4 in the following code for class_Stmt deal_with

 syntax-tree construction The constructor Stmt() does

 nothing since the work is done in the subclasses for the

 individual constructs The static object StmtNull (line_4)

 represents an empty sequence of statements



 footnotesize

 flushleft

 1)_package syntax_File Stmtjava



 2)_public class_Stmt extends Node



 3)_public Stmt()



 4)_public static Stmt Null new Stmt()



 5)_public void_gen()



 6) int begin 0 int after 0



 7) public_static Stmt BreakFrom null



 8)_public static Stmt NextFrom null



 9)_public int getnext() error(can't enclose a continue) return 0



 10)



 flushleft

 footnotesize



 Lines 5-9 deal_with the generation of three-address_code The

 method gen() does nothing since the work is done in the

 version of gen in the subclasses The fields begin and

 after (line_5) are for labels to mark the beginning of the

 code for the statement and the first instruction after the code

 for the statement see for example the layout for if statements in

 Fig if-layout-fig



 The static object StmtBreakFrom (line 7) is used to

 implement break statements it will be discussed along with the

 code for class Break Similarly StmtNextFrom and

 method getnext() will be discussed along with the code for

 class Continue



 The code for class If is based_on the pseudo-code in

 Fig if-pseudo-fig One line 3 expr in lower-case

 letters is a field of class Expr similarly stmt in

 lower-case letters is a field of class_Stmt



 footnotesize

 flushleft

 1)_package syntax_File Ifjava



 2)_import symbols



 3)_public class If extends_Stmt



 4)_Expr expr Stmt_stmt



 5)_public If(Expr x Stmt s)



 6) expr x stmt s after newlabel()



 7) if(_exprtype TypeBool_) exprerror(boolean_required in if)



 8)



 9)_public void_gen()



 10) Expr_x exprrvalue()



 11) emit( iffalse xtoString() goto_L after)



 12) stmtgen() emitlabel(after)



 13)



 14)

 flushleft

 footnotesize



 The construction of a While object is split between the

 constructor While() which creates a node with null children

 (line_5) and an initialization function init(xs) which

 sets child expr to x and child stmt to s

 (lines 6-9) Function gen() for generating three-address

 code (line 10-14) is in the spirit of the corresponding function

 gen() in class If Function getnext() named

 after next iteration returns label begin which marks

 the beginning of the code for the while construct



 footnotesize

 flushleft

 1)_package syntax_File Whilejava



 2)_import symbols



 3)_public class While extends_Stmt



 4)_Expr expr Stmt_stmt



 5)_public While() expr null stmt null



 6)_public void init(Expr x Stmt s)



 7) expr x stmt s begin newlabel() after newlabel()



 8) if(_exprtype TypeBool_) exprerror(boolean_required in while)



 9)



 10)_public void_gen()



 11) emitlabel(begin) Expr_x exprrvalue()



 12) emit( iffalse xtoString() goto_L after)



 13) stmtgen() emit(_goto L begin) emitlabel(after)



 14)



 15)_public int getnext() return begin



 16)

 flushleft

 footnotesize



 Class Do is very similar to class While



 footnotesize

 flushleft

 1)_package syntax_File Dojava



 2)_import symbols



 3)_public class Do extends_Stmt



 4)_Expr expr Stmt_stmt



 5)_public Do() expr null stmt null



 6)_public void init(Stmt s Expr x)



 7) expr x stmt s begin newlabel()



 8) if(_exprtype TypeBool_) exprerror(boolean_required in do)



 9)



 10)_public void_gen()



 11) emitlabel(begin) stmtgen() Expr_x exprrvalue()



 12) emit( iftrue xtoString() goto_L begin)



 13)



 14) public_int getnext() return begin



 15)

 flushleft

 footnotesize



 The result of evaluating the expression in an expression statement

 is discarded Note_that method gen() in class Eval

 calls exprrvalue() but does nothing with the result



 footnotesize

 flushleft

 1)_package syntax_File Evaljava



 2)_public class Eval extends_Stmt



 3) Expr_expr



 4)_public Eval(Expr x) expr x



 5)_public void_gen() exprrvalue()



 6)

 flushleft

 footnotesize



 Class Seq implements a sequence of statements Note_that a

 statement can be the null statement StmtNull in which case

 no code is generated



 footnotesize

 flushleft

 1)_package syntax_File Seqjava



 2)_public class Seq extends_Stmt



 3) Stmt stmt1 Stmt stmt2



 4)_public Seq(Stmt s1 Stmt s2) stmt1 s1 stmt2 s2



 5)_public void_gen() stmt1gen() stmt2gen()



 6)

 flushleft

 footnotesize



 A break statement sends control out of an enclosing_loop or switch

 statement Class Break uses field stmt to save the

 enclosing statement construct (the parser ensures that StmtBreakFrom denotes the syntax-tree_node for the enclosing

 construct) The code for a Break object is a jump to the

 label stmtafter which marks the instruction immediately

 after the code for stmt



 footnotesize

 flushleft

 1)_package syntax_File Breakjava



 2)_public class Break extends_Stmt



 3) Stmt_stmt



 4)_public Break()



 5) if( StmtBreakFrom null_) error(unenclosed break)



 6) stmt StmtBreakFrom



 7)



 8)_public void_gen() emit(_goto L stmtafter)



 9)

 flushleft

 footnotesize



 Continue statements are implemented just like break statements

 Since the next iteration need not be the first instruction

 generated for the enclosing construct method gen() uses

 stmtgetnext() to get the label that marks the next

 iteration of the enclosing statement



 footnotesize

 flushleft

 1)_package syntax_File Continuejava



 2)_public class Continue extends_Stmt



 3) Stmt_stmt



 4)_public Continue()



 5) if( StmtNextFrom null_) error(unenclosed continue)



 6) stmt StmtNextFrom



 7)



 8)_public void_gen() emit(_goto L stmtgetnext() )



 9)

 flushleft

 footnotesize



 Class For will be discussed in

 Chapter intermediate-ch it's code is included for

 completeness



 footnotesize

 flushleft

 1)_package syntax_File Forjava



 2)_import symbols



 3)_public class For extends_Stmt



 4)_Expr expr1 expr2 expr3 Stmt_stmt int next 0



 5)_public For() expr1 expr2 expr3 null stmt null



 6)_public void init(Expr x1_Expr x2 Expr x3 Stmt s)



 7) expr1 x1 expr2 x2 expr3 x3 stmt s



 8) begin newlabel()



 9) if( expr3 null_) next newlabel()



 10) else next begin



 11) after newlabel()



 12)



 13) public_void gen()



 14) if( expr1 null_) expr1rvalue()



 15) emitlabel(begin)



 16) if( expr2 null_)



 17) Expr_x2 expr2rvalue()



 18) emit( iffalse x2toString() goto_L after)



 19)



 20) stmtgen()



 21) if( expr3 null_)



 22) emitlabel(next) expr3rvalue()



 23)



 24) emit(_goto L begin)



 25) emitlabel(after)



 26)



 27) public_int getnext() return next



 28)

 flushleft

 footnotesize





 Intermediate_Code for Expressions

 expr-java-subsect



 The class_hierarchy for expressions is shown in

 Fig expr-classes-fig Class Expr extends Node



 figurehtfb



 Class hierarchy for expression objects

 expr-classes-fig

 figure



 footnotesize

 flushleft

 1)_package syntax_File Exprjava



 2)_import lexer_import symbols



 3)_public class Expr extends Node



 4)_public Token op public_Type type



 5)_public Expr(Token tok Type typ) op tok type typ



 6)_public Expr rvalue() return this



 7) public_Expr prune() return rvalue()



 8)_public Expr lvalue()



 9) error(toString() is not an lvalue )_return null



 10)



 11) public_String toString()_return optoString()



 12)

 flushleft

 footnotesize



 Class Expr has two fields op and type (line_4)

 Field op holds a token that identifies the actual operator

 represented_by an object of class Expr Field type

 holds the type of the expression object The methods rvalue

 prune and lvalue are as in

 Section 3code-subsect where we discussed the generation of

 three-address_code for expressions



 The constructor in class Constant simply sets the fields

 op and type Constants include numbers which are

 known to have type integer the values true and false

 which are known to have type bool and so on



 footnotesize

 flushleft

 1)_package syntax_File Constantjava



 2)_import lexer_import symbols



 3)_public class Constant extends_Expr



 4)_public Constant(Token c Type t) super(c t)



 5)

 flushleft

 footnotesize



 The constructor in class Id also sets the fields op

 and type In_addition method lvalue returns this

 identifier since the identifier is used to refer to both its

 -value and -value



 footnotesize

 flushleft

 1)_package syntax_File Idjava



 2)_import lexer_import symbols



 3)_public class Id extends_Expr



 4)_public Id(Word id Type t) super(id t)



 5)_public Expr lvalue() return this



 6)

 flushleft

 footnotesize



 Class Temp is for compiler-generated_temporary names which

 have the form t1_t2 The static field count tracks the number of temporaries that have_already been

 generated field number tracks the number of this temporary

 name A temporary_name takes its type and lexical position from

 the value it holds On line 6 construct Temp(x) copies the

 type and lexical position of the parameter x



 footnotesize

 flushleft

 1)_package syntax_File Tempjava



 2)_import lexer_import symbols



 3)_public class Temp extends_Expr



 4) static_int count 0 int number 0



 5)_public Temp(Expr x)



 6) super(WordTemp xtype) number count lexline xlexline



 7)



 8)_public String_toString() return t IntegertoString(number)



 9)_public Expr lvalue() return this



 10)

 flushleft

 footnotesize



 Class Op is not meant to be used directly Hence its

 constructor is not public which means that it cannot be used from

 outside the package



 footnotesize

 flushleft

 1)_package syntax_File Opjava



 2)_import lexer_import symbols



 3)_public class Op extends_Expr



 4)_public Expr expr1 expr2



 5) Op(Token t Expr_x1 Expr_x2)



 6) super(tnull) expr1 x1 expr2 x2 type check(x1typex2type)



 7) if (type null_) error(type_error)



 8)



 9)_public Type_check(Type t1_Type t2) return_null overridden



 10)_public Expr rvalue()



 11) Expr_x prune() Temp t new Temp(x)



 12) emit( ttoString() xtoString() )



 13) return t



 14)



 15)_public String_toString()



 16) return expr1toString() optoString() expr2toString()



 17)



 18)

 flushleft

 footnotesize



 The subclasses of Op represent binary operators hence the

 two fields expr1 and expr2 (line_4) Method check is used on line 6 to both check the types of the operands

 and to compute the resulting type for this expression Each

 subclass of Op redefines check



 Class Arith implements arithmetic operators Note the use of

 Typemax in method check (line_5) to determine_whether

 the two operands can be coerced to a common numeric type For

 example when applied to an_integer and a floating_point value an

 arithmetic operator returns a floating_point result



 footnotesize

 flushleft

 1)_package syntax_File Arithjava



 2)_import lexer_import symbols



 3)_public class Arith extends_Op



 4)_public Arith(Token t Expr_x1 Expr_x2) super(t x1_x2)



 5)_public Type_check(Type t1_Type t2) return Typemax(t1 t2)



 6)_public Expr prune()



 7) Expr_x1 expr1rvalue() Expr_x2 expr2rvalue()



 8) if( x1 expr1 x2 expr2 )_return this



 9) return_new Arith(op x1_x2)



 10)



 11)

 flushleft

 footnotesize



 Class Rel is similar in spirit to Op It's type

 checking function check expects relational operators like

 and to be applied to basic types (not arrays)

 that are equal



 footnotesize

 flushleft

 1)_package syntax_File Reljava



 2)_import lexer_import symbols



 3)_public class Rel extends_Op



 4)_public Rel(Token t Expr_x1 Expr_x2) super(t x1_x2)



 5)_public Type_check(Type t1_Type t2)



 6) if_( t1 instanceof_Array t2 instanceof_Array )_return null



 7) else_if( t1_t2 )_return TypeBool



 8) else_return null



 9)



 10)_public Expr prune()



 11) Expr_x1 expr1rvalue() Expr_x2 expr2rvalue()



 12) if( x1 expr1 x2 expr2 )_return this



 13) return_new Rel(op x1_x2)



 14)



 15)



 flushleft

 footnotesize



 footnotesize

 flushleft

 1)_package syntax_File Accessjava



 2)_import lexer_import symbols



 3)_public class Access extends_Op



 4)_public Access(Token t Expr_x1 Expr_x2) super(t x1_x2)



 5)_public Type_check(Type t1_Type t2)



 6) if_( t1 instanceof_Array t2 TypeInt )_return ((Array)t1)of



 7) else_return null



 8)



 9)_public Expr prune() return lvalue()



 10)_public Expr lvalue()



 11) Expr_x1 expr1lvalue() Expr_x2 expr2rvalue()



 12) if( x1 expr1 x2 expr2 )_return this



 13) return_new Access(op x1_x2)



 14)



 15)_public String_toString()



 16) return expr1toString() expr2toString()



 17)



 18)

 flushleft

 footnotesize



 footnotesize

 flushleft

 1)_package syntax_File Assignjava



 2)_import lexer_import symbols



 3)_public class Assign extends_Op



 4)_public Assign(Token t Expr_x1 Expr_x2) super(t x1_x2)



 5)_public Type_check(Type t1_Type t2)



 6) if_( t1 instanceof_Array t2 instanceof_Array )_return null



 7) else if_( t1_t2 )_return t2



 8) else if_( Typenumeric(t1) Typenumeric(t2) )_return t2



 9) else_return null



 10)



 11) public_Expr rvalue()



 12) Expr_x1 expr1lvalue() Expr_x2



 13) if( x1 instanceof Id ) Id leaves two addresses for expr2



 14) x2 expr2prune()



 15) emit( x1toString() x2toString() )



 16) return x1



 17)



 18) x2 expr2rvalue() Assume one address for expr2 hence rvalue



 19) emit( x1toString() x2toString() )



 20) return x2



 21)



 22) public_Expr prune() return rvalue()



 23)



 flushleft

 footnotesize



 In the condition expression is

 evaluated only if evaluates to true The code for such

 conditions therefore has jumps in it The string condition

 determines whether the generated code is for or It signifies when evaluation of can be skipped it is

 false for and true for



 footnotesize

 flushleft

 1)_package syntax_File Condjava



 2)_import lexer_import symbols



 3)_public class Cond extends_Op



 4) int done 0 int after 0 String condition



 5)_public Cond(Token t Expr_x1 Expr_x2 String s)



 6) super(tx1x2) condition s done newlabel() after newlabel()



 7)



 8)_public Type_check(Type t1_Type t2)



 9) if_( t1 TypeBool t2 TypeBool_) return TypeBool



 10) else_return null



 11)



 12) public_Expr rvalue()



 13) Expr_x1 expr1rvalue()



 14) emit( if condition x1toString() goto_L done )



 15) Expr_x2 expr2prune() Temp t new Temp(x2)



 16) emit( ttoString() x2toString() )



 17) emit(_goto L after )



 18) emitlabel(done) emit( ttoString() condition)



 19) emitlabel(after)



 20) return t



 21)



 22)



 flushleft

 footnotesize



 Parser

 parser-java-subsect



 The parser determines the hierarchical_structure of the source

 program Alternatively the parser determines the objects in the

 abstract_syntax It reads a stream of tokens and calls the

 appropriate constructor functions from

 Sections stmt-java-subsect-expr-java-subsect



 The current symbol_table is maintained as in the translation

 scheme in Fig symtab-scheme-fig in

 Section symtab-sect Variable top (declared on line 5)

 holds the top symbol_table variable saved in method block (line 17) is a link to the previous symbol_table The same

 approach using variables StmtBreakFrom in class_Stmt

 and savedbreak (declared on line 43) is used to maintain the

 current enclosing looping construct for break statements We

 separately track the enclosing_loop for continue statements using

 StmtNextFrom and savednext (also declared on line

 43) In C and Java switch statements enclose break statements

 but not continue statements By separately tracking the enclosing

 loops for break and continue we prepare the way for the addition

 of switch statements to the source_language in the future



 As in Section_postfix-sect where we discussed a simple

 expression translator class_Parser has a procedure for each

 nonterminal The procedures are based_on a grammar formed_by

 removing left_recursion from the source-language grammar in

 Section source-gram-subsect



 footnotesize

 flushleft

 1)_package parser



 2)_import javaio_import lexer_import symbols import syntax



 3)_public class_Parser



 4) private Lexer lex private Token look



 5) Env top null



 6)_public Parser(Lexer l) throws_IOException lex l move()



 7) void move() throws_IOException look lexscan()



 8) void match(int t) throws_IOException



 9) if( looktok t ) move()



 10) else error(syntax error)



 11)



 12) void error(String s) throw_new Error(near line lexline s)



 13) public_void program() throws_IOException



 14) block()gen()



 15)



 16) Stmt block() throws_IOException



 17) match('') Env saved top top new Env(top)



 18) decls() Stmt s stmts()



 19) top saved match('')



 20) return s



 21)



 22) void decls() throws_IOException



 23) while(_looktok TagBASIC )



 24) Type typ type() Token_t look match(TagID) match('')



 25) topput( ttoString() new Id((Word)t typ) )



 26)



 27)



 28) Type type() throws_IOException



 29) Type typ (Type)look move()



 30) while(_looktok ''_)



 31) move() Token_t look match(TagNUM)



 32) typ new Array(((Num)t)value typ) match('')



 33)



 34) return typ



 35)



 36) Stmt stmts() throws_IOException



 37) Stmt s StmtNull



 38) while_( looktok_'' )



 39) s new Seq(s stmt())



 40) return s



 41)



 42) Stmt stmt() throws_IOException



 43) Expr_x Stmt s s1 s2 Stmt savedbreak savednext



 44) switch( looktok )



 45) case_''



 46) move()_return StmtNull



 47) case TagIF



 48) move() match('(')_x expr() match(')') s1_stmt()



 49) return_new If(x s1)



 50) case TagWHILE



 51) While whilenode new While()



 52) savedbreak StmtBreakFrom StmtBreakFrom whilenode



 53) savednext StmtNextFrom StmtNextFrom whilenode



 54) move() match('(')_x expr() match(')') s1_stmt()



 55) whilenodeinit(x s1)



 56) StmtBreakFrom savedbreak StmtNextFrom savednext



 57) return whilenode



 58) case TagDO



 59) Do donode new Do()



 60) savedbreak StmtBreakFrom StmtBreakFrom donode



 61) savednext StmtNextFrom StmtNextFrom donode



 62) move() s1_stmt() match(TagWHILE)



 63) match('(')_x expr() match(')') match('')



 64) donodeinit(s1 x)



 65) StmtBreakFrom savedbreak StmtNextFrom savednext



 66) return donode



 67) case TagFOR



 68) Expr y_z For fornode new For()



 69) savedbreak StmtBreakFrom StmtBreakFrom fornode



 70) savednext StmtNextFrom StmtNextFrom fornode



 71) move() match('(')_x optexpr()_match('')



 72) y optexpr()_match('') z optexpr() match(')')



 73) fornodeinit(x y_z stmt())



 74) StmtBreakFrom savedbreak StmtNextFrom savednext



 75) return fornode



 76) case_''



 77) return block()



 78) default



 79) x_expr() match('')



 80) return_new Eval(x)



 81) case TagBREAK



 82) move() match('')



 83) return_new Break()



 84) case TagCONTINUE



 85) move() match('')



 86) return_new Continue()



 87)



 88)



 89) Expr optexpr() throws_IOException



 90) if( looktok_'' looktok ')' )_return null



 91) else_return expr()



 92)



 93) Expr expr() throws_IOException



 94) Expr_x condition()



 95) if( looktok_'' )



 96) Token_t look_move() x new Assign(t x expr())



 97)



 98) return x



 99)



 100) Expr condition() throws_IOException



 101) Expr_x equals() Token_t



 102) for() switch( looktok )



 103) case TagAND



 104) t_look move()_x new Cond(txequals()false) continue



 105) case TagOR



 106) t_look move()_x new Cond(txequals()true) continue



 107) default



 108) return x



 109)



 110)



 111) Expr equals() throws_IOException



 112) Expr_x relation()



 113) while(_looktok TagEQ looktok TagNE )



 114) Token_t look_move() x new Rel(t x relation())



 115)



 116) return x



 117)



 118) Expr relation() throws_IOException



 119) Expr_x add() Token_t



 120) for() switch( looktok )



 121) case_'' case TagLE case TagGE case_''



 122) t_look move()_x new Rel(t x add()) continue



 123) default



 124) return x



 125)



 126)



 127) Expr add() throws_IOException



 128) Expr_x term()



 129) while(_looktok '' looktok '-' )



 130) Token_t look_move() x new Arith(t x term())



 131)



 132) return x



 133)



 134) Expr term() throws_IOException



 135) Expr_x unary()



 136) while(_looktok '' looktok_'' looktok '

 137) Token_t look_move() x new Arith(t x unary())



 138)



 139) return x



 140)



 141) Expr unary() throws_IOException



 142) if( looktok_'' )



 143) Token_t look_move() return_new Not(t unary())



 144)



 145) else_if( looktok '-' )



 146) move()_return new Minus(WordMinus unary())



 147)



 148) else_return access()



 149)



 150) Expr access() throws_IOException



 151) Expr_x factor()



 152) while(_looktok ''_)



 153) move()_x new Access(WordIndex x expr()) match('')



 154)



 155) return x



 156)



 157) Expr factor() throws_IOException



 158) Expr_x null



 159) switch( looktok )



 160) case '('



 161) move()_x expr() match(')') return x



 162) case TagNUM



 163) x new Constant(look TypeInt) break



 164) case TagTRUE case TagFALSE



 165) x new Constant(look TypeBool) break



 166) case TagREAL



 167) x new Constant(look TypeFloat) break



 168) case TagID



 169) String_s looktoString()



 170) x topget(s)



 171) if( x null_) error(s undeclared)



 172) break



 173) default



 174) return x



 175)



 176) move()



 177) return x



 178)



 179)

 flushleft

 footnotesize

 Recognition of Tokens

 token-rec-sect



 In the previous section we learned how to express patterns using regular

 expressions

 Now we must study how to take the patterns for all the needed tokens

 and build a piece of code that examines the input_string and finds a

 prefix that is a lexeme matching one of the patterns

 Our discussion will make use of the following running_example



 figurehtfb



 center

 tabularr_c l

 stmt if expr_then stmt



 if expr_then stmt_else stmt







 expr_term relop term



 term



 term id



 number



 tabular

 center



 A grammar for branching statements

 if-exp-grammar-fig



 figure



 ex

 token-rec-ex

 The grammar fragment of Fig if-exp-grammar-fig describes a simple

 form of branching statements and conditional expressions

 This syntax is similar to that of the language Pascal in that then appears explicitly after conditions

 For relop

 we use the comparison_operators of languages_like Pascal or SQL

 where is equals and is not equals because it

 presents an interesting structure of lexemes



 The terminals of the grammar which are if then else relop id and number are the names of tokens

 as far as the lexical_analyzer is concerned

 The patterns for these tokens are described using regular

 definitions as in Fig if-exp-tokens-fig

 The patterns for id and number are similar to what we saw in

 Example re-short-ex



 figurehtfb



 center

 tabularr_c l

 digit digits digit



 number digits ( digits)

 ( E - digits )



 letter A-Za-z



 id letter ( letter digit





 if if



 then then



 else else



 relop



 tabular

 center



 Patterns for tokens of Example token-rec-ex

 if-exp-tokens-fig



 figure



 For this language the lexical_analyzer will recognize the keywords if then and else as_well as lexemes that match the

 patterns for relop id and number

 To simplify matters we make the common assumption that keywords are

 also reserved_words that is they are not identifiers even

 though their lexemes match the pattern for identifiers



 In_addition we assign the lexical_analyzer the job of stripping out

 whitespace by recognizing the token ws defined by



 center

 ws ( blank tab newline

 center

 Here blank tab and newline are abstract symbols

 that we use to express the ASCII characters of the same

 names

 Token ws is different from the other tokens in that when we

 recognize it we do_not return it to the parser but rather restart the

 lexical analysis from the character that follows the whitespace

 It is the following token that gets returned to the parser



 Our_goal for the lexical_analyzer is summarized in

 Fig if-exp-token-fig

 That table shows for each lexeme or family of lexemes which token_name

 is returned to the parser and what attribute value as discussed in

 Section token-attr-subsect is returned

 Note_that for the six relational operators symbolic_constants LT

 LE and so on are used as the attribute value in order to

 indicate which instance of the token relop we have found

 The particular operator found will influence the code that is output

 from the compiler

 ex



 figurehtfb



 center

 tabularccc

 Lexemes Token Name Attribute

 Value



 Any ws - -



 if if -



 then then -



 else else -



 Any id_id Pointer to table entry



 Any number number Pointer to table entry



 relop LT



 relop LE



 relop EQ



 relop NE



 relop GT



 relop GE



 tabular

 center



 Tokens their patterns and attribute values

 if-exp-token-fig



 figure



 Transition Diagrams

 trans-diag-subsect



 As an intermediate step in the construction of a lexical_analyzer we

 first convert patterns into stylized flowcharts called transition

 diagrams

 In this_section we perform the conversion from regular-expression

 patterns to transition_diagrams by hand

 but in Section fa-sect we_shall

 see that there is a mechanical way to construct these diagrams from

 collections of regular_expressions



 Transition_diagrams have a collection of nodes or circles

 called states

 Each state represents a condition that could occur during the process of

 scanning the input looking for a lexeme that matches one of several

 patterns

 We may think of a state as summarizing all we need to know about what

 characters we have_seen between the lexemeBegin pointer

 and the forward pointer (as in the

 situation of Fig buffer1-fig)



 Edges are directed from one state of the transition_diagram to another

 Each edge is labeled by a symbol or set of symbols

 If we are in some state and the next_input symbol is we look

 for an edge out of state labeled by (and perhaps by other symbols

 as well)

 If we find such an

 edge we advance the forward pointer and enter the state

 of the transition_diagram to which that edge leads

 We_shall assume that all our transition_diagrams are deterministic meaning that there is never more_than one edge out of a

 given state with a given symbol among its labels

 Starting in Section lex-sect we_shall relax the condition of

 determinism making life much_easier for the designer of a lexical

 analyzer although trickier for the implementer

 Some important conventions about transition_diagrams are



 enumerate



 Certain states are said to be accepting or final

 These states indicate that a lexeme has_been found although the actual

 lexeme may not consist of all positions between the lexemeBegin

 and forward pointers

 We always indicate an accepting_state by a double circle and if there

 is an action to be taken - typically returning a token and an

 attribute value to the parser - we_shall attach that action to the

 accepting_state



 In_addition if it is necessary to retract the forward pointer one

 position (ie the lexeme does_not include the symbol that got us to

 the accepting state) then we_shall additionally place a near that

 accepting_state

 In our example it is never necessary to retract forward by more

 than one position but if it were we could attach any number of 's to

 the accepting_state



 One state is designated the start_state or initial_state

 it is indicated by an edge

 labeled start entering from nowhere

 The transition_diagram always begins in the start_state before any input

 symbols have_been read



 enumerate



 ex

 gt-ge-ex

 Figure relop-td-fig is a transition_diagram that recognizes the

 lexemes matching the token relop

 We begin in state 0 the start_state

 If we see as the first input_symbol then among the lexemes that

 match the pattern for relop we can only be looking_at

 or



 We therefore go to state 1 and look_at the next_character

 If it is then we recognize lexeme enter state 2

 and return the token relop with attribute LE the symbolic

 constant representing this particular comparison operator

 If in state 1 the next_character is then instead we have lexeme

 and enter state 3 to return an indication that the not-equals

 operator has_been found

 On any other character the lexeme is and we enter state 4 to

 return that information

 Note_however that state 4 has a to indicate that we must retract the

 input one position



 figurehtfb



 fileuullmanalsuch3figsrelop-tdeps

 Transition diagram for relop

 relop-td-fig

 figure



 On the other_hand if in state 0 the first character we see is then this one character must_be the lexeme

 We immediately return that fact from state 5

 The remaining possibility is that the first character is

 Then we must enter state 6 and decide on the basis of the next

 character whether the lexeme is (if we next see the sign) or just (on any other character)

 Note_that if in state 0 we see any character besides or we can not possibly be seeing a relop lexeme so this

 transition_diagram will not be used

 ex



 Recognition of Reserved Words and Identifiers

 res-words-subsect



 Recognizing keywords and identifiers presents a problem

 Usually keywords_like if or then are reserved (as they are

 in our_running example) so they are

 not identifiers even_though they look_like identifiers

 Thus although we typically use a transition_diagram like that of

 Fig id-td-fig to search for identifier lexemes this diagram will

 also recognize the keywords if then and else of our

 running_example



 figurehtfb

 fileuullmanalsuch3figsid-tdeps

 A transition_diagram for id's and keywords

 id-td-fig

 figure



 There_are two ways that we can handle reserved_words that look_like

 identifiers



 enumerate



 Install the reserved_words in the symbol_table initially

 A field of the symbol-table_entry indicates that these strings are never

 ordinary identifiers and tells which token they represent

 We have supposed that this method is in use in Fig id-td-fig

 When we find an_identifier a call to installID places it in the

 symbol_table if it is not already there and returns a pointer to the

 symbol-table_entry for the lexeme found

 Of_course any identifier not in the symbol_table during lexical

 analysis cannot be a reserved word so its token is id

 The function getToken examines the symbol_table entry for the

 lexeme found and returns whatever token_name the symbol_table says this

 lexeme represents - either id or one of the keyword tokens that was

 initially installed in the table



 Create separate transition_diagrams for each keyword an example for the

 keyword then is shown in Fig then-td-fig

 Note_that such a transition_diagram consists of states representing

 the situation after each successive letter of the keyword is seen

 followed_by a test for a nonletter-or-digit ie any character

 that cannot be the continuation of an_identifier

 It is necessary to check that the identifier has ended or else we would

 return token then in situations_where the correct token was id with a lexeme like thenextvalue that has then as a

 proper prefix

 If we adopt this approach then we must prioritize the tokens so that

 the reserved-word tokens are recognized in preference to id when

 the lexeme matches both patterns

 We do_not use this approach in our example which is why the

 states in Fig then-td-fig are unnumbered



 enumerate



 figurehtfb

 fileuullmanalsuch3figsthen-tdeps

 Hypothetical transition_diagram for the keyword then

 then-td-fig

 figure



 Completion of the Running Example

 td-complete-subsect



 The transition_diagram for id's that we saw in

 Fig id-td-fig has a simple structure

 Starting in state 9 it checks that the lexeme begins_with a letter and

 goes to state 10 if so

 We stay in state 10 as_long as the input contains letters and digits

 When we first encounter anything but a letter or digit we go to state 11

 and accept the lexeme found

 Since the last character is not part of the identifier we must retract

 the input one position and as discussed in Section res-words-subsect

 we enter what we have found in the symbol_table and determine_whether we

 have a keyword or a true identifier



 The transition_diagram for token number is shown in

 Fig num-td-fig and is so_far the most complex diagram we have_seen

 Beginning in state 12 if we see a digit we go to state 13

 In that state we can read any number of additional digits

 However if we see anything but a digit dot or E we have_seen a

 number in the form of an_integer 123 is an example

 That case is handled by entering state 20 where we return token number and a pointer to a table of constants where the found lexeme is

 entered

 These mechanics are not shown on the diagram but are analogous to the

 way we handled identifiers



 figurehtfb

 fileuullmanalsuch3figsnum-tdeps

 A transition_diagram for unsigned numbers

 num-td-fig

 figure



 If we instead see a dot in state 13 then we have an_optional fraction

 State 14 is entered and we look for one or_more additional digits

 state 15 is used for that purpose

 If we see an E then we have an_optional exponent whose

 recognition is the job of states 16 through 19

 Should we in state 15 instead see anything but E or a digit

 then we have come to the end of the fraction there is no exponent and

 we return the lexeme found via state 21



 The final transition_diagram shown in Fig ws-td-fig is for

 whitespace

 In that diagram we look for one or_more whitespace characters

 represented_by delim in that diagram - typically these

 characters would be blank tab newline and perhaps other characters

 that are not considered by the language design to be part of any token



 figurehtfb

 fileuullmanalsuch3figsws-tdeps

 A transition_diagram for whitespace

 ws-td-fig

 figure



 Note_that in state 24 we have found a block of consecutive whitespace

 characters followed_by a nonwhitespace character

 We retract the input to begin at the nonwhitespace but we do_not

 return to the parser

 Rather we must restart the process of lexical analysis after the

 whitespace



 Architecture of a Transition-Diagram-Based Lexical

 Analyzer

 td-arch-subsect



 There_are several ways that a collection of transition_diagrams can be

 used to build a lexical_analyzer

 Regardless of the overall strategy each state is represented_by a piece

 of code

 We may imagine a variable state holding the number of the current

 state for a transition_diagram

 A switch based_on the value of state takes us to code for each of

 the possible states where we find the action of that state

 Often the code for a state is itself a switch statement or multiway

 branch that determines the next state by reading and examining the next

 input_character



 ex

 relop-switch-ex

 In Fig relop-switch-fig we see a sketch of

 getRelop() a C function whose job is to simulate the transition

 diagram of Fig relop-td-fig and return an object of type TOKEN that is a pair consisting of the token_name (which must_be relop in this case) and an attribute value (the code for one of the six

 comparison_operators in this case)

 getRelop()

 first creates a new object retToken and initializes its first

 component to RELOP the symbolic code for token relop



 figurehtfb



 verbatim

 TOKEN getRelop()



 TOKEN retToken new(RELOP)

 while(1) repeat character processing until a return

 or failure occurs

 switch(state)

 case 0 c nextChar()

 if_( c ''_) state 1

 else if_( c ''_) state 5

 else if_( c ''_) state 6

 else fail() lexeme is not a relop

 break

 case 1



 case 8 retract()

 retTokenattribute GT

 return(retToken)







 verbatim



 Sketch of implementation of relop transition_diagram

 relop-switch-fig



 figure



 We see the typical behavior of a state in case 0 the case_where the

 current state is 0

 A function nextChar() obtains the next_character from the input and

 assigns it to local variable

 We then check for the three characters we expect to find making the

 state transition dictated by the transition_diagram of

 Fig relop-td-fig in each case

 For_example if the next_input character is we go to state 5



 If the next_input character is not one that can begin a comparison

 operator then a function fail() is called

 What fail() does depends_on the global

 error-recovery strategy of the lexical_analyzer

 It should reset the forward pointer to lexemeBegin in order

 to allow another transition_diagram to be applied to the true beginning

 of the unprocessed input

 It might then change the value of state to be the start_state for

 another transition_diagram which will search for another token

 Alternatively if there is no other transition_diagram that remains

 unused fail() could initiate an error-correction phase that will

 try to repair the input and find a lexeme as discussed in

 Section lex-error-subsect



 We also show the action for state 8 in Fig relop-switch-fig

 Because state 8 bears a we must retract the input

 pointer one position (ie put back on the input stream)

 That task is accomplished by the function retract()

 Since state 8 represents the recognition of lexeme we set the

 second_component of the returned object which we suppose is named attribute to GT the code for this operator

 ex



 To place the simulation of one transition_diagram in perspective let_us

 consider the ways code like Fig relop-switch-fig could fit into

 the entire lexical_analyzer



 enumerate



 We could arrange for the transition_diagrams for each token to be tried

 sequentially

 Then the function fail() of Example relop-switch-ex resets

 the pointer forward and starts the next transition_diagram each

 time it is called

 This method allows_us to use transition_diagrams for the individual

 keywords_like the one suggested in Fig then-td-fig

 We have only to use these before we use the diagram for id in

 order for the keywords to be reserved_words



 We could run the various transition_diagrams in parallel feeding

 the next_input character to all of them and allowing each one to make

 whatever transitions it required

 If we use this strategy we must_be careful to resolve the case_where

 one diagram finds a lexeme that matches its pattern while one or_more

 other diagrams are still able to process input

 The normal strategy is to take the longest_prefix of the input that

 matches any pattern

 That rule allows_us to prefer identifier thenext to keyword then or the operator to for example



 The preferred approach and the one we_shall take_up in the following

 sections is to combine all the transition_diagrams into one

 We allow the transition_diagram to read input until there is no possible

 next state and then take the longest lexeme that matched any pattern

 as we discussed in item (2) above

 In our_running example this combination is easy because no two tokens

 can start with the same character ie the first character immediately

 tells_us which token we are looking for

 Thus we could simply combine states 0 9 12 and 22 into one start

 state leaving other transitions intact

 However in general the problem of combining transition_diagrams for

 several tokens is more_complex as we_shall see shortly



 enumerate



 exer

 Provide transition_diagrams to recognize the same languages as each of

 the regular_expressions in Exercise re-list-exer

 exer



 exer

 Provide transition_diagrams to recognize the same languages as each of

 the regular_expressions in Exercise reg-def-exer

 exer



 The following exercises up to Exercise kmp-end-exer introduce

 the Aho-Corasick algorithm for recognizing a collection of

 keywords in a text_string in time proportional to the length of the

 text and the sum of the length of the keywords

 This algorithm uses a special form of transition_diagram

 called a trie A trie is a tree-structured transition_diagram

 with distinct labels on the edges leading_from a node to its_children

 Leaves of the trie represent recognized keywords



 Knuth Morris and Pratt presented an algorithm for recognizing

 a single keyword in a text_string

 Here the trie is a transition_diagram with states 0 through State

 0 is the initial_state and state represents acceptance that is discovery

 of the keyword From each state from 0 through there is a transition to

 state labeled by symbol For_example the trie for the keyword

 ababaa is



 fileuullmanalsuch3figstrie1eps



 In order to process text strings rapidly and search those strings for a

 keyword it is useful to define for keyword and position

 in that keyword (corresponding to state of its trie)

 a failure_function computed as in Fig failure-fn-fig

 The objective is that is the longest proper prefix of

 that is also a suffix of The_reason

 is important is that if we are trying to match a text_string for

 and we have matched the first positions but we then

 fail (ie the next position of the text_string does_not hold ) then

 is the longest_prefix of that could possibly match the

 text_string up to the point we are at Of_course the next_character of the

 text_string must_be or else we still have problems and must

 consider a yet shorter prefix which will be



 figurehtfb



 center

 tabularl_l

 1) 0



 2) 0



 3) for (_)



 4) while_( )



 5) if_( )



 6)



 7)







 8) else 0







 tabular

 center



 Algorithm to compute the failure_function for keyword



 failure-fn-fig



 figure



 As an example the failure_function for the trie constructed above for

 ababaa is



 center

 tabularccccccc

 1_2 3 4_5 6



 0_0 1_2 3 1



 tabular

 center

 For_instance states 3 and 1 represent prefixes aba and a

 respectively because a is the longest proper prefix of

 aba that is also a suffix of aba

 Also because the longest proper prefix of ab that is

 also a suffix is the empty_string



 exer

 kmp-begin-exer

 Construct the failure_function for the strings



 itemize



 a) abababaab

 b) aaaaaa

 c) abbaabb



 itemize

 exer



 hexer

 Prove by induction on that the algorithm of

 Fig failure-fn-fig correctly computes the failure_function

 hexer



 vhexer

 Show that the assignment

 in line_(4) of Fig failure-fn-fig

 is executed at most times Show that therefore the entire

 algorithm takes only time on a keyword of length

 vhexer



 Having computed the failure_function for a keyword we

 can scan a string in time to tell_whether the

 keyword occurs in the string The algorithm shown in

 Fig kmp-match-fig slides the keyword along the string trying to

 make progress by matching the next_character of the keyword with the

 next_character of the string If it cannot do so after matching

 characters then it slides the keyword right positions so

 only the first characters of the keyword are considered matched

 with the string



 figurehtfb



 center

 tabularl_l

 1) 0



 2) for (_)



 3)_while (_)



 4) if_( )



 5) if_( )_return yes







 6) return no



 tabular

 center



 The KMP algorithm tests whether string

 contains a single keyword as a substring in time

 kmp-match-fig



 figure



 exer

 Apply_Algorithm KMP to test whether keyword ababaa is a substring

 of



 itemize



 a) abababaab

 b) abababbaa



 itemize

 exer



 vhexer

 Show that the algorithm of Fig kmp-match-fig correctly tells

 whether the keyword is a substring of the given string

 Hint proceed by induction on Show that for all the

 value of after line_(4) is the length of the longest_prefix of the

 keyword that is a suffix of

 vhexer



 vhexer

 Show that the algorithm of Fig kmp-match-fig runs in time

 assuming that function is already_computed and its values

 stored in an array indexed by

 vhexer



 exer

 The Fibonacci strings are defined as_follows



 enumerate



 b

 a

 for



 enumerate

 For_example ab aba and abaab



 itemize



 a) What is the length of



 b) Construct the failure_function for



 c) Construct the failure_function for



 d) Show that the failure_function for any can be

 expressed by and for is

 where is the largest integer such that



 e) In the KMP algorithm what is the largest_number of

 consecutive applications of the failure_function when we try to

 determine_whether keyword appears in text_string



 itemize

 exer



 Aho and Corasick generalized the KMP algorithm to recognize any

 of a set of keywords in a text_string

 In this case the trie is a true tree with branching

 from the root There is one state for every string that is a prefix

 (not_necessarily proper) of any keyword The parent of a state

 corresponding to string is the state that corresponds

 to A state is accepting if it corresponds to a

 complete keyword For_example Fig trie2-fig shows the trie for

 the keywords he she his and hers



 figurehtfb

 fileuullmanalsuch3figstrie2eps

 Trie for keywords he she his hers

 trie2-fig

 figure



 The failure_function for the general trie is defined as_follows

 Suppose is the state that corresponds to string

 Then is the state that corresponds to the longest proper suffix

 of that is also a prefix of some keyword

 For_example the failure_function for the trie of Fig trie2-fig

 is



 center

 tabularcccccccccc

 1_2 3 4_5 6_7 8_9



 0_0 0_1 2 0 3 0 3



 tabular

 center



 hsexer

 gen-failure-fn-exer

 Modify the algorithm of Fig failure-fn-fig to compute the failure

 function for general tries Hint The major difference is that

 we cannot simply test for equality or inequality of and

 in lines_(4) and (5) of Fig failure-fn-fig Rather

 from any state there may be several transitions out on several

 characters as there are transitions on both e and i from

 state 1 in Fig trie2-fig Any of those transitions could lead to

 a state that represents the longest suffix that is also a prefix

 hsexer



 exer

 Construct the tries and compute the failure_function for the following

 sets of keywords



 itemize



 a)

 aaa abaaa and ababaaa



 b)

 all fall fatal llama and lame



 c)

 pipe pet item temper and perpetual



 itemize

 exer



 hexer

 kmp-end-exer

 Show that your algorithm from Exercise gen-failure-fn-exer still runs

 in time that is linear in the sum of the lengths of the keywords

 hexer

 Top-Down Parsing

 top-down-sect



 Top-down parsing can be_viewed as an attempt to construct a parse

 tree for the input_string starting from the root and creating the

 nodes of the parse_tree in preorder that is depth-first from

 left to right The sequence of parse_trees constructed in

 Fig parse-tree-seq-fig for the input -(id

 id) is in fact a top-down parse Equivalently

 top-down_parsing can be_viewed as an attempt to find a leftmost

 derivation for an input_string



 The key problem during top-down_parsing is that of determining the

 production to be applied for a nonterminal at each step of a

 parse For nonterminal once an -production is chosen the

 rest of the parsing problem consists of matching the symbols on

 the right_side with the input_string as in the following

 pseudo-code (note that this pseudo-code is nondeterministic since

 it begins by choosing the -production to apply)



 tabbing





 procedure A()



 Choose an -production



 for from to



 if is a nonterminal then



 call procedure ()



 else if equals the current_input symbol

 then



 advance the input to the next symbol



 else



 error







 return





 tabbing

 This pseudo-code is part of a parsing program that consists of a

 set of procedures one for each nonterminal Execution begins

 with the procedure for the start_symbol which halts and announces

 success if its procedure body scans the entire input_string



 This_section begins_with a general form of top-down_parsing

 called recursive-descent_parsing which may require backtracking

 to find the right -production to be applied Section OLD24

 introduced predictive_parsing a special_case of recursive-descent

 parsing where no backtracking is required Predictive parsing

 chooses the right -production by_looking ahead at the input

 Efficient predictive_parsers can be constructed for the class of

 grammars called LL(1)_grammars



 This_section concludes with a discussion of error_recovery

 Recursive-Descent Parsing

 General recursive-descent may require backtracking that is it

 may require repeated scans over the input However backtracking

 is rarely needed to parse programming_language constructs so

 backtracking parsers are not seen frequently Even for situations

 like natural language parsing backtracking is not very efficient

 and tabular methods such_as the dynamic_programming algorithm of

 Exercise OLD 463 or the method of Earley 1970 are preferred



 Backtracking is required in the next example and we_shall suggest

 a way of keeping_track of the input when backtracking takes place



 ex

 Consider the grammar

 disp

 tabularr_c l







 tabular

 disp

 and the input_string To construct a parse_tree for

 this string top-down begin_with a tree consisting of a single

 node_labeled An input pointer points to the first

 symbol of has only one production so we use it to

 expand and obtain the tree of Fig backtrack-fig(a)



 figurehtfb

 center



 Steps in a top-down parse

 backtrack-fig

 center

 figure



 The leftmost leaf_labeled matches the first symbol of input

 so advance the input pointer to the second symbol of

 and consider the next leaf_labeled



 Now expand using the first alternative to

 obtain the tree of Fig backtrack-fig(b) We have a match

 for the second input_symbol so advance the input pointer to

 the third input_symbol and compare against the next leaf

 labeled Since does_not match we report failure and

 go back to to see whether there is another alternative for

 that has not been tried but that might produce a match



 In going back to we must reset the input pointer to position

 2 the position it had when we first came to which means that

 the procedure for (analogous to the procedures for

 nonterminals in Fig OLD217) must store the input pointer in a

 local variable



 The second alternative for produces the tree of

 Fig backtrack-fig(c) The leaf matches the second

 symbol of and the leaf matches the third symbol Since we

 have produced a parse_tree for we halt and announce

 successful_completion of parsing

 ex



 A left_recursive grammar can cause a recursive-descent_parser

 even one with backtracking to go into an_infinite loop That is

 when we try to expand a nonterminal we may eventually find

 ourselves again trying to expand without_having consumed any

 input



 FIRST and FOLLOW first-follow-subsect



 The construction of both top-down and bottom-up parsers is aided

 by two functions and associated_with a grammar

 During top-down_parsing is used to choose which

 production to apply based_on the next_input symbol During

 panic-mode error_recovery sets of tokens yielded by the

 function can be used as synchronizing_tokens



 Define () where is any string of

 grammar_symbols to be the set of terminals that begin strings

 derived_from If then

 is also in () For_example in

 Fig first-follow-fig so is

 in ()



 figurehtfb

 center



 Terminal is in FIRST() and is in FOLLOW()

 first-follow-fig

 center

 figure



 For a preview of how can be used during predictive

 parsing consider two -productions

 where () and () are disjoint

 sets We can then choose between these -productions by_looking

 at the next_input symbol since can be in at most one of

 () and () not both If is in

 () choose the production This

 idea will be explored when LL(1)_grammars are defined



 Define () for nonterminal to be the set of

 terminals that can appear immediately to the right of in

 some sentential_form that is the set of terminals such that

 there_exists a derivation of the form

 for some and as in

 Fig first-follow-fig Note_that there may have_been

 symbols between and at some time during the derivation

 but if so they derived and disappeared If can be

 the rightmost symbol in some sentential_form then is in

 ()



 To_compute () for all grammar_symbols apply the

 following rules until_no more terminals or can be added

 to any set

 enumerate



 If is a terminal then () is



 If is a production then add

 to ()



 If is a nonterminal and is a

 production then place in () if for some is

 in () and is in all of

 () () that is

 If is in ()

 for all then add to () For

 example everything in () is surely in () If

 does_not derive then we add nothing more to

 () but if then we add

 () and so on

 enumerate



 Now we can compute for any string

 as_follows Add to () all non-

 symbols of () Also add the non-_symbols of

 () if is in () the

 non-_symbols of () if is in

 () and () and so on Finally add

 to () if for all

 is in ()



 To_compute () for all nonterminals apply the

 following rules until nothing can be added to any set

 enumerate



 Place in () where is the start_symbol and is

 the input right_endmarker



 If there is a production then

 everything in () except is in

 ()



 If there is a production or a production

 where () contains

 then everything in () is in ()

 enumerate



 exfirst-follow-ex

 Consider_again grammar_(topdown-expr-display) repeated

 here

 disp

 tabularl_c l

 E_T E



 E_T E



 T_F T



 T_F T



 F ( E

 ) id

 tabular

 disp

 Then

 disp

 (E) (T) (F)

 ( id



 (E)



 (T)



 (E) (E) )



 (T) (T)

 )



 (F) )

 disp

 ex





 LL(1) Grammars

 Predictive parsers that is recursive-descent parsers that need

 no backtracking can be constructed for a class of grammars called

 LL(1) The first L in LL(1) stands_for scanning the input from

 left to right the second L for producing a leftmost

 derivation and the 1 for using one input_symbol of lookahead

 at each step to make parsing_action decisions



 LL(1)_grammars have several distinctive properties No ambiguous

 or left_recursive grammar can be LL(1) It can be shown that a

 grammar is LL(1) if and only if whenever

 are two distinct productions of the following

 conditions hold

 enumerate



 For no terminal do both and derive strings

 beginning with



 At most one of and can derive the empty_string



 If can derive then does_not derive

 any string beginning with a terminal in ()

 enumerate



 The first two conditions are equivalent to the statement that

 () and () are disjoint_sets The third

 condition is equivalent to stating that if is in

 () then () and () are

 disjoint_sets



 Predictive parsers can be constructed for LL(1)_grammars since

 the proper -production to apply can be selected by_looking only

 at the current_input symbol That is given the current_input

 symbol and the nonterminal we can select the unique

 production to apply to derive a string beginning with



 Flow-of-control constructs in most programming_languages with

 their distinguishing keywords are usually detectable in this way

 For_example if we have the productions

 disp

 tabularr_c l

 stmt if expr_then

 stmt_else stmt



 while expr do stmt



 begin stmtlist end

 tabular

 disp

 then the keywords if while and begin tell_us

 which alternative is the only one that could possibly succeed if

 we are to find a statement



 In many_cases by carefully writing a grammar eliminating left

 recursion from it and left_factoring the resulting grammar we

 can obtain an LL(1)_grammar Unfortunately there are some

 grammars for which no amount of alteration will lead to an LL(1)

 grammar The abstract dangling-else_grammar

 (dangling-gram-display) is one such example its language

 has no LL(1)_grammar at all Parser generators therefore use the

 more general bottom-up LR_parsing techniques discussed later in

 this_chapter



 Transition Diagrams for Predictive Parsers

 Hand-written parsers are typically predictive_parsers and

 transition_diagrams are useful for planning them Transition

 diagrams can be used whether or not a grammar is LL(1) If it is

 LL(1) then and sets can be used to guide

 parsing_decisions



 Transition_diagrams for predictive_parsers differ from those for

 lexical_analyzers in several ways Parsers have one diagram for

 each nonterminal The labels of edges can be tokens or

 nonterminals A transition on a token (terminal) means that we

 take that transition if that token is the next_input symbol A

 transition on a nonterminal is a call of the procedure for





 To construct the transition_diagram from a grammar first

 eliminate_left recursion from the grammar and then left factor the

 grammar Then for each nonterminal do the following

 enumerate



 Create an initial and final (return) state



 For each production create a

 path from the initial to the final state with edges labeled

 If the

 path is an edge labeled

 enumerate



 A predictive_parser based_on a transition_diagram attempts to

 match terminal_symbols against the input and makes a potentially

 recursive_procedure call whenever it has to follow an edge labeled

 with a nonterminal This_approach works if the transition_diagram

 is deterministic that is there is only one possible transition

 from a state on the same input If more_than one transition is

 possible we may be_able to resolve the nondeterminism as in the

 next example



 extransition-diagrams-ex

 Figure transition-diagrams-fig contains the transition

 diagrams for nonterminals E and E from the

 nonrecursive expression grammar_(topdown-expr-display) The

 transition_diagrams for nonterminals T and T are

 analogous to those for E and E The diagrams for

 F will be given shortly



 figurehtfb

 center



 Transition_diagrams for nonterminals E and

 E of Example transition-diagrams-ex

 transition-diagrams-fig

 center

 figure



 The only ambiguities with this grammar concern whether or not to

 take an -edge In the initial_state for E

 if we take the transition whenever that is the next_input

 and take the transition on otherwise and make the

 analogous assumption for T then the ambiguity is

 removed and we can write a predictive_parsing program for this

 grammar



 This grammar is in fact LL(1) so it is safe to remove ambiguities

 by making the -transitions the default transitions

 ex



 Transition_diagrams can be simplified For_example in

 Fig simplify-transitions-fig(a) the call of E

 on itself has_been replaced_by a jump to the beginning of the

 diagram for E Figure

 simplify-transitions-fig(b) shows an equivalent transition

 diagram for E



 figurehtfb

 center



 Simplified transition_diagrams

 simplify-transitions-fig

 center

 figure



 Diagrams may be substituted in one another We may substitute the

 diagram for E in Fig simplify-transitions-fig(b)

 into that for E in Fig transition-diagrams-fig to get

 the diagram in Fig simplify-transitions-fig(c) which

 further simplifies to the diagram in

 Fig simplify-transitions-fig(d)



 The complete set of resulting diagrams is shown in

 Fig expr-transitions-fig A C implementation of this

 predictive_parser runs 20-25 faster_than a C implementation

 based_on Fig transition-diagrams-fig



 figurehtfb

 center



 Simplified transition_diagrams for arithmetic_expressions

 expr-transitions-fig

 center

 figure



 Table-Driven Predictive_Parsing

 It is possible to build a nonrecursive predictive_parser by

 maintaining a stack explicitly rather_than implicitly via

 recursive_calls The idea in terms of leftmost_derivations is

 to use the stack to hold the grammar_symbols of a

 sentential_form



 Again the key problem during predictive_parsing is that of

 determining the production to be applied for a nonterminal The

 nonrecursive parser in Fig nonrec-model-fig looks up the

 production to be applied in a parsing_table



 figurehtfb

 center



 Model of a nonrecursive predictive_parser

 nonrec-model-fig

 center

 figure



 A table-driven predictive_parser has an input_buffer a stack a

 parsing_table and an output stream The input_buffer contains

 the string to be_parsed followed_by a symbol used as a right

 endmarker to indicate the end of the input_string The stack

 contains a sequence of grammar_symbols with at the bottom

 indicating the bottom of the stack Initially the stack contains

 the start_symbol of the grammar on top of The parsing_table

 is a two-dimensional_array where is a nonterminal

 and is a terminal or the symbol



 The parser is controlled by a program that considers the

 symbol on top of the stack and the current_input symbol

 These two symbols determine the action of the parser There_are

 three possibilities

 enumerate



 If the parser halts and announces successful

 completion of parsing



 If the parser pops off the stack and advances

 the input pointer to the next_input symbol



 If is a nonterminal the parser consults entry of the

 parsing_table This entry will be either an -production of

 the grammar or an error entry If for example

 the parser replaces on top of the stack by

 (with on top) As output we_shall assume that the

 parser just prints the production used any other code could be

 executed here If error the parser calls an

 error_recovery routine

 enumerate



 The behavior of the parser can be described in terms of its

 configurations which give the stack_contents and the remaining

 input



 algpredictive-alg Nonrecursive predictive_parsing





 A string and a parsing_table for grammar





 If is in a leftmost_derivation of

 otherwise an error indication





 Initially the parser is in a configuration with in

 the input_buffer and on the stack where the start

 symbol of is on the top of the stack The program in

 Fig predictive-fig uses the predictive_parsing table to

 produce a predictive parse for the input

 alg



 figurehtfb

 tabbing

 set to point to the first symbol of



 set to the top stack symbol



 while stack is not empty





 if is then pop the stack

 and advance



 else if is a terminal then error()



 else if is an error entry then error()



 else if

 then



 output the production





 pop the stack



 push_onto the stack with

 on top







 set to the top stack symbol





 tabbing

 Predictive parsing algorithmpredictive-fig

 figure



 exexpr-pred-parsing-ex

 A predictive_parsing table for the nonrecursive expression grammar

 (topdown-expr-display) appears in

 Fig expr-pred-parsing-fig Blanks are error_entries

 non-blanks indicate a production with which to expand the top

 nonterminal on the stack Note_that we have not_yet indicated how

 these entries could be selected but we_shall do so shortly



 With input ididid the predictive

 parser makes the sequence of moves in

 Fig pred-parsing-moves-fig The top of the stack is to the

 right The input pointer points to the leftmost symbol of the

 string in the INPUT column



 figurehtfb

 center

 small

 tabularcc_c c_c c_c



 NON- 6c0pt6ptINPUT SYMBOL



 2-7

 TERMINAL 0pt6 ptid (_)





 E ETE ETE



 E ETE E_E



 T TFT TFT



 T_T TFT T_T



 F Fid F(E)





 tabular

 small

 center

 Parsing_table for Example

 expr-pred-parsing-exexpr-pred-parsing-fig

 figure



 The productions output by this parser are in fact those of a

 leftmost_derivation The left-sentential forms of the derivation

 consist of the input symbols that have_already been scanned

 followed_by the grammar_symbols on the stack

 ex



 figurehtfb

 center

 tabularl r_l c_l

 1ptSTACK INPUT 3lOutput



 1ptE ididid



 ET ididid ETE



 ETF ididid TFT



 ETid ididid Fid



 ET idid



 E idid T



 E'T idid ETE



 E'T idid



 ETF idid TFT



 ETid idid Fid



 ET id



 ETF id TFT



 ETF id



 ETid id Fid



 ET



 E_T



 E





 tabular

 center

 Moves made by a predictive_parser on input id

 ididpred-parsing-moves-fig

 figure



 Construction of Predictive_Parsing Tables

 The following algorithm can be used to construct a predictive

 parsing_table for a grammar The idea_behind the algorithm is

 the following Suppose is a production

 with in () Then the parser will expand by

 when the current_input symbol is The only

 complication occurs_when or

 In this case we should again expand

 by if the current_input symbol is in () or if

 the on the input has_been reached and is in

 ()



 algpred-table-alg Construction of a predictive

 parsing_table





 Grammar





 Parsing_table





 enumerate





 For each production of the grammar do

 steps 2 and 3





 For each terminal in () add

 to





 If is in () then for each terminal

 in () add to If

 is in () and is in () add

 to as_well





 Set each undefined entry of to error

 enumerate

 alg



 ex

 On the expression grammar_(topdown-expr-display) Algorithm

 pred-table-alg produces the parsing_table in

 Fig expr-pred-parsing-fig



 Consider production ETE Since

 (TE)(T)(

 id this production is added to E

 ( and Eid



 Production ETE is

 added to E Since (

 E)) production

 E is added to

 E) and E

 ex



 It can be shown that Algorithm pred-table-alg produces for

 every LL(1)_grammar a parsing_table that parses all and only the

 sentences of The algorithm can be applied to any grammar

 to produce a parsing_table however for some grammars

 may have some entries that are multiply defined For_example if

 is left_recursive or ambiguous then will have at_least

 one multiply-defined entry Parsing tables for LL(1)_grammars

 have no multiply-defined entries



 The main difficulty in using predictive_parsing is in writing a

 suitable grammar for the source_language Although left-recursion

 elimination and left_factoring are easy to do there are some

 grammars for which no amount of alteration will produce an LL(1)

 grammar The language in the following example has no LL(1)

 grammar at all



 exambig-parsing-table-ex

 The following grammar which abstracts the dangling-else problem

 is repeated_here from Example left-factoring-ex

 disp

 tabularl_c l











 tabular

 disp

 The parsing_table for this grammar appears in

 Fig ambig-parsing-table-fig The entry for

 contains both and



 figurehtfb

 center

 tabularccccccc

 -3ptnon- 6cInputSymbol



 2-7

 TERMINAL -2pt



 -2pt



 0pt0pt0pt







 -2pt





 tabular

 center

 Parsing_table for Example

 expr-pred-parsing-exambig-parsing-table-fig

 figure



 The grammar is ambiguous and the ambiguity is manifested by a

 choice in what production to use when an (else) is seen

 We can resolve this ambiguity by choosing

 This choice corresponds to associating else's with the

 closest previous then's Note_that the choice

 would prevent from ever being put on the

 stack or removed_from the input and is surely wrong

 ex



 Error_Recovery in Predictive_Parsing



 The stack of a nonrecursive predictive_parser makes explicit the

 terminals and nonterminals that the parser hopes to match with the

 remainder of the input The following discussion of error

 recovery therefore refers to symbols on the parser stack An

 error is detected during predictive_parsing when the terminal on

 top of the stack does_not match the next_input symbol or when

 nonterminal is on top of the stack is the next_input

 symbol and the parsing_table entry is empty



 Panic-mode error_recovery is based_on the idea of skipping symbols

 on the the input until a token in a selected set of synchronizing

 tokens appears Its effectiveness depends_on the choice of

 synchronizing_set The sets should be chosen so that the parser

 recovers quickly from errors that are likely to occur in practice

 Some heuristics are as_follows



 enumerate





 As a starting point place all symbols in () into the

 synchronizing_set for nonterminal If we skip tokens until an

 element of () is seen and pop from the stack it is

 likely that parsing can continue





 It is not enough to use () as the synchronizing_set for

 For_example if semicolons terminate statements as in C

 then keywords that begin statements may not appear in the

 set of the nonterminal generating expressions A missing semicolon

 after an assignment may therefore result in the keyword beginning

 the next statement being skipped Often there is a hierarchical

 structure on constructs in a language eg expressions appear

 within statements which appear within blocks and so on We can

 add to the synchronizing_set of a lower construct the symbols that

 begin higher constructs For_example we might add keywords that

 begin statements to the synchronizing sets for the nonterminals

 generating expressions





 If we add symbols in () to the synchronizing_set for

 nonterminal then it may be possible to resume parsing

 according to if a symbol in () appears in the input





 If a nonterminal can generate the empty_string then the

 production deriving can be used as a default Doing_so

 may postpone some error_detection but cannot cause an error to be

 missed This_approach reduces the number of nonterminals that have

 to be considered during error_recovery





 If a terminal on top of the stack cannot be matched a simple idea

 is to pop the terminal issue a message saying that the terminal

 was inserted and continue parsing In effect this approach takes

 the synchronizing_set of a token to consist of all other tokens

 enumerate



 ex

 Using and symbols as synchronizing_tokens works

 reasonably well when expressions are parsed according to the usual

 grammar_(topdown-expr-display) The parsing_table for this

 grammar in Fig expr-pred-parsing-fig is repeated in

 Fig topdown-recovery-table-fig with synch indicating

 synchronizing_tokens obtained from the set of the

 nonterminal in question The sets for the nonterminal

 are obtained from Example first-follow-ex



 figurehtfb

 center

 small

 tabularcc_c c_c c_c



 NON- 6c0pt6ptINPUT SYMBOL



 2-7

 TERMINAL 0pt6 ptid (_)





 E ETE ETE synch_synch



 E ETE E_E



 T TFT synch

 TFT synch_synch



 T_T TFT T_T



 F Fid synch_synch F(E) synch_synch





 tabular

 small

 Synchronizing tokens added to the parsing_table of

 Fig expr-pred-parsing-figtopdown-recovery-table-fig

 center

 figure



 The table in Fig topdown-recovery-table-fig is to be used

 as_follows If the parser looks up entry and finds that

 it is blank then the input_symbol is skipped If the entry is

 synch then the nonterminal on top of the stack is popped in an

 attempt to resume parsing If a token on top of the stack does_not

 match the input_symbol then we pop the token from the stack as

 mentioned above



 On the erroneous_input )idid the parser

 and error_recovery mechanism of

 Fig topdown-recovery-table-fig behave as in

 Fig pred-recovery-moves-fig

 ex



 figurehtfb

 center

 tabularl r_l



 STACK_INPUT REMARK



 -2ptE )idid error skip )



 E idid id is in (E)



 ET idid



 ETF idid



 ETid idid



 ET id



 ETF id



 ETF id error F synch



 ET id F has_been popped



 E_id



 ET id



 ET id



 ETF id



 ETid id



 ET



 E









 tabular

 center

 Parsing and error_recovery moves made by a predictive

 parserpred-recovery-moves-fig

 figure



 The above discussion of panic-mode recovery does_not address the

 important issue of error_messages In_general informative error

 messages have to be supplied_by the compiler_designer



 Phrase-level recovery Phrase-level recovery is implemented

 by filling in the blank entries in the predictive_parsing table

 with pointers to error_routines These routines may change

 insert or delete symbols on the input and issue appropriate error

 messages They may also pop from the stack Alteration of stack

 symbols or the pushing of new symbols onto the stack is

 questionable for a couple of reasons First the steps carried

 out by the parser might then not correspond to the derivation of

 any word in the language at all Second we must ensure_that there

 is no possibility of an_infinite loop Checking that any recovery

 action eventually results in an input_symbol being consumed (or

 the stack being shortened if the end of the input has_been

 reached) is a good way to protect against such loops

 Introduction to Trace-Based Collection

 trace-sect



 Instead of collecting garbage as it is created trace-based

 collectors run periodically to find unreachable_objects and

 reclaim their space Typically we run the trace-based collector

 whenever the free_space is exhausted or its amount drops below

 some threshold



 We begin this_section by introducing the simplest

 mark-and-sweep garbage_collection algorithm We then

 describe the variety of trace-based algorithms in terms of four

 states that chunks of memory can be put in This_section also

 contains a number of improvements on the basic algorithm

 including those in which object relocation is a part of the

 garbage-collection function



 A Basic Mark-and-Sweep Collector

 secgcsimple



 Mark-and-sweep garbage-collection algorithms are

 straightforward stop-the-world algorithms that find all the

 unreachable_objects and put them on the list of free_space

 Algorithm_algmark-sweep visits and marks all the

 reachable_objects in the first tracing step and then sweeps

 the entire heap to free up unreachable_objects

 Algorithm algmod-mark-sweep which we consider after

 introducing a general framework for trace-based algorithms is an

 optimization of Algorithm_algmark-sweep By using an

 additional list to hold all the allocated_objects it visits the

 reachable_objects only once



 alg

 algmark-sweep

 Mark-and-sweep garbage_collection



 A root_set of objects a heap and a free_list called

 Free with all the unallocated chunks of the heap As in

 Section alloc-frag-subsect all chunks of space are marked

 with boundary_tags to indicate their freeused status and size



 A modified Free_list after all the garbage has_been removed



 figurehtb

 center

 tabularr_l

 marking phase



 1) add each object referenced by the root_set to list Unscanned



 and set its reached-bit to 1



 2) while (Unscanned )



 3) remove some object from Unscanned



 4) for_(each object referenced in )



 5) if_( is unreached ie its reached-bit is 0)



 6) set the reached-bit of to 1



 7) put in Unscanned















 sweeping phase



 8) Free



 9) for_(each chunk of memory in the heap)



 10) if_( is unreached ie its reached-bit

 is 0)

 add to Free



 11) else

 set the reached-bit of to 0







 tabular

 center

 A Mark-and-Sweep Garbage Collector

 figmark-and-sweep

 figure



 The algorithm shown in Fig figmark-and-sweep

 uses several simple data_structures List Free holds objects

 known to be free A list called Unscanned holds objects

 that we have determined are reached but whose successors we have

 not_yet considered That is we have not scanned these objects to

 see what other objects can be reached through them The Unscanned_list is empty initially Additionally each object

 includes a bit to indicate whether it has_been reached (the reached-bit) Before the algorithm begins all allocated_objects

 have the reached-bit set to 0



 In line_(1) of Fig figmark-and-sweep we initialize the

 Unscanned_list by placing there all the objects referenced

 by the root_set The reached-bit for these objects is also set to

 1 Lines (2) through_(7) are a loop in which we in turn examine

 each object that is ever placed on the Unscanned_list



 The for-loop of lines_(4) through_(7) implements the scanning of

 object We examine each object for which we find a

 reference within If has already_been reached (its

 reached-bit is 1) then there is no need to do anything about

 it either has_been scanned previously or it is on the Unscanned_list to be scanned later However if was not

 reached already then we need to set its reached-bit to 1 in

 line_(6) and add to the Unscanned_list in line_(7)

 Figure mark-sweep-fig illustrates this process It shows an

 Unscanned_list with four objects The first object on this

 list corresponding to object in the discussion_above is in

 the process of being scanned The dashed lines correspond to the

 three kinds of objects that might be reached from



 enumerate



 A previously scanned object that need not be scanned again



 An object currently on the Unscanned_list



 An item that is reachable but was previously thought to be

 unreached



 enumerate



 figurehtfb





 The relationships among objects during the marking phase

 of a mark-and-sweep garbage_collector mark-sweep-fig

 figure



 Lines (8) through (11) the sweeping phase reclaim the space of

 all the objects that remain unreached at the end of the marking

 phase Note_that these will include any objects that were on the

 Free_list originally Because the set of unreached objects

 cannot be enumerated directly the algorithm sweeps through the

 entire heap Line (10) puts free and unreached objects on the Free_list one at a time Line (11) handles the reachable

 objects We set their reached-bit to 0 in order to maintain the

 proper preconditions for the next execution of the

 garbage-collection algorithm

 alg



 Basic Abstraction

 secgcabstraction



 All trace-based algorithms compute the set of reachable_objects

 and then take the complement of this set Memory is therefore

 recycled as_follows



 itemize



 a) The program or mutator runs and makes allocation

 requests



 b) The garbage_collector discovers reachability by tracing



 c) The garbage_collector reclaims the storage for

 unreachable_objects



 itemize



 This cycle is illustrated in Fig figmemory-cycle

 in terms of four states for chunks of memory Free Unreached Unscanned and Scanned The state of a

 chunk might be stored in the chunk itself or it might be implicit

 in the data_structures used by the garbage-collection algorithm



 figurehtb





 States of memory in a garbage_collection cycle

 figmemory-cycle

 figure



 While trace-based algorithms may differ in their implementation

 they can all be described in terms of the following states



 enumerate



 Free A chunk is in the Free state if it is

 ready to be allocated Thus a Free chunk must not hold a

 reachable object



 Unreached Chunks are presumed unreachable unless

 proven reachable by tracing A chunk is in the Unreached

 state at any point during garbage_collection if its reachability

 has not_yet been established Whenever a chunk is allocated by

 the memory_manager its state is set to Unreached as

 illustrated in Fig figmemory-cycle(a) Also after a round

 of garbage_collection the state of a reachable object is reset to

 Unreached to get ready for the next round see the

 transition from Scanned to Unreached which is shown

 dashed to emphasize that it prepares for the next round



 Unscanned Chunks that are known to be reachable are

 either in state Unscanned or state Scanned A chunk

 is in the Unscanned_state if it is known to be reachable

 but its pointers have not_yet been scanned The transition to

 Unscanned from Unreached occurs_when we discover that

 a chunk is reachable see Fig figmemory-cycle(b)



 Scanned Every Unscanned object will_eventually

 be scanned and transition to the Scanned state To scan an object we examine each of the pointers within it and

 follow those pointers to the objects to which they refer If a

 reference is to an Unreached object then that object is put

 in the Unscanned_state When the scan of an object is

 completed that object is placed in the Scanned state see

 the lower transition in Fig figmemory-cycle(b) A Scanned object can only contain references to other Scanned

 or Unscanned objects and never to Unreached objects



 enumerate



 When no objects are left in the Unscanned_state the

 computation of reachability is complete Objects left in the Unreached state at the end are truly unreachable The garbage

 collector reclaims the space they occupy and places the chunks in

 the Free state as illustrated by the solid transition in

 Fig figmemory-cycle(c) To get ready for the next cycle of

 garbage_collection objects in the Scanned state are

 returned to the Unreached state see the dashed transition

 in Fig figmemory-cycle(c) Again remember that these

 objects really are reachable right now The Unreachable

 state is appropriate because we_shall want to start all objects

 out in this state when garbage_collection next begins by which

 time any of the currently reachable_objects may indeed have_been

 rendered unreachable



 ex

 Let_us see_how the data_structures of

 Algorithm_algmark-sweep relate to the four states

 introduced above Using the reached-bit and membership on lists

 Free and Unscanned we can distinguish_among all four

 states The table of Fig gc-state-fig summarizes the

 characterization of the four states in terms of the data_structure

 for Algorithm_algmark-sweep

 ex



 figurehtfb



 center

 tabularl_c c_c

 State OnFree OnUnscanned Reached-bit



 Free Yes No 0



 Unreached No No 0



 Unscanned No Yes 1



 Scanned No No 1



 tabular

 center



 Representation of states in Algorithm_algmark-sweep

 gc-state-fig



 figure



 Optimizing Mark-and-Sweep



 The final_step in the basic mark-and-sweep algorithm is expensive

 because there is no easy way to find only the unreachable_objects

 without examining the entire heap An improved algorithm due to

 Baker keeps a list of all allocated_objects To_find the set of

 unreachable_objects which we must return to free_space we take

 the set difference of the allocated_objects and the reached

 objects



 alg

 algmod-mark-sweep

 Baker's mark-and-sweep collector



 A root_set of objects a heap

 a free_list Free and a list of allocated_objects which we refer

 to as Unreached



 Modified lists Free and Unreached which holds

 allocated_objects



 In this algorithm shown in Fig baker-fig the data

 structure for garbage_collection is four lists named Free

 Unreached Unscanned and Scanned each of which

 holds all the objects in the state of the same name These lists

 may be_implemented by embedded doubly_linked lists as was

 discussed in Section alloc-frag-subsect A reached-bit in

 objects is not used but we assume that each object contains bits

 telling which of the four states it is in Initially Free

 is the free_list maintained by the memory_manager and all

 allocated_objects are on the Unreached list (also maintained

 by the memory_manager as it allocates chunks to objects)



 figurehtfb

 center

 tabularr_l

 1) Scanned Unscanned



 2) move objects referenced by the root_set from Unreached to Unscanned



 3)_while (Unscanned )



 4) move object from Unscanned to Scanned



 5) for_(each object referenced in )



 6) if_( is in Unreached)



 7) move from Unreached to Unscanned











 8) Free Free Unreached



 9) Unreached Scanned



 tabular

 center



 Baker's mark-and-sweep algorithm

 baker-fig

 figure



 Lines (1) and (2) initialize Scanned to be the empty list

 and Unscanned to have only the objects reached from the root

 set Note_that these objects were presumably on the list Unreached and must_be removed_from there Lines (3) through_(7)

 are a straightforward implementation of the basic marking

 algorithm using these lists That is the for-loop of lines (5)

 through_(7) examines the references in one unscanned object

 and if any of those references have not_yet been reached

 line_(7) changes to the Unscanned_state



 At the end line_(8) takes those objects that are still on the Unreached list and deallocates their chunks by moving them to the Free_list

 Then line_(9) takes all the objects in state Scanned which are

 the reachable_objects and reinitializes the Unreached list to be

 exactly those objects

 Presumably as the memory_manager creates new objects those too will

 be added to the Unreached list and removed_from the Free

 list

 alg



 In both algorithms of this_section we have assumed that chunks

 returned to the free_list remain as they_were before deallocation

 However as discussed in Section alloc-frag-subsect it is

 often advantageous to combine adjacent free_chunks into larger

 chunks If we_wish to do so then every time we return a chunk to

 the free_list either at line (10) of

 Fig figmark-and-sweep or line_(8) of Fig baker-fig

 we examine the chunks to its left and right and merge if one is

 free



 Mark-and-Compact Garbage Collectors

 secgcrelocate



 Relocating collectors move reachable_objects around in the

 heap to eliminate memory fragmentation It is common that the

 space occupied by reachable_objects is much_smaller than the freed

 space Thus after identifying all the holes instead of freeing

 them individually one attractive alternative is to relocate all

 the reachable_objects into one end of the heap leaving the entire

 rest of the heap as one free chunk After all the garbage

 collector has already analyzed every reference within the

 reachable_objects so updating them to point to the new locations

 does_not require much more work These plus the references in the

 root_set are all the references we need to change



 Having all the reachable_objects in contiguous locations reduces

 fragmentation of the memory space making it easier to house large

 objects Also by making the data occupy fewer cache_lines and

 pages relocation improves a program's temporal and spatial

 locality since new objects_created at about the same time are

 allocated nearby chunks Objects in nearby chunks can benefit from

 prefetching if they are used together Further the data_structure

 for maintaining free_space is simplified instead of a free_list

 all we need is a pointer free to the beginning of the one

 free block



 Relocating collectors vary in whether they relocate in place or

 reserve space ahead of time for the relocation



 itemize



 A mark-and-compact collector described in this

 section compacts objects in place Relocating in place reduces

 memory_usage



 The more_efficient and popular copying collector in

 Section copying-coll-subsect moves objects from one region

 of memory to another Reserving extra space for relocation allows

 reachable_objects to be moved as they are discovered



 itemize



 The mark-and-compact collector in Algorithm algmark-compact

 has three phases



 enumerate



 First is a marking phase similar to that of the

 mark-and-sweep algorithms described previously



 Second the algorithm scans the allocated section of the

 heap and computes a new address for each of the reachable_objects

 New addresses are assigned from the low end of the heap so there

 are no holes between reachable_objects The new address for each

 object is recorded in a structure called NewLocation



 Finally the algorithm copies objects to their new

 locations updating all references in the objects to point to the

 corresponding new locations The needed addresses are found in

 NewLocation



 enumerate



 alg

 algmark-compact

 A mark-and-compact garbage_collector



 A root_set of objects a heap and free a pointer

 marking the start of free_space



 The new value of pointer free



 The algorithm is in Fig figmark-and-compact it

 uses the following data_structures



 enumerate



 An Unscanned_list as in Algorithm_algmark-sweep



 Reached bits in all objects also as in

 Algorithm_algmark-sweep To keep our description simple we

 refer to objects as reached or unreached when we mean

 that their reached-bit is 1 or 0 respectively Initially all

 objects are unreached



 The pointer free which marks the beginning of

 unallocated space in the heap



 The table NewLocation This structure could be a hash

 table search tree or another structure that implements the two

 operations

 enumerate

 Set NewLocation to a new address for object

 Given object get the value of NewLocation

 enumerate

 We_shall not concern ourselves with the exact structure used

 although you may assume that NewLocation is a hash_table

 and therefore the set and get operations are each

 performed in average constant time independent of how many

 objects are in the heap



 enumerate



 figurehtb

 center

 tabularr_l

 mark



 1) Unscanned set of objects referenced by the root_set



 2) while (Unscanned )



 3) remove object from Unscanned



 4) for_(each object referenced in )



 5) if_( is unreached)



 6) mark as reached



 7) put on list Unscanned















 compute new locations



 8) free starting location of heap_storage



 9) for_(each chunk of memory in the heap from the low end)



 10) if_( is reached)



 11) NewLocation() free



 12) free free sizeof











 retarget references and move reached objects



 13) for_(each chunk of memory in the heap from the low end)



 14) if_( is reached)



 15) for_(each reference in )



 16) NewLocation



 17) copy to NewLocation()











 18) for_(each reference in the root set)



 19) NewLocation



 tabular

 center

 A Mark-and-Compact Collector

 figmark-and-compact

 figure



 The first or marking phase of lines_(1) through_(7) is

 essentially the same as the first phase of

 Algorithm_algmark-sweep The second phase lines (8)

 through (12) visits each chunk in the allocated part of the heap

 from the left or low end As a result chunks are assigned new

 addresses that increase in the same order as their old addresses

 This ordering is important since when we relocate objects we can

 do so in a way that assures we only move objects left into space

 that was formerly occupied by objects we have moved already



 Line (8) starts the free pointer at the low end of the heap

 In this phase we use free to indicate the first available

 new address We create a new address only for those objects

 that are marked as reached Object is given the next available

 address at line (10) and at line (11) we increment free by

 the amount of storage that object requires so free

 again points to the beginning of free_space



 In the final phase lines (13) through (17) we again visit the

 reached objects in the same from-the-left order as in the second

 phase Lines (15) and (16) replace all internal pointers of a

 reached object by their proper new values using the NewLocation table to determine the replacement Then line (17)

 moves the object with the revised internal references to its

 new location Finally lines (18) and (19) retarget pointers in

 the elements of the root_set that are not themselves heap_objects

 eg statically allocated or stack-allocated objects

 Figure mark-compact-fig suggests how the reachable_objects

 (those that are not shaded) are moved down the heap while the

 internal pointers are changed to point to the new locations of the

 reached objects

 alg



 figurehtfb





 Moving reached objects to the front of the heap while

 preserving internal pointers mark-compact-fig

 figure



 Copying collectors

 copying-coll-subsect



 A copying collector reserves ahead of time space to which the

 objects can move thus breaking the dependency between tracing and

 finding free_space The memory space is partitioned_into two semispaces and The mutator allocates memory in one semispace say

 until it fills up at which point the mutator is stopped and the

 garbage_collector copies the reachable_objects to the other space say

 When garbage_collection completes the roles of the semispaces

 are reversed The mutator is allowed to resume and allocate objects in

 space and the next round of garbage_collection moves reachable

 objects to space

 The following algorithm is due to C J Cheney



 alg

 algcopying-collector

 Cheney's copying collector



 A root_set of objects and a heap consisting of the From_semispace containing allocated_objects and the To

 semispace all of which is free



 At the end the To_semispace holds the allocated

 objects A free pointer indicates the start of free_space

 remaining in the To_semispace The From_semispace is

 completely free



 The algorithm is shown in Fig figcheney Cheney's

 algorithm finds reachable_objects in the From_semispace and

 copies them as_soon as they are reached to the To

 semispace This placement groups related objects together and may

 improve spatial_locality



 figurehtb



 center

 tabularr_l

 1) CopyingCollector ()



 2) for (all objects in From space) NewLocationNULL



 3) unscanned free starting address of To space



 4) for_(each reference in the root set)



 5) replace with LookupNewLocation



 6) while (unscanned free)



 7) object at location unscanned



 8) for_(each reference within )



 9) LookupNewLocation



 10) unscanned unscanned sizeof















 Look up the new location for object if it has_been moved



 Place object in Unscanned_state otherwise



 11) LookupNewLocation



 12) if (NewLocation NULL)



 13) NewLocation free



 14) free free sizeof



 15) copy to NewLocation







 16) return NewLocation







 tabular

 center

 A Copying Garbage Collector

 figcheney

 figure



 Before examining the algorithm itself which is the function CopyingCollector in Fig figcheney consider the auxiliary

 function LookupNewLocation in lines (11) through (16) This

 function takes an object and finds a new location for it in

 the To space if has no location there yet All new

 locations are recorded in a structure NewLocation and a

 value of indicates has no assigned

 locationfootnoteIn a typical data_structure such_as a

 hash_table if is not assigned a location then there simply

 would be no mention of it in the structurefootnote As in

 Algorithm algmark-compact the exact form of structure NewLocation may vary but it is fine to assume that it is a hash

 table



 If we find at line (12) that has no location then it is

 assigned the beginning of the free_space within the To

 semispace at line (13) Line (14) increments the free

 pointer by the amount of space taken by and at line (15) we

 copy from the From space to the To space Thus

 the movement of objects from one semispace to the other occurs as

 a side_effect the first time we look up the new location for the

 object Regardless of whether the location of was or was not

 previously established line (16) returns the location of in

 the To space



 Now we can consider the algorithm itself Line (2) establishes

 that none of the objects in the From space have new

 addresses yet At line_(3) we initialize two pointers unscanned and free to the beginning of the To

 semispace Pointer free will always indicate the beginning

 of free_space within the To space As we add objects to the

 To space those with addresses below unscanned will be

 in the Scanned state while those between unscanned

 and free are in the Unscanned_state Thus free

 always leads unscanned and when the latter catches up to

 the former there are no more Unscanned objects and we are

 done with the garbage_collection Notice_that we do our work

 within the To space although all references within objects

 examined at line_(8) lead us back to the From space



 Lines (4) and (5) handle the objects reached from the root_set

 Note_that as a side_effect some of the calls to LookupNewLocation at line_(5) will increase free as chunks

 for these objects are allocated within To Thus the loop

 of lines (6) through (10) will be entered the first time it is

 reached unless there are no objects referenced by the root_set

 (in which case the entire heap is garbage) This loop then scans

 each of the objects that has_been added to To and is in the

 Unscanned_state Line (7) takes the next unscanned object

 Then at lines (8) and (9) each reference within is

 translated from its value in the From_semispace to its value

 in the To_semispace Notice_that as a side_effect if a

 reference within is to an object we have not reached

 previously then the call to LookupNewLocation at line_(9)

 creates space for that object in the To space and moves the

 object there Finally line (10) increments unscanned to

 point to the next object just beyond in the To space

 alg



 Comparing Costs



 Cheney's algorithm has the advantage that it does_not touch any of

 the unreachable_objects On the other_hand a copying garbage

 collector must move the contents of all the reachable_objects

 This process is especially expensive for large objects and for

 long-lived objects that survive multiple rounds of garbage

 collection We can summarize the running_time of each of the four

 algorithms described in this_section as_follows Each estimate

 ignores the cost of processing the root_set



 itemize



 Basic Mark-and-Sweep (Algorithm algmark-sweep)

 Proportional to the number of chunks in the heap



 Baker's Mark-and-Sweep (Algorithm algmod-mark-sweep)

 Proportional to the number of reached objects



 Basic Mark-and-Compact (Algorithm algmark-compact)

 Proportional to the number of chunks in the heap plus the total size of

 the reached objects



 Cheney's Copying Collector

 (Algorithm algcopying-collector)

 Proportional to the total size of the reached objects



 itemize



 exer

 mark-sweep-exer

 Show the steps of a mark-and-sweep garbage_collector on



 itemize

 a) Fig gc-exer-fig with the pointer

 deleted

 b) Fig gc-exer-fig with the pointer deleted

 c) Fig gc2-exer-fig with the pointer deleted

 d) Fig gc2-exer-fig with the object deleted

 itemize

 exer



 exer

 The Baker mark-and-sweep algorithm moves objects among four lists Free Unreached Unscanned and Scanned For each of the object

 networks of Exercise mark-sweep-exer indicate for each object the

 sequence of lists on which it finds itself from just_before garbage

 collection begins until just after it finishes

 exer



 exer

 Suppose we perform a mark-and-compact garbage_collection on each of the

 networks of Exercise mark-sweep-exer Also suppose that



 itemize



 Each object has size 100 bytes and



 Initially the nine objects in the heap are arranged in alphabetical

 order starting_at byte 0 of the heap

 itemize

 What is the address of each object after garbage_collection



 exer



 exer

 Suppose we execute Cheney's copying garbage_collection algorithm

 on each of the

 networks of Exercise mark-sweep-exer Also suppose that



 itemize

 Each object has size 100 bytes



 The unscanned list is managed as a queue and when an

 object has more_than one pointer the reached objects are added to

 the queue in alphabetical order and



 The From_semispace starts at location 0 and

 the To_semispace starts at location 10000

 itemize

 What is the value of NewLocation for each object that

 remains after garbage_collection



 exer

 Syntax-Directed Translation

 trans-sect



 Syntax-directed_translation is done by_attaching rules or program

 fragments to productions in a grammar For_example consider the

 production



 center



 center

 The fact that is the sum of the two subexpressions

 and can be used to translate (The

 subscript in is used only to distinguish the instance of

 in the production_body from the head of the production)



 This_section introduces two concepts related to syntax-directed

 translation



 itemize



 Attributes Any quantity associated_with a programming

 construct (a terminal or nonterminal of a grammar)

 is called an attribute of the construct In the

 process of generating code to implement the programming construct represented_by a

 nonterminal a compiler might use

 attributes for quantities such_as data types of expressions the number of

 instructions in the generated code or the location of the first

 instruction in the generated code among many other possibilities



 (Syntax-directed)

 translation_schemes A translation_scheme is a

 notation for attaching program_fragments to the productions

 of a grammar The program_fragments are executed when the production

 is used during syntax analysis The combined

 result of all these fragment executions in the order induced by the syntax

 analysis

 is the

 translation of the program to which this analysissynthesis process is applied

 itemize



 Syntax-directed translations will be used_throughout this_chapter

 to translate infix expressions into_postfix notation to evaluate

 expressions and to build syntax_trees for programming_constructs

 A more detailed discussion of syntax-directed formalisms appears

 in Chapter_sdt-ch



 Postfix Notation



 The examples in this_section deal_with translation into_postfix

 notation The postfix_notation for an expression can be

 defined inductively as_follows



 enumerate

 If is a variable or constant then the postfix_notation

 for is itself



 If is an expression of the form

 where op is any binary_operator then the postfix_notation

 for is where and are

 the postfix notations for and respectively



 If is a parenthesized expression of the form

 then the postfix_notation for is the same as the postfix

 notation for

 enumerate



 ex

 The postfix

 notation for (9-5)2 is 95-2

 That is the translations of 9 5 and 2 are themselves

 by rule (1)

 Then the translation of is by rule_(2)

 The translation of (9-5) is the same by rule_(3)

 Having translated the parenthesized subexpression we may apply rule_(2)

 to the entire expression with (9-5) in the role of to

 get the result As_another example the postfix_notation

 for 9-(52) is 952-

 That is 52 is first translated_into 52 and this expression

 becomes the second argument of the minus_sign

 ex



 No parentheses are needed in postfix_notation because the position

 and arity (number of arguments) of the operators permits only one

 decoding of a postfix expression

 The trick is to repeatedly scan the postfix string from the left

 until you find an operator Then look to the left for the proper

 number of operands and group this operator with its operands

 Evaluate the operator on the operands and replace them by the result

 Then repeat the process continuing to the right and searchnig for

 another operator



 ex

 Consider the postfix expression 952-3

 Scanning from the left we first encounter the plus sign

 Looking to its left we find operands 5 and 2 Their sum 7

 replaces 52 and we have the string Now the leftmost operator is the minus_sign and its operands are

 9 and 7 Replacing these by the result of the subtraction

 leaves 23 Last the multiplication sign applies to 2 and

 3 giving the result 6

 ex



 Synthesized Attributes

 synth-attr-subsect



 The idea of associating quantities with programming

 constructs-eg values and types with expressions-can be

 expressed in terms of grammars We associate attributes with

 nonterminals and terminals

 Then we attach rules to the productions of the grammar these rules

 describe_how the attributes are computed at those

 nodes of the parse_tree where the production in question is used to

 relate a node to its_children



 A syntax-directed_definition associates



 enumerate



 With each grammar symbol a set of attributes and



 With each production a set of semantic_rules for

 computing the values of the attributes associated_with the symbols

 appearing in the production



 enumerate



 Attributes can be evaluated as_follows For a given input_string

 construct a parse_tree for Then apply the semantic

 rules to evaluate attributes at each node in the parse_tree as_follows



 Suppose a node in a parse_tree is labeled by the grammar

 symbol We write to denote the value of attribute of

 at that node A parse_tree showing the attribute values at

 each node is called an annotated_parse tree For_example

 Fig attr-tree-fig shows an annotated_parse tree for 9-52 with an attribute associated_with the nonterminals expr and term The value 95-2 of the attribute at

 the root is the postfix_notation for 9-52

 We_shall see shortly how these expressions are computed



 figurehtfb

 fileuullmanalsuch2figsattr-treeeps

 Attribute values at nodes in a parse_tree

 attr-tree-fig

 figure



 An attribute is said to be synthesized if its value at a

 parse-tree_node is determined from attribute values at the

 children of the node Synthesized attributes have the desirable

 property that they can be evaluated during a single bottom-up

 traversal of a parse_tree

 In Section inher-synth-subsect we_shall discuss another important

 kind of attribute the inherited_attribute

 Informally inherited_attributes have their value at a

 parse-tree_node determined from attribute values at the node

 itself its parent and its siblings in the parse_tree



 ex

 postfix-ex The annotated_parse tree in

 Fig attr-tree-fig is based_on the syntax-directed

 definition in Fig post-sdt-fig for translating expressions

 consisting of digits separated_by plus or minus_signs into_postfix

 notation Associated with each nonterminal is a string-valued

 attribute that represents the postfix_notation for the

 expression generated_by that nonterminal in a parse_tree



 figurehtfb



 center

 tabularll

 Production_Semantic Rules































 hline

 tabular

 center



 Syntax-directed definition for infix to postfix

 translation post-sdt-fig



 figure



 The postfix form of a digit is the digit itself eg the

 semantic_rule associated_with the production

 defines to be 9 itself whenever this

 production is used at a node in a parse_tree The other digits are

 translated similarly

 As_another example when the production

 is applied the value of becomes

 the value of



 Convention Distinguishing Uses of a Nonterminal

 As in Example postfix-ex often we have a need in rules to

 distinguish_among several uses of the same nonterminal in the head

 andor body of a production

 The_reason is that in the parse_tree different nodes labeled by

 the same nonterminal usually have different values for their

 translations

 We_shall adopt the convention used there the nonterminal appears

 unsubscripted in the head and with distinct subscripts in the body

 These are all occurrences of the same nonterminal and the subscript is

 not part of its name

 However the reader should be alert to the difference_between examples

 of specific translations where this convention is used and generic

 productions like where the subscripted

 's represent an arbitrary list of grammar_symbols and are not

 instances of one particular nonterminal called



 The production derives an

 expression containing a plus operatorfootnoteIn this and many

 other rules the same nonterminal (expr here)

 appears several_times The purpose of the subscript 1 in is

 to distinguish the two_occurrences and the 1 is not part of the

 nonterminal See the box on Convention Distinguishing Uses of a

 Nonterminal for more detailsfootnote

 The left_operand of the

 plus operator is given by and the right operand by

 Using the operator to represent string

 concatenation the semantic_rule



 center



 center

 associated_with this production constructs the value of attribute

 by concatenating the postfix forms and

 of the left and right operands respectively and then

 appending the plus sign

 Notice_that this rule is a formalization of the definition of postfix

 expression

 ex



 Simple Syntax-Directed Definitions



 The syntax-directed_definition in Example postfix-ex has the

 following important_property the string representing the

 translation of the nonterminal at the head of each production is

 the concatenation of the translations of the nonterminals in the

 production_body in the same order as in the production with some

 optional additional strings interleaved A syntax-directed

 definition with this property is termed simple



 ex

 Consider the first production and semantic_rule from

 Fig post-sdt-fig



 equation

 arrayc c

 Production_Semantic Rule



 1 t 1t t ' 'eqplus-sdd

 array

 equation

 Here the translation is the concatenation of the

 translations of and followed_by the symbol Notice_that and appear in the same order in

 both the production_body and the semantic_rule

 There_are no additional symbols before or between their translations

 In this example the only extra symbol occurs at the end

 ex



 When translation_schemes are discussed we_shall see that a simple

 syntax-directed_definition can be_implemented by printing only the

 additional strings in the order they appear in the definition



 Tree Traversals

 dfs-subsect



 Tree traversals are useful for describing attribute evaluation and

 for specifying the execution of code fragments in a translation

 scheme

 A traversal of a tree starts at the root and visits each

 node of the tree in some order



 figurehtfb

 center

 tabularl

 procedure visit(node )



 for ( each child of from left to right )



 visit()







 evaluate semantic_rules at node





 tabular

 center

 A depth-first_traversal of a tree

 depth-visit-fig

 figure



 A depth-first_traversal traversal) is

 defined in Fig depth-visit-fig It starts at the root and

 recursively visits the children of each node in left-to-right

 order as shown in Fig depth-trav-fig It is called

 depth-first because it visits an unvisited child of a node

 whenever it can so it tries to visit nodes as far away from the

 root (as deep) as quickly as it can

 In this traversal we have included the action of evaluating

 translations at each node just_before we finish with the node (ie

 after translations at the children have surely been computed)

 In_general the actions associated_with a depth-first_traversal can be

 whatever we choose or nothing at all



 figurehtfb



 Example of a depth-first_traversal of a tree

 depth-trav-fig

 figure



 A syntax-directed_definition does_not impose any specific order

 for the evaluation of attributes on a parse_tree any evaluation

 order that computes an attribute after all the other

 attributes that depends_on is acceptable Synthesized

 attributes can be evaluated during any bottom-up traversal

 that is a traversal that evaluates attributes at a node after having

 evaluated attributes at its_children

 The matter of evaluation order is quite complex in general and it is

 taken up again in Section eval-sdd-subsect



 Preorder and Postorder Traversals

 Related to the idea of depth-first_traversal of a tree are the concepts

 of preorder and postorder traversals or orderings of the nodes of a

 tree

 The preorder of the nodes of the tree is the same as the order in

 which a depth-first_traversal first visits the nodes That is the

 preorder_traversal of a (sub)tree rooted_at node is then the

 preorder traversals of each of its_children if any from the left

 The postorder for the nodes of a tree is the order in which a

 depth-first_traversal last visits the nodes That is the postorder

 traversal of a (sub)tree rooted_at is the postorder_traversal of

 each of the children of if any from the left followed_by

 itself



 Often we traverse a tree to perform some particular action at each

 node If the action is done when we first visit a node then we may

 refer to the traversal as a preorder_traversal Similarly if the

 action is done just_before we leave a node for the last time then we

 say it is a postorder_traversal of the tree



 Translation Schemes

 sdt-intro-subsect



 Syntax-directed_translation schemes

 are a notation for attaching program_fragments

 to productions in a grammar As_mentioned at the beginning of this

 section a translation_scheme is like a syntax-directed

 definition except that the order of evaluation of the semantic

 rules is explicitly shown



 Program fragments embedded within production_bodies are called

 semantic_actions The position at which an action is to be

 executed is shown by enclosing it between curly_braces and writing it

 within the production_body as in



 center

 ' rest1













 rest1















































































 1 print()

 1 print()



 0print()

 1print()



 9print()



























 print(' ')

































































 Conclusion

 secconclusion























 This_paper shows that BDDs have made possible an extremely simple

 approach to context_sensitivity We can clone all the methods in a

 call_graph one per context of interest and simply_apply a

 context-insensitive_analysis over the cloned graph to get

 context-sensitive_results We can handle exponentially_many contexts

 if their commonalities can be_exploited Our_approach first computes

 a call_graph based_on a context-insensitive point-to analysis We

 then assign the context_numbers such that contexts of a node are given

 contiguous values and pairs of source and destination context of an

 invocation graph edge have a constant distance We showed that this

 approach can be applied to type_inference thread_escape analysis and

 even fully context-sensitive_points-to analysis on large_programs



 This_paper shows that we can create efficient BDD-based analyses

 easily We represent all the program information in relations and

 express our analyses as Datalog_rules The system we have

 developed produces BDD programs that are even more_efficient than

 those painstakingly hand-tuned



 Context-sensitive pointer_analysis is the corner stone of deep program

 analysis for modern programming_languages By combining (1)

 context-sensitive_points-to results (2) a simple approach to context

 sensitivity and (3) a simple logic-programming based query framework

 we believe we have made it much_easier to create advanced program

 analyses





 In this_paper we presented a context-sensitive_inclusion-based

 pointer_alias analysis that scales to hundreds of thousands of Java

 bytecodes Our technique is based_on cloning the call_paths in the

 original_program and applying a simple context-insensitive algorithm

 to the expanded_call graph As a program gets large there is an

 exponential_blowup in the number of paths however we can represent

 the large_number of contexts efficiently using_BDDs and the set-based

 semantics of BDD_operations allow_us to compute the results for all

 contexts in parallel In essence the BDD data_structure automatically

 exploits the commonalities between different_contexts leading to

 an efficient_representation and an efficient algorithm



 We have implemented this technique on top of a BDD-based

 deductive database system that automatically_translates our

 declarative specifications into highly_efficient BDD code

 allows_us to abstract away the implementation details and reduce the

 analysis to just a few lines automatically handles the

 optimizations and transformations With we were_able to

 quickly and easily develop efficient implementations of other

 context-sensitive analyses and queries Our context-sensitive

 points-to_analysis can analyze even the largest of our benchmarks

 in under 19 minutes





 Our_approach to represent all analysis constraints and computed

 relations as BDDs and handle context_sensitivity by cloning has two

 important advantages (1) the results are readily accessible with BDD

 queries (2) it is easy to develop and implement new algorithms

 These advantages are significant in enabling researchers to create

 more and better techniques We hope to have demonstrated this point

 by developing two context-sensitive_pointer alias_analyses and four

 meaningful context-sensitive analyses that use the pointer_alias

 analysis results and applying them to twenty real large_programs





 Our implementation is available publicly at

 httpjoeqsourceforgenet We will post the information collected

 on the applications on their Sourceforce site when this_paper is

 published



 Type Checking

 type-check-sect



 To do type_checking a compiler needs to assign a type

 expression to each component of the source_program

 The compiler must then determine that these type expressions

 conform to a collection of logical rules that is called

 the type system for the source_language



 Type_checking has the potential for catching errors in programs

 In principle any check can be done dynamically if the target

 code carries the type of an element along with the value of the

 element A sound type system eliminates the need for

 dynamic checking for type errors because it allows_us to determine

 statically that these errors cannot occur when the target program

 runs An implementation of a language is strongly typed if

 a compiler guarantees that the programs it accepts will run

 without type errors



 Besides their use for compiling ideas from type_checking have

 been used to improve the security of systems that allow software

 modules to be imported and executed Java programs compile into

 machine-independent bytecodes that include detailed type

 information_about the operations in the bytecodes Imported code

 is checked before it is allowed to execute to guard against both

 inadvertent errors and malicious misbehavior



 Rules for Type Checking



 Type_checking can take on two forms synthesis and inference

 Type synthesis builds up the type of an expression from the

 types of its subexpressions It requires names to be declared

 before they are used The type of is defined in terms of

 the types of and A_typical rule for type synthesis

 has the form



 equationtype-syn-eq

 arrayl

 if has type and has type



 then expression has type



 array

 equation

 Here and denote expressions and

 denotes a function from to This rule for functions with

 one argument carries over to functions with several arguments

 The rule (type-syn-eq) can be adapted for by

 viewing it as a function application

 (We shall use the term

 synthesis even if some context information is used to

 determine types With overloaded functions where the same name

 is given to more_than one function the context of may

 also need to be considered in some languages)



 Type_inference determines the type of a language construct

 from the way it is used Looking ahead to the examples in

 Section poly-subsect let null be a function that

 tests whether a list is empty Then from the usage

 we can tell that must_be a list The type

 of the elements of is not known all we know is that must

 be a list of elements of some type that is presently unknown



 Variables representing type expressions allow_us to talk_about

 unknown types We_shall use Greek letters

 for type variables in type expressions



 A_typical rule for type_inference has the form



 equationtype-inf-eq

 arrayl

 if is an expression



 then for some and has type and has type



 array

 equation

 Type_inference is needed for languages_like ML which

 check types but do_not require names to be declared



 In this_section we consider type_checking of expressions The

 rules for checking statements are similar to those for

 expressions For_example we treat the conditional_statement

 as if it were the application of a

 function if to and Let the special type void

 denote the absence of a value Then function if expects to

 be applied to a boolean and a void the result of the

 application is a void



 Type Conversions

 coerce-3code-subsect



 Consider expressions like where is of type float and

 is of type integer Since the representation of integers and

 floating-point_numbers is different within a computer and different machine

 instructions are used for operations on integers and floats the

 compiler may need to convert one of the operands of to ensure

 that both operands are of the same type when the addition occurs



 Suppose that integers are converted to floats when necessary using

 a unary operator (float) For_example the integer 2 is

 converted to a float in the code for the expression





 center

 tabularl

 (float) 2



 314

 tabular

 center

 We can extend such examples to consider integer and float versions

 of the operators for example int for integer operands and

 float for floats



 Type synthesis will be illustrated by extending the scheme in

 Section expr-incr-subsect for translating expressions We

 introduce another attribute whose value is either

 integer or float The rule_associated with

 builds on the pseudocode



 center

 tabularl











 tabular

 center

 As the number of types subject to conversion increases the number

 of cases increases rapidly Therefore with large_numbers of types

 careful organization of the semantic_actions becomes important



 Type conversion rules vary from language to language The rules

 for Java in Fig coerce-fig distinguish_between widening conversions which are intended to preserve information

 and narrowing conversions which can lose information The

 widening rules are given by the hierarchy in

 Fig coerce-fig(a) any type lower in the hierarchy can be

 widened to a higher type Thus a char can be widened to an

 int or to a float but a char cannot be widened

 to a short The narrowing rules are illustrated by the graph

 in Fig coerce-fig(b) a type can be narrowed to a type

 if there is a path from to Note_that char short and byte are pairwise convertible to each other



 figurehtbf



 Conversions between primitive types in

 Javacoerce-fig

 figure



 Conversion from one type to another is said to be implicit

 if it is done automatically by the compiler Implicit type

 conversions also called coercions are limited in many

 languages to widening conversions Conversion is said to be explicit if the programmer must write something to cause the

 conversion Explicit conversions are also called casts



 The semantic_action for checking uses two

 functions



 enumerate



 takes two types and and

 returns the maximum (or least upper bound) of the two types in the

 widening hierarchy It declares an error if either or

 is not in the hierarchy eg if either type is an array or a

 pointer type



 generates type_conversions if needed

 to widen the contents of

 an address of type into a value of type It

 returns itself if and are the same type Otherwise

 it generates an instruction to do the conversion and place the

 result in a temporary which is returned as the result

 Pseudocode for widen assuming that the only types are integer and float appears in Fig widen-fig



 enumerate



 figurehtbf

 center

 tabularl



































 tabular

 center

 Pseudocode for function widenwiden-fig

 figure



 The semantic_action for in

 Fig conv-plus-fig illustrates_how type_conversions can be

 added to the scheme in Fig_expr-1pass-fig for translating

 expressions In the semantic_action temporary_variable is

 either if the type of does_not need to be

 converted to the type of or a new_temporary variable returned

 by widen if this conversion is necessary Similarly

 is either or a new_temporary holding the

 type-converted value of Neither conversion is needed if

 both types are integer or both are float In_general

 however we could find that the only way to add values of two

 different types is to convert them both to a third type



 figurehtfb



 center

 tabularr_c l_l l













 new







 tabular

 center



 Introducing type_conversions into expression evaluation

 conv-plus-fig



 figure



 Overloading of Functions and Operators

 overload-subsect



 An overloaded symbol has different meanings depending_on its

 context Overloading is resolved when a unique meaning is

 determined for each occurrence of a name In this_section we

 restrict attention to overloading that can be_resolved by_looking

 only at the arguments of a function as in Java



 exoverload-ex

 The operator in Java denotes either string concatenation or

 addition depending_on the types of its operands User-defined

 functions can be overloaded as_well as in

 center

 tabularl

 'void err() '' '



 'void err(String s) '' '



 tabular

 center

 Note_that we can choose between these two versions of a function

 err by_looking at their arguments

 ex



 The following is a type-synthesis rule for overloaded functions



 equation

 arrayl

 if can have type for where for



 and has type for some



 then expression has type

 array

 equation



 The value-number method of Section val-num-subsect can be

 applied to type expressions to resolve overloading

 based_on argument types efficiently In a DAG representing a type expression

 we assign an_integer index called a value number to each node

 Using Algorithm val-num-alg we construct a signature for a

 node consisting of its label and the value numbers of its

 children in order from left to right The signature for a

 function consists of the function name and the types of its

 arguments The assumption that we can resolve overloading based_on

 the types of arguments is equivalent to saying that we can resolve

 overloading based_on signatures



 It is not always possible to resolve overloading by_looking only

 at the arguments of a function In Ada instead of a single type

 a subexpression standing alone may have a set of possible types

 for which the context must provide sufficient information to narrow

 the choice down to a single type (see Exercise ada-exer)



 Type Inference and Polymorphic Functions

 poly-subsect



 Type_inference is useful for a language like ML which is strongly

 typed but does_not require names to be declared before they are

 used Type_inference ensures that names are used consistently



 The term polymorphic refers to any code_fragment that can be

 executed with arguments of different types In this_section we

 consider parametric polymorphism where the polymorphism is

 characterized by parameters or type variables The running_example

 is the ML program in Fig length-ml-fig which defines a

 function length The type of length can be described

 as for any type length maps a list of elements

 of type to an_integer



 figurehtbf

 center

 tabularl







 tabular

 center

 ML program for the length of a listlength-ml-fig

 figure





 exlength-ml-ex1

 In Fig length-ml-fig the keyword fun introduces a

 function definition functions can be recursive The program

 fragment defines function length with one parameter The

 body of the function consists of a conditional expression The

 predefined function null tests whether a list is empty and

 the predefined function tl (short for tail) returns the

 remainder of a list after the first element is removed



 The function length determines the length or number of

 elements of a list All elements of a list must have the same

 type but length can be applied to lists whose elements are of any

 one type

 In the following expression length is applied to two

 different types of lists (list elements are enclosed within

 and )



 equationlength-poly-eq

 length(sun mon tue)

 length(10987)

 equation

 The list of strings has length 3 and the list of

 integers has length 4 so expression (length-poly-eq)

 evaluates to 7

 ex



 Using the symbol (read as for any type) and the type

 constructor list the type of length can be written as

 equationlength-type-eq

 list() integer

 equation



 The symbol is the universal quantifier and the

 type variable to which it is applied is said to be bound by

 it Bound variables can be renamed at will provided all

 occurrences of the variable are renamed Thus the type

 expression

 center



 center

 is equivalent to (length-type-eq) A type expression with a

 symbol in it will be referred to informally as a

 polymorphic type



 Each time a polymorphic function is applied its bound type

 variables can denote a different type During type_checking

 at each use of a polymorphic type

 we replace the bound variables by fresh variables and

 remove the universal quantifiers



 The next example informally infers a type for length

 implicitly using type_inference rules like (type-inf-eq)

 which is repeated_here



 center

 tabularl

 if is an expression



 then for some and has type and has type



 tabular

 center



 exlength-ml-ex2

 The abstract_syntax tree in Fig length-ast-fig represents

 the definition of length in Fig length-ml-fig The

 root of the tree labeled fun represents the function

 definition The remaining nonleaf nodes can be_viewed as function

 applications The node_labeled represents the application of

 the operator to a pair of children Similarly the node

 labeled if represents the application of an operator if to a triple formed_by its_children (for type_checking it does

 not matter that either the then or the else part will

 be evaluated but not both)



 figurehtfb



 Abstract syntax_tree for the function definition in

 Fig length-ml-fig length-ast-fig

 figure



 From the body of function length we can infer its type

 Consider the children of the node_labeled if from left to

 right Since null expects to be applied to lists must

 be a list Let_us use variable as a placeholder for the

 type of the list elements that is has type list of





 If is true then is Thus

 the type of length must_be function from list of

 to integer This inferred_type is consistent_with the usage of

 length in the else part

 ex



 Since variables can appear in type expressions we have to

 re-examine the notion of equivalence of types Suppose of

 type is applied to of type Instead of

 simply determining the equality of and we must_unify

 them Informally we determine_whether and can be made

 structurally equivalent by_replacing the type variables in and

 by type expressions



 A substitution is a mapping from type variables to type

 expressions We write for the result of applying the

 substitution to the variables in type expression see the

 box on Substitutions Instances and Unification Two type

 expressions and unify if there_exists some

 substitution such that In_practice we are

 interested in the most general unifier which is a substitution

 that imposes the fewest constraints on the variables in the

 expressions

 See Section unify-subsect

 for a unification algorithm



 Substitutions Instances and

 Unification

 If is a type expression and is a substitution

 (a mapping from type variables to type expressions) then we write

 for the result of consistently replacing all occurrences of

 each type variable in by is

 called an instance of For_example

 is an instance of

 since it is the result of substituting integer for in Note_however

 that is not an instance of

 since a substitution must replace all

 occurrences of by the same type expression



 Substitution is a unifier of type expressions and

 if is the most general unifier

 of and if for any other unifier of and

 say it is the case that for any

 is an instance of In simpler words imposes more

 constraints on than does



 alg

 type-infer-alg

 Type_inference for polymorphic

 functions



 A program consisting of a sequence of function definitions

 followed_by an expression to be evaluated An expression is made

 up of function applications and names where names can have

 predefined polymorphic types



 Inferred types for the names in the program



 For_simplicity we_shall deal_with unary

 functions only The type of a function with two parameters

 can be represented_by a type expression

 where and are the types of and

 respectively and is the type of the result An

 expression can be checked by matching the type of

 with and the type of with



 Check the function definitions and the expression in the input

 sequence Use the inferred_type of a function if it is

 subsequently used in an expression



 itemize



 For a function definition

 create fresh type variables and

 Associate the type with the function

 and the type with the parameter

 Then infer a type for expression Suppose

 denotes type and denotes type after type

 inference for The inferred_type of function is

 Bind any type variables that remain unconstrained in

 by quantifiers



 For a function application infer types for

 and Since is used as a function its type must have

 the form (Technically the type of must_unify

 with where and are new type

 variables) Let be the inferred_type of Unify and

 If unification fails the expression has a type error Otherwise the

 inferred_type of is



 For each occurrence of a polymorphic function replace the

 bound variables in its type by distinct fresh variables and remove

 the quantifiers The resulting type expression is the

 inferred_type of this occurrence



 For a name that is encountered for the first time introduce

 a fresh variable for its type



 itemize

 alg



 exlength-typevar-ex

 In Fig length-infer-fig we infer a type for function length The root of the syntax_tree in Fig length-ast-fig

 is for a function definition so we introduce variables

 and associate the type with

 function length and the type with see

 lines 1-2 of Fig length-infer-fig



 figurehtfb

 center

 tabularrr lr l

 LINE EXPRESSION type 2cunify



 1)



 2)



 3)



 4)



 5)



 6)



 7)



 8)



 9)



 10)



 11)



 12)



 13)



 tabular

 center

 Inferring a type for the function length of

 Fig length-ml-fig length-infer-fig

 figure



 At the right_child of the root we view if as a polymorphic

 function that is applied to a triple consisting of a boolean and

 two expressions that represent the then and else

 parts Its type is





 Each application of a polymorphic function can be to a different

 type so we make up a fresh variable (where is from

 if) and remove the see line 3 of

 Fig length-infer-fig The type of the left_child of if must_unify with boolean and the types of its other two

 children must_unify with



 The predefined function null has type

 We use a fresh type

 variable (where is for null) in place of the

 bound variable see line_4 From the application of

 to we infer that the type of must

 match see line 5



 At the first child of if the type boolean for

 matches the type expected by if At the

 second child the type unifies with integer see

 line 6



 Now_consider the subexpression

 We make up a fresh variable (where is for tail)

 for the bound variable in the type of tl see

 line 8 From the application we infer

 see line 9



 Since is an operand of its type

 must_unify with integer see line 10 It follows

 that the type of length is

 After the function definition is checked the

 type variable remains in the type of length

 Since no assumptions were made about any type can be

 substituted for it when the function is used We therefore make it

 a bound variable and write

 center



 center

 for the type of length

 ex





 An Algorithm for Unification

 unify-subsect



 Informally unification is the problem of determining whether two

 expressions and can be made identical by substituting

 expressions for the variables in and Testing equality of

 expressions is a special_case of unification if and have

 constants but no variables then and unify if and only if

 they are identical The unification algorithm in this_section

 extends to graphs with cycles so it can be used to test

 structural equivalence of circular types(In some

 applications it is an error to unify a variable with an

 expression containing that variable Algorithm unify-alg

 permits such substitutions)



 We_shall implement a graph-theoretic formulation of unification

 where types are represented_by graphs Type variables are

 represented_by leaves and type constructors are represented_by

 interior_nodes Nodes are grouped into equivalence_classes if two

 nodes are in the same equivalence_class then the type expressions

 they represent must_unify Thus all interior_nodes in the same

 class must_be for the same type_constructor and their

 corresponding children must_be equivalent



 exmgu-ex

 Consider the two type expressions

 center

 tabularl









 tabular

 center

 The following substitution is the most general unifier for

 these expressions



 center

 tabularc_c

























 tabular

 center



 This substitution maps the two type expressions to the

 following expression



 center





 center



 The two expressions are represented_by the two nodes

 labeled in Fig mgu-fig The integers at the

 nodes indicate the equivalence_classes that the nodes belong to

 after the nodes numbered 1 are unified

 ex



 figurehtfb



 Equivalence classes after unification mgu-fig

 figure



 algunify-alg Unification of a pair of

 nodes in a type graph



 A graph representing a type and a pair of nodes and to be unified





 Boolean value true if the expressions represented_by the

 nodes and unify false otherwise



 A node is implemented_by a record with fields for a binary

 operator and pointers to the left and right children The sets of

 equivalent nodes are maintained using the set field One

 node in each equivalence_class is chosen to be the unique

 representative of the equivalence_class by making its set

 field contain a null pointer The set fields of the remaining

 nodes in the equivalence_class will point (possibly indirectly

 through other nodes in the set) to the representative Initially

 each node is in an equivalence_class by itself with as

 its_own representative node



 figurehtbf

 center

 tabularl













 else if_( nodes and represent the

 same basic type )_return true



 else if_( is an op-node with children

 and and



 is an op-node with children and )









 return and









 else if_( or represents a variable)







 return true







 else_return false







 tabular

 center

 Unification algorithmunify-alg-fig

 figure



 The unification algorithm shown in Fig unify-alg-fig uses

 the following two operations on nodes



 itemize



 returns the representative node of the

 equivalence_class currently containing node



 merges the equivalence_classes

 containing nodes and If one of the representatives for

 the equivalence_classes of and is a nonvariable node

 union makes that nonvariable node be the representative for

 the merged equivalence_class otherwise union makes one or

 the other of the original representatives be the new

 representative This asymmetry in the specification of union

 is important because a variable cannot be used as the

 representative for an equivalence_class for an expression

 containing a type_constructor or basic type Otherwise two

 inequivalent expressions may be unified through that variable



 itemize



 The union operation on sets is implemented_by simply

 changing the set field of the representative of one

 equivalence_class so that it points to the representative of the

 other To_find the equivalence_class that a node belongs to we

 follow the set pointers of nodes until the representative

 (the node with a null pointer in the set field) is reached



 Note_that the algorithm in Fig unify-alg-fig uses

 and rather_than and

 respectively The representative nodes and are equal if

 and are in the same equivalence_class If and

 represent the same basic type the call

 returns true If and are both interior_nodes for a binary

 type_constructor we merge their equivalence_classes on

 speculation and recursively check that their_respective children

 are equivalent By merging first we decrease the number of

 equivalence_classes before recursively checking the children so

 the algorithm_terminates



 The substitution of an expression for a variable is implemented_by

 adding the leaf for the variable to the equivalence_class

 containing the node for that expression Suppose either or is a

 leaf for a variable

 Suppose also that this leaf has_been put into an equivalence_class

 with a node representing an expression with a type

 constructor or a basic type

 Then find will return a

 representative that reflects that type_constructor or basic type

 so that a variable cannot be unified with two different

 expressions

 alg



 exunify-ex

 Suppose that the two expressions in Example mgu-ex are

 represented_by the initial graph in Fig unify-ex-fig where

 each node is in its_own equivalence_class When

 Algorithm unify-alg is applied to compute

 it notes that nodes 1 and 9 both represent

 the same operator It therefore merges 1 and 9 into the same

 equivalence_class and calls and

 The result of computing

 is the graph previously shown in Fig mgu-fig

 ex



 figurehtfb

 Initial

 graph with each node in its_own equivalence

 classunify-ex-fig

 figure



 If Algorithm unify-alg returns true we can construct a

 substitution that acts as the unifier as_follows For each

 variable gives the node that

 is the representative of the equivalence_class of The

 expression represented_by is For_example in

 Fig mgu-fig we see that the representative for

 is node 4 which represents The representative for

 is node 8 which represents The

 resulting substitution is as in Example mgu-ex



 exer

 Assuming that function widen in Fig widen-fig can

 handle any of the types in the hierarchy of

 Fig coerce-fig(a) translate the expressions below

 Assume that and are characters and are short

 integers and are integers and is a float



 itemize

 a) x s c

 b) i s c

 c) x (s c) (t d)

 itemize

 exer



 exerada-exer

 As in Ada suppose that each expression must have a unique type

 but that from a subexpression by itself all we can deduce is a

 set of possible types That is the application of function

 to argument represented_by has

 the associated rule

 center

 for some in

 is in

 center

 Describe an that determines a unique type for

 each subexpression by using an attribute type to synthesize

 a set of possible types bottom-up and once the unique type of

 the overall expression is determined proceeds top-down to

 determine attribute unique for the type of each

 subexpression

 exer

 ch1apptex

 ch1sum1tex

 ch1studytex

 ch1tex

 ch1bib1tex

 ch1ch1tex

 ch1epltex

 ch1booktex

 ch1plbasicstex

 ch1emphasistex

 ch1structuretex

 ch1lptex

 ch10ch10tex

 ch10bib10tex

 ch10sum10tex

 ch10booktex

 ch10constraintstex

 ch10machinetex

 ch10globaltex

 ch10bbtex

 ch10tex

 ch10footex

 ch10sptex

 ch11indextex

 ch11sum11tex

 ch11affinetex

 ch11overalltex

 ch11it-spacetex

 ch11pipelinetex

 ch11const-synctex

 ch11bib11tex

 ch11workspacesynctex

 ch11workspaceopipelinetex

 ch11workspacepipeline-savetex

 ch11time-algtex

 ch11apptex

 ch11bib11tex

 ch11r2blocktex

 ch11r2affinetex

 ch11r2introtex

 ch11r2boundstex

 ch11r2mmtex

 ch11r2reusetex

 ch11r2synctex

 ch11r2booktex

 ch11r2ch11tex

 ch11r2it-spacetex

 ch11r2datadeptex

 ch11r2time-algtex

 ch11r2indextex

 ch11booktex

 ch11ch11tex

 ch11reusetex

 ch11originalsynctex

 ch11originalbooktex

 ch11originaltime-algtex

 ch11originalblocktex

 ch11originalaffinetex

 ch11originalintrotex

 ch11originalch10tex

 ch11originalindextex

 ch11r1time-algtex

 ch11r1it-spacetex

 ch11r1indextex

 ch11r1introtex

 ch11r1synctex

 ch11r1ch11tex

 ch11r1booktex

 ch11r1affinetex

 ch11r1blocktex

 ch11r1boundstex

 ch11r1mmtex

 ch11introtex

 ch11datadeptex

 ch11mmtex

 ch11blocktex

 ch12booktex

 ch12andersen-newtex

 ch12cg-discoverytex

 ch12xtex

 ch12jun10ipa-introtex

 ch12jun10ip-appltex

 ch12jun10csenstex

 ch12jun10booktex

 ch12jun10andersentex

 ch12jun10ch12tex

 ch12jun10bib12tex

 ch12jun10datalogtex

 ch12jun10bddtex

 ch12jun10sum12tex

 ch12jun10cg-discoverytex

 ch12bddtex

 ch12tex

 ch12ip-appltex

 ch12csenstex

 ch12datalogtex

 ch12ipa-introtex

 ch12saveandersentex

 ch12bib12tex

 ch12pldi04jacktex

 ch12pldi04jprogramrepresentationtex

 ch12pldi04jrelatedtex

 ch12pldi04jsolvertex

 ch12pldi04josavetex

 ch12pldi04joverviewtex

 ch12pldi04jold-introtex

 ch12pldi04jcgtex

 ch12pldi04jappendixtex

 ch12pldi04jformulationtex

 ch12pldi04jqueriestex

 ch12pldi04jintrotex

 ch12pldi04jcontextoldtex

 ch12pldi04jcspointertex

 ch12pldi04jresultstex

 ch12pldi04jconclusiontex

 ch12pldi04jcontexttex

 ch12pldi04jtttex

 ch12pldi04japproachtex

 ch12pldi04jmacrostex

 ch12pldi04jpapertex

 ch12pldi04joldcontexttex

 ch12pldi04jabstracttex

 ch12andersentex

 ch12cg-discovery-newtex

 ch12sum12tex

 ch12ch12tex

 ch15parser-java-secttex

 ch15stmt-java-secttex

 ch15front-java-secttex

 ch15jumping-java-secttex

 ch15ch15tex

 ch15tex

 ch15booktex

 ch15symbols-java-secttex

 ch15expr-java-secttex

 ch15main-java-secttex

 ch15source-lang-secttex

 ch15lexer-java-secttex

 ch16ch16tex

 ch16booktex

 ch2oldabstractiontex

 ch2oldch2-oldtex

 ch2oldirtex

 ch2oldch2tex

 ch2oldopttex

 ch2oldbooktex

 ch2parse-secttex

 ch2syntree-secttex

 ch2ch2tex

 ch2lexan-secttex

 ch2syntax-secttex

 ch2lexan-secttexbaktex

 ch2symtab-secttex

 ch2trans-secttex

 ch2postfix-secttex

 ch2sum2tex

 ch2ch2-030722together-secttex

 ch2ch2-030722lexan-secttex

 ch2ch2-030722overview-secttex

 ch2ch2-030722symtab-secttex

 ch2ch2-030722parse-secttex

 ch2ch2-030722postfix-secttex

 ch2ch2-030722ch2tex

 ch2ch2-030722syntax-secttex

 ch2ch2-030722syntree-secttex

 ch2ch2-030722inherit-secttex

 ch2ch2-030722trans-secttex

 ch2bib2tex

 ch2overview-secttex

 ch2booktex

 ch3bib3tex

 ch3lex-archtex

 ch3spectex

 ch3lextex

 ch3sum3tex

 ch3tex

 ch3booktex

 ch3roletex

 ch3re-fatex

 ch3fa-opttex

 ch3buffertex

 ch3ch3tex

 ch3fatex

 ch3token-rectex

 ch4ch4writing-grammars-secttex

 ch4ch4lr-parsers-secttex

 ch4ch4syntax-intro-secttex

 ch4ch4bottom-up-secttex

 ch4ch4grammars-secttex

 ch4ch4ambig-gram-secttex

 ch4ch4yacc-worktex

 ch4ch4ch4tex

 ch4ch4top-down-secttex

 ch4ch4yacc-secttex

 ch4yacc-worktex

 ch4sum4tex

 ch4booktex

 ch4ch4tex

 ch4ambigtex

 ch4lrtex

 ch4bib4tex

 ch4butex

 ch4grammarstex

 ch4obib4tex

 ch4slrtex

 ch4syntax-introtex

 ch4writingtex

 ch4tdtex

 ch4yacctex

 ch5sddtex

 ch5ch5tex

 ch5sum5tex

 ch5sdttex

 ch5l-atttex

 ch5bib5tex

 ch5booktex

 ch5eval-sddtex

 ch5apps-sddtex

 ch6switchtex

 ch6ch6tex

 ch6booktex

 ch6expr-3codetex

 ch6bib6tex

 ch6control-3codetex

 ch6decltex

 ch63addrtex

 ch6proc-3codetex

 ch6backpatchtex

 ch6syn-treetex

 ch6sum6tex

 ch6type-checktex

 ch6newbooktex

 ch6newtex

 ch7sum7tex

 ch7ch7tex

 ch7tracetex

 ch7st-accesstex

 ch7stor-orgtex

 ch7scopetex

 ch7alloctex

 ch7bib7tex

 ch7heaptex

 ch7other-gctex

 ch7short-pausetex

 ch7gctex

 ch7booktex

 ch7rt-stacktex

 ch7newbooktex

 ch8ershovtex

 ch8simple-cgtex

 ch8tiletex

 ch8ch8tex

 ch8sum8tex

 ch8peepholetex

 ch8bib8tex

 ch8qtex

 ch8targettex

 ch8rt-storagetex

 ch8bbtex

 ch8issuestex

 ch8tex

 ch8booktex

 ch8ratex

 ch8dyn-progtex

 ch8bb-opttex

 ch8newbooktex

 ch9loopstex

 ch9booktex

 ch9tex

 ch9consttex

 ch9oldolddf-introtex

 ch9oldoldpretex

 ch9oldoldloopstex

 ch9oldoldconsttex

 ch9oldregion-jefftex

 ch9oldoldregiontex

 ch9oldoldch9tex

 ch9oldoldopt-sourcestex

 ch9oldolddftex

 ch9oldintrotex

 ch9oldinductiontex

 ch9oldoldsymbolictex

 ch9df-introtex

 ch9archiveopt-rejecttex

 ch9archivedf-introbackuptex

 ch9archiveregion-savetex

 ch9archivedf-savetex

 ch9archiveinterval-rejecttex

 ch9cs243df-intro-04tex

 ch9cs243mych9-04tex

 ch9cs243loops-04tex

 ch9cs243mych9tex

 ch9cs243old-pretex

 ch9bib9tex

 ch9pretex

 ch9dftex

 ch9opt-sourcestex

 ch9ch9tex

 ch9symbolictex

 ch9sum9tex

 ch9regiontex

 Writing a Grammar

 writing-grammars-sect



 Grammars are capable of describing most but not all of the

 syntax of programming_languages A limited amount of syntax

 analysis is done by a lexical_analyzer as it produces the sequence

 of tokens from the input_characters Certain constraints on the

 input such_as the requirement that identifiers be declared before

 they are used cannot be described by a context-free_grammar

 Therefore the sequences of tokens accepted_by a parser form a

 superset of a programming_language subsequent phases must analyze

 the output of the parser to ensure compliance with rules that are

 not checked by the parser



 This_section begins_with the division of work between a lexical

 analyzer and a parser Since each parsing method can handle

 grammars only of a certain formthe initial grammar may have to be

 rewritten to make it parsable by the method chosen Suitable

 grammars for expressions can often be constructed using

 associativity and precedence information (The grammar

 (expr-gram-display) properly handles associativity and

 precedence) In this_section we consider transformations that

 are useful for rewriting grammars so they become suitable for

 top-down_parsing We_conclude this_section by considering some

 programming_language constructs that cannot be described by any

 grammar



 Lexical vs Syntactic Analysis

 Every construct that can be described by a regular_expression can

 also be described by a grammar We may therefore reasonably ask

 Why use regular_expressions to define the lexical syntax of a

 language There_are several reasons

 enumerate



 The lexical rules of a language are frequently quite_simple and

 to describe them we do_not need a notation as powerful as

 grammars



 Regular_expressions generally provide a more concise and easier to

 understand notation for tokens than grammars



 More efficient lexical_analyzers can be constructed automatically

 from regular_expressions than from arbitrary grammars



 Separating the syntactic_structure of a language into lexical and

 non-lexical parts provides a convenient_way of modularizing the

 front_end of a compiler into two manageable-sized components

 enumerate



 There_are no firm guidelines as to what to put into the lexical

 rules as opposed to the syntactic rules Regular_expressions are

 most useful for describing the structure of lexical constructs

 such_as identifiers constants keywords and white_space

 Grammars on the other_hand are most useful in describing nested

 structures such_as balanced parentheses matching begin-end's

 corresponding if-then-else's and so on These nested structures

 cannot be described by regular_expressions



 Eliminating Ambiguity

 Sometimes an ambiguous_grammar can be_rewritten to eliminate the

 ambiguity As an example we_shall eliminate the ambiguity from

 the following dangling-else_grammar



 displayif-gram-display

 317ptstabularr c_l

 stmt if expr_then stmt



 if expr_then stmt_else stmt



 other

 tabular

 (if-gram-display)

 display



 Here other stands_for any other statement According to

 this grammar the compound conditional_statement

 disp

 if then else if

 then else

 disp

 has the parse_tree shown in Fig if-tree-fig



 figurehtfb

 center



 Parse_tree for conditional_statement

 if-tree-fig

 center

 figure



 Grammar (if-gram-display) is ambiguous since the string

 displayambig-display

 if then if then

 else (ambig-display)

 display

 has the two parse_trees in Fig if-ambig-fig



 figurehtfb

 center



 Two parse_trees for an ambiguous sentence

 if-ambig-fig

 center

 figure



 In all programming_languages with conditional_statements of this

 form the first parse_tree is preferred The general rule is

 Match each else with the closest unmatched then

 This disambiguating rule can theoretically be incorporated

 directly into a grammar but in practice it is rarely built into

 the productions



 exmatched-if-ex

 We can rewrite the dangling-else_grammar (if-gram-display)

 as the following unambiguous_grammar The idea is that a

 statement appearing between a then and an else must_be

 matched that is it must not end with an unmatched or open

 then A matched statement is either an if-

 then-else statement containing no open statements or it is

 any other kind of unconditional statement Thus we may use the

 grammar



 disp

 tabularr_c l

 stmt matchedstmt



 openstmt



 matchedstmt if expr

 then matchedstmt else matchedstmt



 other



 openstmt if expr

 then_stmt



 if expr_then matchedstmt

 else openstmt

 tabular

 disp

 This grammar generates the same strings as the dangling-else

 grammar_(if-gram-display) but it allows only one parsing

 for string (ambig-display) namely the one that associates

 each else with the closest previous unmatched else

 ex



 Elimination of Left Recursion

 A grammar is left_recursive if it has a nonterminal such

 that there is a derivation for

 some string Top-down parsing methods cannot handle

 left-recursive grammars so a transformation is needed to

 eliminate_left recursion In Section OLD24 we discussed

 immediate_left recursion where there is a production of the form

 Here we study the general case In

 Section OLD24 we showed how the left-recursive pair of

 productions could be replaced

 by the non-left-recursive productions

 disp

 tabularl_c l







 tabular

 disp

 without_changing the strings derivable_from This rule by

 itself suffices for many grammars



 exnonrec-expr-ex

 The non-left-recursive expression grammar

 (topdown-expr-display) repeated_here

 disp

 tabularl_c l

 E_T E



 E_T E



 T_F T



 T_F T



 F ( E

 ) id

 tabular

 disp

 is obtained_by eliminating_immediate left_recursion from the

 expression grammar_(expr-gram-display) The left-recursive

 pair of productions for EE

 T_T are replaced_by E_T

 E and E TE

 The new productions for T and T

 are similarly obtained_by eliminating_immediate left_recursion

 ex



 Immediate left_recursion can be_eliminated by the following

 technique which works for any number of -productions First

 group the productions as



 disp

 tabularl_c l







 tabular

 disp

 where no begins_with an Then replace the

 -productions by



 disp

 tabularl_c l









 tabular

 disp

 The nonterminal generates the same strings as before but is no

 longer left_recursive This procedure eliminates all left

 recursion from the and productions (provided no

 is ) but it does_not eliminate_left

 recursion involving derivations of two or_more steps For

 example consider the grammar



 displayleftrec2-display

 317ptstabularl_c l







 tabular

 (leftrec2-display)

 display

 The nonterminal is left-recursive because

 but it is not immediately left_recursive



 Algorithm left-recursion-alg below systematically

 eliminates left_recursion from a grammar It is guaranteed to

 work if the grammar has no cycles (derivations of the form

 ) or -productions (productions of the

 form ) Cycles can be systematically

 eliminated from a grammar as can -productions (see the

 exercises at the end of this chapter)





 algleft-recursion-alg Eliminating left_recursion



 Input Grammar with no cycles or

 -productions



 Output An_equivalent grammar with no

 left_recursion



 Method Apply the algorithm in

 Fig left-recursion-fig to Note_that the resulting

 non-left-recursive grammar may have -productions

 alg



 figurehtfb

 tabbing

 1 Arrange the nonterminals in some order







 2For each from to do



 For each from to do



 replace each production of the form

 by



 the productions







 where

 are all the current -productions







 Eliminate the immediate_left recursion_among the

 -productions





 tabbing

 Algorithm to eliminate_left recursion from a grammar

 left-recursion-fig

 figure



 The procedure in Fig left-recursion-fig works as_follows

 In the first iteration for in the outer for-loop of step

 (2) it eliminates any immediate_left recursion_among

 productions Any remaining productions of the form

 must therefore have Sweeping

 through the nonterminals after the iteration of

 the other for-loop for all cleaned nonterminals where

 any production must have

 As a result on the next iteration the inner_loop (on

 ) progressively raises the lower limit in any production

 until we have Then

 eliminating_immediate left-recursion for the productions

 forces to be greater_than



 exleft-recursion-ex

 Let_us apply this procedure to the grammar

 (leftrec2-display) Technically Algorithm

 left-recursion-alg is not guaranteed to work because of the

 -production but in this case the production

 turns_out to be harmless



 We order the nonterminals There is no immediate_left

 recursion_among the -productions so nothing_happens during

 step_(2) for the case For we substitute for in

 to obtain the following -productions



 disp

 tabularr_c l





 tabular

 disp

 Eliminating the immediate_left recursion_among these

 -productions yields the following grammar



 disp

 tabularl_c l











 tabular

 disp

 ex



 Left Factoring

 Left factoring is a grammar transformation that is useful for

 producing a grammar suitable for predictive_parsing When the

 choice between two alternative -productions is not clear we

 may be_able to rewrite the productions to defer the decision until

 enough of the input is seen to make the right choice



 For_example if we have the two productions

 disp

 tabularr_c l

 stmt if expr_then

 stmt_else stmt



 if expr_then stmt

 tabular

 disp

 on seeing the input if we cannot immediately tell which

 production to choose to expand stmt In_general if

 are two

 -productions and the input begins_with a nonempty string

 derived_from we do_not know whether to expand to

 or However we may defer the

 decision by expanding to Then after seeing the

 input derived_from we expand to or to

 That is left-factored the original productions

 become



 disp

 tabularl_c l







 tabular

 disp



 algleft-factor-alg Left factoring a grammar



 Input Grammar



 Output An_equivalent left-factored

 grammar



 Method For each nonterminal find the

 longest_prefix common to two or_more of its alternatives

 If ie there is a nontrivial common

 prefix replace all of the -productions



 where represents all alternatives that do

 not begin_with by



 disp

 tabularl_c l









 tabular

 disp

 Here is a new nonterminal Repeatedly apply this

 transformation until_no two alternatives for a nonterminal have a

 common prefix

 alg



 exleft-factoring-ex

 The following grammar abstracts the dangling-else problem



 displaydangling-gram-display

 317ptstabularl_c l







 tabular

 (dangling-gram-display)

 display

 Here and stand_for if then and

 else and for expression and statement

 Left-factored this grammar becomes



 displaydangling-factored-display

 317ptstabularl_c l











 tabular

 (dangling-factored-display)

 display

 Thus we may expand to on input and wait_until

 has_been seen to decide_whether to expand to or

 to Of_course these grammars are both ambiguous and

 on input it will not be clear which alternative for

 should be chosen Example ambig-parsing-table-ex discusses

 a way out of this dilemma

 ex



 Non-Context-Free Language Constructs

 A few syntactic_constructs found in many programming_languages

 cannot be specified using grammars alone Here we consider a

 couple of these constructs using simple abstract languages to

 illustrate the difficulties



 ex

 The language in this example abstracts the problem of checking

 that identifiers are declared before they are used in a program

 The language consists of strings of the form where the

 first represents the declaration of an_identifier

 represents an intervening program_fragment and the second

 represents the use of the identifier



 The abstract language is

 consists of all words composed of a

 repeated string of 's and 's separated_by such_as

 While it is beyond the scope of this_book to prove it

 the non-context-freedom of directly implies the

 non-context-freedom of programming_languages like C and Java

 which require declaration of identifiers before their use and

 which allow identifiers of arbitrary length



 For this reason a grammar for C or Java does_not specify the

 characters in an_identifier Instead all identifiers are

 represented_by a token such_as id in the grammar In a

 compiler for such a language the semantic analysis phase checks

 that identifiers are declared before they are used

 ex



 ex

 The non-context-free language in this example abstracts the

 problem of checking that the number of formal_parameters in the

 declaration of a function agrees with the number of actual

 parameters in a use of the function The language consists of

 strings of the form (Recall means

 written times) Here and could represent the

 formal_parameter lists of two functions declared to have and

 arguments respectively and and represent the

 actual_parameter lists in calls to these two functions



 The abstract language is

 That is consists of strings

 in the language generated_by the regular_expression

 such that the number of 's and 's are equal and the number

 of 's and 's are equal This language is not context free



 Again the typical syntax of function declarations and uses does

 not concern itself with counting the number of parameters For

 example a function call in C-like language might be specified_by



 disp

 tabularr_c l

 stmt id ( exprlist

 )



 exprlist exprlist

 expr



 expr

 tabular

 disp

 with suitable productions for expr Checking that the

 number of parameters in a call is correct is usually done during

 the semantic analysis phase

 ex

 Writing a Grammar

 writing-grammars-sect



 Grammars are capable of describing most but not all of the

 syntax of programming_languages For_instance the requirement

 that identifiers be declared before they are used cannot be

 described by a context-free_grammar Therefore the sequences of

 tokens accepted_by a parser form a superset of the programming

 language subsequent phases of the compiler must analyze the

 output of the parser to ensure compliance with rules that are not

 checked by the parser



 This_section begins_with a discussion of how to divide work

 between a lexical_analyzer and a parser We then consider several

 transformations that could be applied to get a grammar more

 suitable for parsing One technique can eliminate ambiguity in the

 grammar and other techniques - left-recursion elimination and

 left_factoring - are useful for rewriting grammars so they

 become suitable for top-down_parsing We_conclude this_section by

 considering some programming_language constructs that cannot be

 described by any grammar



 Lexical Versus Syntactic Analysis

 lex-syn-subsect



 As we observed in Section cfg-re-subsect everything that

 can be described by a regular_expression can also be described by

 a grammar We may therefore reasonably ask Why use regular

 expressions to define the lexical syntax of a language There

 are several reasons

 enumerate

 Separating the syntactic_structure of a language into

 lexical and non-lexical parts provides a convenient_way of

 modularizing the front_end of a compiler into two manageable-sized

 components

 The lexical rules of a language are frequently

 quite_simple and to describe them we do_not need a notation as

 powerful as grammars

 Regular_expressions generally provide

 a more concise and easier-to-understand notation for tokens than

 grammars

 More efficient lexical_analyzers can be

 constructed automatically from regular_expressions than from

 arbitrary grammars



 enumerate



 There_are no firm guidelines as to what to put into the lexical

 rules as opposed to the syntactic rules Regular_expressions are

 most useful for describing the structure of constructs such_as

 identifiers constants keywords and white_space Grammars on

 the other_hand are most useful for describing nested structures

 such_as balanced parentheses matching begin-end's corresponding

 if-then-else's and so on These nested structures cannot be

 described by regular_expressions



 Eliminating Ambiguity

 elim-amb-subsect



 Sometimes an ambiguous_grammar can be_rewritten to eliminate the

 ambiguity As an example we_shall eliminate the ambiguity from

 the following dangling-else_grammar



 displayif-gram-display

 317ptstabularr c_l

 stmt if expr_then stmt



 if expr_then stmt_else stmt



 other

 tabular

 (if-gram-display)

 display

 Here other stands_for any other statement According to

 this grammar the compound conditional_statement

 center

 if then else if then else

 center



 figurehtfb

 center



 Parse_tree for a conditional_statement

 if-tree-fig

 center

 figure



 has the parse_tree shown in

 Fig if-tree-figfootnoteThe subscripts on and

 are just to distinguish different occurrences of the same

 nonterminal and do_not imply distinct

 nonterminalsfootnote Grammar (if-gram-display) is

 ambiguous since the string

 displayambig-display

 if then if then

 else (ambig-display)

 display

 has the two parse_trees shown in Fig if-ambig-fig



 figurehtfb

 center



 Two parse_trees for an ambiguous sentence

 if-ambig-fig

 center

 figure



 In all programming_languages with conditional_statements of this

 form the first parse_tree is preferred The general rule is

 Match each else with the closest unmatched thenfootnoteWe should note_that C and its derivatives

 are included in this class Even_though the C family of languages

 do_not use the keyword then its role is played by the

 closing parenthesis for the condition that follows iffootnote This disambiguating rule can theoretically be

 incorporated directly into a grammar but in practice it is rarely

 built into the productions



 exmatched-if-ex

 We can rewrite the dangling-else_grammar (if-gram-display)

 as the following unambiguous_grammar The idea is that a

 statement appearing between a then and an else must_be

 matched that is the interior statement

 must not end with an unmatched or open

 then A matched statement is either an if-then-else statement containing no open statements or it is

 any other kind of unconditional statement Thus we may use the

 grammar in Fig unamb-ite-fig

 This grammar generates the same strings as the dangling-else

 grammar_(if-gram-display) but it allows only one parsing

 for string (ambig-display) namely the one that associates

 each else with the closest previous unmatched then

 ex



 figurehtfb



 center

 tabularr_c l

 stmt matchedstmt



 openstmt



 matchedstmt if expr_then matchedstmt else matchedstmt



 other



 openstmt if expr_then stmt



 if expr_then matchedstmt else openstmt

 tabular

 center



 Unambiguous grammar for if-then-else statements

 unamb-ite-fig



 figure



 Elimination of Left Recursion

 left-rec-elim-subsect



 A grammar is left_recursive if it has a nonterminal such

 that there is a derivation for

 some string Top-down parsing methods cannot handle

 left-recursive grammars so a transformation is needed to

 eliminate_left recursion In Section left-rec-subsect

 we discussed immediate_left recursion where there is a production of the form

 Here we study the general case In

 Section left-rec-subsect we showed how the left-recursive pair of

 productions could be replaced

 by the non-left-recursive productions



 center

 tabularl









 tabular

 center

 without_changing the strings derivable_from This rule by

 itself suffices for many grammars



 exnonrec-expr-ex

 The non-left-recursive expression grammar

 (topdown-expr-display) repeated_here



 center

 tabularl





















 tabular

 center



 is obtained_by eliminating_immediate left_recursion from

 the expression grammar_(expr-gram-display) The

 left-recursive pair of productions are

 replaced_by and

 The new productions for and are obtained similarly by

 eliminating_immediate left_recursion

 ex



 Immediate left_recursion can be_eliminated by the following

 technique which works for any number of -productions First

 group the productions as



 center

 tabularl_c l







 tabular

 center

 where no begins_with an Then replace the

 -productions by



 center

 tabularl











 tabular

 center

 The nonterminal generates the same strings as before but is no

 longer left_recursive This procedure eliminates all left

 recursion from the and productions (provided no

 is ) but it does_not eliminate_left

 recursion involving derivations of two or_more steps For

 example consider the grammar



 displayleftrec2-display

 317ptstabularl









 tabular

 (leftrec2-display)

 display

 The nonterminal is left_recursive because

 but it is not immediately left_recursive



 Algorithm left-recursion-alg below systematically

 eliminates left_recursion from a grammar It is guaranteed to

 work if the grammar has no cycles (derivations of the form

 ) or -productions (productions of the

 form ) Cycles can be

 eliminated systematically

 from a grammar as can -productions

 (see Exercises ep-free-exer and no-cycles-exer)



 alg

 left-recursion-alg

 Eliminating left_recursion



 Grammar with no cycles or

 -productions



 An_equivalent grammar with no

 left_recursion



 Apply the algorithm in

 Fig left-recursion-fig to Note_that the resulting

 non-left-recursive grammar may have -productions

 alg



 figurehtfb



 center

 tabularr_l

 1) arrange the nonterminals in some order





 2) for ( each from to )



 3) for ( each from to )



 4) replace each production of the form

 by the



 productions

 where





 are all current -productions



 5)



 6) eliminate the immediate_left recursion_among the

 -productions



 7)



 tabular

 center



 Algorithm to eliminate_left recursion from a grammar

 left-recursion-fig



 figure



 The procedure in Fig left-recursion-fig works as_follows

 In the first iteration for the outer for-loop of lines_(2)

 through_(7)

 eliminates any immediate_left recursion_among

 -productions Any remaining productions of the form

 must therefore have

 After the st iteration of

 the outer for-loop all nonterminals where

 are cleaned that is

 any production must have

 As a result on the th_iteration the inner_loop of lines

 (3) through (5)

 progressively raises the lower limit in any production

 until we have Then

 eliminating_immediate left_recursion for the productions

 at line_(6) forces to be greater_than



 ex

 left-recursion-ex

 Let_us apply_Algorithm left-recursion-alg to the grammar

 (leftrec2-display) Technically the algorithm

 is not guaranteed to work because of the

 -production but in this case the production

 turns_out to be harmless



 We order the nonterminals There is no immediate_left

 recursion_among the -productions so nothing_happens during

 the outer_loop for For we substitute for in

 to obtain the following -productions



 center

 tabularl





 tabular

 center

 Eliminating the immediate_left recursion_among these

 -productions yields the following grammar



 center

 tabularl













 tabular

 center

 ex



 Left Factoring

 left-factor-subsect



 Left factoring is a grammar transformation that is useful for

 producing a grammar suitable for predictive or top-down_parsing When the

 choice between two alternative -productions is not clear we

 may be_able to rewrite the productions to defer the decision until

 enough of the input has_been seen that we can make the right choice



 For_example if we have the two productions



 center

 tabularl_l l

 stmt if expr_then stmt_else stmt



 if expr_then stmt

 tabular

 center

 on seeing the input if we cannot immediately tell which

 production to choose to expand stmt In_general if

 are two

 -productions and the input begins_with a nonempty string

 derived_from we do_not know whether to expand to

 or However we may defer the

 decision by expanding to Then after seeing the

 input derived_from we expand to or to

 That is left-factored the original productions

 become



 center

 tabularl









 tabular

 center



 alg

 left-factor-alg

 Left factoring a grammar



 Grammar



 An_equivalent left-factored

 grammar



 For each nonterminal find the

 longest_prefix common to two or_more of its alternatives

 If - ie there is a nontrivial common

 prefix - replace all of the -productions



 where represents all alternatives that do

 not begin_with by



 center

 tabularl











 tabular

 center

 Here is a new nonterminal Repeatedly apply this

 transformation until_no two alternatives for a nonterminal have a

 common prefix

 alg



 exleft-factoring-ex

 The following grammar abstracts the dangling-else problem



 displaydangling-gram-display

 317ptstabularl









 tabular

 (dangling-gram-display)

 display

 Here and stand_for if then and else and stand_for conditional expression and statement

 Left-factored this grammar becomes



 displaydangling-factored-display

 317ptstabularl











 tabular

 (dangling-factored-display)

 display

 Thus we may expand to on input and wait_until

 has_been seen to decide_whether to expand to or

 to Of_course these grammars are both ambiguous and

 on input it will not be clear which alternative for

 should be chosen Example ambig-parsing-table-ex discusses

 a way out of this dilemma

 ex



 Non-Context-Free Language Constructs

 non-cfl-subsect



 A few syntactic_constructs found in typical programming_languages

 cannot be specified using grammars alone Here we consider

 two of these constructs using simple abstract languages to

 illustrate the difficulties



 ex

 decl-before-use-ex

 The language in this example abstracts the problem of checking

 that identifiers are declared before they are used in a program

 The language consists of strings of the form where the

 first represents the declaration of an_identifier

 represents an intervening program_fragment and the second

 represents the use of the identifier



 The abstract language is is in



 consists of all words composed of a

 repeated string of 's and 's separated_by such_as

 While it is beyond the scope of this_book to prove it

 the non-context-freedom of directly implies the

 non-context-freedom of programming_languages like C and Java

 which require declaration of identifiers before their use and

 which allow identifiers of arbitrary length



 For this reason a grammar for C or Java does_not distinguish_among

 identifiers that are different character_strings

 Instead all identifiers are

 represented_by a token such_as id in the grammar In a

 compiler for such a language the semantic-analysis phase checks

 that identifiers are declared before they are used

 ex



 ex

 The non-context-free language in this example abstracts the

 problem of checking that the number of formal_parameters in the

 declaration of a function agrees with the number of actual

 parameters in a use of the function The language consists of

 strings of the form (Recall means

 written times) Here and could represent the

 formal-parameter lists of two functions declared to have and

 arguments respectively while and represent the

 actual-parameter lists in calls to these two functions



 The abstract language is

 That is consists of strings

 in the language generated_by the regular_expression





 such that the number of 's and 's are equal and the number

 of 's and 's are equal This language is not context free



 Again the typical syntax of function declarations and uses does

 not concern itself with counting the number of parameters For

 example a function call in C-like language might be specified_by



 center

 tabularr_c l

 stmt id ( exprlist

 )



 exprlist exprlist_expr



 expr

 tabular

 center

 with suitable productions for expr Checking that the

 number of parameters in a call is correct is usually done during

 the semantic-analysis phase

 ex



 sexer

 rewrite-gram-exer

 The following is a grammar for regular_expressions over symbols and

 only using in place

 of for union to avoid conflict with the use of vertical bar as a

 metasymbol in grammars



 center

 tabularl_l l

 rexpr rexpr rterm rterm



 rterm rterm rfactor rfactor



 rfactor rfactor rprimary



 rprimary a b



 tabular

 center



 itemize



 a) Left factor this grammar

 b) Does left_factoring make the grammar suitable for top-down

 parsing

 c) In_addition to left_factoring eliminate_left recursion from the

 original grammar

 d)_Is the resulting grammar suitable for top-down_parsing



 itemize

 sexer



 exer

 Repeat_Exercise rewrite-gram-exer on the following grammars



 itemize



 a) The grammar of Exercise_cfg-exer

 b) The grammar of Exercise more-cfgs-exer(a)

 c) The grammar of Exercise more-cfgs-exer(c)

 d) The grammar of Exercise more-cfgs-exer(e)

 e) The grammar of Exercise more-cfgs-exer(g)



 itemize

 exer



 hexer

 The following grammar is proposed to remove the dangling-else

 ambiguity discussed in Section elim-amb-subsect



 center

 tabularl_c l

 stmt if expr_then stmt



 matchedStmt



 matchedStmt if expr_then matchedStmt else_stmt



 other



 tabular

 center

 Show that this grammar is still ambiguous

 hexer



 tocchapter12Interprocedural Analysis903

 lof10

 lot10

 ptr-ch12903

 tocsection121Basic Concepts904

 tocsubsection1211Call Graphs904

 fn-ptr-ex121904

 loffigure121A program with a function pointer905

 fn-ptr1-fig121905

 loffigure122Call graphs derived_from Fig121906

 fn-ptr2-fig122906

 tocsubsection1212Context Sensitivity906

 secipa-intro-cs1212906

 ipa-1-ex122906

 loffigure123A program_fragment illustrating the need for context-sensitive analysis907

 ipa-1-fig123907

 loffigure124The control-flow_graph for Fig123 treating function calls as control flow908

 supergraph-fig124908

 tocsubsection1213Call Strings908

 ipa-call-strings1213908

 ipa-2-ex124908

 loffigure125Program fragment illustrating call strings909

 ipa-2-fig125909

 loffigure126Recursive program requiring analysis of complete call strings909

 ipa-recurse-fig126909

 ipa-recurse-ex125910

 tocsubsection1214Cloning-Based Context-Sensitive Analysis910

 ipa-intro-cloning1214910

 ipa-2clone-ex126910

 loffigure127Cloned version of Fig125911

 ipa-2clone-fig127911

 ipa-recurse-clone-ex127911

 tocsubsection1215Summary-Based Context-Sensitive Analysis911

 ipa-intro-summary1215911

 loffigure128Cloned version of Fig126912

 ipa-recurse-clone-fig128912

 ipa-2-mod-ex128913

 loffigure129Result of propagating all possible constant arguments to the function 914

 ipa-2-mod-fig129914

 tocsubsection1216Exercises for Section 121914

 call-graph-exer1211914

 loffigure1210Program for Exercise1211915

 call-graph-exer-fig1210915

 clone-exer1212915

 loffigure1211Code fragment for Exercise1212916

 clone-exer-fig1211916

 tocsection122Why Interprocedural Analysis916

 tocsubsection1221Virtual Method Invocation916

 tocsubsection1222Pointer Alias Analysis917

 tocsubsection1223Parallelization917

 tocsubsection1224Detection of Software Errors and Vulnerabilities917

 tocsubsection1225SQL Injection918

 sql-inj-ex1210918

 tocsubsection1226Buffer Overflow920

 tocsection123A Logical Representation of Data Flow921

 datalog-sect123921

 tocsubsection1231Introduction to Datalog921

 atom-ex1211922

 loffigure1212Representing the value of a predicate by a relation922

 atom-rel-fig1212922

 tocsubsection1232Datalog Rules922

 path-ex1212923

 tocsubsection1233Intensional and Extensional Predicates924

 drd-ex1213924

 loffigure1213A basic_block with points between statements925

 drd1-fig1213925

 loffigure1214Rules for predicate 925

 drd2-fig1214925

 drd2-ex1214926

 loffigure1215Rules for predicates and 927

 drd3-fig1215927

 tocsubsection1234Execution of Datalog Programs927

 datalog-eval-subsect1234927

 naive-eval-alg1215927

 naive-eval-ex1216927

 loffigure1216Evaluation of Datalog programs928

 naive-eval-fig1216928

 tocsubsection1235Incremental Evaluation of Datalog Programs928

 seminaive-ex1217929

 loffigure1217Incremental rules for the path Datalog program929

 seminaive1-fig1217929

 seminaive-alg1218929

 loffigure1218Evaluation of Datalog programs930

 seminaive2-fig1218930

 tocsubsection1236Problematic Datalog Rules930

 tocsubsubsectionRule Safety931

 safety-ex1219931

 tocsubsubsectionStratified Datalog931

 stratified-ex1220931

 tocsubsection1237Exercises for Section 123932

 dl-rd-exer1231932

 loffigure1219Datalog program for a simple reaching-definitions analysis932

 dl-rd-exer-fig1219932

 tocsection124A Simple Pointer-Analysis Algorithm933

 andersen-sect124933

 tocsubsection1241Why is Pointer_Analysis Difficult934

 tocsubsection1242A Model for Pointers and References935

 tocsubsection1243Flow Insensitivity936

 sens-ex1222936

 loffigure1220Java code for Example1222936

 sens-fig1220936

 insens-ex1223937

 tocsubsection1244The Formulation in Datalog937

 copy-item2937

 loffigure1221Datalog program for flow-insensitive pointer analysis938

 andersen-dl-fig1221938

 tocsubsection1245Using Type Information938

 loffigure1222Java program with a type error939

 andersen-type-fig1222939

 tocsubsection1246Exercises for Section 124939

 andersen-exer1241939

 loffigure1223Adding type restrictions to flow-insensitive pointer analysis940

 andersen-types-dl-fig1223940

 loffigure1224Code for Exercise1241940

 andersen-exer-fig1224940

 loffigure1225Example code for pointer analysis941

 andersen2-fig1225941

 tocsection125Context-Insensitive Interprocedural-Analysis941

 virt-meth-sect125941

 tocsubsection1251Effects of a Method Invocation941

 virt-meth-ex1224942

 loffigure1226A virtual_method invocation943

 virt-meth-code-fig1226943

 tocsubsection1252Call Graph_Discovery in Datalog943

 cg-discovery-sect1252943

 loffigure1227Datalog program for call-graph discovery944

 cgdiscovery-fig1227944

 tocsubsection1253Dynamic Loading and Reflection944

 reflection-ex1225944

 tocsubsection1254Exercises for Section 125945

 virt-meth-exer1251945

 tocsection126Context-Sensitive Pointer Analysis945

 csens-sect126945

 tocsubsection1261Contexts and Call Strings946

 context-call-subsect1261946

 context-ex1226947

 loffigure1228Methods and call_sites for a running example948

 five-fns-fig1228948

 tocsubsection1262Adding Context to Datalog Rules949

 loffigure1229Datalog program for context-sensitive_points-to analysis949

 csens-fig1229949

 tocsubsection1263Additional Observations About Sensitivity949

 tocsubsection1264Exercises for Section 126950

 loffigure1230Code for Exercises1261 and 1262950

 csens-exer-fig1230950

 csens1-exer1261950

 csens2-exer1262951

 tocsection127Datalog Implementation by BDD's951

 bdd-sect127951

 tocsubsection1271Binary Decision Diagrams951

 loffigure1231A binary_decision diagram952

 bdd1-fig1231952

 tocsubsection1272Transformations on BDD's953

 loffigure1232Transformations on BDD's953

 bdd2-fig1232953

 tocsubsection1273Representing Relations by BDD's954

 rel-bdd-ex1228954

 tocsubsection1274Relational Operations as BDD Operations954

 tocsubsubsectionBDD's for Single Tuples955

 tocsubsubsectionUnion956

 bdd-union-alg1229956

 bdd-union-ex1230956

 loffigure1233Constructing the BDD for a logical OR957

 bdd3-fig1233957

 tocsubsection1275Using BDD's for Points-to Analysis957

 tocsubsection1276Exercises for Section 127958

 tocsection128Summary of Chapter 12958

 tocsection129References for Chapter 12961

 Allen741962

 and942962

 ad053962

 br004962

 b065962

 banning796962

 barth787962

 b038962

 bryant869962

 bp0010963

 cc8611963

 ec0012963

 eg9413963

 fr0014963

 ht0115963

 l0516963

 ll0517963

 mr0218963

 rc0419963

 rl0420963

 sp81x21964

 steensgaard9622964

 uw0223964

 wl0424964

 zhu0225964

 uullmanalsuch12ch12

 page965

 equation0

 enumi25

 enumii2

 enumiii0

 enumiv0

 footnote3

 mpfootnote0

 part0

 chapter12

 section9

 subsection0

 subsubsection0

 paragraph0

 subparagraph0

 figure33

 table0

 parentequation0

 theorem30

 exerz0



 Parser Generators

 yacc-sect



 This_section shows_how a parser_generator can be used to

 facilitate the construction of the front_end of a compiler We

 shall use the LALR_parser generator Yacc as the basis of our

 discussion since it implements many of the concepts discussed in

 the previous two sections and it is widely available Yacc stands

 for yet another compiler-compiler reflecting the popularity of

 parser_generators in the early 1970's when the first version of

 Yacc was created by S_C Johnson Yacc is available as a command

 on the UNIX_system and has_been used to help implement

 hundreds of compilers



 The Parser_Generator Yacc

 A translator can be constructed using Yacc in the manner

 illustrated in Fig yacc-how-fig First a file say

 translatey containing a Yacc_specification of the translator is

 prepared The UNIX_system command

 disp

 verbatim

 yacc translatey

 verbatim

 disp

 figurehtfb

 center



 Creating an inputoutput translator with Yacc

 yacc-how-figcenter

 figure

 transforms the file translatey into a C program called

 ytabc using the LALR method outlined in

 Algorithm_propagate-alg The program ytabc is a

 representation of an LALR_parser written in C along with other C

 routines that the user may have prepared The LALR_parsing table

 is compacted as described in Section_lr-parsers-sect By

 compiling ytabc along with the ly library that

 contains the LR_parsing program using the command

 disp

 verbatim

 cc ytabc -ly

 verbatim

 disp

 we obtain the desired object program aout that performs the

 translation specified_by the original Yacc program(The

 name ly is system dependent) If other procedures are

 needed they can be compiled or loaded with ytabc just as

 with any C program



 A Yacc source_program has three_parts

 disp

 declarations







 translation rules







 supporting C routines

 disp



 exyacc-calc-ex

 To illustrate_how to prepare a Yacc source_program let_us

 construct a simple desk_calculator that reads an arithmetic

 expression evaluates it and then prints its numeric value We

 shall build the desk_calculator starting_with the with the

 following grammar for arithmetic_expressions



 disp

 tabularl_c l

 E_E T_T



 T_T F_F



 F ( E )

 digit

 tabular

 disp

 The token digit is a single digit between 0 and

 9 A Yacc desk_calculator program derived_from this grammar is

 shown in Fig yacc-calc-fig

 ex





 The declarations part There_are two optional sections in

 the declarations part of a Yacc program In the first section we

 put ordinary C declarations delimited by '

 '

 translation rules or procedures of the second and third sections

 In Fig yacc-calc-fig this_section contains only the

 include-statement

 figurehtb

 verbatim



 include ctypeh



 verbatim

 verbatim



 verbatim

 verbatim



 line expr '_printf(



 expr_expr '' term





 3

 factor



 factor '('_expr ')'





















































































































































 ' refers to the attribute

 value associated_with the nonterminal on the left while

 refers to the value associated_with the th grammar

 symbol (terminal or nonterminal) on the right The semantic_action

 is performed whenever we reduce by the associated production so

 normally the semantic_action computes a value for '

















 3



 term





 disp

 Note_that the nonterminal term in the first production is

 the third grammar symbol on the right while '' is the

 second The semantic_action associated_with the first production

 adds the value of the expr and the term on the right

 and assigns the result as the value for the nonterminal expr

 on the left We have omitted the semantic_action for the second

 production altogether since copying the value is the default

 action for productions with a single grammar symbol on the right

 In_general '' '













































































































































































 3



 expr '-' expr

 3



 expr_'' expr



 2



 NUMBER









 yylex()



 int c



 while_( ( c getchar() ) '_' )



 if_( (c '') (isdigit(c)) )



 ungetc(c stdin)



 scanf(

 return NUMBER







 return c







 disp

 Yacc_specification for a more advanced desk_calculator

 yacc-ambig-fig

 figure



 Since the grammar in the Yacc_specification in

 Fig_yacc-ambig-fig is ambiguous the LALR algorithm will

 generate parsing_action conflicts Yacc reports the number of

 parsing_action conflicts that are generated A description of the

 sets of items and the parsing_action conflicts can be obtained_by

 invoking Yacc with a -v option This option generates an

 additional file youtput that contains the kernels of the

 sets of items found for the grammar a description of the parsing

 action_conflicts generated_by the LALR algorithm and a readable

 representation of the LR_parsing table showing how the parsing

 action_conflicts were resolved Whenever Yacc reports that it has

 found parsing_action conflicts it is wise to create and consult

 the file youtput to see_why the parsing_action conflicts

 were generated and to see whether they_were resolved correctly



 Unless otherwise instructed Yacc will resolve all parsing_action

 conflicts using the following two rules

 enumerate



 A reducereduce_conflict is resolved by choosing the conflicting

 production listed_first in the Yacc_specification





 A shiftreduce_conflict is resolved in favor of shift This rule

 resolves the shiftreduce_conflict arising from the dangling-else

 ambiguity correctly

 enumerate



 Since these default rules may not always be what the compiler

 writer wants Yacc provides a general mechanism for resolving

 shiftreduce_conflicts In the declarations portion we can assign

 precedences and associativities to terminals The declaration

 disp

 verbatim



 verbatim

 disp

 makes and - be of the same precedence and be left

 associative We can declare an operator to be right associative by

 writing

 disp

 verbatim



 verbatim

 disp

 and we can force an operator to be a nonassociative binary

 operator (ie two_occurrences of the operator cannot be_combined

 at all) by writing

 disp

 verbatim



 verbatim

 disp



 The tokens are given precedences in the order in which they appear

 in the declarations part lowest first Tokens in the same

 declaration have the same precedence Thus the declaration

 disp

 verbatim



 verbatim

 disp

 in Fig_yacc-ambig-fig gives the token UMINUS a

 precedence_level higher_than that of the five preceding terminals



 Yacc resolves shiftreduce_conflicts by_attaching a precedence and

 associativity to each production involved in a conflict as_well

 as to each terminal involved in a conflict If it must choose

 between shifting input_symbol and reducing by production

 Yacc reduces if the precedence of the

 production is greater_than that of or if the precedences are

 the same and the associativity of the production is left

 Otherwise shift is the chosen action



 Normally the precedence of a production is taken to be the same

 as that of its rightmost terminal This is the sensible decision

 in most cases For_example given productions

 disp

 E_E

 E_EE

 disp

 we would prefer to reduce by E

 EE with lookahead because the

 in the right_side has the same precedence as the lookahead but is

 left_associative With lookahead we would prefer to

 shift because the lookahead has higher_precedence than the

 in the production



 In those situations_where the rightmost terminal does_not supply

 the proper precedence to a production we can force a precedence

 by appending to a production the tag

 disp



 disp

 The precedence and associativity of the production will then be

 the same as that of the terminal which presumably is defined in

 the declaration section Yacc does_not report shiftreduce

 conflicts that are resolved using this precedence and

 associativity mechanism



 This terminal can be a placeholder like UMINUS in

 Fig_yacc-ambig-fig this terminal is not returned by the

 lexical_analyzer but is declared solely to define a precedence

 for a production In Fig_yacc-ambig-fig the declaration

 disp



 disp

 assigns to the token UMINUS a precedence that is higher_than

 that of and In the translation rules part the

 tag

 disp



 disp

 at the end of the production

 disp

 expr '-' expr

 disp

 makes the unary_minus operator in this production have a higher

 precedence_than any other operator



 Creating Yacc Lexical Analyzers with Lex



 Lex was designed to produce lexical_analyzers that could be used

 with Yacc The Lex library ll will provide a driver program

 named yylex() the name required by Yacc for its lexical

 analyzer If Lex is used to produce the lexical_analyzer we

 replace the routine yylex() in the third part of the Yacc

 specification by the statement

 disp

 verbatim

 include lexyyc

 verbatim

 disp

 and we have each Lex action return a terminal known to Yacc By

 using the include lexyyc statement the program

 yylex has access to Yacc's names for tokens since the Lex

 output_file is compiled as part of the Yacc output_file

 ytabc



 Under the UNIX_system if the Lex_specification is in the

 file firstl and the Yacc_specification in secondy

 we can say

 disp

 lex firstl



 yacc secondy



 cc ytabc -ly -ll

 disp

 to obtain the desired translator



 The Lex_specification in Fig lex-spec-fig can be used in

 place of the lexical_analyzer in Fig_yacc-ambig-fig The

 last pattern is since in Lex matches any

 character but a newline



 figurehtfb

 disp

 number 0-90-90-9





 skip blanks



 number sscanf(yytext

 return NUMBER



 return yytext0

 disp

 Lex_specification for yylex() in

 Fig_yacc-ambig-fig lex-spec-fig

 figure



 Error_Recovery in Yacc



 In Yacc error_recovery can be performed using a form of error

 productions First the user decides what major nonterminals

 will have error_recovery associated_with them Typical choices are

 some subset of the nonterminals generating expressions

 statements blocks and procedures The user then adds to the

 grammar error productions of the form

 where is a major nonterminal and

 is a string of grammar_symbols perhaps the empty_string

 error is a Yacc reserved word Yacc will generate a parser

 from such a specification treating the error productions as

 ordinary productions



 However when the parser generated_by Yacc encounters an error it

 treats the states whose sets of items contain error productions in

 a special way On encountering an error Yacc pops symbols from

 its stack until it finds the topmost state on its stack whose

 underlying set of items includes an item of the form

 The parser then

 shifts a fictitious token error onto the stack as though

 it saw the token error on its input



 When is a reduction to occurs immediately

 and the semantic_action associated_with the production

 (which might be a

 user-specified error-recovery routine) is invoked The parser then

 discards input symbols until it finds an input_symbol on which

 normal_parsing can proceed



 If is not empty Yacc skips ahead on the input looking

 for a substring that can be reduced to If

 consists entirely of terminals then it looks for this string of

 terminals on the input and reduces them by shifting them onto

 the stack At this point the parser will have error

 on top of its stack The parser will then reduce

 error to and resume normal_parsing



 For_example an error production of the form

 disp

 stmt error

 disp

 would specify to the parser that it should skip just beyond the

 next semicolon on seeing an error and assume that a statement had

 been_found The semantic routine for this error production would

 not need to manipulate the input but could generate a diagnostic

 message and set a flag to inhibit generation of object code for

 example



 figurehtfb

 disp



 include ctypeh



 include stdioh



 define YYSTYPE double double type

 for Yacc stack





















 lines lines expr '_printf(

 lines ' empty



 error ' yyerror(reenter last line)



 yyerrok







 expr_expr ''_expr

 3



 expr_'' expr

 3



 '('_expr ')'

 -







































































 Parser Generators

 yacc-sect



 This_section shows_how a parser_generator can be used to

 facilitate the construction of the front_end of a compiler We

 shall use the LALR_parser generator Yacc as the basis of our

 discussion since it implements many of the concepts discussed in

 the previous two sections and it is widely available Yacc

 stands_for yet another compiler-compiler reflecting the

 popularity of parser_generators in the early 1970s when the first

 version of Yacc was created by S_C Johnson Yacc is

 available as a command on the UNIX_system and has_been

 used to help implement many production compilers



 The Parser_Generator Yacc

 A translator can be constructed using Yacc in the manner

 illustrated in Fig yacc-how-fig First a file say translatey containing a Yacc_specification of the translator is

 prepared The UNIX_system command



 center

 'yacc translatey'

 center

 transforms the file translatey into a C program called

 ytabc using the LALR method outlined in

 Algorithm_propagate-alg The program ytabc is a

 representation of an LALR_parser written in C along with other C

 routines that the user may have prepared The LALR_parsing table

 is compacted as described in Section_lr-parsers-sect By

 compiling ytabc along with the ly library that

 contains the LR_parsing program using the command



 center

 'cc ytabc -ly'

 center

 we obtain the desired object program aout that performs the

 translation specified_by the original Yacc

 program(The name ly is system dependent) If other

 procedures are needed they can be compiled or loaded with ytabc just as with any C program



 figurehtfb

 center



 Creating an inputoutput translator with Yacc

 yacc-how-figcenter

 figure



 A Yacc source_program has three_parts



 center

 tabularl

 declarations







 translation rules







 supporting C routines

 tabular

 center



 ex

 yacc-calc-ex

 To illustrate_how to prepare a Yacc source_program let_us

 construct a simple desk_calculator that reads an arithmetic

 expression evaluates it and then prints its numeric value We

 shall build the desk_calculator starting_with the

 following grammar for arithmetic_expressions



 center

 tabularr_c l













 tabular

 center

 The token digit is a single digit between 0 and 9

 A Yacc desk_calculator program derived_from this grammar is

 shown in Fig yacc-calc-fig

 ex



 The Declarations Part



 There_are two sections in

 the declarations part of a Yacc program both

 are optional In the first section we

 put ordinary C declarations delimited by 123 and

 125 Here we place declarations of any temporaries used by the

 translation rules or procedures of the second and third sections

 In Fig yacc-calc-fig this_section contains only the

 include-statement



 figurehtb

 center

 tabularl



 include ctypeh









 line expr '_printf(





 expr_expr '' term





 3



 factor







 factor '('_expr ')'









































































































































































 ' refers to the attribute

 value associated_with the nonterminal of the head while refers to the value associated_with the th grammar

 symbol (terminal or nonterminal) of the body The semantic_action

 is performed whenever we reduce by the associated production so

 normally the semantic_action computes a value for '





















 3



 term





 tabular

 center

 Note_that the nonterminal term in the first production is

 the third grammar symbol of the body while is the

 second The semantic_action associated_with the first production

 adds the value of the expr and the term of the body

 and assigns the result as the value for the nonterminal expr

 of the head We have omitted the semantic_action for the second

 production altogether since copying the value is the default

 action for productions with a single grammar symbol in the body

 In_general '' '





















































































































































































 3



 expr '-' expr

 3



 expr_'' expr



 2



 NUMBER









 yylex()



 int c



 while_( ( c getchar() ) '_' )



 if_( (c '') (isdigit(c)) )



 ungetc(c stdin)



 scanf(

 return NUMBER







 return c







 tabular

 center

 Yacc_specification for a more advanced desk_calculator

 yacc-ambig-fig

 figure



 Since the grammar in the Yacc_specification in

 Fig_yacc-ambig-fig is ambiguous the LALR algorithm will

 generate parsing-action_conflicts Yacc reports the number of

 parsing-action_conflicts that are generated A description of the

 sets of items and the parsing-action_conflicts can be obtained_by

 invoking Yacc with a -v option This option generates an

 additional file youtput that contains the kernels of the

 sets of items found for the grammar a description of the parsing

 action_conflicts generated_by the LALR algorithm and a readable

 representation of the LR_parsing table showing how the parsing

 action_conflicts were resolved Whenever Yacc reports that it has

 found parsing-action_conflicts it is wise to create and consult

 the file youtput to see_why the parsing-action_conflicts

 were generated and to see whether they_were resolved correctly



 Unless otherwise instructed Yacc will resolve all parsing_action

 conflicts using the following two rules

 enumerate

 A reducereduce_conflict is resolved by choosing the conflicting

 production listed_first in the Yacc_specification



 A shiftreduce_conflict is resolved in favor of shift This rule

 resolves the shiftreduce_conflict arising from the dangling-else

 ambiguity correctly

 enumerate



 Since these default rules may not always be what the compiler

 writer wants Yacc provides a general mechanism for resolving

 shiftreduce_conflicts In the declarations portion we can assign

 precedences and associativities to terminals The declaration

 center



 center

 makes and - be of the same precedence and be

 left_associative We can declare an operator to be right associative by

 writing

 center



 center

 and we can force an operator to be a nonassociative binary

 operator (ie two_occurrences of the operator cannot be_combined

 at all) by writing

 center



 center



 The tokens are given precedences in the order in which they appear

 in the declarations part lowest first Tokens in the same

 declaration have the same precedence Thus the declaration

 center



 center

 in Fig_yacc-ambig-fig gives the token UMINUS a

 precedence_level higher_than that of the five preceding terminals



 Yacc resolves shiftreduce_conflicts by_attaching a precedence and

 associativity to each production involved in a conflict as_well

 as to each terminal involved in a conflict If it must choose

 between shifting input_symbol and reducing by production

 Yacc reduces if the precedence of the

 production is greater_than that of or if the precedences are

 the same and the associativity of the production is left

 Otherwise shift is the chosen action



 Normally the precedence of a production is taken to be the same

 as that of its rightmost terminal This is the sensible decision

 in most cases For_example given productions

 center

 E_EE EE

 center

 we would prefer to reduce by EEE with lookahead because the

 in the body has the same precedence as the lookahead but is

 left_associative With lookahead we would prefer to

 shift because the lookahead has higher_precedence than the in the production



 In those situations_where the rightmost terminal does_not supply

 the proper precedence to a production we can force a precedence

 by appending to a production the tag

 center



 center

 The precedence and associativity of the production will then be

 the same as that of the terminal which presumably is defined in

 the declaration section Yacc does_not report shiftreduce

 conflicts that are resolved using this precedence and

 associativity mechanism



 This terminal can be a placeholder like UMINUS in

 Fig_yacc-ambig-fig this terminal is not returned by the

 lexical_analyzer but is declared solely to define a precedence

 for a production In Fig_yacc-ambig-fig the declaration

 center



 center

 assigns to the token UMINUS a precedence that is higher_than

 that of and In the translation rules part the

 tag



 center



 center

 at the end of the production



 center

 expr '-' expr

 center

 makes the unary-minus operator in this production have a higher

 precedence_than any other operator



 Creating Yacc Lexical Analyzers with Lex

 lex-yacc-subsect



 Lex was designed to produce lexical_analyzers that could be used

 with Yacc The Lex library ll will provide a driver program

 named yylex() the name required by Yacc for its lexical

 analyzer If Lex is used to produce the lexical_analyzer we

 replace the routine yylex() in the third part of the Yacc

 specification by the statement

 center

 include lexyyc

 center

 and we have each Lex action return a terminal known to Yacc By

 using the include lexyyc statement the program

 yylex has access to Yacc's names for tokens since the Lex

 output_file is compiled as part of the Yacc output_file ytabc



 Under the UNIX_system if the Lex_specification is in the

 file firstl and the Yacc_specification in secondy

 we can say

 center

 tabularl

 lex firstl



 yacc secondy



 cc ytabc -ly -ll

 tabular

 center

 to obtain the desired translator



 The Lex_specification in Fig lex-spec-fig can be used in

 place of the lexical_analyzer in Fig_yacc-ambig-fig The

 last pattern meaning any character

 must_be written since the dot in Lex matches any

 character except newline



 figurehtfb

 center

 tabularl

 number 0-90-90-9





 skip blanks



 number sscanf(yytext

 return NUMBER



 return yytext0

 tabular

 center

 Lex_specification for yylex() in

 Fig_yacc-ambig-fig lex-spec-fig

 figure



 Error_Recovery in Yacc

 yacc-error-subsect



 In Yacc error_recovery uses a form of error

 productions First the user decides what major nonterminals

 will have error_recovery associated_with them Typical choices are

 some subset of the nonterminals generating expressions

 statements blocks and functions The user then adds to the

 grammar error productions of the form

 where is a major nonterminal and

 is a string of grammar_symbols perhaps the empty_string

 error is a Yacc reserved word Yacc will generate a parser

 from such a specification treating the error productions as

 ordinary productions



 However when the parser generated_by Yacc encounters an error it

 treats the states whose sets of items contain error productions in

 a special way On encountering an error Yacc pops symbols from

 its stack until it finds the topmost state on its stack whose

 underlying set of items includes an item of the form

 The parser then

 shifts a fictitious token error onto the stack as though

 it saw the token error on its input



 When is a reduction to occurs immediately

 and the semantic_action associated_with the production

 (which might be a

 user-specified error-recovery routine) is invoked The parser then

 discards input symbols until it finds an input_symbol on which

 normal_parsing can proceed



 If is not empty Yacc skips ahead on the input looking

 for a substring that can be reduced to If

 consists entirely of terminals then it looks for this string of

 terminals on the input and reduces them by shifting them onto

 the stack At this point the parser will have error

 on top of its stack The parser will then reduce error to and resume normal_parsing



 For_example an error production of the form

 center

 stmt error

 center

 would specify to the parser that it should skip just beyond the

 next semicolon on seeing an error and assume that a statement had

 been_found The semantic routine for this error production would

 not need to manipulate the input but could generate a diagnostic

 message and set a flag to inhibit generation of object code for

 example



 figure

 center

 tabularl



 include ctypeh



 include stdioh



 define YYSTYPE double double type

 for Yacc stack

















 lines lines expr '_printf(

 lines ' empty



 error ' yyerror(reenter previous line)



 yyerrok







 expr_expr ''_expr

 3



 expr_'' expr

 3



 '('_expr ')'

 -

































































































 a



























 a









 TS lw(20p) 0 c1 c1 l

 'E' '-' 'E T T' TE and their associated semantic_actions as

 P expr_expr '' term























 1)

 P to the Yacc_specification This production says_that an input

 to the desk_calculator is to be an expression followed_by a

 newline character The semantic_action associated_with this

 production prints the decimal value of the expression followed_by

 a newline character sp 05 PAGE 261 EQ delim EN PP

 supporting C-routines part The third part of a Yacc

 specification consists of supporting C-routines A lexical

 analyzer by the name 8yylex()must be provided Other

 procedures such_as error_recovery routines may be added as

 necessary PP The lexical_analyzer 8yylex()produces pairs

 consisting of a token and its associated attribute value If a

 token such_as 8DIGITis returned the token must_be

 declared in the first section of the Yacc_specification hw

 yylval The attribute value associated_with a token is communicated

 to the parser through a Yacc-defined variable 8yylval PP

 The lexical_analyzer in Fig FI456 is very crude It reads

 input_characters one at a time using the C-function

 8getchar() If the character is a digit the value of the

 digit is stored in the variable 8yylval and the token

 8DIGITis returned Otherwise the character itself is

 returned as the token sp SH Using Yacc with Ambiguous Grammars

 sp 05 LP Let_us now modify the Yacc_specification so that the

 resulting desk_calculator becomes more useful First we_shall

 allow the desk_calculator to evaluate a sequence of expressions

 one to a line We_shall also allow blank lines between

 expressions We do this by changing the first rule to P

 lines lines expr '_printf(

 lines ' P In Yacc an empty alternative as the

 third line is denotes 'epsilon' PP Second we_shall enlarge the

 class of expressions to include numbers instead of single digits

 and to include the arithmetic operators '' '-' (both binary and

 unary) 'star' and '' The easiest way to specify this class of

 expressions is to use the ambiguous_grammar TS lw(20p) 0 c1 c1 l

 'E' '-' 'EE E-E E star

 E_EE (E) -E ' TE The resulting Yacc

 specification is shown in Fig FI457 KF sp 05 P



 include ctypeh include stdioh define YYSTYPE double

 doubletypeforYaccstack



 sp 05









 sp 05



 lines lines expr '_printf(

 lines ' 'epsilon' expr_expr ''_expr

 3 expr_''

 expr 3

 '('_expr ')'

 -











































































































































































































































 A - bold

 error alphaAalpha

















 A - cdot bold error alpha



 alpha

 epsilonA

 A - bold

 error



 alpha



 alphaalpha





 alpha

 alpha

 A

 stmt - bold error



































 2)

 lines ' empty error ' yyerror(reenter

 last line) yyerrok expr_expr ''_expr

 3 expr_''

 expr 3

 '('_expr ')'

 -

























 lines - cdot











 lines





