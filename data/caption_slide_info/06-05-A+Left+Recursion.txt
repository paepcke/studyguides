3.57
slide
Compilers 1 Left Recursion
In this video, we&#39;re gonna talk about the main difficulty with Recursive Descent Parsing, a problem known as Left Recursion. Let&#39;s consider a very simple 

15.879
slide
Consider a production 5 S a bool 51 term a bool 5 return 810 50 goes into an infinite loop Left Recursion ex mm


15.879
slide
Left Recursion Max m
grammar that consist of only one production, s goes to s followed by a. So 

22.48
slide
Left Recursion Consider the grammar S a so I E 5 95 S kok s L h 54m 1 9 1 S generates all strings starting with a B and followed by any number of
the Recursive Descent Algorithm for this production is the following. So, we just 

28.58
slide
Left Recursion Ingeneral 0 1 Iii E1 i3 m All strings derived from S start with one of 31 3m and continue with several instances of 11 0 n Rewrite as Ems oc1 8 ex mm
have a function called s1 for the first production of s. And it&#39;s going to succeed if the function s succeeds and then after that succeeds we see a terminal a in the input stream. And then we have to write a function for the symbol s itself and since there&#39;s only one alternative, there&#39;s only one production for s we don&#39;t need to worry about backtracking or anything. So as we&#39;ll succeed exactly when as one succeed. There&#39;s only one possibility in this case and now I think you can see the problem what&#39;s going to happen. Well, when we go to parse an input string, we&#39;re going to call the function s which is going to call the function s1. And then what the function as one gonna do well, the very first thing it&#39;s going to do is to call the function s again. And as a result, the function s is going to go into an infinite loop and we&#39;re never going to succeed in parsing any input. This will always go into an infinite loop. So, The reason that this, this grammar doesn&#39;t behave very well is because it is left recursive. And a left recursive grammar is any grammar that has a non-terminal where if you start with that non-terminal and you do some non-empty sequence of re-writes. Notice the plus there. You have to do more than one re-write. So, if you&#39;re actually doing a sequence of replacements, you get back to a situation where you have the same symbol still in the left most position. And you can see, this is not going to be good for parsing. So, in the case of this grammar up here, what happens while we get s goes to sa it goes to saa goes to saaa. And so on and we can always get to a situation where we have a long string of a&#39;s and an s on the left end of the string. And if we always have an s on the left end of the string, we can never manage any input because the only way we manage input is if the first thing we generate is a terminal symbol. But if the first thing is always a non-terminal, we will never make any progress. And, it just doesn&#39;t work. I mean, Recursive Descent does not work with Left-Recursive Grammars. Well, this seems like a major problem with recursive to same parsing. It is a problem but as we&#39;ll see shortly it&#39;s really not so major. So let&#39;s consider a left-recursive grammar that slightly more general form. So here we have two productions now for s, s goes to s followed by something alpha or it goes to something else that doesn&#39;t mention s and let&#39;s call that Beta. And if you think about the language that this generates, it&#39;s gonna join all strings that start with a beta and then follow, and followed by any number of alphas. And, but it does it in a very particular way. So if I write out some, a derivation here where I used a few, where I used the first production a few times. You can see what&#39;s going on. So again, s goes to s followed by alpha. And then s goes to s followed by alpha, alpha and then s goes to s followed by alpha, alpha, alpha and if I repeat this, I get. S followed by any number of alphas and then in one more step, I can. Put in beta and I get beta followed by any number of alpha. So, that&#39;s the proof that it generates that language. That language that begins with a beta and has some sequence of alphas but you can see that it does it right to left, it produces the right under the string first and in fact the very last thing it produces if the first thing that appears in the input and that&#39;s why. It doesn&#39;t work with Recursive Descent Parsing because Recursive Descent Parsing wants to see the first part of the input first and then work left to right. And this grammar is built to produce the string right to left. And therein lies the idea that allow us to fix the problem so we can generate exactly the same language producing the strings from left to right instead of right to left and th e way we do that is to replace left-recursion by right-recursion. And this requires us to add one more symbol in this case to the grammar so instead of having s go to something involving s on the left, we&#39;ll have s go to beta so the first thing notice in the very first position and then it goes to s prime and what does s prime do, well s prime produce what you would expect, a sequence of alphas and it could be the empty sequence. And if you write out some you know? Example derivation here we&#39;ll have s goes to beta s prime. Which goes to now using the rules for s prime goes to beta alpha s prime, Goes to beta alpha. Alpha s prime goes to and after any number of sequent, any number of rewrites we get beta followed by sub sequence of alphas followed by s prime. And then in one more step we use the Epsilon Rule here and we wind up with beta followed by some number of alphas. And so you can see it generates exactly the same string as the first grammar but it does so in a right-recursive way instead of a left-recursive way. So in general, we may have many productions some of which are left-recursive and some of which are not. And the language produced by this particular form of grammar here is gonna be all the strings. They are derived from asst start with one of the betas. So one of the things here that doesn&#39;t involve s and it continues with zero or more instances of the alphas. And we can do exactly the same trick. This is just generalizing the idea that we had before where we only have one beta and one alpha to many betas and many alphas and so the general form of rewriting this left-recursive grammar in using right-recursion is given here. So here each of the betas appears as an alternative in the first position. We only need one additional symbol s prime and then the s prime rules is take care of generating any sequence of the alpha i. Now it turns out that, that isn&#39;t the most general form of left recursion. There are even other ways to encode left recursion in a grammar and here&#39;s another way that&#39;s important. So, we may have a grammar that where nothing is obviously left-recursive. So if you look here, you see that the s doesn&#39;t even appear on the right hand side here. And if you look at this production here, a doesn&#39;t appear anywhere on the right hand side so there&#39;s no what&#39;s called Immediate Left-Recursion in this grammar. But on the other hand, there is left-recursion because s goes to a alpha and then a can go to s beta. And so there we have in, in two steps produce another string with s at the left end and so this is still a Left-Recursive Grammar. We just delayed it by inserting other non-terminals at the left most position before we got back to s. So, this left recursion can also be eliminated. In fact, this can be eliminated automatically, it doesn&#39;t even require human intervention. And if you look at any of the text pretty quickly in the Dragon Book, you&#39;ll find algorithms were doing that. 

