3.169
slide
Compilers 1 Structure of a Compiler
Welcome back, in this second half of the lecture we&#39;ll continue with our overview of the structure of a compiler. Recall that a compiler has five major phases, 

16.199
slide
S Lexical Analysis Parsing Semantic Analysis Optimization Code Generation Intro to Compilers ex Aiken
lexical analysis, parsing, semantic analysis, optimization, and code generation. And now we&#39;re going to briefly talk about each one, and we&#39;re going to explain how a compiler understands these with an analogy to how humans understand English. The first step at understanding a program, both for a compiler and for a 

38.379
slide
Intro to Compilers First step recognize words Smallest unit above letters This is a sentence
human, is to understand the words. Now, humans can look at this example sentence 

45.069
writing

and immediately recognize that there are four words &#39;this is a&#39; and &#39;sentence. And this is so automatic that you don&#39;t even think about it but there is [inaudible] real computation going on here. You have to recognize the separators, namely the blanks. And the punctuation, things like the periods, and also clues like capital letters. And these help you to divide up this group of letters into, a bunch of words that you can understand. And just to emphasize that this is not completely trivial, let&#39;s take a look at this sentence. And you can read this, but it takes a little bit of time Because I&#39;ve put the separators in, in odd places. So you can see the word is, the word this, the word a, and the word sentence. But again this isn&#39;t something that comes to you immediately. You actually have to do some work to see where the divisions lie Because they&#39;re not given to you in the way that you&#39;re used to. The goal of lexical analysis, then, is to divide the program text into its words, or what we call in compiler speak, the tokens. So, 

116.259
slide
Intro to Compilers Lexical analysis divides program text into words or ifx yihenz l elsez 2


116.259
writing

here&#39;s an, an example piece of program text now, instead of a piece of English text, and let&#39;s walk through this and identify the tokens. So, there&#39;s some obvious ones that are keywords, like if, and then. &gt;&gt; And else that we want to identify. And then there are variable names, things like X, and Y, and Z. There&#39;s also constants, things like number one, and the number two. And then there are some operators, double equals is one, and the assignment operator is another. And here&#39;s already an interesting question. How do we know that double equals is not two individual equals signs? How do we know that we want this? To be a double equal so we want, and not two single equals. Well, we don&#39;t know right now but we&#39;ll talk about that. &gt;&gt; In the lecture on how we implement Lexico analysis. But we&#39;re not done with all the tokens in this example either, there&#39;s a few more. The semi colons, the punctuation are also tokens, and then the separators are also tokens, so here&#39;s a blank, that&#39;s a token, here&#39;s another blank, that&#39;s another token, and then there are lots of blanks here that serve to separate things like the keywords and the variable names and other symbols from each other. And those are the tokens of this example. So for humans, once the words are understood, the next step is to understand the structure of the sentence, and this is 

202.659
slide
Intro to Compilers Once words are understood the next step is to understand sentence structure Parsing Diagramming Sentences The diagram is a tree Mex mm
called parsing. And as we all learned in elementary school, this means diagramming sentences, and these diagrams are trees, and it&#39;s a very simple procedure. Let&#39;s look at this example. This line is a longer sentence. The first step in parsing 

216.969
slide
This line is O longer Intro to Compilers sentence
is to identify the role of each word in the sentence. So we have things like nouns 

222.79
writing

and verbs and adjectives. But then, the actual work of parsing is to group these words together into higher level constructs. So for example, this particular sentence consists of a subject, a verb, and an object, okay? And that actually forms an entire sentence. So, right here we have the root of the tree called a sentence, and that&#39;s broken down into constituent parts. The high level structure, as we said, is subject verb to object. And then the subject is more complicated, as is the object. And this is an example of parsing an English sentence. The analogy between parsing English text and parsing program text is very strong. 

264.42
slide
Intro to Compilers ifx yfhenz 1 elsez 2
In fact, they&#39;re exactly the same thing. So here&#39;s our little example piece of code again, so let&#39;s work through parsing it. So, clearly, this is an if then else statement, and so, the root of our diagram, of our parse tree, is gonna be if 

278.16
writing

then else. [inaudible] Nothing else consists of three parts. There&#39;s a predicate, a then statement, and an L statement. And now let?s look at the predicate which consists of three pieces. There&#39;s a variable, a comparison operator and another variable and together those form a relation. So the comparison between two things is one of the things you can have as a valid predicate. Similarly, the then statement consists of an assignment where Z gets one, and the else statement also has the form of an assignment, Z gets two. And to, all together this is a parse tree, of the if-then-else, showing its structure, breaking it up into its constituent pieces. Now, once we&#39;ve understood the sentence structure, the 

326.6
slide
Intro to Compilers Once sentence structure is understood we can try to understand is hard Compilers perform limited semantic analysis to catch inconsistencies Alex Nken
next step is to try to understand the meaning, of what has been written. And, this is hard. So, actually we don&#39;t know how this works for humans still. We don&#39;t understand, what happens after lexical analysis and parsing. We do know that people do lexical analysis and parsing in much the same way, that compilers lexically analyze and parse programs. But frankly, understanding meaning is something that is simply too hard for compilers. So, the first important thing to understand about, semantic analysis is that compilers can only do very limited kinds of semantic analysis. And in particular the kinds of things that compilers generally do are try to catch inconsistencies. So, if the program is somehow self inconsistent, [inaudible] compilers can often notice that and report errors. But they don&#39;t really know what the program is supposed to do. As an 

382.13
slide
Intro to Compilers Example Jack said Jerry left his assignment at home Even worse Jack said Jack left his assignment at home
example of the kind of thing that we do in semantic analysis, again, using an analogy 

387.59
writing

in English, let&#39;s consider the following sentence. So, Jack said Jerry left his assignment at home. And the question is, what, who does his, refer to here? It could be that his refers to Jerry, in which case we would read, Jack said Jerry left Jerry&#39;s assignment at home. Or it could refer to Jack. In which case, we could read the sentence as, Jack said Jerry left Jack&#39;s assignment at home. And without more information, we actually don&#39;t know which one. His is referring to, whether it&#39;s Jack, or it&#39;s Jerry. And even worse, let&#39;s take a look at this sentence down here. Jack said, Jack left his assignment at home. And the question is how many people are actually involved in this sentence? It could be as many as three, there could be two separate Jacks and his, could even refer to somebody completely different. We don&#39;t know without seeing the rest of the story. That surrounds this sentence, all the possibilities for his. But it could also be as few as, only a single person. It could be that Jack and Jack and his are all the same person in this sentence. And so this kind of ambiguity is a real problem, in semantic analysis. And the analogy in programming languages is 

466.25
slide
Intro to Compilers Programming languages define strict rules to Jock 3 avoid such ambiguities 4 COUT JOCK
variable bindings. So we would have variables, in this case, a variable called Jack or maybe more than one variable called Jack. And a programming language is going to have very strict rules to prevent the kind of ambiguities we had in the English sentences on the previous slide. So you know, in this example. Question is what value is printed by this output statement, and the answer is it&#39;s going to print four because this use of the variable Jack binds to this definition here. And the outer definition is hidden. So the outer definition is not active in this scope because it is hidden by the inner definition and that is just a standard rule of a lot of lexically scoped programming languages. Now the pilots perform many semantic texts besides analyzing the variable bindings. And so 

519.299
slide
Intro to Compilers Compilers perform many semantic checks besides variable bindings Example Jack left her homework at home A type mismatch between her and Jack we know they are different people
here&#39;s another example in English. So Jack looked her homework at home. And, under the usual naming conventions, assuming that Jack is male, we know there&#39;s a type mismatch between Jack and her. So we know that, whatever her is, it is not Jack. And, and therefore we known that this sentence is talking about two different people. And so this is, analogous to type checking. The fourth compiler phase, 

546.569
slide
Intro to Compilers Optimization has no strong counterpart in English But a little bit like editing Automatically modify programs so that they Run faster Use less memory
optimization, doesn&#39;t have a very strong counterpart in everyday English usage but it&#39;s a little bit like editing. And, in fact, it&#39;s a lot like what professional editors do when they have to reduce the length of an article to get it down to some word budget. So, for example, I have this phrase right here, but a little bit like ending; and if I didn&#39;t like it, if I thought it was too long, I could replace 

569.06
writing

the middle four words, with two words. Akin to. So now it says, but akin to editing, and that means exactly the same thing as the original phrase, but uses fewer words. And the goal in program optimization Is to modify the program so that it uses less of some resource. Maybe we want to use less time, we want the program to run faster maybe we want it to use less space so that we can fit more data in memory. For a handheld device we might be interested in reducing the amount of power that it uses. If we have external communication we might be interested in reducing the number of network messages or the number of database accesses. And there&#39;s any number of resources that we might want to improve other program&#39;s use of. So here&#39;s a simple example of the kinds of optimizations a program might do. 

628.019
slide
Intro to Compilers X Y 0 isthesomeos X O


628.019
writing

We can have a rule in our compiler that says X equals Y times zero, is the same as X equals zero. And this seems like a real improvement, because instead of doing the multiply, we can just do an assignment. So we save some computation by doing that. Now unfortunately this is not a correct rule. And this is one of the important things to know about compiling optimization, is that it&#39;s not always obvious when it&#39;s legal to do certain optimizations or not. Now it turns out That this particular rule is valid for integers. Okay, so if X and Y are integers, then multiplying by zero is always the same thing as just signing zero. But, it&#39;s invalid for floating point. And why is that, well because you have to know some details of the IEEE floating point standard, but there is a special number in the IEEE standard called not a number and it turns out that not a number called a NaN times zero is equal to not a number. Any particular non-number times zero is not equal to zero If X and Y are plotting point numbers, you can&#39;t do this optimization. In fact, if you did this optimization, it would break certain very important algorithms that rely on the proper propagation of not a number. Finally, the last compiler phase is code generation, often 

720.689
slide
Intro to Compilers Produces assembly code usually A translation into another language Analogous to human translation
referred to as Code Gen, and Code Gen, can produce assembly code. That&#39;s the most common thing that a compiler would produce. But in general, it&#39;s a translation into some other language. And this is, entirely analogous to human translation. So just as a human translator might translate, English into French, a compiler will translate a high level program into assembly code. To wrap up, almost every compiler has the five phases that we outlined. However, the proportions 

754.36
slide
Intro to Compilers The overall structure of almost every compiler adheres to our outline The proportions have changed since FORTRAN
have changed a lot over the years, and if we were to go back to FORTRAN I and look inside of that compiler, we would probably see a size and complexity that 

763.809
writing

looks something like this. We have a fairly complex lexical analysis phase, an equally complicated parsing phase, a very small semantic analysis phase, a. A fairly involved optimization phase and another fairly involved code generation phase. And so we see a compiler where the complexity was sp, spread fairly evenly throughout except for its semantic analysis which is very weak in the early days. And today if we look at a modern compiler you&#39;ll see almost nothing in lengthening, very little in parsing, because we have extremely good tools to help us write those two phases. We would see a fairly involved thematic analysis phase. We would see a very large optimization phase, and this is in fact the dominant component off all modern compilers, and the a small code-generation phase because again we understand that phase very, very well. That&#39;s it for this lecture. Future lectures, we&#39;ll look at each of these phases in detail. 

