3.629
slide
Compilers 1 Managing Caches
In the last few videos, we&#39;ve talked about managing registers. In this video, we&#39;re going take a few moments to talk about another very important resource, the cache and what compilers can and can&#39;t do to manage them. Modern computer systems have 

20.269
slide
Managing Caches n 1 cycle 256 8000 bytes Cache 3 cycles 256k 1M W Main memory 20 100 cycles 32M46 Disk 0 5 5M cycles 4G 1T Alex mm
quite elaborate memory hierarchies. And so, if we were to start at the closest 

25.3
writing

level to the processor itself, we would find that on the chip there are some number of registers. And these are very fast access. So, typically that can be accessed in a single cycle so at the same rate as the clock frequency. And the problem is that it&#39;s very expensive to build such high performance memory. And so, we don&#39;t get to have very much of it, typically. You know, you might have 256, say, to 8K bytes of registers total available to you on a given processor. Now, a very significant portion of the die area and the modern processor would be devoted to the cache. And the cache is also quite high performance but not quite as high performance as registers. Maybe on average, it would take three cycles just service something from the cache but you get a lot more of it. And modern processors would have up to a megabyte of cache. Then, much further away from the processor is the main memory, the DRAM, and this is much more expensive to allocate to access in time you know, typical values would be twenty to 100 cycles and I think, you know, it&#39;s more on 100 toward the 120 these days in most processors but you get quite a lot of it. You get between 32 megabytes. That would be fairly small machine up to four gigabytes for maximally provisions processor. And finally, farthest away is typically disk. And this takes a very, very long time to get to hundreds of thousands or millions of cycles but you can have enormous amounts of storage out there, gigabytes to terabytes of storage. As I said, there are limitations on the size and speed of registers and caches. 

131.26
slide
Managing Caches Power usage limits Size and speed of registers caches Speed of processors Mex mm
And these are limited as much by power actually as, as anything e lse these days. 

136.33
writing

And I, and so it&#39;s, you know, very important people would like to have as much register and cache as possible but there are real constraints on how big and how fast we can make these relative to the speeds of the processors. Now unfortunately, the cost of a cache miss is very high as we saw in the previous slide. If you, you could get something in a couple of cycles from the cache. But if it&#39;s not in the cache, then it could take you a couple of orders of magnitude longer to get it out of the main memory. And so for this reason people, you know, try to build caches in between the processor and the main memory to hide that latency of the main memory so that most of the data is in the cache. And typically, it requires more than one level of cache these days to match a fast processor well with the speed of a very large main memory. So, you know, very common now to have two levels of cache and processors and some processors even have three levels of cache. So the bottom line is that it&#39;s very important to for high performance to manage these resources properly. Particular to manage the registers and the cache as well if you want your program to perform well. Compilers have become very 

211.49
slide
Managing Caches Compilers are very good at managing registers Much better than a programmer could be Compilers are not good at managing caches This problem is still left to programmers It is still an open question how much a compiler can do to improve cache performance Compilers can and a few do perform some cache optimizations
good in managing registers and in fact, I think today, most people would agree that for almost all programs, compilers do a better job at managing registers than programmers can. And so, it&#39;s very worthwhile to leave the job of allocating registers or assigning registers to the compiler. However, compilers are not good at managing caches. And while there&#39;s a little bit that compilers can do and that&#39;s what we&#39;re going to talk about in this rest of this video for the most part, if programmers want to get good cache performance, they have to understand the behavior of the cache is on the machine and have to understand what their program is doing, you have to understand a little bit about what the compiler is capable of doing and then they still have to , write the program in such a way that is going to, to be cache friendly. So, it&#39;s still very much an open question. How much a compiler can do to improve cache performance? Although, there are a few things that we&#39;ve found compilers can do reliably. So, to see one of those things that compilers can actually do let&#39;s take a look at this example loop. So, what we 

279.59
slide
Consider the loop f0r j 1 J 10 j for i 1 i 1000000 i 3U W Managing Caches ex Nken
have here, we have an outer loop on j and inner loop on i and then in each iteration 

285.22
writing

of the inner loop we&#39;re reading from , some vector you know, performing some computational net value and storing the results into the ith element of the A vector. Now, as it turns out, this particular program has really, really terrible cache performance. This is going to behave very badly. And so, let&#39;s think about what&#39;s going to happen. So, let&#39;s imagine our cache, you know, as some block of memory, okay. And so, what&#39;s going to happen here. I mean, what&#39;s, what&#39;s the first iteration going to be? Well, we&#39;re going to, you know load and, store some function of that into . And so, what&#39;s going to get loaded into the cache is and . All right, let&#39;s assume they just go into different elements in this just for the sake of argument, let&#39;s say they land in the first two elements in the cache. And then we&#39;re going to do the second iteration of this and, we&#39;ll, we&#39;ll load and write it into and so and will be loaded into the cache, all right and so on. And this will repeat over and over and over again, loading one element of a and one element of b the important thing to notice is that all of these references to a and to b are misses, okay. Every single one of these is a cache miss because on each iteration of the loop we refer to new elements, okay. So, we&#39;re not referring to the same elements as we were on the previous ones. So, now let&#39;s ignore for the moment the fact that there may be multiple elements in the same cache line, okay. So, some of you probably are aware already. That when we fetch data from memory we don&#39;t just fetch the one word, okay. So, typically when we refer to for example you know, is stored here will fetch an entire cache line which will be some block of memory and that may well have, you know, other elements of b in it. So, we might get a couple other elements of b into the cache at the same time but the important thing here is that on every iteration of the loop, we&#39;re referring to fresh data, okay. And, and if these data values are large enough, if they take up an entire cache line, then each iteration of the loop is going to be a cache miss for both elements, and we won&#39;t get any benefit of the cache. And this loop will run at the rate of at the rate of the main memory and not at the rate of the cache. Now, the other thing that&#39;s important here is that this loop bound here is very large and I picked it to be very large to suggest that it&#39;s much larger than the size of the cache. So, as we get towards the end of the loop what&#39;s going to happen is we will have filled up the whole cache, so this whole cache will be filled with values from a and b, and then it&#39;s going to start clobbering values that are already in the cache. And if this loop, you know, if the size of these vectors, let&#39;s say twice the size of the cache by the time we come around and complete the entire execution of the. Inner loop. What&#39;s in the cache is the second half of the a and b arrays, it&#39;s not the first half of the a and b arrays. And so, then when we go back around and execute another iteration of the outer loop, now what&#39;s in the cache is also, going to be not the data that we&#39;re referencing. And so when we come back around and begin the execution of the inner loop the second time. And we refer to and, and, and . What&#39;s in the cache is the values from the high numbered elements of the a and b vector and not the low numbered elements. And so, these references are all misses again. And so, the, the basic problem with this loop is, a loop that&#39;s structured like this, is that almost every memory reference and if, and if the data values are big enough again that they fill an entire cache line then it will be every single memory reference is a cache miss. Now, instead, let&#39;s consider an alternative structure for the same 

530.81
slide
Managing Caches Consider the program for i 1 i 1000000 i for j 1 j lO j a i b i Computes the same thing But with much better cache behavior Might be more than 10x faster A compiler can perform this loop interchange optimization
program. Here, I&#39;ve put the i loop at as the outer loop and the j loop as the inner 

536.17
writing

loop. And here what we do is we load . And we write and then we repeat that computation ten times on the same data values. And so here we&#39;ll get excellent cash performance. We&#39;ll, we&#39;ll have a miss on the first reference, but then on the subsequent nine references the data will be in the cache or will completely exhaust our computation on those particular a and b values. And then we&#39;ll go on to the next a and b values. We&#39;ll finish the inner loop and go on to the other and do one more iteration of the outer loop. And so, the advantage of this structure is that it brings the data into the cache and then it uses that data as much as possible, before going on to the next data. Rather than doing a little bit on every data item and then going back, you know, doing one pass and then going back and sweeping over all items, items again and doing another little bit. Alright, so this particular structure, where we&#39;ve exchanged the order of the outer loops sorry, exchanged the order of the inner and outer loops, it computes exactly the same thing but it has much better cache behavior. And it probably run more than ten times faster. Now compilers can preform this simple loop interchange optimization. This particular kind of optimization is called loop interchange, where you just switch in the order of loops. In this particular case, it&#39;s very easy to see that that&#39;s legal and the compiler could actually figure it out. Not many compilers actually implement this optimization because in general, it&#39;s not easy to decide whe ther you can reverse the orders of, of the loops. And so usually, a programmer would have to figure out that they wanted to do this, in order to improve the performance in the 

