Welcome to this course on compilers. My name is Alex Aiken. Im a professor here at Stanford University. And were going to be talking about the implementation of programming languages. &gt;&gt; There are two major approaches to implementing programming languages compilers and interpreters. Now this class is mostly about compilers. But I do want to say a few words about interpreters here in the first lecture. So what does an interpreter do? Well Im gonna draw a picture here this box is the interpreter and it takes let me label it with a big I it takes as input your program. That you wrote And whatever data that you want to run the program on. And it produces the output directly. Meaning that it doesnt do any processing of the program before it executes it on the input So you just write the program and you invoke the interpreter on the data and the program immediately begins running. And so we can say that the interpreter is is online meaning it the work that it does is all part of running your program. Now a compiler is structured differently. So we can draw a picture here. Which well label with a big C for the compiler And the compiler takes as input just your program. And then it produces an executable. And this executable is another program might be assembly language it might be bytecode. It could be in any number of different implementation languages. But now this can be run separately on your data. And that will produce the output. Okay? And so in this structure the compiler is offline Meaning that we pre-process the program first. The compiler is essentially a pre-processing step that produces the executable and then we can run that same executable on many many different inputs on many different data sets without having to recompile or do any other processing of the program. I think its helpful to give a little bit of history about how compilers and interpreters were first developed. So the story begins in the their first commercially successful machine although there had been some earlier machines that they had tried out. But anyway the interesting thing about the software costs exceeded the hardware costs. And not just by a little bit but by a lot And This is important because these the hardware in these those days was extremely expensive. And even then when hardware cost the most in absolute and relative terms more than they would ever cost again already the software was the dominant expense in in making good use out of computers. And this led a number of people to think about how they could do a better job of writing software. How they could make programming more productive. Where the earliest efforts to improve the productivity of programming was called speed coding developed in 1953 by John Backus. Now speed coding is what we call today an early example of an interpreter. And like all interpreters it had some advantages and disadvantages. The primary advantage was that it was much faster to develop the programs. So the in that sense the programmer was much more productive But among its disadvantages code written speed code programs were ten to twenty times slower. Then handwritten programs and thats also true of interpreted programs today. So if you have an implementation that uses an interpreter theyre going to be much slower than either a compiler or writing code by hand. And also the speed code interpreter took up 300 bytes of memory. And that doesnt seem like very much. In fact 300 bytes today would seem like an incredibly tiny program. But in those days you have to keep in mind that this was 30 Percent Of the memory on the machine. So this was 30 percent of the entire memory of the 704. And so the amount of space that the interpreter took up was itself a concern. Now speed coding did not become popular but John Backus thought it was promising and it gave him the idea for another project. The most important applications in those days were scientific computations and programmers thought in terms of writing down formulas in a form that the machine could execute. John thought that the problem with speed coding was that the formulas were in fact interpreted and he thought if first the formulas were translated in to a form that the machine could execute directly. That the code would be faster And while still allowing the programmer to write the the the programs at a high level and thus was the Formula Translation Project or FORTRAN Project born. Now FORTRAN ran from 1954 To 1957 And interestingly they thought it would only take them one year to build the compiler but it would end up taking three. So just like today they werent very good at predicting how long software projects would take. But it was a very successful project. By 1958 over 50 percent of all code was written in FORTRAN. So 50 percent of programs were in FORTRAN And that is very rapid adoption of a new technology. We would be happy with that kind of success today and of course at that time they were ecstatic And everybody thought that FORTRAN both raised the level of abstraction improved programmer productivity and allowed everyone to make much better use of these machines. So FORTRAN one was the first successful high level language and it had a huge impact on computer science. In particular it led to an enormous body of theoretical work. And one of the interesting things about programming languages actually is the combination of theory. And practice because its not really possible in programming languages to do a good job without having both a a very good grasp of fairly deep theory and also good engineering skills. So theres a lot of very good systems building material in programming languages and typically it involves a very subtle and fruitful interaction with theory. And so and this is one of the things I think thats most attractive about the areas the subject of studying computer science. And the impact of FORTRAN was not just on computer science research of course but also on the development of practical compilers. And in fact its influence was so profound that today auto compilers still preserve the outlines of FORTRAN one. So what was the structure of FORTRAN one? Well it consists five phases lexical analysis and parsing which together take care of the syntactic aspects of the language semantic analysis which of course takes care of more semantic aspects things like types and scope rules. Optimization Which is a collection of transformations on the program to either make it run faster or use less memory. And finally code generation which actually does the translation to another generation. And depending on our goals that translation might be to machine codes. It might be to a bite code for a virtual machine. It might be to another high level programming language. Well thats it for this lecture and next time well pick up here and talk about these five phases in more detail. Welcome back in this second half of the lecture well continue with our overview of the structure of a compiler. Recall that a compiler has five major phases lexical analysis parsing semantic analysis optimization and code generation. And now were going to briefly talk about each one and were going to explain how a compiler understands these with an analogy to how humans understand English. The first step at understanding a program both for a compiler and for a human is to understand the words. Now humans can look at this example sentence and immediately recognize that there are four words this is a and sentence. And this is so automatic that you dont even think about it but there is [inaudible] real computation going on here. You have to recognize the separators namely the blanks. And the punctuation things like the periods and also clues like capital letters. And these help you to divide up this group of letters into a bunch of words that you can understand. And just to emphasize that this is not completely trivial lets take a look at this sentence. And you can read this but it takes a little bit of time Because Ive put the separators in in odd places. So you can see the word is the word this the word a and the word sentence. But again this isnt something that comes to you immediately. You actually have to do some work to see where the divisions lie Because theyre not given to you in the way that youre used to. The goal of lexical analysis then is to divide the program text into its words or what we call in compiler speak the tokens. So heres an an example piece of program text now instead of a piece of English text and lets walk through this and identify the tokens. So theres some obvious ones that are keywords like if and then. &gt;&gt; And else that we want to identify. And then there are variable names things like X and Y and Z. Theres also constants things like number one and the number two. And then there are some operators double equals is one and the assignment operator is another. And heres already an interesting question. How do we know that double equals is not two individual equals signs? How do we know that we want this? To be a double equal so we want and not two single equals. Well we dont know right now but well talk about that. &gt;&gt; In the lecture on how we implement Lexico analysis. But were not done with all the tokens in this example either theres a few more. The semi colons the punctuation are also tokens and then the separators are also tokens so heres a blank thats a token heres another blank thats another token and then there are lots of blanks here that serve to separate things like the keywords and the variable names and other symbols from each other. And those are the tokens of this example. So for humans once the words are understood the next step is to understand the structure of the sentence and this is called parsing. And as we all learned in elementary school this means diagramming sentences and these diagrams are trees and its a very simple procedure. Lets look at this example. This line is a longer sentence. The first step in parsing is to identify the role of each word in the sentence. So we have things like nouns and verbs and adjectives. But then the actual work of parsing is to group these words together into higher level constructs. So for example this particular sentence consists of a subject a verb and an object okay? And that actually forms an entire sentence. So right here we have the root of the tree called a sentence and thats broken down into constituent parts. The high level structure as we said is subject verb to object. And then the subject is more complicated as is the object. And this is an example of parsing an English sentence. The analogy between parsing English text and parsing program text is very strong. In fact theyre exactly the same thing. So heres our little example piece of code again so lets work through parsing it. So clearly this is an if then else statement and so the root of our diagram of our parse tree is gonna be if then else. [inaudible] Nothing else consists of three parts. Theres a predicate a then statement and an L statement. And now let?s look at the predicate which consists of three pieces. Theres a variable a comparison operator and another variable and together those form a relation. So the comparison between two things is one of the things you can have as a valid predicate. Similarly the then statement consists of an assignment where Z gets one and the else statement also has the form of an assignment Z gets two. And to all together this is a parse tree of the if-then-else showing its structure breaking it up into its constituent pieces. Now once weve understood the sentence structure the next step is to try to understand the meaning of what has been written. And this is hard. So actually we dont know how this works for humans still. We dont understand what happens after lexical analysis and parsing. We do know that people do lexical analysis and parsing in much the same way that compilers lexically analyze and parse programs. But frankly understanding meaning is something that is simply too hard for compilers. So the first important thing to understand about semantic analysis is that compilers can only do very limited kinds of semantic analysis. And in particular the kinds of things that compilers generally do are try to catch inconsistencies. So if the program is somehow self inconsistent [inaudible] compilers can often notice that and report errors. But they dont really know what the program is supposed to do. As an example of the kind of thing that we do in semantic analysis again using an analogy in English lets consider the following sentence. So Jack said Jerry left his assignment at home. And the question is what who does his refer to here? It could be that his refers to Jerry in which case we would read Jack said Jerry left Jerrys assignment at home. Or it could refer to Jack. In which case we could read the sentence as Jack said Jerry left Jacks assignment at home. And without more information we actually dont know which one. His is referring to whether its Jack or its Jerry. And even worse lets take a look at this sentence down here. Jack said Jack left his assignment at home. And the question is how many people are actually involved in this sentence? It could be as many as three there could be two separate Jacks and his could even refer to somebody completely different. We dont know without seeing the rest of the story. That surrounds this sentence all the possibilities for his. But it could also be as few as only a single person. It could be that Jack and Jack and his are all the same person in this sentence. And so this kind of ambiguity is a real problem in semantic analysis. And the analogy in programming languages is variable bindings. So we would have variables in this case a variable called Jack or maybe more than one variable called Jack. And a programming language is going to have very strict rules to prevent the kind of ambiguities we had in the English sentences on the previous slide. So you know in this example. Question is what value is printed by this output statement and the answer is its going to print four because this use of the variable Jack binds to this definition here. And the outer definition is hidden. So the outer definition is not active in this scope because it is hidden by the inner definition and that is just a standard rule of a lot of lexically scoped programming languages. Now the pilots perform many semantic texts besides analyzing the variable bindings. And so heres another example in English. So Jack looked her homework at home. And under the usual naming conventions assuming that Jack is male we know theres a type mismatch between Jack and her. So we know that whatever her is it is not Jack. And and therefore we known that this sentence is talking about two different people. And so this is analogous to type checking. The fourth compiler phase optimization doesnt have a very strong counterpart in everyday English usage but its a little bit like editing. And in fact its a lot like what professional editors do when they have to reduce the length of an article to get it down to some word budget. So for example I have this phrase right here but a little bit like ending; and if I didnt like it if I thought it was too long I could replace the middle four words with two words. Akin to. So now it says but akin to editing and that means exactly the same thing as the original phrase but uses fewer words. And the goal in program optimization Is to modify the program so that it uses less of some resource. Maybe we want to use less time we want the program to run faster maybe we want it to use less space so that we can fit more data in memory. For a handheld device we might be interested in reducing the amount of power that it uses. If we have external communication we might be interested in reducing the number of network messages or the number of database accesses. And theres any number of resources that we might want to improve other programs use of. So heres a simple example of the kinds of optimizations a program might do. We can have a rule in our compiler that says X equals Y times zero is the same as X equals zero. And this seems like a real improvement because instead of doing the multiply we can just do an assignment. So we save some computation by doing that. Now unfortunately this is not a correct rule. And this is one of the important things to know about compiling optimization is that its not always obvious when its legal to do certain optimizations or not. Now it turns out That this particular rule is valid for integers. Okay so if X and Y are integers then multiplying by zero is always the same thing as just signing zero. But its invalid for floating point. And why is that well because you have to know some details of the IEEE floating point standard but there is a special number in the IEEE standard called not a number and it turns out that not a number called a NaN times zero is equal to not a number. Any particular non-number times zero is not equal to zero If X and Y are plotting point numbers you cant do this optimization. In fact if you did this optimization it would break certain very important algorithms that rely on the proper propagation of not a number. Finally the last compiler phase is code generation often referred to as Code Gen and Code Gen can produce assembly code. Thats the most common thing that a compiler would produce. But in general its a translation into some other language. And this is entirely analogous to human translation. So just as a human translator might translate English into French a compiler will translate a high level program into assembly code. To wrap up almost every compiler has the five phases that we outlined. However the proportions have changed a lot over the years and if we were to go back to FORTRAN I and look inside of that compiler we would probably see a size and complexity that looks something like this. We have a fairly complex lexical analysis phase an equally complicated parsing phase a very small semantic analysis phase a. A fairly involved optimization phase and another fairly involved code generation phase. And so we see a compiler where the complexity was sp spread fairly evenly throughout except for its semantic analysis which is very weak in the early days. And today if we look at a modern compiler youll see almost nothing in lengthening very little in parsing because we have extremely good tools to help us write those two phases. We would see a fairly involved thematic analysis phase. We would see a very large optimization phase and this is in fact the dominant component off all modern compilers and the a small code-generation phase because again we understand that phase very very well. Thats it for this lecture. Future lectures well look at each of these phases in detail. Hello. In this video were going to talk about something that Ive referred to as the economy of programming languages. So the idea behind this video is that before we get into the details of how languages are implemented or designed I wanted to say something about how languages work in the real world and why certain languages are used and others are not. And if you look around theres actually a few obvious questions that come up to anybody who thinks about programming languages for more than a few minutes. One question is why are there so many of these things? We have hundreds if not thousands of programming languages in everyday use and why do all these things need to exist? Why wouldnt one programming language for example be enough? A related question but slightly different is why are there new programming languages? That given that we have so many programming languages already what is the need for new ones to be created? And finally how do we know a good programming language when we see it? So what makes a good programming language and what makes a bad programming language? I just want to spend this video talking about these three questions. And as well see I think the answers to these questions are largely independent of the technical aspects of language design and implementation. But very interesting in their own right. So lets begin with the question of why there are so many programming languages. And at least a partial answer to this question is not too hard to come by. If you think for a few minutes youd realize that the application domains for programming have very distinctive and conflicting needs. That is its very hard to design one language that would actually do everything in every situation for all programmers. And lets just go through some examples. One domain that you might not think about very much is scientific computing. So these are all the big calculations that are done for engineering applications primarily but also big science and long running experiments simulation experiments. And what are the needs for such computations? Well typically you need very good floating point support. Ill abbreviate that as FP. You need good support for arrays and operations on arrays because the most common data type in most scientific applications is larger arrays of floating point numbers. And then you also need parallelism. Today to get sufficient performance you really have to exploit parallelism in these applications. And its not every language actually supports all of these things well. This is actually not an exhaustive list of the things you need but a few distinctive things that are needed. But one language that has traditionally done a very good job of supporting these things is Fortran. And Fortran is still heavily used in the scientific community. It was originally designed for scientific applications. If you recall that the name means formula translation. And it has evolved over time. It doesnt really look much like the original language anymore but its always retain this core constituency in scientific computing and remains one of the leading languages in that domain. Now a completely different kind of a domain is business applications. And so what do you need here? Well so here youre going to need things like persistence. You dont want to lose your data. Businesses go to a lot of trouble to get the data and they need a way to hold onto it and they want that to be extremely reliable. Youre going to need good report facilities. Because typically you want to do something with the data. So you need good facilities for report generation. And also you want to be able to exploit the data. The datas actually in many modern businesses one of the most valuable assets and so you need good facilities for asking questions about your data. Lets call it data analysis. And again this is not an exhaustive list of things that you need but it is representative I would say. And probably the most common or one of the most common used languages for this class of applications is SQL the database query language. So relational databases and their associated programming language-- languages I should say but most notably SQL-- really dominate in this application domain. And then another domain lets do one more is systems programming. So by this I mean things like embedded systems things to control devices operating systems things like that. And what are the characteristics here? Well we need very low level control of the resources. The whole point of systems programming is to do a good job of managing resources and so we really want fine grained control over the resources. And often theres a time aspect so you might have some real time constraints. So you need to be able to reason about time. Because these are actually again devices and they need to be able to react within certain amount of time - if its a network device or something like that - you need to be responsive to the network. Lots and lots of examples where timing is important. And these are just two aspects and Im a little bit - Im running out of space here so Ill just stop with that. But again these are representative of the kinds of things you need in systems programming. And probably today still the most widely used systems programming language or family systems of programming languages is the C and to some extent C++ family of languages. And as you can see the requirements in these different domains are just completely different from each other. Whats important in one domain or most important in one domain is not the same as in another domain. And its easy I think to imagine at least that it would be difficult to integrate all of these into one system that would do a good job on all of these things. That brings us to our second question: Why are there new programming languages? There are so many languages in existence why would we ever need to design a new one? And Im going to begin the answer to this question with an observation that at first glance has nothing to do with the question at all. So let me just take a moment to explain it. I claim that programmer training is the dominant cost for a programming language. And I think this is really important so just going to emphasize the bit thats important here. Its the programmer training. The cost of educating the programmers in the language. So we think about a programming language there are several things that have to happen for that language to get used. Somebody has to design it. But thats really not very expensive. Thats just one or very few people typically. Somebody has to build a compiler but that is also not actually all that expensive. Maybe 10 to 20 people for a really large compiler project can build quite a good compiler. The real cost is in all the users and educating them. So if you have thousands or hundreds of thousands or millions of users of the language the time and money that it takes to teach them all the language is really the dominant cost. And here I dont mean just the actual dollar expense of buying textbooks and taking classes and things like that. Its also the fact that the programmers have to decide its worth it for them to learn this language and many programmers learn on their own time but thats a use of their time and the expense of their time is a real economic cost. And so if you think about the number of hours that it takes to teach a population of a million programmers a language thats really quite a significant economic investment. Now from this observation we can make a couple of predictions pretty easily. And again these are just predictions now that follow from this claim. If you believe that its true. So let me erase it and fix it. So first prediction is that widely used languages will be slow to change. And why should that be true? Well if I make a change to a language of lots of people use I have to educate everybody in that community about the change. And so even relatively minor language extensions small changes to syntax small new features even just simple changes in the interface of the compiler if you have a lot of users it takes a very long time and is quite expensive to teach them all about that. So as languages become widely used the rate of change their rate of change will slow down. And this predicts over time as the world of programming grows as we have more more programmers in the world we would expect the most popular languages which will have larger and larger user bases so larger and larger programmer basis to become more and more ossified. To evolve more and more slowly. And I think actually what you see in practice is very consistent with that prediction. Now at the other end of the spectrum this same observation makes an almost what appears to be contradictory prediction which is that easy to start its easy to start a new language. That in fact the cost of starting up a new language is very low. And why is that? Well because you start with zero users and so there is essentially zero training cost at the beginning and then even when you have just a few users the cost of teaching them the changes in the language is not very high. And its so new languages can evolve much more quickly. They can adapt much more quickly to changing situations. And its just not very costly to experiment with a new language at all. And theres a tension between these two things. When is a programmer going to choose between a widely used existing language that perhaps doesnt change very quickly and a brand new language? Well theyre going to choose it if the productivity if their productivity exceeds the training cost. So if they perceive that by spending a little bit of time and money to learn this new language theyre going to be much more productive over a relatively short period of time then theyre going to make the switch. So when is this likely to happen? Well putting this all together languages are most likely to be adopted  to fill a void. And again this is a prediction that follows from the fact that programmer training is the main cost. What do I mean by this? Well what I mean is that programming languages exist for purpose people use them to get work done. And because were still in the middle of the information revolution and there are new application domains coming along all the time. So there are new kinds of programming that emerge every few years or even more often than that. So just in terms of recent history mobile applications are now something thats relatively new. And theres a lot of new technology being built up to support mobile computing. A few years ago it was the internet itself was a new programming platform and a bunch of new programming languages like Java in particular got started during that time. So a new programming niche is open up because the technology changes that what people want to do with software changes. And this creates new opportunities for languages. The old languages are slow to change and so they have some difficulty in adapting to fit these new domains. And they arent really necessarily well suited to them for the reasons we talked about on the previous slide with the previous question because its hard to have one language that incorporates all the features you would want. And so there are so these languages are not necessarily perfect for these application domains. Theyre slow to adapt to the new situation. And this tends to call forth new languages. So when theres a new opportunity and some application domain. If there are enough programmers to support the language often a new language will arise. Just want to point out another prediction that can be made from this one observation. That programmer training and Ill underline that is a dominant cost per programming language. And that is that new languages  tend to look like old languages. That is that new languages are rarely if ever completely new. They have a family resemblance to some predecessor language sometimes a number of predecessor languages. And why is that? Well partly that its hard to think of truly new things. But also I think if theres an economic benefit to this namely that it reduces the training cost by having your new language look like an old language by leveraging off what people already know about the old language you make it easier for people to learn the language and make them learn it more quickly. And the most classic example of this is a Java versus C++ where Java was designed to look a lot like C++. And that was I think very conscious to make it easy for all of the existing C++ programmers to start programming in Java. Finally we can ask ourselves what is a good programming language. And here unfortunately the situation is much less clear. I would just make one claim that there is no and Ill emphasize no universally accepted metric for language design. And what I mean by that? Well I guess the most important part of this statement is the universally accepted bit. So I mean that people dont agree on what makes a good language. There are lots of metrics out there and people have proposed lots of ways of measuring programming languages but most people dont believe that these are very good measures and there is certainly no consensus. If you just look at the world of programmers they cant agree on what the best language is and to convince yourself of this just go and take a look at any of the many news group posts where people get into a semi religious arguments about why one group of languages or particular language is better than another language. But even in the research community in the scientific community in among people who design languages I would say there is no universally accepted consensus on what makes a good language. And to just kind of illustrate the difficulties in trying to come up with such a metric let me discuss one that Ive heard people propose in all seriousness and that is that a good language  is one people use. And let me put a question mark on that because I dont believe this statement. And I think a moments reflection with a moments reflection I can convince you that this isnt a great measure. On the positive side I guess the argument for this is that its a very clear measure. It measures the popularity of the language. How many people are actually using it and presumably languages are more widely used for a good reason. In some sense perhaps they are better languages. But this would imply if you believe this and follow it its logical conclusion that Visual Basic is the best language above all other programming languages. And Ive nothing against Visual Basic. Its a well designed system but I dont even think the designers of Visual Basic would claim that it is in fact the worlds best programming language. And as we saw in the discussion that we just had there are many many other factors besides technical excellence that go into whether a programming languages is widely used or not. And in fact technical excellence is probably not even the most important reason that a language might be used. It has much more to do with whether it addresses a niche or application domain for which there isnt a better tool. And then once its established and has lots of users of course theres inertia in history that aided in surviving. And thats why we still have Fortran and Cobalt and lots of other languages from long long ago that we could if we were starting over today designed much better. So to conclude this video on the economy of programming languages I think the two most important things to remember are that application domains have conflicting needs and therefore it is difficult to design one system that incorporates everything that you would like to have. So you cant get all the features that you would like into a single system in a coherent design at least its very hard to do that and so it takes a lot of time to add new features to existing systems. And the second point is that programmer training is the dominant cost for programming language. And together these two things these two observations these really explain why we get new programming languages because the old languages are difficult to change and when we have new opportunities its often easier and more direct to just design a new language for those rather than trying to move the entire community of programmers and an existing systems to accommodate those new applications. Hello in this and the next few videos Im going to be giving a overview of COOL the programming language in which youll be writing a compiler. Cool is the Classroom Object Oriented Language and the acronym of course is COOL. And the unique design requirement for COOL is that the compiler has to be able to be written in a relatively short period of time. We only have one quarter or in some cases a semester for students to write the compilers. And so COOL has to be implementable quickly. And actually since its used primarily for teaching compilers the number of COOL compilers in the world vastly exceeds the number of COOL programs. So there many many more compilers have been written thousands of compilers maybe tens of thousands of compilers have been written for COOL but probably only some dozens or hundreds COOL programs. And so its probably the only language in existence for which this is true That that the number of compilers actually exceeds the number of programs but it does Tell you about the main design requirement. Its much more important in COOL that the compiler be easy to write then that it be easy to write programs in. And so there are some quirks in the language Things that have been done specifically to make it easier to implement where that wouldnt take away from the the teaching value of the of the language. But that would make it inconvenient to use the language on a day-to-day basis as a working programmer. So what is in the language? Well its weve tried to design it so that it will give you a taste of modern notions of extraction static typing reuse through inheritance automatic memory management. And theres actually a few more things that well talk about when we come to them. But many things are left out. Were not gonna be able to put everything in the language and have it be implementable quickly. Well be able to cover some things in lectures but unfortunately therell even be some interesting language ideas that we wont be able to get to in this class. So the course project is to build a complete compiler. And specifically youre going to compile COOL into MIPS assembly language. So MIPS is a real instruction set It was for a machine that was designed in the 1980s. And there is a simulator for MIPS that runs on just about any kind of hardware. And so this makes the the whole project very portable We can run your compiler or you can generate MIPS assembly language and then that MIPS assembly language can be simulated on just about whatever kind of machine you have access to. The project is broken up into five assignments. First youre gonna write a COOL program. And that program itself will be an interpreter to give you a little bit of experience with writing a simple interpreter. And then the compiler itself will consist of the four the phases that we discussed lexical analysis parsing semantic analysis and code generation. And all of these phases I should emphasize are [inaudible] compatible. Meaning that we have separate implementations separate reference implementations of each of these. And so for example when you are working on semantic analysis you will be able to take the lexical analysis parsing and code generation components from the reference compiler and plug your semantic analysis into that. Framework and and test it against the reference components. And so this way if you have trouble with one component or arent sure that your components is working very well you wont have a problem in working on a different component because youll be able to test that independently. And finally theres no required optimization assignment But we do have some suggestions for optimizations that you can do. And many people have written optimizations for COOL. And so this is an optional assignment if youre interested in learning something about program optimization. So lets write the simplest possible COOL program. And the first thing to know is that COOL source files and in the extension dot CL for COOL and you can use whatever editor you like to write your programs. I happen to use Emacs you can use some other editor if you like. And every COOL program has to have a class called main. And lets talk about that business in a second. So a class declaration in COOL begins with the key word class Followed by the name of the class So in this case main Followed by a pair of curly braces And inside the curly braces is where all the stuff that belongs to the class goes And every class declaration must be terminated by a semi-colon. So a program consists of a list of class declarations. Each class declaration terminated by a semi-colon. So thats the structure of a class. And now we need this class to actually do something so were going to have a method in this class and lets call the method main. In fact the main method of the main class must always exist. This is the method thats run to start the program and furthermore this method must take no arguments. So the empty argument list for the main method is always empty. And lets say the main method its body always goes in a pair of curly braces. So the main method always goes inside curly braces. And a class consists of a list of such declarations. And again those declarations must all be separated by semicolons. So in or terminated excuse me by semicolons. So in this case we only have one method in the class. But still has to have its semi-colon and now we can say what we want the method actually do so this is the place for the code for the method goes and lets just have the simplest possible method the one that just event evaluates to the number one. Okay so [inaudible] an expression language which means that wherever a piece of code can go you can put an arbitrary expression any expression can go there theres no explicit return statement for a method. Its just a value of the method body is the value of the methods. So in this case we just put the number one in there and that will be the value of this method when we run it. So lets save that. And now we can try compiling this simple program so how do we compile the compiler is called a COOL c for the COOL compiler and you just give the COOL compiler a list of COOL source files. So in this case theres just one file 1.CL hit enter and ooh we got a syntax error so we have to come back and fix that and the error said at or near the open curly brace on line three theres a mistake. And I know what the mistake is because Im a competent COOL programmer at least somewhat competent COOL programmer. Cool methods must declare their return type. So we need to put a type here. And the syntax for the declaration is to put a colon after the name of the method and the argument list and then the name of a type. And since were returning the number one for this program for sorry for this method we might as well say that the main method is going to return an integer So save that Go back to our compilation window and lets compile the program again. And this time it compiles successfully. And now if we look in our directory we see that there is a new file called 1.s. Thats the assembly code for the program one. And now we could try to run this code. And the The the Mitch simulator is called spin and it just takes a assembly file to to simulate And so we just give it one as you can see it says part way down that the COOL program successfully executed so thats good and then afterwards there are some statistics and things like number of instructions executed a number of loads and stores a number of branches those things would be interesting if were worried about performance if we were to say working on the optimization of the compiled code but were not doing that right now. Were just running programs. And we can see if this program works. So the program ran. It terminated successfully. But it didnt actually produce any output. And thats because we didnt ask it to produce any output If we want to have output. We have to go back and modify the program again. So so what this program does currently is that it just returns its value but that but nothing is done with that value. Its not printed out or anything like that. If you wanted to have something printed out in a COOL program you have to do that explicitly. So theres a special class built in a primitive class called IO. And we can declare whats called a attribute of this class it will be a IO attribute and it will be called I okay and I will be a object that we use to do IO. So now in our main method Here we could add a call to out-string I dot out-string is how we invoke a method. Okay so out-string is a method of the IO class so we use I to invoke that method and then we can pass it a string that we want printed out on the screen. So for example we could say hello world. Okay And now we have to decide what to do with our with our number one there. And let me show you one more feature of COOL. Lets leave the one there and lets make it part of a statement block. So a statement block consists of a sequence of expressions separated by semicolons. And you can have any number of expressions and the semantics of a statement block or an expression block is to just evaluate the expressions in order. And the value of the block is the value of the last expression. But now a statement or an expression block has to be included in its own set of curly braces. Okay so that now is a valid COOL program so let me just read this for you so the body of the program is a block of expressions. The first one executes. A out string call to the object I which is going to print hello world for us. And then the second one evaluates to one which is the value of the entire of the entire method. Okay actually I should say its the value of the block okay and then because the block is the body of the method the value of the block becomes the value of the entire method So one will be returned from this method call. So lets save this. Go back over here and lets compile this again. So Looks like I failed to save it. Lets compile this and we see we have a syntax error. And so it says on line four we have a syntax error at or near our closing curly brace. And the problem here is that a statement block or expression block consists of a series or a sequence of expressions terminated by semi-colons and we forgot to terminate the last expression in the sequence by its semi-colon So we have to add that. And now we should be able to compile this and lo and behold it compiles correctly and then we can run it. And now we see oh we got another mistake. So we have an when the program ran it complained that we have a dispatched void. So that on line four our dispatch was to an object that didnt exist. And you can see the dispatch call right here to I and it doesnt exist because in fact we forgot to allocate an object for I. So here we declare I to be of type IO but that doesnt actually create any objects. That just says that it creates a variable name I but I doesnt actually have a value. So if you want I to actually have a value we have to initialize it to something. So we can initialize it to a new IO object. And new here is the way you allocate new objects in COOL and new always take a type argument so in this case were creating a new object in type IO and were assigning it To this object i. And notice here that I is a is a is what would be called a field name in Java. Its what we call an attribute in COOL. So so these are the data el the data elements of the of the class. And so the class can have both of names of things that are so attributes or fields that hold values as well as methods that can perform computation. [sound] Lets save this and switch back. And now well compile this again. So and it still compiles. And now we can run it. And now it runs and low and behold as you can see down there third line from the the top it prints out hello world. And that looks a little bit ugly because the the successful execution message is on the same line as our hello world message. So lets fix that. Lets come back over here. And in our string here we can add a new line. Okay at the end of the string so backslash N is how you write a new line character in the string. Save that come back over here lets compile. So if you dont know Unix bang will repeat the previous expression the previous command that began with the same prefix that you type after the bang. So I want to run the last command that began with C which is to compile and then I want to run the last command that began with S which is to run spin. And now we can see there it is all nice hello world is on a line by itself. Lets continue now lets [sound] clear all this out [sound]. So let me just show you a few variations on the same program. What Im going to do here is just rewrite it in a couple of different ways. So I just illustrate a couple of features of COOL and get you more familiar with the syntax and also just show some alternative ways to do the same thing. So you know this this. A block here of of expressions is kind of a clumsy way to to implement the Hello World program. So lets get rid of that. Lets get rid of the the block. Lets get rid of the one here at the end. Okay lets just make the statement body a single expression again and and now the problem were going to have is that the types wont match. But just to illustrate that let me show it to you so lets do COOL C of one dot CL and youll see here that in complains that the inferred return type of the IO of the method main does not conform to the declared return type INT. So coming back over here the to the program The the compiler figured out that this expression I dot out string yields an object of type IO. So it returns the i object as the results evaluating this expression. And that does not match the type it. And so naturally the compiler says hey somethings wrong with the types. Well thats easily repaired. We can just change the return type or the main method to say it returns something of type IO. So lets go back over here and see if that now works. So we compile the program. And then we run spin on the output and yes everything still works as expected. Now We dont have to be so specific about the type over here since were not actually using the result of the method body for anything. I mean the program just exits once it prints the string. We could have allowed ourselves more flexibility here. We couldve just declared the result type of main to be of type Object. So Object is the root of the class hierarchy in COOL. Every other class is a subclass of Object. So lets come back over h lets save this first. And then we can come back over to our compilation window. We can compile it. And we can run it and it still works. So now another thing we can do if we want is we could observe. Here that this attribute that we declare this field I isnt really necessary. Here we we allocate you know we have a special name I when the main object is constructed to run the program a new [inaudible] object is allocated to I and then that gets used in the main method. We can actually just do all of that inside the main method itself by just allocating a new [inaudible] object right here and then calling out string on that object. Alright So this should also work. And lets check it out. So it compiles. And lo and behold it rots. Alright So coming back over here lets illustrate one more or a couple more things that we could do. So we could also say that [inaudible] inherits From IO. So we have to have the IO functionality somewhere in order to call the out string method. So we have been doing that by creating a separate object of type IO. But now we can say well just the main object is itself. And something that has all the capabilities of IO by inheriting from IO. And if youve seen any [inaudible] language before this will be a familiar concept. So main here gets all the attributes and methods of IO in addition to whatever attributes and methods of its own that it will have. And now Instead of of having to allocate a new IO object in order to call out string we can just invoke it on self Which is the name of the current object when the main method runs In other languages self is called this. Okay and so lets we saved it so lets go over and compile this. So it compiles it compiles and and it runs right? So last example here we dont have to name self actually in this dispatch. Theres a feature that allows us to call a method without explicitly naming the object on which its dispatched and defaults to self so if no object is named in a dispatch then its just a dispatched self. So this should also work. [sound] And indeed it does. So that concludes our first example. In the next couple of videos well look at some more complex examples of COOL programming. Welcome back In this video were going to look at another example of cool programing. This time lets move beyond the simple hello world kind of examples and on to something more exciting say the ever popular factorial function. So in order to write factorial well need to open a file which we can write some code. Let me start that. And recall from last time that every cool program has to have a main class and the main class is required to have a main method. And we dont care what the main method return so well just have it return something a type object and then well just fill in a skeleton here on the file. And so now were ready to write some codes. So what are we going to have the main method do? Well before we can actually write factorially before we can get to the guts of this program which is actually not very difficult. We need to talk about IO some more. Because were going to need to be able to read and write numbers. We need to be able to read numbers from the the user whos running the program and print them back out. So lets just review little bit about IO also. In order to invoke the IO functions we need an IO object. And one of the IO functions is something that prints out a string. So lets just write a program that we already know how to do just to confirm that we remember that. And we can compile this program. And it should just print one. And lets see. Indeed it does. Okay? So it prints out the number one. And so now lets come back here and lets talk about how to do input. So instead of just printing out the number one lets print out a strain that the user types in. So ins- in here were going to read a strain. And in order to do that we need an IO object because there is another function another method called in-strain. Okay. And so this will renew this string. And return to string. And to make sure that we get the nice output lets concatenate on to that string a new line. So this is just to. When it prints this string back out it will be printed on i ts own line. So lets try compiling this And steak. It compiles And now we can run spin. Remember the bang command in UNIX runs the previous command that began with the same letters. And now the program runs and it waits. Cause its waiting for me to type something. And if I type it type in one it prints back one and if I type in 42 it gives me back 42. And so now The next thing we need to talk about is how to convert strings into integers because if were going to do a factorial we want to work on integers and not strings. And at the moment were just reading and writing strings. So there is a library written in cool that does conversion between integers and strings. And were going to give the main class here the functionality of that class. Which is called A2I for ASCII to integer. And that defines a bunch of methods that can convert between strings and integers. So lets add those commands in here. So here heres our string. That weve read in. And what we want to do now is to convert this into an integer. So let me just add a couple of parens here. So theres our string okay? And now were going to invoke. On that the method Im sorry were going to call on that the function the method A to I. Okay? And lets just double check here that weve got friends in the right place. So thats the argument to A2I. Now we recall that when we have a dispatch to a method and its just sitting by itself with not object its dispatched to the self object. And the self object is the object of the current class that were in. In this case the main object which has inherited the A2I methods And so the A2I functions should be defined in there. Now we have an integer. And we can do something with that integer if we like. So lets add some more [inaudible] here and lets just say we add just one to the integer. Okay. And then once were done with our integer. Whatever operation it is that we want to do with the integer we need to convert it back to a string so that we can print it out. An theres an inverse function I2A that will do. So I dont know if we have all the parens in the right places at this point. So lets just check. Yes. That looks like that should work. So this will read in a string convert convert it to an integer add one to it convert it back to a string concatenate on a new line and print it out. Now lets see if all that actually works so lets run the compiler and we have a problem here. It says that we have an undefined class A to I. And the reason is we didnt supply the code for A2I. So if we look in our directory here well see Ive already copied in the class file for A2I. And I encourage you to go and look at that code. Its actually interesting code to see how the conversions are written in COOL. But now we need to talk about how to compile a program that uses a library And the way you do it is very simple. You just list all the class files on the command line when you call up the compiler. And it will read them all in and treat them as a single program. So in this case we compile compile fact together with A2I. And that complies. And then we can run it. [sound]. And now if I type in three it prints four. And if I type in one it prints two. And so the program seems to be working. And now were almost ready to write our factorial function. So what do we want to do in factorial? Well we want to do something other than just adding one. Instead we want to call our special function factorial. So lets insert a call to factorial in here. Okay and lets get rid of the plus one. And then lets check that we have all the parens that we need. So we need to close off the the A2I call the factorial call The I2A call. And then that last one should be the out string call And it is Okay. So now we can add a method fact to this class And fact is gonna take an integer argument. So we need a parameter here And its type is in force. And the whole thing is gonna return an integer. And then we have body of our function And probably a good idea here just to make sure that we got this much right to do something simple. So lets just try to make a function that returns one more than its argument. So this will do exactly the same thing that we had before and lets just confirm that that is working. So EW compile with the A2I library and now we have a syntax error. And we see that I forgot the closing semicolon here for the method. Remember the the class body is a list of methods and each method is terminated by a semicolon Must try compiling that again. Now it compiles. Lets run it. We type in four gives a spec five. Alright So looks right were ready now to actually right the code for factorial. And this is gonna be anti climactic because its actually a very simple code if we write it recursively. So lets do that. So hows that going to work? Well everybody knows the definition by Hardy Hope. If I is equal to zero then the factorial of zero is one. And we have a keyword there then one otherwise the factorial is going to be I times the factorial of I minus one. Right and then if statements in should have a program that actually computes factorial it compiles so now lets run it. So factorial of three is six. And factorial of six is 720 and that looks right. And if we try it one more time with a bigger number we get a we get a large number we think thats probably correct. And so anyway our factorial function is working. So now lets come back here and just as an exercise lets rewrite this code iteratively. So instead of using a recursive function lets write it using a loop. And in order to that [inaudible] gonna get rid of that code. What are we going to need? Well were going to need an accumulator here. Were going to need a a local variable that we can use to accumulate the results of the factorial computation. And the way you declare local variables in [inaudible] is with [inaudible] Statements or naudibl expressions. So were gonna have lets call this variable fact for the result of factorial. And notice here that I can have a variable that has the same name as the function. And the programming language [inaudible] will not get confused about that Because variables and functions play different roles. So well have the factorial fact excuse me its of type Int and we do initialize it to one. Alright so that multiplication will work. I think that the default for integers is to be initialized to zero and that would not be good if were going to be multiplying up fact with other with other numbers. Alright so then the a let has two parts. It has the variable or variables that you are declaring. This could actually be a list of variables. Well only have one this time. And then it has a body. The the expression or the computation in which the fact variable is available. And what do we want to do? So I think were gonna need to have this be a statement block cause were gonna need to have more than one statement in the sequence. And well see why in just a minute. But then we wanna have a loop. And so what is our loop going to do? Well were gonna say while I is not equal to zero. What do we and what do we need to do the opening for the loop body the opening keyword is called loop. [sound]. And now I think were going to need another statement block here. So lets open up a block. Were gonna probably need to do more than one thing. The first thing we want to do is we want to have fact. Be fact times I so we know that I is not zero so we need to multiply the current value of I into fact to accumulate the result and then we want to subtract one from I and notice that the assignment statement in cool is this backwards arrow. Thats how you do assignment. Its also how you do initialization. So initialization and assignment look the same. Then we can close off our statement block. Okay so the body of a while loop is always a single expression. In this case that expression is a block that consists of two statements. [sound] And then we can close the loop. And the closing for a loop is the pool key word. And then now were in a statement block so this has to end with a semicolon. Notice the statement block up there from the let And now we want the result of the let block or the let expression to be factorial. So whatever the whatever we got out of the while loop whatever we computed in the while loop we want that to be the result of the entire let expression. [sound] so thats the last statement of our blog. Remember the last statement of a statement blog is the value of the blog. The body of the led is the the result of the led so fact will also be result of the whole led statements its just the result of the statement blog. And since the body of the factorial method itself is just the led expression fact will be the result of the whole thing. And so this if weve written havent made any mistakes should be an iterative version of factorial. So lets compile this. And amazingly it complies on the first try. And now lets run it. [sound] And whoa! It actually works. So we got six. And lets just do one more test to see that to convince ourselves that things are working reasonably well and they are. Now let me just point out one common mistake that you can easily make and that I make when I havent written cool programs for a little while. If youre a C or programmer or a Java programmer you might think about writing assignments like this. So I just use the equal sign to write assignment. That looks completely fine if youre if youre familiar with those languages or used to programming in those languages. And now lets see what happens when we try to compile this. Oh it compiles just fine. And then what happens when we try to run it. Well it runs [inaudible] input so lets give it input and. Then we see that weve run out of heat. And that looks like an infinite loop. So were going around and around the loop And consuming memory for some reason. And well we [laugh] get to that much later in the class. Why why this loop actually ends up consuming memory. But clearly we dont have enough memory in the loop and and eventually we run out. And so so thats a sure sign Of an infinite look. So what is going on here? Well the thing is that equals. Equals operator in cool is the comparison operators. [inaudible] Well we compared I with O and that returns a boullion. So these are perfectly valid cool expressions. They just happen to be boullions. So you dont ever actually I or factorial in this program. Your just comparing fact with factoid body and I with I minus one and the program is perfectly happy to do that. It just doesnt compute the factorial function. And it never terminates because I never reaches zero. So that concludes our factorial example. And well do one more example next time of a more complicated of a of a cool program with some non-trivial data structures. Hello again. In this video were gonna wrap up our overview of cool with one more example of writing a cool program. For our final example lets look at a program that actually manipulates some interesting data structure. So well begin here by opening up a file. And lets call our program list dot CL this time. And as usual I will begin by writing our main routine and our main method. And once again lets lets make this inherit from I/O so we can do the I/O routines here. And lets just begin with something very simple as always. Lets just have something that prints out &quot;Hello World&quot; but in a little bit of an unusual way. Lets were going to end up writing a list a list abstraction. And lets first build a list by hand or at least build the elements of a list by hand and then well actually build the list abstraction and put them in a list. So lets have some strings. So we will have our string hello and this will also illustrate how you do multiple LET bindings simultaneously. I shouldnt say simultaneously. How you do multiple LET bindings in one LET expression. So you do them by just listing them and notice that this uses commas as a separator rather than semicolons as a terminator. So this left binding is going to define three names. Hello world and new line all of which are strings. And then Were going to now print these out on the screen so we are going to want to be able to do out string and since main inherits from self we can do that without an object there because it just again dispatches to the self object. And we want to concatenate these strings together in the right order. So well do hello dot and since hello is a string it can be concatenated to world and world is a string so it can be concatenated to newline And that should do the job. And just like probably one more thing about this lead this these lead bindings here. This notice if it the comma is the comma is the separator here meaning it doesnt come after the last one on the list. So it just separates that into the list so its not a terminator. And now we can close up our main procedure. Close up our class definition. Save it and now see if it compiles. Oh amazing! First try. And we run it and it prints hello world as expected. So now lets instead of introducing the three strings separately and then concatenating them together lets write an abstraction where we can build a list of strings. And then that abstraction will have a function within it to do the to do the concatenation. Alright so well have a class called list. And every list needs to I think to have two components. So first its gonna have the item thats in the list and thatll be a string. And then when you have a pointer to the next to the to the tail of the list to the rest of the list. And so I have a next field that points or is another list is another list of strings. Now we need a couple of methods in order to in order to use this list. Well need to be able to initialize a list in some way. So the initialization function will take an item and the rest of the list the next part of the list. And what is it going to do? Well its gonna need to set the fields of the object And so this one has to be done as a series of assignment statements. So well need a statement block and we will set the item to be the I argument. Well set the next attribute to be the N argument. And now we actually want this initialized object here this this method here to return the object itself. So that and thatll be convenient for chained together calls to [inaudible]. So well have it return self. Itll return the self object. And thats the end of out statement block. And then that is the end of our method. And I made a mistake up here. Weve gotta declare the return type of a knit. And what its going to return of course it returns a a object of type list. Ill need to put a list declaration there. Alright so that takes care of of a knit . And now we can use this to build build a list down here. So what should we do? Lets actually have a new variable called list. That well introduce here in this lead this series of lead bindings. And lets just build a list out of these three objects. So well say well have a new list and then well initialize to contain the string hello. And. What should the rest of the list be? Well that should be another list which is initialized to have the string the world. And what should be inside of that list file is another new list object which well initialize to have new line. And now what do we put here actually theres a little bit of problem here isnt there? We need to put a list object here. But we dont want to allocate a a new list object we want that to be really the equivalent of a of a null pointer. And theres no name for that in Cool actually you cant write down the name of a null pointer. Its called void in Cool. Theres no theres no no special symbol for that. So well have to create a variable that is just not initialized. And that will be as well uninitialized variable of type list will in fact be void. Itll be a null pointer. So lets call that nil. And itll be a typed list and no initializer. And so nil there will point to nothing or the the void point. And then we can use nil to terminate our list here. And then we have to close off all the params for all the nesting here. And I think thats it. And so that will be our list. Okay so we have a list of three strings. And now what we want to do with that is to print it out. And so what we would like to do is to have a list fall off the list and then a function thats gonna flatten that list and well just print it. So that is the what the what the main program should do. And now. You have to write the flatten function. So flatten takes no arguments. And its going to return a string. Its going to return a single string. And flatten is a pretty simple function. U h what do we have to do. Well. Theres really two cases. One is if were at the end of the string and the other is if were not yet at the end of the string. So lets test for that. So how do we know if were at the end of the string? Well if the next pointer is is void then there is no nothing more in the string. And there actually is a special test for that in Cool. Its called the isvoid function and its written like this. So if isvoid of next okay? So [inaudible] next field. So if the next field is void then what are we going to return? Keeps. Well then the result here is just the item whatever the item was in this last element of the list. And otherwise What we want to do Well otherwise we want to take the item and we want to concatenate onto it the result of flattening the rest of the list. And that is our flattened method. So lets see if that works. So lets compile this. And we got a couple of syntax errors here. So lets go back and see whats going on. So we have a syntax error here at the end of the the flatten method. And we see that we left out the [inaudible] to close a conditional. So a conditional has to be ended with with [inaudible]. Alright and lets see if thats working now. And we still have a syntax error at line 29. And the mistake here is that we forgot to declare the type of this variable which is a list. And then it gets initialized to this to this big expression that we wrote out. Lemme just do the indentation a little more nicely here. And notice something actually is worth mentioning here that this definition here this definition of the variable list. Depends on the definition of the previous variables in the let. So each of so when a let binding is made the name of the variable thats bound is actually available in subsequent let expressions. So in this case this variable list makes use of all of hello world and newline Which were defining earlier in the same lead constr uct. Lets save this and come over here and compile it. And we see we got another bug in the code. So we come up here. We see we Ive made a mistake here. Ive used functional notation here calling flattened of next and what I actually wanted to do was to dispatch to next on the method flattened. So that should be written like that. All right Probably getting close now. Lets see if it works yet. Well it compiles and now lets see if it runs. And indeed it does Prints out hello world Just as we expected. Now lets go back to our program and lets generalize this list abstraction in one way. Lets say that we can have an arbitrary list of objects not just strings. And that will require us to change a few things so it can be initialized now with an object. And now when it comes time to flatten this list we want to reduce a string. We want to present produce a print representation. But not everything in the in the list is necessarily a string. And we need a way to traverse the list and do different things for different kinds of things that might be in the list for different types of things that might be in the list. And so theres a constructing cool for the type of and of an object at run time and this is called the case construct. So let me first introduce a let expression here well let the string that were gonna construct Which is of type string. And thats going to be initialized to something and now its going to be a case. And what do we put a case on? Well its going to depend on the kind of thing the item is. So the item in the list could be it could be different kinds of types and we want to do a different operation depending on what item actually is. So the case item and then the key word is of and now we have different branches of the case expression for different kinds of things that could be in the list. So lets say that if its an [inaudible]. Okay so what this does is this says that if the item is an INT then were going to rename it to I were going to bind I to that integer and then we can do something with I. And what would we want to do with I? Well wed probably want to convert it to a string. So well do I to A &quot;I&quot; And what if in fact. That item happened to be of type string. The items in the list have to be of type string. Well then we can just use the item itself as the string representation. And we can do this for other kinds of types. If we had other kinds of types in our system we could continue to list out other cases here and how to convert them into a string representation. But lets just have a default case here. Well say if its any other kind of type which would which would be covered by having a branch saying that if its of type object well call it O then we should just abort. And so we should just call the abort function and quit. And thats our case. It needs to be terminated with a closing keyword called esag Again the reverse of of case. And now we can use that [inaudible] we constructed in our little function here. So if if the next field is void then were just gonna return the string. Otherwise were gonna return this string incatenated with the flattening out of the rest of the list. Kay. Now theres a couple of things we have to fix up. We used the I to A method here which means that list needs to inherit. From the conversion class A to I. And theres another issue here I see. And thats right here. So if you notice The the. The case statement needs to produce a string. Okay and it turns out that abort dose not return a string abort actually terminates the program but its type is that it returns an object. And so here we have to convince the type checker to convince to accept this piece of code and we need to get this branch here to type as a string. So what we can do and this is ugly but its the way to do it is we put it in a block in a statement block. We call abort first and a gain that will just terminate the program. And now we can put any string expression we want after that and thatll be the thatll give a type string to the entire block. So we can just put the empty string here for example and that has to be terminated with a semicolon since this isnt a block. And we can close that with a curly brace. Okay? So this is just something we have to do to make the type checker happy. And that may be everything we needed to do. So lets try compiling this. And we have to include the diversion library. And we have one syntax error. So far. And thats because we forgot to put the semicolon terminator on on each of our each of the each of the variables that we were introducing in the let. Okay got to save that. Lets try this again. And oops I didnt actually manage to fix the syntax error. And thats because I put the semicolon in the wrong place. Actually I I forgot. Variables that are bond in a lent are separated by by commas. But the branches of the case have to be terminated by semicolons. So what I said before was incorrect about using semicolons to terminate lent bindings. Its just in case branches where we need it In this example. Alright anyway coming back to this lets see if it compiles. And it does. And now lets run it And it works. Now of course we havent actually exploited the ability to have different types of objects in the list. So lets lets do that. Lets add an integer in here. Type ints and lets give it a number 42. And we can insert it in here. And now we can pass any object to a knit in the first position. So well just put in 42 right there. And when we compile and run this it just print hello world 42. If we if everything goes as expected And it does And that concludes our little tour of [inaudible]. There are a few features that we havent shown in these examples but you can look in the examples directory for lots more programs - many more pro grams that will show you all the different ins and outs and details of the other language features as well as the ones weve covered here. Welcome back. This is the first video in our long series of the implementation of compilers The call from last time that a compiler has five phases. Were gonna begin by talking about lexical analysis and this will probably take us three to four videos to get through at least and then well we will be moving on in order to the other phases. Lets start by looking at a small code fragment. The goal of lexical analysis is to divide this piece of code up. Into lexical units so things like the keyword if the variable names i n j and the relational operator double-equals and so on. Now as a human being this is. As we discussed last time this is a very easy thing to do because there are all kinds of visual clues about where the units lie Where the boundaries between the different units lie but a program like lexical analyzer. It doesnt have that kind of luxury. In fact what the what the likes of analyzer will see is something that looks more like this. So here I overwritten the code out just as a string with all the white space symbols included and is from from this representation this is a linear string you can think of this as bytes in the file that the lexical analyzer has to work and its going to mark through placing dividers between the different units. So it will recognize that theres a division there between the white space and the keyword. Then a division after the keyword and theres more a wide space the open paren the i another wide space double equals and so on and it goes through drawing these lines diving up. The the string into its lexical unit So I wont finish the whole thing but you should get the idea. Now it doesnt just place these dividers in the string however. It doesnt just recognize the substrings. It also needs to classify the different elements of the string according to their role. We call these token classes. Or sometimes Ill just call it the class of the token. And in English these roles are things like noun verb adjective. Okay and there is ther e are many more or at least or some more. And in the programming language the classes the token classes would be things like identifiers Keywords. I and then individual pieces of syntax like an open paren or a close paren those are the classes by themselves. A numbers. And again there are more classes but theres a thick set of classes and each one of these corresponds to some set of strings that could appear in a program. So token classes correspond to sets of strings And [inaudible] strings can be described relatively straightforwardly so for example. The token class of identifiers in most programming languages might be something like strings of letters or digits starting with a letter. So for example a variable name or identifier could be something like a1 or it could be f00 or it could be b17 all of those would be be valid identifiers and often often theyll be additional characters that allowed identifiers but thats the basic idea Very very often The main restriction identifiers that they have to start with a letter An integer and typical definition of an integer is a non-empty string of digits. So something like zero or twelve. Okay. One followed by two I should say is actually a string of number in this case. And and yeah it is actually whether admit some numbers you might not think of. Things like 001 would be a valid representation of a number or even 00 could be a valid integer according to this definition. Keywords are typically just a fix set of reserved words and so here Ive listed a few else if begin and so on. And then white space as itself a token class so we actually have to say in that string which is the representation of the program what every character in that string what token or what token class its a part of. What every substring is a part of and that includes the white space. So for example if we have a series of three blanks if I say if and then an open paren and I have three blanks in here these three blank s would be grouped together as white space. So the goal of lexical analysis is to classify substrings of the program according to their role. This is the the token class okay? Is it a keyword a variable identifier And then to communicate these tokens to the parser. So drawing a picture here lets switch colors. The lexical analyzer communicates with the parser. Okay and the functionality here is that the lexical analyzer takes in a string. Typically stored up also just a sequence of bytes and then when [inaudible] to the parser is sequence or pairs which are the token class. And substring which I would say string here that that of which is the sets of string which is a part of the input along with the class the role that it plays in the in the language and this pair together is called a token. So for example if my string is that f00 = 42 all right then that will go to the lexical analyzer and that will come Ill write down here three tokens. And these would be identifier. Who? Operator say equals. And. Integer excuse me 42. And here I just left these things as strings to to emphasize that these are strings. So this is not the number 42 at this point in time its its the string 42 which is a plays an integer role in the programming language. And then these and when the price that takes this input is this sequence of pairs. So the lexical analyzer essentially runs over the input string and chunks it up into the sequence of pairs where each pair is a token class and a substring of the original input. As we turn to the example from the beginning of the video here it is written out as a string. And our goal now is to lexically analyze this fragment of code. We want to go through and identify the substrings that are tokens and also their token classes. So to do this were gonna need some token classes. So lets give ourselves some of those to work with. Well need white space. And and so this is sequences of blanks new lines tab things like that with the keywords. And well need variables which well call identifiers. And well need integers and now Ill call those numbers. Here and then were going to have some other operations some other classes things like open paren close paren and semi colon and these are interesting. These three ae interesting because theyre single character token classes that is is a set of strings but is only is only one string in the set so the open paren corresponds to exact the language are in token classes all by themselves. Another piece of punctuation that well add here is is assignments. That will be a token class by itself because its such an important operation. But the double equals will class as a relational operator with this class as an operator put it up here. Alright So now what were going to do is were gonna go through and tokenized this string and Im going to write down for each substring. What class it is. You know Im just gonna use the first letter here of the class. Its indicated just to save time so I dont have to write everything up. Hence we change colors so we can do this in a different color. So the first token here is white space token and then that followed by the F keyword. So okay And then we have a blank here which is another white space and then the open paren which is its own token class so Ill just leave it to identify itself there and then we have an identifier. Okay White space and then an operator the double-equals. Another blank so thats white space followed by another identifier followed by close parens Again a punctuation mark in a token class by itself. And then we have three white space characters so those are group together as a white space token Followed by another identifier and more white space and then another single character token the assignment operator white space and a number And then sem i colon again and punctuation mark and a token class by itself. Two white space characters can group together. What follows in is a keyword so it gets classified as in the keyword token class. Another run of white space characters and then another identifier. Theres actually a blank there where we almost covered it up without marks. The assignment operator by itself in a token class white space a number and finally the semi colon by itself. And there is our tokenization. Weve identified the substrings and weve also labeled each one with its token class. Welcome back In this video were going to continue our lecture on lexical analysis with some examples from past programming languages where interesting lexical problems arose. So weve already talked a little bit about Fortran and what are the interesting lexical rules in Fortran is the white space is insignificant so white space doesnt matter and something like VAR1 to which could be a variable name VAR1 is exactly the same as VA R1 so these two program fragments have to mean exactly the same thing sand the idea in Fortran is that you can take your program and you could delete all the blanks from it and that shouldnt change what the program means at all. Lets take a look at an example of how Fortrans white space rule affects lexical analysis. Here are a couple of Fortran code fragments and I should say that this example is taken from the dragon book and actually couple of the later examples were also taken from an older edition of the dragon book. But anyway what we have here this is actually the header of a Fortran loop. And you know its a loop because it has the key word do which is like four in modern C or C++ so Id say loop key word And then we have out iteration variable I and we have a range that I will vary between. So in this case I will go from one up to 25. And then this number five here this is a little bit odd something you dont see in modern languages. In the old days in Fortran you would have your do statement at the top of the loop and then the size of the loop or all the statements included in the loop Were named by a label they came right after the do statement. So the loop will extend from the the header the do statement down to the label five. So whatever statement was able with five all of the statements in between would be part of the loop. And so the loop would execute those statements then well go back around to the header and then we keep executing those until it had done so for every one of the values of the iteration variable in this case one to 25. Now heres a nother code fragment and as you can see this one is almost exactly the same as the one above. The only difference is let me just switch colors is here that this particular fragment has a comma in that position and this fragment has a period. And it turns out that this difference makes all the difference that these two fragment of code mean completely different things. So this fragment the first one is in fact a do loop as I said before so it has the keyword do the label five the variable I and the range one to 25. Now this fragment down here this is actually a variable name do 5I So far as writing without the blanks. Remember the blanks dont matter This would be do 5I and this is an assignment equals the number 1.25. Okay And so you can see here these symbols the sequence the first sequence of symbols is interpreted completely differently depending on whether theres a period or a comma further on. And so lets just be a little more precise about that. How do we know what do is? So lets just focus on the keyword here do and when were at this point when our focus is here right after the zero. And keep in mind that that the way this is going to be implemented is by a left to right scan so were going to be walking in this direction over the over the input looking at each character successfully and when our focus reaches this point we can make a decision. Is this a is this a keyword cause weve seen the entire keyword too. And the problem is that we dont have information to make that decision. We dont know whether this is do or whether its going to be eventually be part of a variable name like do 5I. And the only way to know is to look ahead in the input to this position to see whether theres a comma or a period there. So this is an example of lexical analysis that requires look ahead. In order to understand the role of due as were going left to right. We have to pick ahead of the input to see some symbols that come later on. And we cant possibly disambiguate role of do until that poin t because up to this point the sequence and the symbols are exactly the same and so the only thing that distinguishes them is something thats much much further on. And as you can imagine having lots of look ahead complicates the implementation of lexical analysis and so one of the goals in the design of lexical systems is to minimize the amount of the look ahead or bound the amount of look ahead that is required. So you might wonder why Fortran has this funny rule about white space. It turns out that on punch card machines it was easy to add extra blanks by accidents and as a result they added this rule to the language so the punch card operator wouldnt have to redo their work all the time. Fortunately today we dont enter our programs anymore on punch cards. But this example does help us understand better what were trying to do in lexical analysis so as I said the goal is to partition the string. Were trying to buy the string up into the logically units of the language. And this is implemented by reading left to right. So were doing a left to right scan over the input recognizing one token at a time. And because of that look ahead may be required to decide where one token ends and the next token begins. And again I want to stress that look ahead is always needed but we would like to minimize the amount of look ahead. And in fact we like to bound it to some constant to this because it will simplify the implementation of lexical analyzer quite a bit. Now just to illustrate to look ahead is something that we always have to worry about. Lets consider this example which weve looked at before and just notice that when were reading left to right lets look at this keyword else here when we read the E. We have to decide is that a variable name or some symbol but itself or do we want to consider it together with the symbols that follow them. And so theres a look ahead issue here. After we scanned E we have to decide does that sit by itself or is it part of a larger lexical unit? And you know there a re single character variable names in this example like I J and Z and so its not unreasonable that E could also be one and another example is this double-equals. When we read a single equal sign how do we decide whether thats a single equals like these other assignments or that its really a double-equals. Well in order to do that if our focus point is right here we have to look ahead and see. Theres another = coming up and thats how we know or how we will know. That we wanted to combine it into a single symbol instead of considering this equals by itself. Another example from a a language from long ago PL [inaudible] is a interesting language. It was designed by IBM and it stands for Programming Language One. Alright It was designed to be the programming language. At least with an IBM that would be used by everybody and is supposed to encompass all the features that every programmer would ever need. And as such it was supposed to be very very general and have very few restrictions. And so one of the features of PL [inaudible] is that Keywords are not reserved. So in PL [inaudible] you can use a keyword both as a keyword and also as a variable. So you can use keywords and other roles other than keywords and that means you can write interesting interesting sentences or interesting programs like this. And let me just read this out loud because it sounds interesting if else then then = else else = then. And the correct organization here of course is that this is a keyword this is a keyword and this is a keyword. And the other things switch colors here are all variables. These are all variable names. And as you can imagine this mix a lexical analysis somewhat difficult because when were just scanning left to right like when were coming through here when we say were at to this point you know how do we decide whether these things are going to be variable names or keywords without seeing whats going on in the rest of the expression so lexical analysis in PL [inaudible] was quite challenging. So heres another example from PL [inaudible]. Here we have a program fragment we have the word declare and then an open paren and a close paren encompassing a bunch of arguments so well point out the balance parens here and then just a list of n things inside the parens. And it turns out that the pending on the larger context in which this whole expressions sits this could be either a keyword. Or it could be in array reference that mean when yeah that mean declare here could either be a keyword or it could be a name of an array and this could be the end [inaudible] to the array. And as it happens there is no way looking at just this much that we can decide. This fragment is valid is a valid declaration and its also a valid array reference. So it would depend on what came next. It might depend on for example whether there was an equal sign here in which cases would be interpreted as an assignment and and declare would be the name of an array. And the interesting thing about this example is that because the number of arguments in here is unbounded. There could be n of them for any n. This requires unbounded look ahead. Okay So to implement this properly as youre scanning left to right to decide whether declare again is a keyword or re-reference we would have to scan beyond this entire argument list to see what came next. Fortren and PL [inaudible] were designed in the 1950s and 1960s respectively and those experiences taught us a lot about what not to do in the lexical design of programming languages. So things are a lot better today but the problems have not gone away completely and Ill use an example from C++ that illustrate this. So heres an example of C++ template syntax which you may be familiar with or you may have seen the similar syntax in Java. And C++ has another operator called Stream Input. So this operator here reads from an input stream and stores the results in a variable. And the problem is here that theres a conflict with nested templates So for example if I have a template o peration that looks like this. Okay. Notice what happens here. So my intention here is to have a nested application of templates but I wind up with two great than signs together at the end and this looks just like the stream operator and the question is what should the lexical analyzer do? Should it interpret this as two close brackets for template or should it interpret it as a two greater than signs stuck together as a stream operator. And it turns out that for a very long time I think most C++ compilers have now fixed this. The C++ compiler in this situation would regard this as a stream operator and you would get a syntax there. And what do you think the solution was it turns out that the only fix that you could really do to make this lexically analyzed the correct way was to insert a blank so you would have to write this and you would have to remember to put the blank in there so that the two greater than signs were not together. And you know thats kind of ugly that we have to put in white space to fix the lexical analysis of the program. So to summarize the goal of lexical analysis is to partition the input streams into lexemes okay. So we have drop down dividing lines in the string to decide where the lexemes lie and we want to identify the token of each lexeme And because exactly because were doing a left to right scan sometimes we have to have look ahead. Sometimes we have to peek ahead in the input string to figure out what the current string were looking at what the current substring were looking at what role it plays in the language? In this video were gonna talk about regular languages which are used to specify the lexical structure of programming languages. To briefly review the lexical structure of a programming language is a set of token classes. And each one of the token classes consists of some set of strings. Now we need a way to specify which set of strings belongs to each token class and the usual tool or doing that is to use regular languages. So in this video were going to present like regular languages and define what they are and then in subsequent videos were going to look at some examples using them in actual programming languages. To define the regular languages we generally use something called regular expressions. And each regular expression team now its a set. There are two basic regular expressions. If I write the single character C thats an expression and what at the notes is a language containing one string. Which is the single character C okay Thats one basic form so for any single character I get a language with a one string language with just and then the only string is that character. Another basic building block of regular languages is the regular expression epsilon which is the language. That contains again just a single string this time the empty string. And one thing thats important to keep in mind is that epsilon is not the empty language okay? So this is not correspond to the empty string and the empty set of strings. It is a language that has a single string namely the empty string. Besides the two base regular expressions there are three compound regular expressions and well just go through them here in order. The first is a + b which corresponds to the union of the languages a and b. So this would be the set a such that a is in the language of big A little a is in the language of big A union little b such that b is in the language of little b so just the union of the two sets of strings. Concatenation is like string concatenation. So if I have two languages a and b or two regula r expressions a and b then the concatenation of a and b Is equal to all of the strings. Little a concatenate with little b where a is drawn from the language big A and little b is drawn from the language big B. And so this is cross sporadic operation. Choose a string from a. Choose a string from capital B and then combine put them together with the string from a first and choosing strings at all possible ways from all possible combined strings and thats the language a concatenated with b. And finally theres a kind of looping Kleene closure. And a star is equal to the union. For i greater than = zero of a to the i a to the i-th power. Whats that mean? Well a to the i-th power is just a to concatenated with itself By times. So this is [inaudible]. And note that because i can be = zero one of the possibilities here is a to the zero so a concatenate with itself zero times and what is that well thats the language epsilon. So thats the language contain the empty string. So the empty string is always an element of a star. To summarize the last couple of slides the regular expressions over some alphabet sigma. The smallest of that expressions that include the following. So lets define it so the regular expression r are equal to epsilon is always a regular expression. Or another possibility is the single character c where c is an element of our alphabet okay? So this is important the regular expressions define with respect to some alphabet. So we have to pick a family of characters that will form the base cases of the regular expression and here you know? We have one base regular expression for each character in the alphabet. And then we have the compound expressions. So another possibility Is that a regular expression is the union of two regular expressions. Another one is that the concatenation of two regular expressions. And the last one is that it could be the iteration of a regular expre ssion. So these five cases are the set of regular expressions over a given alphabet. Now this syntax here for describing the regular expressions with these vertical bars and these different cases on the right hand side in this recursive definition of r If you havent seen this before this is called the grammar. And thats not important for this lecture. Its not what this this lecture is about but were talking about grammars when we get to parsing. Next Id like to do a few examples of actually building regular languages writing the mountain and thinking about what they mean. And as we said whenever were talking about a regular language we first have to say what the alphabet is. And so for these examples lets just use the alphabet zero and one. So these are going to be languages which consists of strings of 0s and 1s. And lets start with a very simple example. Lets think about the language one star And what language that to note. So well we know the definition of star. If you remember that was the union over i greater than = zero of one to the i. Okay. And what is that equal to? Well thats just one. Repeated i thats what the concatenation of one to the i means okay. It means one concatenated with itself i and so this is going to be the empty string. Thats one concatenated with itself zero followed by one followed by eleven followed by one concatenated with itself three followed by one concatenated with itself four followed by one concatenated with itself any number of times. Okay And this and so we can see that this is just equal to all strings Of 1s All right? Now lets do a second example lets think about the language one. Plus zero concatenated with the language one okay? And remember how concatenation works is across products we take every string in the first expression and combining with every string in the second expression. So this is going to be equal to the strings a b where a is drawn from one + zero and b is drawn from one. All right? And what can that be when theres two traces for a. A could be one or zero and b could be one so in fact this is equal to the set one one and the strings one one the second [inaudible] of the strings one one and one zero. All right? Lets do another examples slightly more complex. Lets build up here to having two iterations in a union so have zero + one and think of about whats that equal to but weve already know what one is equal to. Thats equal to all strings of ones and so by analogy zero must be all strings of zeroes then we take the union of those two things so this is actually really easy to write out. Lets write them out in this notation so we have zero to the i for i again equal to zero okay. Thats zero union with. One to the i or greater than = zero. Thats the strings of all one. So theres a set at this expression to nodes. And for our last example lets think about zero + one. Now that iterated. Okay? So we put the star around the union of the two individual character instead of having the star on each character individually in taking the union of the two things. So what is the what is this expression equal to? Well lets work with the definition of star. So we know. That this is the union over i greater than or equal to zero of zero + one to the i. And what does that look like well that looks like first of all theres the empty string right? And then another string in this language is is. Excuse me is drawn from zero + one and so this I shouldnt say another string but another set of strings is the language zero + one. And then zero + one concatenated with itself okay? And in general is going to be zero + one concatenated by itself i times. Now what does that mean? That means that every position if we have a string of length i at every position we could pick a zero or a one to plug in and this works for any length string. This is gonna be true of strings of every length and so in fact this language is just going to be all strings Of 0s and 1s. In fact what that means is this is the cycle effect on our alphabet. Our alphabet that consists of zero and one and so this is the set of all strings that you can form over the entire alphabet And that has a special name when that happens when you have a regular expression that denotes the set of all strings you can form out of the alphabet we write that as sigma star okay? So just meaning that all the strings of the alphabet integrated as many times as you like One last point I wanna make on this before we go on here is that there are actually lots of ways to write each of these different languages. Theres not a unique way to write these. So for example lets just take this language here. The second one that we did and let me switch colors. Another alternative way to write this since we know the meaning of it is these two strings one one and one zero I could have written it as one one. + one zero and that would mean exactly the same thing. We used two expressions denote exactly the same set similarly with one star I could write this as one &lt;i&gt;. + one.&lt;/i&gt; And cuz this wouldnt change anything. Adding in the single string one wouldnt change anything since one is already included in one&lt;i&gt;. This might be kind of a&lt;/i&gt; silly way to write that set but it doesnt matter it has a meaning and it means exactly the same things as one&lt;i&gt;. The point again is that there is more than one way&lt;/i&gt; to write down the same set to write to write you can write multiple regular expressions that denote the same set. Welcome back. In this video were going to take a little digression and talk about formal languages. A formal language has played a big role in theoretical computer science but theyre also very important in compilers because inside of the compiler we typically have several different formal languages that were manipulating. A regular expressions are one example of a formal language but its actually helpful I think in understanding regular languages and all the formal languages well see later on in later videos to talk about what the formal languages in general So lets begin with the definition. A formal language has an alphabet So some set of letter sigma. And then a language over that alphabet is just a set of strings of the characters drawn from the alphabet. So in the case or regular languages we had certain ways of building up sets of strings of characters but other kinds of languages would have different sets of strings. And in general a formal language is just any set of strings over some alphabet. An example of a language that youre familiar with is a form from the alphabet of English characters and it is just the set of English sentences. Now This is not quite a formal language and that we might disagree in which string of English characters are in fact valid English sentences but one could imagine that we could define some rules that we would say the certain strings are English sentences and others arent. And if we could come to this on agreement this would be a fully formal language. Now a much more rigorous formal language would be something like the following; we could pick our alphabet to be the asking character set and the language to be the set of all Valid C program. So this is definitely a very well defined language. This is exactly the set of inputs that C compilers will accept. And the the important contrast I want to draw here is that the alphabet is actually interesting. So different formal languages you know? Have a very very different alphabets and we cant really talk a bout what the formal language is or what sort of strings were interested in unless to find that alphabet. Another important concept for many formal languages is a meaning function. Typically we have one of the strings in our language and lets call that some expression e and the expression e by itself is just a piece of syntax. Its a program in some sense or it represents something else that were Which is the thing were actually interested in. And so we have a Function L that maps the strings in the language to their meanings. And so for example in the case of the regular expressions this would be a regular expression. And that would be map to a set of strings. The regular language that that regular expression to notes and we saw an example where we wrote out the meeting function for regular expression last time so lets use regular expressions as an example and Im gonna first write down the meaning of the regular expressions. The way I wrote it down in the last video so if you recall we had a regular expression epsilon and that denoted a set Which contain just one string namely the empty string. And then we had a regular expression C for every character in the alphabet which also do need a socketing just one string namely the single character C. And then we had a bunch of compound expressions. So for example A + B. That was equal to the union of the sets A and B and we had the concatenation so I could I could [inaudible] A and B and that was equal to a cross product where I selected a string from each set in order and concatenated them together. And finally there was iteration so I could write a star and that was the union over I. Greater than zero of all the sets A to the I I ends. An interesting thing about this definition is you can see that they were mapping over we have expressions and let me switch colors here over here we have expressions and over here we have the sets. But theres something kind of odd about the way this is written and not quite right cuz you can see here we clearly we have an expression. We have a piece of syntax A + B and then somehow on the other side this this A this A and this B have magically turned into sets that were taking the union of and similarly down here were choosing an element from this set but this set is also an expression and what does that mean? Somehow were conflating the sets in the expressions and this is what. The meaning function is intended to fix and this what they or or or intended to make clear. So we what we really wanted to say is that theres some mapping That the language L epsilon is the set so the so L maps from expressions into sets of strings. Okay Its a function that maps one to the other and it you havent seen this notation before this is a standard notation for describing functions. It does says that L is a function from things in the domain in this domain to this range okay. And similarly the language of this expression is the set and it becomes really useful for the compound expressions cuz here we say the language of this expression. Is equal to the language of a union with the language of B and now you can see the recursion. First we interpret A and B using L and we take the union of the result. Okay so now its clear whats asset and whats an expression and similarly here the language of a concatenated with B we are going to select elements from the language of these two expressions and then were going to form another set from those two sets. And finally for iteration The language of a star is equal to the union over the meaning of a bunch of expressions A to the I is an expression. This is a a piece of syntax and we have to convert it to A set N order to take the union. And so about this is. The proper definition of the meaning of regular expressions where weve made the meaning function L explicit and weve shown exactly how recursively we apply L to decompose the compound expressions into several expressions that we compute the meaning of and then computed the sets from those from those separate smaller s ets. So theres other reasons for using a meeting function. We just saw one of them which is to make clear. What is syntax and what is semantics in our definitions. Some parts of the definition are expression and some parts are the the meanings or the sets and the using L makes it clear that the arguments to L are the the programs or the expressions and the results Are the the sets. The outputs are the sets But there are a couple of other reasons for separating syntax and semantics. One is that it allows us to consider notation as a separate issue. That is if we have syntax and semantics being different then we can vary the syntax while we keep the semantics the same and we might discover. That there that some kinds of syntax are better than others for the problems that were interested in for the languages that were interested in. And another reason for separating the two is because of expressions and meanings because syntax and semantics are not in one to one correspondents. And I actually illustrated this with regular expressions in the previous video but I want to iterate here that that there are generally many more expressions than there are meanings so that means there maybe multiple way. To write an expression that means the same thing. Id like to take a moment to illustrate why separating syntax from semantics is beneficial for a notation. So everybodys familiar with the the r number system so I can write numbers like zero one. 42 and 107 and there are very nice algorithms for describing how you add and subtract and multiply such numbers but there are older systems of notation for numbers. Things like the Roman numerals. I could have the number one. I could have the number four the number ten and say the number 40 I think is written like that and. And an issue with this number system first of all let me stress that these two have the same meaning. So the the meanings of expressions in this language are. Are the integers and its exactly the same in this language. So the idea the mean ing of these two systems are just the numbers but the notation is extremely different. The number written in Roman numerals was completely different from a number written in Arabic numerals. And the fact is that the Roman numerals are really painfully to do addition and subtraction and multiplication and in fact. Back in ancient times when this was a common system was not very well known how to do it and very few people were actually good at doing arithmetic with with the system because of because the algorithms were kind of complicated. And when we moved to the the Arabic system later That it was a big improvement because people it was easier for people to learn how to do basic arithmetic with these kinds of numbers. And the only thing that changed between one system and the other was the system of notation. And so notation is extremely important because it governs how you think and it governs the kinds of things you can say and the sort of procedures that you will use. So dont underestimate the importance of notation and this is one reason for separating syntax from semantics because we can leave the idea of what were trying to do than numbers alone. And play with with different ways of representing them and we might discover that some ways are better than others. The third reason I gave for separating syntax and semantics is that in many interesting languages multiple expressions multiple pieces of syntax will have the same semantics. Now going back again to regular expressions lets consider the regular expression zero&lt;i&gt;. Now there are many ways to write the same language which is the&lt;/i&gt; language of all strings of zeroes so string of zeroes of any length. So for example I could also write that as zero + zero&lt;i&gt;. Another way to write it is as&lt;/i&gt; epsilon + zero zero and here you can see that that this expression is all the strings of 0s of at least link one and then we get the empty string for epsilon so that is = zero and then just you know? Any combination of these things would also amount to an eq uivalent language for example that one and so on. So theres actually an unbounded unlimited number of way I could write this language but all of these mean exactly the same thing and if you think about it. What this means is that in general if I draw the two domains differently I think about different expressions over here and different distinct meanings over here and the function L that maps between them. The function L is many to one. So there are. Yeah. There are points in the space that where many different expressions or pieces of syntax map to the same meaning. And this is just a general characteristic of Interesting formal languages and this is actually extremely important in compilers because this is the basis of optimization. The fact that there are many different programs that are actually functionally equivalent thats what allows us to substitute one program that runs faster than another thats what allows us to replace one program with another if it runs faster and does exactly the same thing. So we couldnt do optimization and you know the reason we can do optimization as precisely because the meaning function is many to one. So meaning is many to one and keep in mind important point here its never one to many. We dont want the opposite situation. If we have the opposite situation Where L could map a single point to two different meanings. Well first of all this would no longer be a function but but also it would mean that the meaning of certain expressions say in our programming language was not well defined. Thats that when you wrote a program was actually ambiguous whether it meant this or it meant that and thats a situation we dont like. So we expect meaning functions to be many to one for nontrivial languages and we dont want them ever to be one too many. And that concludes todays video. Next time Going to go back and continue with our discussion of lexical analysis. Welcome back. In this video Im going to show how to use regular expressions to specify different aspects of programming languages. Must begin with the keywords and this serves a relatively simple case and Ill just do it for three keywords. Ill write a regular expression for if else or then and it would be obvious how to do it for more. So lets write a regular expression for if and that would be under regular expression for i. And followed by the regular expression for f and were taking the concatenation of these two. And then were going to union that with the regular expression for else and what is that? Well else consist of four individual characters so we have to write out a concatenation of those four characters. And as you can see this is a little bit verbose with all of these quotes and kind of messy to read. So in fact theres a short hand thats normally used and let me switch over to that right now. So if I want to write the regular expression for a sequence of single character expressions I could just most of the tools that let you write this I put a quote at the beginning right IF and then I close quote and this means exactly The same thing as this. This is the concatenation of two single character regular expression and similarly for else And similarly for them. And if I have more keywords Ill just write them out and union them altogether. Now lets consider a slightly more complicated example. Lets think about how to specify the integers who want to be the non-empty strings of digits. So the first problem here is to write out what a digit is and thats pretty straight forward. A digit is just any of the individual characters zero through nine and we already know how to write out single character regular expressions. And its just a union of ten of those to specify this and itll take me just a moment to finish. Here we go. So thats a regular expression for the set of strings correspondin g to all the single digits. And because we want to refer to this from time to time and also because as a very common thing to watch too most tools have a facility for naming regular expressions. So for example I can name this one to be digit so a single digital is anything that is generated or is in the set denoted by this regular expression. And now what we want to do is to have multiple digits. Well we know a way to do that. We can just iterate a single digit as many times as we like. And so here we get all strings all possible strings of digits and this is very very close to what we wanted except that the string that we want has to be non-empty. We dont want to count the empty string as an integer. And theres an easy way to do that. We just say that the whole sequence has to begin with a single digit and then its followed by zero or more additional digits so just. Is just to reiterate that we see there has to be at least one digit and is followed by zero more additional digits. And this pattern actually for a given language is extremely common. So if I wanted to say that I have at least one a I write that as a a because this has zero or more a the second part is zero more as and the first part says there has to be at least one a. And because this is so common theres a short hand for it. I think is supported by every regular expression processor and that is to write a plus. An a + means is just is just a short hand for a star. And so we can actually simplify this regular expression a bit and write simply digit plus. Now lets look at yet another example even more sophisticated than the the previous two. Lets think about how to define the identifiers which are strings of letters or digits that begin with a letter. And so we already know how to do digits so lets focus on the letters for a moment. So how we write out a regular expression for the letters while were gonna name it. So well say that the letters or actually a single letter. And now we have to write a regula r expression for all the individual letters and thats you know straightforward but tedious. We have to say little a lower case b lower case c lower case d. And well as you can see this is gonna be rather a large regular expression. Were going to have 26 lower case letters and 26 upper case letters and the whole thing is going to be rather tedious as to write down so lets actually not do that. Instead let me mention a shorthand A tool support to make it easier to write out exactly this kind of regular expression which is called the character range. So inside your square brackets I can write a range of characters. So how do I do that? Well I have the starting character and an ending character and I separate them by a hyphen. And what this means is the union of all the single character regular expressions beginning with the first character and ending with the second character so everything in between. So this is exactly the regular expression for all the lower case letters and then I can have another character range and so at the same square brackets for all the upper case letters. So A through Z Okay? And this regular expression here on the right defines exactly the big union that I didnt wanna write out okay? And that gives us a definition of the single letter and now were in great shape. We are we already have definition for a digit we already now we have definition for letter and so that we can write out the rest of this definition. So we want the whole Regular expression to always begin with a letter. Okay so identify always begins with a letter and after that is allowed to be a string of letters or digits okay? So they are suggest that theres going to be a union. So After the first letter we can have either a letter or a digit and then we can have an arbitrary string of those things. So we put a start on the whole thing and that is the definition of identifier. Begins with a single letter and its followed by zero or more letters and digits. So. Because were doing a complete lexical specification we also have to deal with even the parts of the string that were not really interested in. We have to have at least specification of them so then we can recognize and throw them away. In particular we have to be able to recognize the white space and. Were just gonna take white space to be a non-empty sequence of blanks new lines and tabs even though there are other kinds of white space characters. Things like maybe like rubout. Depending on your keyboard there might be others but these three will suffice to illustrate all the important points. So you know blank is relatively easy to write down. Thats just a single quote around the blank space but theres a problem with new line in tab. Because a new line that carries return in the file has special meaning typically. You know and on the line you end whatever command youre working on in this regular expression tools like SQL tools. And you know tab also is not an easy thing to write down and it doesnt look much different from a blank in a lot of cases. So what tools do is they provide separate name for these and its and typically its done by having some kind of escape character and a backslash. Is the most common one thats used and then followed by a name for the character? So back slash n is typically used for new line and back slash t is typically used for tab. And I just want to stress I mean the reason for doing this example is to illustrate this that. We have to have a way of naming some characters that dont really have a very nice print representation. There are other characters that that dont even have really any kind of print representation and we still need a way to talk about them in our regular expressions because them might be embedded in a file that we have to lexically analyze at some point. And so anyway the way this is done is by providing a separate. Naming scheme for such on principal characters and typically as the one that escape sequence. So something beginning with special character like back slash followed by the name of the character. So n for new line and this case nt for tab. And so to finish off our definition this gives us. You know? One character white space and then we want a non empty sequence or such things so we could wrap the whole union there in parenthesis and put a plus on it and that. Get us what we want. Lets pause for a moment in discussing programming languages and look at another example of using regular expressions from a different domain. Here I have an e-mail address and as it turns out e-mail addresses form a regular language and every e-mail processing device in the world. So your mailer and the mail servers that you use all do regular expression processing to understand what the e-mail address is telling them in the e-mail messages that they go by. And. And so we can think of an e-mail address is being is consisting of four different strings separated by punctuation. Theres a username and then three parts to the domain. Okay. Lets just assume for simplicity that the strings only consist of letters and practice they can consist of other kinds of characters too but lets just keep things simple and we can write out the more complicated things using the regular expressions but the structure would be the same as if we just consider them to be made of letters. And then these four strings are separated by punctuations so theres the @ sign and to decimal points thats. Form the separators of the strings so these is a relatively straightforward thing to write a regular expression for given what we know so far so the user name would be the non-empty sequence of letters and then that would be followed by an @ sign. And then the first part of the domain will also be on empty sequence of letters followed by a dot and then the rest is just the same. Hey so here were quite concisely we had specified large family of e-mail addresses. As I said in reality the e-mail addresses are slightly more complicated but they can be written out with just a slightly more complicated regular expression. Finally for our last example lets look at a fragment of the lexical specification of a real programming language and this case that language is Pascal which is in the its in the same general family as Fortran and C. And in this particular fragment of pascal deals with the definition of numbers and so lets take a look here. Ill start at the bottom and look at what the overall definition of a number is. So a number consist of three things some digits and Ill just read out this abbreviation and optional fraction. And an optional exponent so were dealing here with floating point numbers and so a floating point number have a bunch of digits and then it can be followed possibly by a fraction or not and that could be followed possibly by an exponent or not and. And the idea although we cant see it just for this particular definition is that either of action or the exponent can be present independent of the other. So now lets work briefly from the bottom up lets just check the digits on what we expect. So a single digit is in fact the union of all the common digits just as we would hope. And then a non-empty sequence of digit is a digit plus so this is what weve already seen. And now the interesting bid is to look at how the optional fraction and the optional exponent are defined and the optional fraction looks a little less scary so lets do that one first. So whats going on inside the fraction well if we have a decimal fraction there is gonna be a decimal point and thats gonna be followed by strong of digits so this is just. The fractional parts of the floating point number its just of it comes after the decimal point. And whats this plus epsilon doing out here? Well remember the plus is union and epsilon stands for the empty string. So what this is saying is that either the fractional portion of the number is present or its completely absent. So this is how you say something is optional. You write out the regular expression for the thing and then you do plus epsilon at the end and that means well either everything I said before it can be there or nothing is there. Okay? And the optional exponent is structured similarly but somewhat more complex. So you can see the whole exponent is optional because theres some regular expression here. Thats union with epsilon And so either Something is there and this is the optional or this exponent part or is not present at all. And now lets look inside how the exponents define if its there. So an exponent always begins with e. So this is exponent you know standard exponent notation and it always has a non-empty string of digits. So theres e followed by some digits and in between theres an optional sign. We know the sign is optional because epsilon is one of the possibilities. The whole the whole sign might be absent. And one of the possibilities for the sign well it could be negative or it could be positive. So either theres a positive or negative sign or no sign. In which case presumably its interpreted to be positive. Now this idiom where we have + epsilon indicate that something is optional is also extremely common and so theres another short hand that many tools provide so another way of writing this thats common is to say that. Thats my fractional component and then it might be absent. So the question mark after a regular expression just means exactly this construction that we take that regular expression and we or with epsilon. And so this one this regular expression is a is a little more complicated. Theres two optional components so lets just write out what that would look like so we would have the exponent begins with e and then we have a sign. Which is either + or - and thats optional so we put a question mark after it followed by a non-empty string or digits and then this whole thing is optional. The whole exponent is optional. S o this is an alternative and more compact way to write this expression. To wrap up I I always convinced you in this video that regular expressions can describe many useful languages. Weve seen some fragments from programming languages but also we saw that e-mail addresses could be specified this way. Other things that are regular languages are things like phone numbers and file names are also regular. And there are many many examples in everyday life where regular languages are used to describe some simple set of strings. And I also want to emphasize that so far weve used regular languages as a language specification. We use it to define the set of strings were interested in. But we havent said anything about how to actually implement lexical analysis. We still need an implementation. And thats what well talk about in future videos. In particular in particular were going to look at the problem of given a string as an irregular expression. Or how do we know whether that string is in the language of the regular expression are. Welcome back. In this video were going to talk about finite automata which well see in future videos are a good implementation model for regular expressions. So in the last few videos weve been talking about regular expressions which we use as the specification language for lexical analysis. And in this video were gonna start something new. Were gonna talking about Finite Automata which are the For a convenience as an implementation mechanism for regular expressions. And so regular expressions and finite automaton are very close related. It turns out that they can specify exactly the same languages called the regular languages. We wont prove that in this course but well certainly make use of that fact. So moving right along What is a finite automaton? Well here is a typical definition as you might see in a automaton theory textbook. Finite automaton consists of an input alphabet. So its a set of characters that it can read. It has this finite set of states. We should probably emphasize that. This is what makes it a finite automaton is that it has some set of states that it can be in. One of those states is special and its designated as the start state. Some subset of the states are accepting states so these are the states that. But well well just find that more in a minute but intuitively if the automaton terminates after reading some input on one of these takes that it accepts the input. Otherwise it rejects the input and finally the automaton has some set of state transitions that is in one state they can read some input and go to another state. So lets look at that little more detail so a transition in a finite automaton. If Im in in this case Ive written out one particular transition here. Were in state one and we read the input A then the automaton can move to state two okay. And there could be lots of different transitions for the automaton from different states and different inputs and its read the following way. If were in state one on input A  we would go to state two. And if the automaton ends in an accepting state when it gets to the end of the input that is going to do whats called accepting the string Meaning that it will say yes That string was in the language of this machine. So intuitively the automaton starts in the start date and it repeatedly reads inputs one input character at a time makes a transition. So itll see what kind of transition it can make out of its current state based on that input to another state and if thats done ringing the input its in one of the final states that it will accept. Otherwise is going to reject the input. Now one of the situations in which it rejects well if it terminates In a state S thats no one of the final or accepting states okay? So that ends in any other state besides one of the accepting states and its going to reject. If the machine gets stuck Meaning it finds itself in a state and theres no transition of that state on the input. So in particular lets say that in some state as a news and the input is A and theres no transition. Theres no transition specified per state as an input A so the machine cant move anywhere and it get stuck and thats also a rejecting state. And so in these two situations if if you either get to the end of the input and its not in a final state or. If it never reaches the end of the input because it can stuck and both of those cases it rejects the string. That string is not in the language of the finite automaton. Now theres an alternative notation for Finite Automata that I think is more intuitive for examples and so were going to emphasize that way of writing the mount. In this notation a state is represented as a known graph which just draws a big circle. The start state is represented as a node that has an edge or an arrow into it with no source. So this is a transition into the node but no source node that it comes from and that indicates the unique start state. An accepting state is drawn as a node wit h just double circles like this. And finally a transition is drawn as an edge between two nodes of the graph. So with this as the time in this state that Im circling in blue and I read the input a well then I can move to this state at at the tail of the arrow. So now lets do a simple example. Lets try to write up the automaton that accepts only the single digit one. So all we need is start state. And will probably want an accepting state as well and now the questions is what do we put in between the two? Well there would be some kind of transition here and its a good guess that we should take that transition if the machine reads the one. Now let me take a moment here to talk about how the machine executes so lets label these states. Lets call this state a and lets call this state b okay. So the machine will have some input. Okay and we can write that input out will be here. So lets just say we have the single character one and it begins in some state namely the start state. And so one configuration of the machine is the state that it is in And the input. And typically we would indicate where it is in the input by just a pointers saying what position it is in the input. And the important thing to know about input in [inaudible] the input pointer always advances. So when we or it only advances so when we read a character input the input pointer moves to the right and it never moves back. Alright So from state a we have a rule. We can see that were in state a. The next input character is a one and that allows us to take a transition to state b and so now where b in state b and where as our input point well its beyond the end of the input indicating we are at the end of the input. And so now this is. We are in an accepting state and we pass the end of the input and so we accept. Okay? So lets do another execution. So we start in state a and lets take as our input the string zero. Okay. And Id like to draw the pointer. Actually I should have drawn it before the input so well al ways put the pointer between two input elements. In this case its a merely to the left of the one were about to read. So in this case were about read zero so in state a. Our input is zero. We look at our machine. We see that there is no transition on zero. All right? And so the machine stays stuck. It doesnt make any move at all and this is our final configuration. And we could see that were not at the end of the input and so this is a reject. Okay so in this case the machine rejects that string as not being in the language of the machine. Lets do one more example. Lets say that were in state well were always beginning in state a and the start state and lets say our input this time is the string ten okay? And our input pointer is there. All right? So again were in state a. The input is a one and so well move to state b. And now the input doesnt change. Just the input point changes but Ill just copy the input over to show the difference. Now the input pointer has advanced cuz weve read one character of input and now were in another state. And now we can see that were in state b. Our next input is zero and there is no transition on zero from state b and so even though were in an accepting state b as a final state its one of the accept state and we havent consumed the entire input. And so this The machine also rejects this string so this is also a reject. And in general we can talk about the language. Of a finite automata that is equal to the set of...accepted strings. Okay? So the language of a finite automaton when Im talking about the language of a finite automaton I mean the set of strings that the automaton accepts. So now lets do a more complex example. Lets try to write out an automaton that accepts any number of one followed by a single zero. So once again well need a start state and well also need a final state and now lets start by thinking about whats the shortest string is thats in the language of this machine. So in this case we know it has to end in a singl e zero. So a zero definitely has to be a zero transition has to be the last move and before that zero can come any number of what? In a particular there could be no 1s. So one transition in this machine is that from start state on input zero we can definitely go to the final state because the single string consisting of a single zero isnt the language of this machine. And now the only question is how do we encode the fact that any number of 1s can proceed to zero? Well there is an easy way to do that. We can just add a [inaudible] by the start state. And take that transition if we read at one. And what does this mean? This means that well stay in the state state as longer are were reading 1s and as soon as we read zero well move to the final state because that has to be the end of the string if the machine is going to accept it. So lets do a couple of examples to convince ourselves that this works. Let me label this state?s again. So this is state a and thats stat b. So Lets write out here states and input. So well begin in state a and lets take as input 110 okay. So lets do an accepting case first. All right So our input pointer begins to the left of the first character. So were in state a in start state. Were reading a one and that says we should take a transition that puts us back in state a. And so we advance the input pointer. And now we consume the first one and and again were in state a and the next input is a one so well make another transition to state a. And the input cleaner will advance. So now were in state a and the next input is a zero and so well take the transition to b and now in this configuration so the input pointer has reached the end of the input were in an accepting state and so the machine accepts. 110 is in the language of this machine. So now lets do an example where we will reject the input. And what configuration do we begin in and again a configuration for a finite automaton that just means you know a point in the execution it alwa ys consist of a state and a position of the the input pointer. So our initial state is a and now lets just choose the string. I dont know lets take 100 and lets confirm that this is not in the language of the machine. All right So we begin in state a and our input pointer is there. Now we read a one and that means well you know. So its from state a transition of one. We stay in state a and the input pointer advances. And now we see a zero. So from state a and input zero we make a transition to state b. And now the input point is here so now were in state b and we have an input of zero but there is no transition the b and zero there are no transitions out of b at all and so the machine gets stuck it cant get to the en of the input and again even though were in an accepting state we havent read the entire input yet and so that means the machine will reject. And so 100 is not in the language of this machine. expressions are used to construct a full lexical specification on the programming language. Before we get started I want to quickly summarize the notation for regular expressions. One of the shorthand?s we talked about last time is a+ which means a sequence of at least 1a or the language aa&lt;i&gt;. Sometimes youll see a vertical bar&lt;/i&gt; used instead of unions or a + b. Can also be written a vertical bar b and optional a is abbreviation for the regular expression a + epsilon. And then we have character ranges which allows us to do a big union a bunch of characters in order. And then one more thats used thats thats actually fairly important but we didnt discussed last time is the compliment of the character range. So this expression here means any character except the characters a through z. So the last lecture we talked about a specification for the following predicate. Given a string s is it in the language as the function l of a regular expression. As we we define the language of regular expressions and talked about their semantics in terms of sets of strings. And so for any given regular expression we could reason out by hand whether a given string was in that language or not and this turns out not to be enough for what we wanted to do. So just to review what is it we wanted to do when were given an input which is a bunch of characters so heres a string of characters And it can be much longer than just setting characters and. But we actually wanted to do is to partition the string. We want to drop lines in the strings divide up into the words of language. Now of course each one of these words are to be The language at some regular expression. But just having a a a definition or a yes no answers not quite the same thing as having a method for partitioning a string into its constituting parts. And so were gonna have to adapt regular expressions to this problem and and this requires some small extensions and thats what this video is all about. So lets talk about how to do this. The first thing were going to do when we want to design the lexical specification of the language is we have to write the regular expression for the lexing to be to the [inaudible] classes and we we talked about how to do this last time. So for the numbers we might use digit plus desire as our regular expression and we might have a category of keywords which is just the list of all the keywords in the language. We would have some category perhaps of identifiers. There is the definitely we talked about it last time. Sequences of letters or digits that begin with with the letter and then were having a bunch of. Bunch of punctuations things like open parens close parens etc. So we write down a whole set of regular expressions. One for each syntactic category in the language and thats the starting point for our lexical specification. The second step what were going to do is were going to construct a gigantic regular expression which just matches all the lexings for all the tokens and this is just the union of all the regular expressions that we wrote out on the previous slides. So well just take the union of all those regular expressions and that forms the lexical specification of the language. And well just write this out we dont really care what these regular expressions are but theyre just some set r1 r2 and so on and the whole thing were going to call r. And now heres the heart of how we actually use this bicycles specification to perform lexical analysis. So lets consider an input. We input the string x1 up to xn. And now for every prefix of that input okay. Were going to check whether its in the language of the regular expression. So were gonna look at some prefix trying with the first character and were gonna ask ourselves is it in the language of that big regular expression. And if it is if it is in the language well then we know in particular that that prefix is in the language of one in the constituen t regular expressions cuz remember that r =. The sum of all the different talking classes of our language okay. So we know that this prefix x1 through xi is in the language of sum rj. Okay And so that we know that thats a word. In our language is one of. Is in one of the talking classes that were interested in And so what we do is we simply delete that prefix from the input and then we go back to three and we repeat. And in this way we keep biting off prefixes of the input and well do this until the string is empty and then we have [inaudible] analyzed the entire program. Now this algorithm has a couple of ambiguities or a couple of things that are under specified and those are actually interesting. So lets take a moment and talk about those. The first question is how much input is actually used? So lets consider the following situation. Lets say that we have the x1 to xi is in the language of our lexical specification. And lets say theres a different prefix thats also in the language of our lexical specification and of course your I is is not equal to J. What does that look like? Well it would look like the following kind of situation; we would have our input string. And we have two different prefixes of the input that are both valid talking classes and the question is which one of these do we want? And you know just be kind of [inaudible] here to have a concrete example lets consider. What happens when a = = is at the is at the beginning of the input. After we chopped off a bunch of other input and perhaps we have this sub-string or this prefix of the input that were looking at and the question is you know should this be regarded as a single = which would be an assignment operator in most languages or would it be regards to = = which in some language is a comparison operator? And and this is an example weve looked at before and discussed and theres actually a well defined answer to this question. And it is that we should always take the longer one and this has a name thats c alled the maximal munch. So the rule is that when faced with a choice of two different prefixes of the input either which would be a valid token we should always choose the longer one. And the reason for this is thats just the way humans themselves read things so when we see = = we dont see two different equal operators we see = = and if I. Look at you know that the sentence that I wrote up here you know when we look at HOW we dont see three letters. We gather that altogether in one long token. We go as far as we can until we see a separator and so because this is the way humans work; we make the tools work the same way and this normally or almost always does the right thing. Second question is which token should be used if more than one token matches? So what do I mean by that? Well again we have our prefix of the input and its in the language of our lexical specification and just remember that the lexical specification itself again is made up as the union a bunch of regular expressions for token classes. Now since that since this prefix is in the language of the lexical lexical specification that means that it again it must be in the language of some particular token class rj. But nothing says that it isnt also in the language of a completely different token class. Meaning at the same string could be interpreted as a as one of two different tokens and the question is if this happens which one should we pick? So for example just to make this concrete Recall that we could have a lexical specification for key words which would be things like if and else and so on and also for identifiers. And then the identifier was the letter Followed by a letter or a digit. Repeat it okay. And if you look at these two specifications youll see that the string if IF is both of them. So IF is in the language of keywords And its also in the language of the identifiers. And so should we treat it as a keyword or an identifier. Now the normal rule in most languages is that if its a keyword then i ts always a keyword and you know the identifier is actually dont include the keywords. And but actually its a real pain to write out the identifiers in such a way that you explicitly exclude the key words. This is a much more natural definition Ive written here for the identifiers. And so the way this gets resolved is by a priority ordering and typically the rule is to choose the one Listed first. Okay. So when there is a choice when there is more than one token class which the string might be long the one that is listed first is given higher priority. So in our file defining our lexical specification we would put the key words before the identifiers just as we have done here. The final question is what to do if no rule matches. What if I have the prefix of the input? That is not in the language Of my lexical specification. Now this can actually arise. Certainly there are lots and lots of strings that are not gonna be in the language of the lexical specification of most languages. And the question is how to handle that situation? So its very important for compilers to do good error handling. They cant simply crash. They need to be able to give the user the programmer a feedback about where the error is and what kind of error it is so we do need to handle this gracefully. And the best solution for lexical analysis is to not do this so dont let this ever happen. And so what we wanted to do instead is to write a category of arrow strings So all of the strings. Not in the lexical specification of the language. So we write out a regular expression. Again this is another regular expression here. For all the possible error strings all the possible erroneous strings that could occur as you know invalid lexical input and then we put it last. Put it last in priority. So that it will match after everything else matches and and the reason for putting it last. Is that this actually allows us to be a little bit sloppy in in how we define the error strings. It can actually overlap with earlier regular expressi ons. We can include things in the error strings that are in fact not errors. But if we put it last in priority then it will only match if no earlier regular expression match and so in fact they will only catch the error strings. Then the action that well take when we match the error string will be the prints in the error message and give device a feedback like where it is in the file and such. To wrap up this video regular expressions are very nice and concise notation for string patterns but to use them in lexical analysis requires a couple of small extensions. Some particulars a couple of ambiguities we have to resolve we want our matches to be as long as possible. So we take. As much input at a time as we can and we also want to choose the highest Priority match. So the regular expressions are given a priority. The different token classes have priorities and when theres tie when the same prefix of the input could match more than one we pick the one that has the highest priority. Typically this has done just by listing them in order in a file and the ones listed first have higher priority over the ones listed later. I just wanna warn you that when you go to right lexical specifications when you go to actually implement lexor for a language the interaction of these two rules that we take longest possible matches and we break ties and favor of the highest priority rules. That this lead to some tricky situations and its not always obvious that youre getting exactly what you want You have to think carefully about the Ordering of the rules and its actually how you write the rules so that you get the behavior that you desire. And finally to handle errors we typically write out. Catch all regular expression that soaks up all the possible erroneous strings and give it the lowest priority so that it only triggers if no valid token class matches some piece of the input. If I leave we havent discussed these yet but they are very good algorithm to know for implementing all of these and in fact well be able to do it in only single pass over the input and with very few operations per character. Just a few Just a simple table look up and this would be the subject of our future videos. Welcome back In this video were going to talk about converting regular expressions into non-deterministic finite automata. Before we get started I wanna give you an overview of the plan for the next few videos. We have a lexical specification that we want to implement and the first step is for someone to write that down as a set of regular expressions. Now that bites all the courses not implementation thats just specification. So we have to translate that into a program that can actually do lexical analysis and this actually happens in several steps. The first part is a translate Those regular expressions into non-deterministic finite automata that recognize the same exactly the same thing. And then those non-deterministic automata are translated into deterministic automata and finally those deterministic automata are implemented as a set of Lookup tables and a little bit of code for traversing those tables. So in previous videos we talked about this piece and weve also defined this piece. And so now were ready to put the whole thing together and in this particular video were going to focus on this component right here The translation of regular expressions to non-deterministic finite automata. So the plan is that for each kind of regular expression were going to find an equivalent Non-deterministic Automata automaton that accepts exactly the same language as the language or a regular expression. And heres a little bit of notation were gonna use. Well define these automaton for regular expressions and usually what were going to be doing is needing to modify their start states and their final states so well just indicate the start state with the l and the final state. With the double circle and we wont worry too much about the overall structure of the machine as long as we have a handle on the start state and the final state. I should say that in the machines well build here there will only be one final state. Okay so lets begin. So for the epsilon regular expression once the machine that accepts that well this is a very simple machine. We can just have a start state and a final state and epsilon transition between them so this machine accepts exactly the empty string. Certainly for a single character A we can define a one transition two state machine that accepts that one character. So from the start state we can move to the final state if it only if we read that particular character okay. So those are out two simple regular expressions and now we have to do the compound regular expressions. And these are little [inaudible] involved. So lets talk about concatenation first. And so because were gonna build these machines up from smaller. Regular expressions to larger ones we can assume that weve already converted A and B separately in two machines. So I have the machine for A. And to have a machine for B and now all I have to do is say how Im going to paste together these two machines to form a machine a compound machine that recognizes the same language as a concatenated with B. And heres the construction the start state for the compound machine will be the start state for A so well just keep that start state for A the same and then we modify the final state of A. So we make the final state of A no longer a final state and Ive done that here by removing the double circle on the final state of A andthe epsilon transition to the start state of B. Now if we think about it that does exactly the right thing but that says is that first you recognize some portion of the input that belongs to the language of A and when we get to that what would been the final state of A we can jump to the start state of B without consuming any input and then try to read the rest of the string as part of the language as as a string in the language of B. And for union we have a similar way of phasing together the machines. Although the the structure is somewhat different so we at a new start state for the compound machine and What does A + B mean? It means either the input is in the language of A or its in the language of B. And epsilon transitions are really good for capturing this because we just make a decision right from the start state is the string going to be in language of A or is it going to be in the language of B. So we make a non-deterministic choice and then we read the string as using that the automaton that we chose and if we get to the final state. Either those machines we can make the epsilon transition to the new final state for the compound machine. Now remember what the notion is of acceptance for Nondeterministic Automata you know? They make these guesses but if theres any guess that works then we say that its in the language of the machine. So if in fact the string is in the union of A or B then either choosing A or choosing B will work and so the machine will accept the string. And finally the most complicated case for iteration is star we have the following construction So heres the machine for A Embedded in here. Weve added a new start state and a new final state And now lets talk about how this works. So one possibility if we remember that epsilon is always in the language of A&lt;i&gt; and so we have this transition here&lt;/i&gt; We can go straight from the start state to the final state and accept the empty string. And so that just guarantees that the empty string is in the language. Otherwise what do we do? Otherwise we can make a transition an epsilon transition to the start state of A. And then we can from the final state of A if we reach it we can go back to the start state of the whole machine and we can do this as many times as we like. Okay So theres the iteration of A Surround this loop right here And when we reach the final state of A we can also decide to just make a transition to the final state of the machine we conclude that the last time. And so this machine recognizes zero or more strings in the language of A. So now lets do an example So heres a regular expression and we want to build a equivalent nondeterministic machine that recognizes the sa me language and were gonna follow our construction Which works by induction on the structure of the regular expressions starting with the simple regular expressions and building up to the compound one so what do we have here? So we have a machine for accepting one okay. So we need a machine that accepts one and if we call it had two states and it just you know made a transition between the two on the number one Similarly a machine for accepting zero. Okay And now we need to put them together in a machine that accepts either one or zero. And the way we did that is we made a choice from A from a start state for the compound machine where you can either move to the machine for accepting one or the machine for accepting zero. And then we have at the end also epsilon moves back to the final state of the compound machine. Okay And now we need to iterate this so we need to be able to accept zero or more of ones or zeros and so were going to take this entire Block here and paste it into the pattern that we had for iteration so how do we do that? Well we have a new start state and a new final state okay. And theres an epsilon move from the start state to the new final state to guarantee that we accept the empty string. And then we can just iterate this inner machine as many times as we like. We can make an epsilon move to the start state We could execute the machine ones and if we decide we want to do it again well we can do that. Okay Go back around for another time Or from the final state we can decide that weve seen enough and we can just move to the final state of the compound machine. So this machine then accepts the language one + zero&lt;i&gt;. And now we have a little bit more&lt;/i&gt; to do. We have to accept we have other machine that accepts just one so we build another machine that accepts The digit one and now we need to compose two of these things to concatenate them and that was very simple. We just have an epsilon move from the final state of the first machine to the start state of the second mac hine and then these are all the states of the final machine. And we just need to now label our final final state or the end of the state that were actually gonna use in the end of the final state of the entire machine which should be that one and the start state Which is this state over here. And thats the entire construction for the non-deterministic automata or a non-deterministic automata that recognizes this language. Welcome back. In this video were going to talk about converting nondeterministic finite automata into deterministic finite automata. Here again is our little diagram of the pipeline of a lexical analyzer how one is constructed. So beginning with the lexical specification we write our regular expressions. Last time we talked about the step the conversion of regular expressions and the non-deterministic finite automata and this time were going to talk about this step. And as you might guess in the final video in the series well talk about the final step which is the implementation of DFAs. So heres the Nondeterministic Finite Automata and we finished up with last time. And the first thing were gonna discuss today is an important idea called the Epsilon Closure of a state. And the basic idea of the epsilon culture is that I pick the states. And it could a set of states but well just do it for a single state. And then I look at all the states that I can reach by following only epsilon moves. And so b is the state that were starting with so b would be included in the set and then theres an epsilon move to c. So c would be included in the set and theres another epsilon move to d so d would be included in the set. So we would say the epsilon closure of b is = the set b c d. And lets do one more as an example. Want to take the epsilon closure of g. And when we switch colors up to this one Ill erase that and to this one in pink Our purple-ish pink. So the epsilon closure of g we always have to follow all the epsilon transitions out of g. So h would be in the epsilon closure of g but its not just single epsilon move. This is recursive. So any number of epsilon moves that I can take all of those states are included in the epsilon closure of g. So in fact i would also be included. A would be included and b and c and d will also be included And now if I look at all of these states that have been colored in the light purple color. I can see that I cant reach any new states from those states using only epsilon moves and so the epsilon closure of g would be equal to and state. Recall from the last video that an NFA maybe in many states any given point in time that is because of the choices it can make for a given input and NFA may reach multiple different states. And the question we want to address now is how many different states can it be in? Well if a non-deterministic automaton has n states. And it winds up in some subset of those states as how big can that subset b will clearly the cardinality of that said has to be less than or equal to n. So the NFA can get into a most and different states. Now instead I want to know the number of different subsets well how many different subsets are there of any things. Well that means there are two to the n - one possible subsets of n states. And theres something very interesting about this number. First of all its a very big number so clearly the NFA can get into lots of different configurations particularly one it has a lot of different states but the important thing is that this is a finite set of possible configurations. And this is going to give us the seed of the idea. For converting an NFA into a DFA or Deterministic Automata because all we have to be able to do to convert a Nondeterministic Automata into Deterministic Automata is come up with a way for the Deterministic Automata to simulate for the [inaudible] of the Nondeterministic Automata and the fact that the Nondeterministic Automata can only get into a finite set of configurations even that configurations is very large is exactly what we will exploit in the construction. Now were ready to give the construction showing how to map an arbitrary nondeterministic finite automaton to an equivalent deterministic finite automaton. So lets begin by saying whats in our NFA. So well have a set of states Which well call s and these are the states of the Nondeterministic machine. Theres a star t state a little s which is one of the states and there is a set of final states F. And then we also have to give the transition function and I want to write out the state transition function. I want to use the state transition function to define a a operator that were going to find handy for defining our DFA. So Id say that a applied to a set of states so x here is a set of states and a is a character in the input language. So a and x is = those states y such that there is some x little x here single state in the set of states such that theres a transition from x to y on input a. Okay. So this is just a way of saying Ive given the transition function at this set level. It says for a given set of state x show me all the states that you can reach on input a. Alright. So now were ready to define our DFA. So what will the DFA be? Well its gonna have to have all of these things. Its gonna have to have perhaps where the states are? What are the start state is? Whats the final states are and whats the transition function is? So lets begin with the set of states. The states will be the subsets Of s. So the states of the DFA will be all possible subsets of the states of the NFA so there will be one state of DFA for each subset of possible each possible subset of states of the NFA. And of course this is potentially a very big number but its still finite and so we can use that set of of subsets of states as the states based of the Deterministic machine So now whats the start state of the DFA. Well thats going to be the  epsilon closure. Now one of the set of final states Well so the final states will be consist of those state x and every member of the states of the DFA are sets of states of the NFA. So that x is a set and is can be every x such that x intersected with the set of final states of the NFA is not empty. And finally we need to define the transition function. And do we do that? Well we we need to say that for a given state x and another state y when is there a transition between them on some input a. Well that there will be such a transition under that conditions and well lets write them out. So remember were in state x. And what do we need to know? Well we need to know the set of states that we can reach on input A and well be justifying that thats A of X and then once weve gotten to where these once weve seen where we can go from the set of states X of input A. Theres also a possibility of making [inaudible] after that so furthermore we have to take the [inaudible] closer of that set of states okay? And So well say that theres a transition from x to y if y is equal to this set of states. Alright And notice that theres only one such set of states for any x and that guarantees of this is a deterministic machine. Each machine each state will only have one possible move on each input so. We can just now it goes to our check list and see if we have a deterministic machine. We have a finite set of states. We have a start state and we have a set of final states and we have a transition function with only one more per input and no epsilon moves. And so that is in fact a deterministic machine and the property that it maintain is that each step of computation. The state of the DFA records the set of possible states that the NFA could have gotten into the same input So lets work to an example of constructing a Deterministic machine from a Nondeterministic machine. Heres the Nondeterministic Finite Automata that we built in the last video and again this is the one that I used at the beginning of the video to define epsilon enclosure. So were gonna do the example slightly differently than the construction I gave on the previous slide. If we actually have to write out all the subsets of this many states it will take us a very very long time. And it turns out that not all of the subsets were actually used by the DFA. So were just going to enumerate the states that we actually need and well do that by beginning with the start sta te of DFA and then working out which additional states are required. So how do we do that? Well we begin with the start state of the NFA which is just this state a. And then recall at the start of the DFA is the epsilon closure of that state so that corresponds to this purple set here. Alright. So the first state of the DFA the start state is the subset of states a b c d h i. And now we have to work out from this particular state from the start state what happens on each of the impossible input values. So the alphabet of this machine is one and zero so you would have to have two transitions out of the state one for an input of one and one for an input of zero. So lets do input zero first. And we can see looking at the purple set that theres only one possible transition and thats from the state D to the state F. So certainly the state s is included in the set of states if the NFA can reach but then once we get the state f theres a lot of epsilon moves that we can take and so in fact the second state of the DFA corresponds to a much larger set. Its all the its the epsilon closure of f and that is this set of states f g h i a b c and d okay. So these are the set of possible states that the NFA could be in after reading a single zero. Next lets consider what happens from the start state on an input of one. Which possible states can the NFA reach? And if we look at the transition function we see there are two possible moves that the NFA could take. It could be in state c. In which case it would move to state e or it could have been state i thats also part of the purple set in which case it would move to state j. So there are two possible states that the NFA can get into as a result of reading a one and then after that theres a bunch of epsilon moves that can take place and in fact it turns out that after reading a one the machine could be in any state except for state f. And thats this set of states and youll notice that this particular set of states the read set includes the final state of the NFA so this is also a final state indicating that after reading one the NFA could be in an accepting state. So this would be an accepting state of the DFA Well we still have to fill in for both of the two states that weve added here. The two states on the right of the machine what they do on input zero what they do on input one. So lets figure that out. So beginning with the red state on input zero what can happen? Well look the red state includes state d and it can move to state f but weve already computed what happens on the epsilon what the epsilon closure that is just the green state. And so if Im in the red state and I read zero I move to the green state. If Im in the red state and I read a one youll see that both states NFA states c and i are in the red set. And so it just takes us back to the red set. And similarly for the green state if I read a one I move to the red state. And if I read a zero I stay in the green state. And so this then is our deterministic machine down here. This is the deterministic machine and again it simulates the NFA. So every move at the deterministic machine it records the set of possible states that the NFA could be in and it will accept a string infinitely if the NFA could accept the string. Welcome back In this video were going to wrap up our presentations on lexical analysis with the discussion of how we implement Finite Automata using a variety of different techniques. Just to review heres our little flow chart of how lexical analyzers are constructed. And today were going to be focusing on this last step. The implementation of DFAs and actually I should say that this chart is not quite completely accurate because sometimes we dont go all the way to DFAs. We stop with NFAs and we implement them directly and so well talk a little bit about that. What if we didnt want to build a DFA and instead wanted to base our lexical analyzers directly on an NFA. So beginning now with DFAs Its very simple to implement a deterministic finite automaton as an array. Theres dimensional array and one of the dimensions will be the state. So we might have states here and the other dimension will be the input symbols. And so I might have a state i and then input A and I simply look up in that position and there will be the next state k to which were going to move. So the table stores at every particular input symbol and state the next state that the machine will move to. So lets do an example of converting a deterministic automaton into a table driven implementation so here is the automaton that we built last time and recall that several videos ago. We started with a regular expression which we convert into a non-deterministic finite automaton and then we converted it to a deterministic automaton just in the last video. And here it is again and now lets talk about how to realize it as a table. So draw 2-dimensional table and there are three states so we will need three rows. And just label these rows S T and U and then there are two inputs zero and one and now lets fill in the entries in the table. So in state S on input zero where do we go? We go to state T. So the entry in the S0 entry will be T. And some really from state S input one will wind up in state U. So on so the S1 entry will be U okay? And then certainly for the other the other rows of the table lets do the T row next on one we go to U and on zero we stay in T. So this this row is also TU. And finally for U what happens well on zero we go to T and on one we stay in U so this row is also TU and theres our table. That describes the transition relation of the automaton. And now if we would think about how we would use this transition relation in a program you can easily imagine. We would start out say with our input index. Porting to the beginning of the input and lets call that zero and we can have to have a current state and we start at the start state lets just say that thats row zero so in this case that would be row S. And then while what we wanted to do we wanted to walk over the input. And check whether and checking on it you know and make the transitions for each element of the input one at a time and we want to stop when the input is empty. So while there is still as input lets say we have an array of characters that is our that is our input and while the entry in that array is not null lets do the following. Were gonna update the state. At each step and what are we gonna update it to lets give this array a name. Lets call this array A. So were gonna look up in our transition relation A and what are we going to use? Well were gonna look up the current state And were going to look up the input. And in that entry I think you know? Using the the current state and the current input were gonna transition to a new state and we also wanted to increment the input pointer. So well do that at the same time. And there is our loop that processes an input according to the transition table A. And as you can see this is a very compact very efficient process. Just really just a little bit of index arithmetic and one two table look ups one for the input and one for the state transition table per character of input. So thats really hard to imagine having a process thats much faster or more compact than this. Now that was one strategy for implementing a deterministic finite automaton and you may have noticed that one disadvantage of that particular approach was that there were a lot of duplicate rows in the table. In fact all the rows of the table were exactly the same and we could save some space by using a slightly different representation. So instead of having a 2-dimensional table we can just have a 1-dimensional table and this table again. Would be one entry for each state so S T and U. And what this table would contain is a pointer to a vector of moves for that particular state. So there will be a pointer here and it would point to another. Table another one dimensional table that would say what we should do zero and what we should do on one. So in the case of S we wanted to move to state T if it was a zero and to state U if it was a one. And now when we go to fill in the entry for T we see we dont need to duplicate this row. We can actually just share this row and similarly for U. And so this table this representation is actually much more compact which just share the rows that are duplicated in the automaton. And it turns out that in the kinds of automata that we look at for lexical analysis its very very common to have duplicated rows. And so this can actually resolve the significant compaction of the table particularly when you consider a number of possible states. Remember there could be. To the N - one states in a DFA. For an NFA with end states. And while the blow up is often not the worst case it can be very substantial. So the two dimensional table we had on the in the previous slide could actually be quite quite large and we keep we can sometimes have a much more compact representation by little tricks like this. Now in this advantage of this particular representation is this extra interaction right here. I mean these pointers actually take time to the reference and so now in our loop will be a little bit slower. We we we have to do table look up the reference. Pointer do another tab le look up and then we can make the move. Finally its also possible that we might not want to convert to a DFA at all. It might be that the particular specification we gave is very expensive to turn into a DFA. The table has just become truly huge and we might be better off just using the NFA directly. And so one can imagine an implementation of an NFA as well. We can also implement that via a table. In this case we would have to have a row for each state in the NFA. And we wont do them all here. But we could have Rows for every state of the NFA and then you know where were going to go if the input is zero or if the input is one. And so in this case And I almost forgot we would also need a transition in the most naive or the most straight forward implementation of where we would go if if an epsilon. And and now remember because these are NFAs in general this will be a set of states because we might have more than one epsilon transition or more than one transition on zero and one. And so in this case an epsilon A can go to B. So this would be the set of states B and B can go To C or D. And C can only go to E and on one alright. And D can go to F on and input of zero and so on. We fill in the rest of the table and this table is guaranteed to be relatively small because the number of states is limited by the number of states in the NFA and the size of the input alphabet. Once again we could do a sharing of common rows and and other tricks like that to compress the table if we like. But now the inner loop for simulating this automaton is gonna be much more expensive because we have to deal with sets of states rather than single states. So in every point in time where we can be tracking a set of states and when were going to do a move we have to look up for every state in the set where it can potentially go including things like the epsilon moves and carry out all possible epsilon moves so we always have an accurate assessment of the complete set of states if the NFA could reach. So while this sav es a lot of space in terms of the tables in terms of the size of the tables it can be much slower to execute than deterministic automaton. Summarize a key idea in the implementation of lexical specifications is the conversion of nondeterministic finite automaton to deterministic finite automaton. This is what takes a general high level specification use of regular expressions and confer to them to something as completely deterministic and only uses a few operations per input character. Now in practice tools provide tradeoffs between speed and space. So so DFAs are faster And less compact so the tables can be very large and and at times thats a practical problem and NFAs are slower to to implement but more concise. And the tools give you generally a series of options often in the form of configuration files or command lines which is that allow you to to choose whether you want to be closer to a full DFA something thats faster and perhaps bigger or to a pure NFAs something thats slower but consumes less space. In this video were going to transition from lexical analysis to parsing and talk a little bit about the relationship between those two compiler phases. Weve already talked about regular languages and its worth mentioning that these are the weakest formal languages that are widely used. But they have of course many applications some of which we saw in previous videos. The difficulty with regular languages is that a lot of languages are simply not regular. And theres some pretty important languages that cant be expressed using regular expressions or finite automata. So lets consider this language which is the set of all balanced parentheses. So some elements of this language would be at the string one open-paren one close-paren two open-parens two close-parens three open-parens three close-parens and so on. And you can imagine that this is actually something thats fairly representative of lots of programming language construct. So for example any kind of nested arithmetic expression would fit into this class but also things like nested if and elses will have this category this characteristic. And here with the nested [inaudible] its just the f statement the functions like an open-paren. Not every languages like cool which has the explicit closing fee as well but theyre implicit in many languages and so there are lots of nesting structure in programming languages constructs and those cannot be handled by regular expressions. So this raises the question of what the regular languages can express. And why they arent sufficient for recognizing arbitrary nesting structure. So we can illustrate the limitations of regular languages and Finite Automaton by looking a simple two state machine. So lets consider this machine. We have one we have start state and then the other state is the accepting state. And well have this machine. Just be a machine that weve already seen actually and itll recognize strings with odd numbers of 1s. So if we see a one and were in the start state we move. We now see an odd number of 1s. We move to the accepting state and we stay there until we see another one. In which case weve seen even number of 1s and then were in the start state. So whenever we see an odd number of 1s were in the final state. Whenever we see an even number of 1s were in the start state. And if we feed this a fairly long string of 1s lets lets select only seven 1s in it. Then whats it going to do is going to go back and forth and back and forth between these states. Its gonna wind up in the final state when it gets to the last one so itll accept but notice that it doesnt know how many times its been to that final state. It doesnt remember the length of the string; it doesnt have any way of counting how many characters the string had in it. And in fact all I can count here is the parity. So in general Finite Automata can really only express things where you can count modulus on k. So they can count mod k for some k where k is the number of states in the machine. And so you know if I have pre-test the machine I can keep track of whether the string length is divisible by three or some other similar property but I cant do things like count to an arbitrary i so if I need to recognize a language that requires counting arbitrarily high like recognizing all strings of balance parentheses we cant do that with the finite set of states. So what does a parser do it takes the sequence of tokens as input from the lexer and it produces a parse tree of the program. And for example in cool heres an input expression that is input to the lexical analyzer. The lexical analyzer produces this sequence of tokens as its output. Thats the input to the parser. Then the parser produces this parse tree where the nesting structure has been made explicit. So we have the if and else and then the three components: the predicate the then branch and the else branch of the if To summarize the lexer takes a string of character as input and produces a string of tokens as output. That string of tok ens is the input to the parser which takes a string of tokens and produces a Parse Tree of the program. And its worth mentioning a couple of thing here. First of all sometimes the Parse Tree is only implicit. So the a compiler may never actually build the full Parse Tree. Well talk more about that later. Many compilers do build an explicit parse tree but many do not. The other thing thats worth mentioning is that there are compilers that do combine these two phases into one where everything is done by the parser. So the parsing technology is generally powerful enough to express lexical analysis in addition to parsing. But most compilers still divide up the work this way because regular expressions are such a good match for lexical analysis and then the parsing is handled separately. In this video were gonna begin our discussion of parsing technology with context-free grammars. Now as we know not all strings of tokens are actually valid programs and the parser has to tell the difference. It has to know which strings of tokens are valid and which ones are invalid and give error messages for the invalid ones. So we need some way of describing the valid strings of tokens and then some kind of algorithm for distinguishing the valid and invalid strings of tokens from each other. Now weve also discussed that programming languages have a natural recursive structure So for example in Cool an expression That can be anyone of a very large number of things. So two of the things that can be are an if expression and a while expression but these expressions are themselves recursively composed of other expressions. So for example the predicate of an if is a a [inaudible] expression as is the then branch and the else branch and in a while loop the termination test is an expression and so is the loop body. And context-free grammars are in natural notation for describing such recursive structures. So within a context-free grammar so formally it consist a set of terminals t a set of nonterminals n a start symbol s and s is one of the nonterminals and a set of productions and whats a production? A production is a symbol followed by an arrow followed by a list of symbols. And these symbols there are certain rules about them so the x thing on the left hand side of the arrow has to be a nonterminal. Thats what it means to be on the left hand side so the set of things on the left hand side of productions are exactly the nonterminals. And then the right hand side every yi on the right hand side can be either a nonterminal or it can be a terminal or it can be the special symbol epsilon. So lets do a simple example of a Context-free Grammar. Strings of balanced parenthesis which we discussed in an earlier video can be expressed as follows. So we have our start symbol and. One possibility for a string o f balanced parentheses is that it consists of an open paren on another string of balanced parentheses and a close paren. And the other possibility for a string of balanced parentheses that is empty because the empty string is also a string of balanced parentheses. So there are two productions for this grammar and just to go over the to to relate this example to the formal definition we gave on the previous slide what is our set of nine terminals its just. The singles nonterminal s what our terminal symbols in this context-free grammar is just open and close paren. No other symbols. Whats the start symbol? Well its s. Its the only nonterminal so it has to be the start symbol but generally we will when we give grammars the first production will name a start symbol so rather than name and explicitly whichever production occurs first the symbol on the left hand side will be the nonterminal for that particular context-free grammar. And then finally what are the productions with the we said there could be a set of productions and here are the two productions for this particular Context-Free Grammar. Now productions can be read as rules. So lets write down one of our productions from the from the example grammar and what does this mean? This means wherever we see an s we can replace it by the string of symbols on the right hand side. So Wherever I see an s I can substitute and I can take the s out. If that important I remove the s that appears on the left side and I replace it by the string of symbols on the right hand side so productions can be read as replacement rule so right hand side replaces the left hand side. So heres a little more formal description of that process. We begin with the string that has only the start symbol s so we always start with just the start symbol. And now we look at our string initially its just a start symbol but it changes overtime and we can replace any non-terminal in the string by the right hand side side of some production for that non-terminal. So for exam ple I can replace a non-terminal x by the right hand side of some production for x. X in this case x goes to y1 through yn. And then we just repeat step two over and over again until there are no non-terminals left until the string consist of only terminals. And at that point were done. So to write this out slightly more formally a single step here consist of a state which is a which is a string of symbols so this can be terminals and non-terminals. And somewhere in the string is a non-terminal x and there is a production for x in our grammar. So this is part of the grammar okay? This is a production And we can use now production to take a step from to a new state Where we have replaced X by the right hand side of the production Okay? So this is one step of a context-free derivation. So now if you wanted to do multiple steps we could have a bunch of steps alpha zero goes to alpha one goes to alpha two and these are strings now. Alpha is are all strings and as we go along we eventually get to some strong alpha n alright. And then we say that alpha zero rewrites in zero or more steps to alpha n so this means n zero greater than or equal to zero steps. Okay. So this is just a short hand for saying there is some sequence of individual productions. Individual rules being applied to a string that gets us from the string alpha string zero to the string alpha n and remember that in general we start with just the start symbol and so we have a whole bunch of sequence of steps like this that get us from start symbol to some other string. So finally we can define the language of a Context-Free Grammar. So [inaudible] context-free grammar has a start symbol s so then the language of the context-free grammar is gonna be the string of symbols alpha one through alpha n such that for all i. Alpha i is an element of the terminals of g okay. So t here is the set of terminals of g and s goes the start symbol s goes in zero or more steps to alpha one Im sorry a1 to an okay. And so were just saying this is just saying that all the strings of terminals that I can derive beginning with just the start symbol those are the strings in the language. So the name terminal comes from the fact that once terminals are included in the string there is no rule of replacing them. That is once the terminal is generated its a permanent feature of the string and in applications to programming languages and context-free grammars the terminals are to be the tokens of the language that we are modeling with our context-free grammar. With that in mind lets try the context-free grammar for a fragment of [inaudible]. So [inaudible] expressions we talked about these earlier but one possibility for a [inaudible] expression is that its an if statement or an if expression. And we call that [inaudible] if statements have three parts. And they end with the keyword [inaudible] which is a little bit unusual. And so looking at this looking at this particular rule we can see some conventions that way that are pretty standard and that well use so that non-terminals are in all caps. Okay so in this case was just [inaudible] well try that in caps and then the terminal symbols are in in lower case all right? And another possibility Is that it could be a while expression. And finally the last possibility Is that it could be identifier id and there actually many many more possibilities and lots of other cases for expressions and let me just show you one bit of notation to make things look a little bit nicer. So we have many we have many productions for the same non-terminal. We usually group those together in the grammar and we only write a non-terminal on the right hand side once and then we write explicit alternative. So this is actually. Completely the same as writing out expert arrow two more times but we here we just is this is just a way of grouping these three productions together and saying that expr- is the non-terminal for all three right hand sides. Lets take a look at some of the strings on the language of this Context- Free Grammar. So a valid Kuhl expression is just a single identifier and thats easy to see because EXPR is our start symbol Ill call it EXPR. And so the production it does says it goes to id. So I can take the start symbol directly to a string of terminals a single variable name is a valid Kuhl expression. Another example is an e-th expression where e-th of the subexpressions is just a variable name. So this is perfectly fine structure for a Kuhl expression. Similarly I can do the same thing with the while expression. I can take the structure of a while and then replace each of the subexpressions just with a single variable name and that would be a syntactically valid cool while loop. There are more complicated expressions so for example here we have a why loop as the predicate of an if expression. Thats something you might normally think or writing but perfectly well form and tactically. Similarly I could have an if statement or an if expression as the predicate of and if its inside of an if expression. So so nested if expressions like this one are also syntactically valid. Lets do another grammar this time for simple arithmetic expressions. So well have our start symbol and only non-terminal for this grammar be called e and one of the possibilities while e could be the sum of expressions. Or and remember this is an alternative notation for e arrow. Its just a way of saying Im going to use the nonterminal for another production. We can have a sum of two expressions or we could have the Multiplication of two expressions. And then we could have expressions that appear inside the parentheses so parenthesized expressions. And just to keep things simple we could just have as our base only base case simple identifier so variable names. And heres a small grammar over plus and times to see and in parentheses and variable names. [inaudible] a few elements of this language. So for example a single variable name is a perfectly good element of the language id + id is also in this language. Which s is id + id id and we could also use parens to group things so we could say id + id id thats also something you can generate using these rules and so on and so forth. There are many many more strings in this language. In this video were gonna continue our discussion of parsing with the idea of a derivation. So a derivation is a sequence of productions so beginning with the start symbol we can apply productions one at a time. In sequence and that produces a derivation. And a derivation can be drawn in a different way instead of as a linear sequence of replacements we can draw it as a tree. So for example if I have a nonterminal x it appears in a derivation then when I replace x I can represent that by making the children Of x the left hand side of the rule that I used to replace x. So I applied production x goes to y1 to yn I add the y1 to yn is children of x in the tree that Im building up. Lets do an example. Here is our simple grammar of arithmetic expressions and lets consider this particular string id id + id. So what were going to do now is were going to parse this string and were going to show how to produce a derivation for the string and also at the same time build the tree. And here it is Over here there is derivation beginning in e and ending in the string that were interested in with one production applied each step along the way and here is the corresponding tree and this is called a parse tree. This is a parse tree of this expression or this input string. So lets walk through this derivation in detail. The right side in red were going to have the tree that were building up. And on the left side in blue were going to have the steps in the derivation that weve taken so far. So initially our derivation consists of just the start symbol e and our tree consists of just the root which is also the start symbol. So the first step is that we have a production e goes to e + e and what that means is over on the tree we take the root of the tree and we make we give it three children e + ne. So now we replace the first t by e z. We use the production e goes to e z and that means we take the first e in the tree and we give it to three children e expression and we replace it by id which means we make id a child of the left most e in the tree that were building. And then we replace the second e by id using the production e goes to id and finally we use the same thing with the third e and now we have completed our Parse Tree. So here again from the start symbol to the string we were interested in parsing and in the process we built up this Parse Tree of the expression. Now there are lots of interesting things to say about parse trees. So first of all parse trees have terminals at the leaves and nonterminals at the interior nodes and furthermore in-order traversal of the leaves is the original input. So lets back up and look at our example and confirm all this. If we look at the leaves we can see that they are all terminals Okay? And the interior nodes are all nonterminals. In this case its only one nonterminal in our language all the interior nodes are e and the leaves are the terminals of the string. And then we can see if we do an inward reversal of the leaves we get exactly this input string that we started with. Furthermore the Parse Tree shows the association of the operations and the input string does not. So you may notice here that the way that this Parse Tree is constructed the times binds more tightly than the plus because the times is a sub-tree. Of the tree containing plus. And so this means that we would do the e e first before we would add e and some some of you may have wondered well how did I know. To pick this Parse Tree because actually if you think about it theres another derivation. Actually there are several derivations that will give me a different Parse Tree where the plus where the times is towards the root and the plus is nested inside the times. So lets not worry about that for right now and lets just say that somehow we knew that this was the Parse Tree we wanted and I gave you a derivation that produces that Parse Tree. Continuing on the previous derivation I showed you is actually a very special derivation. Its whats called a leftmost derivation where each step will replace the leftmost nonterminal in our string of terminals and nonterminals. And theres a natural and equivalent notion of a rightmost derivation and here it is. Here is a rightmost derivation for the same string. Again beginning with the start symbol ending with a string were interested in. And notice that at each step were replacing the rightmost non-terminal. So here we replace the only non-terminal e and we get e + c. And then in the second step we replace the second non-terminal e with id and so on for the rest of the string. So lets just illustrate this entirely with our little picture here of the tree and the derivation simultaneously so once again over here is our tree and this is the root the start symbol e and and in blue is our derivation so we begin by replacing e by e + e. Thats the only nonterminal so its the rightmost one and then working from the right side of the tree we replace the right e by id and then the left id gets replaced by e z. And now the right most e that remains is replaced by id and finally the only e that remains is also replaced by id. Now I want to point out that the rightmost and leftmost derivations I showed you have exactly the same Parse Tree. And this was not an accident. Every Parse Tree has a rightmost and a leftmost derivation. Its just about the order in which the branches are added. So for example if I have the first production e goes to e + e now I have a choice on how to build my tree. I can either work on. This sub-tree or I can work on that sub-tree. And if I build this one first that will be a rightmost derivation. If I continue to always work on the rightmost non-terminal of course And if I work on this one first I can use that to do a leftmost derivation. Now its important also to realize that there are many derivations besides rightmost and leftmost. I could I could choose non-terminals in some random order to do my replacements. But th e rightmost and leftmost ones are the ones that were most concerned with. In this video were going to talk about ambiguous context free grammars in programming languages and what to do about them. Well begin by looking at our favorite grammar for expressions over + and and identifiers and well just look at the string id id + id. Now it turns out that this particular string and lets write it down one more time id id + id. This string has two parse tree using this grammar. Lets do the Parse tree on the left first. We begin with the start symbol e and the first production in this derivation that gives us this Parse tree must be that e goes to e + e. E + e And then we replace the left most e by e e. We use the production e goes to e e and we still have the plus e left over and at this point you can see that were going to get this parse tree. Weve done with those two productions. We have done this much. The construction of the parse tree and the rest of productions are just generating these ids. So thats a three more productions and we can see that you know if we do those well get id id + id no problem alright. So now lets switch and do the derivation on the right or excuse me the parse tree on the right so this begins also with e. But this time we use the production e goes to e e first all right? And now were gonna replace the right most e. By e goes to e + e so we have e e + e and now weve with those two productions weve done this portion of the parse tree and once again with three more productions we can get to id id + id so there you can see weve got two derivations. That produced two distinct Parse trees. And just to be completely clear about this in this case were getting two different Parse trees. Each of these derivation each of these Parse trees has many derivations. Each Parse tree has a left most derivation a right most derivation and many other derivations. This situation is something different. Here we have two derivations that yield completely different Parse trees and that is the sign or the definition of an ambiguous grammar. So a grammar is ambiguous if it has more than one Parse tree for some string. And another way of saying the same thing is that there is more than one right most or left most derivation for some string. So if some string has two right most derivations or more or two left derivations or more then the that string will have two distinct parse trees and that grammar will be ambiguous. Ambiguity is bad. If you have multiple parse trees for some program then that essentially means that youre leaving it up to the compiler to pick which of those two possible interpretations of the program you want it to generate code for and thats not a good idea. We dont like to have ambiguity in our programming languages and leave decisions about what the program means up to the compiler. In this video were going to take a rest a little bit and talk about how compilers handle errors and in particular what kind of error handling functionality is available in parsers. Compiler has two relatively distinct jobs. The first is to translate valid programs. That is if it gets a program from a programmer that is correct is a valid program it needs to produce correct code for that program. Now distinct from that task is the job of giving good feedback. For erroneous program and even just detecting the invalid programs we dont want to compile any program that isnt a valid program in the programming language. And programming languages have many different kinds of errors. Heres just a few. So for example we might have lexical errors thats for using characters that dont even appear in any balanced symbol in the language and these would be detected by lexical analysis phase. We could have syntax errors and this would be the parsing errors when all the individual lexical units are correct but theyre assembled in some way that doesnt make sense and we dont know how to compile it. There could be semantic errors for example when types mismatch. Here Ive declared excess in integer and use it as a function and that would be the job of type checker to catch those. And then Actually there may be many errors in your program that are not errors of the programming language. The program you wrote is actually a valid program but it doesnt do what you intended. Youre likely bugging your program and so while the compiler can detect many kinds of errors it doesnt detect all of them and you know once we get past what the compiler can do then its up to testers and users to find the rest of the problems in the program. So what are the requirements for a good error handling? Well we want the [inaudible] report errors accurately and clearly so that we can identify what the problem is quickly and fix it. The compiler itself should recover from the error error quickly. So when it hits an error it shouldn t take a long time to make a error handling to slow down the compilation of valid code. That is we shouldnt pay a price for the error handling if were not really using it. Im going to talk about three different kinds of error handling. Panic mode and error productions are the two that are used in current compiler. So these are actually things that people use today. Automatic local or global correction is an idea that was pursued excessively in the past. And I think its historically quite interesting particularly as a contrast what people do today and also why people try to do it a long ago. [inaudible] is the simplest and most popular method of error recovery thats widely used and the basic ideas that when an error is detected the parser is going to begin discarding tokens until one that has a clear role in the language is found and thats going to try to restart itself and continue from that point on. And these tokens the ones that its looking for are called the Synchronizing Tokens. And these are just tokens that have a well-known role in the language and so that we can reliably identify where we are. So a typical strategy might be to try to skip to the end of a statement or to the end of a function if an error is found in a statement or function and then begin parsing either the next statement or the next function. So lets look at a simple hypothetical example of panic mode error recovery. So heres an expression. Clearly it has a problem. We shouldnt have two plus signs in a row so something has gone wrong here at the second plus and whats going to happen is the parser is going to come along. The parser is going to be proceeding from left to right. Its gonna see the open-paren its gonna see the number one its gonna see the plus everything is good and then its gonna see the second plus and its not gonna know what to do. Its going to realize. That its stuck and that theres no expression in the language that has two p lus signs in a row and it needs to do something to recover. Its encountered a parsing error and it has to take some error action at this point. So in panic mode recovery what its going to do is its going to hit the panic button. So right at this point its going to say I give up Im not parsing normally anymore. It goes into a different mode. Where is simply throwing away input until it finds something that it recognizes and for example we could say that the policy in this particular for this particular kind of error is to skip ahead to the next integer and then try to continue. So well just throw away the plus in this case and then it would restart here at the two expecting to see another integer. Try to finish off this expression and it would treat that as one just fine. Now in tools such Bison which is widely used parser generator and one that you may use for the project there is a special terminal symbol called error to describe how much input to skip and the productions that are given in Bison look like this. So you would say at the possibilities for e are that e could be an integer e could be an. The sum of the two es two expressions it could be a parenthesized expression or if none of this work okay. So these are the normal productions. Alright If none of those work then we could try some of these productions that have error in them. And error is a special symbol for Bison and it says well these are the alternatives to try if these things over here didnt work. So if you find an error. Lets focus on this one right here so if this says that if you find an error while youre trying to parse an e. Okay we havent actually said how that works yet. Well see that in the future videos but conceptually the parser is trying to recognize one of these kinds of expressions here. Its in a state where it thinks it wanted to see an integer or a + or a parenthesized expression and if that isnt working out i f it get stuck well then hit the panic button and you can declare that its in error state and it can throw away all the input. This error will match all the input up to the next integer. And then this whole thing could be counted as an e. As one of these things and then we will try to continue the parsing. Similarly if we encounter an error somewhere inside a pair of match parenthesis well we could just throw away the whole thing and just reset at the parenthesis boundaries and they continue parsing. So these are two possible error recovery strategies if we find an error for this particular kind of symbol in the grammar. And you can have these error These productions that involved the error token for for as many different kinds of symbols in the language as you like. Another common strategy is to use what are known as error productions and this specify known common mistakes that programmers make adjust as alternative productions in the grammar. So heres a simple example lets say we were working on a programming language or compiler for a programming language that was used by a lot of mathematicians and instead of writing Five x like computer scientists do these guys always wanted to write five x to just juxtapose the five and the x to look more like normal mathematical notation. And they complain that this is always giving them parser errors. If the parser is just complaining over and over again then this is not a well formed expression. Well we could just go in to our grammar and add a production that made of of well form. We could just say well now its legal if I have that one kind of expression is just to have two expressions that [inaudible] opposed next to each other with no intervening operator. And this has a disadvantage obviously of complicating the grammar. If we do this a lot our grammar is gonna get a lot harder to understand. Its gonna be a lot harder to maintain and essentially all this is doing is promoting common mistakes to alternative syntax but this is used in practice. Peo ple do this sort of thing and you will see for example when you use TCC and other production C compilers they will often warn you about things youre not supposed to do but theyll accept them anyway and this is essentially the mechanism by which they do that. Last strategy I want to talk about a little bit is error correction. So so far weve just talked about strategies for detecting errors but we could also try to fix errors that is if the program has mistakes in it the compiler could try to help the program out and say oh you obviously didnt mean to write that. Let me try to find a program for you that that works. And these kind of corrections in some sense we wanted to find programs that are nearby programs that arent too different from the programs at the that the programmer supplied but we couldnt compile correctly. And theres a few different things that you can do to the things that people have tried are things like token insertions and deletions. So here youd like to minimize the edit distance. That would be the metric that you would use to determine whether a program was close to the original program that the programmer supplied. You could also do exhaustive search within some bounce to try all possible programs that are close to the program that was supplied. And a couple of disadvantages to this actually number of disadvantages. You can imagine that this is hard to implement. It its actually quite complex. This will slow down the parsing of correct programs because we need to keep enough state around that we can manage the search or or the editing in the case of that way actually doing counter and error and of course nearby is not really is not really that clear what that means and various notions of nearby may or may not actually take us to a program of the the programmer would actually be happy with. The best one example of error correction is the compiler PL [crosstalk]. This is PL [inaudible] compiler thats the PL part and the C stands for either correction or Cornell which is where the compiler was built and PL [crosstalk] is well-known for being willing to compile absolutely anything. You could you could give it the phone book. You can and people did give it things like speech from Hamlet soliloquy and it would print out a lot of error messages. Sometimes these error messages would be very funny to read. And it would in the end do correction and produce always a valid running PL [inaudible] program. And you might ask why do people bother with that? It doesnt seem but that may not seem very compelling To us today. And have to realize that when this work was done back in the 1970s people live in a very different world. There was a very slow recompilation cycle. It could take a whole day To get your program To compile and run you would essentially submit your program in the morning and you might not get results back until the afternoon. And with that kind of turnaround cycle Even one syntax error in your program was devastating. You can lose a whole day because you mistype the keyword and having the compiler try to take a stab at finding a working program for you if the correction was small and you save an entire day you know to think it can fix that one small mistake you made and give you a valid run that was actually a useful thing to do. And so the goal then was to find as many errors in one cycle as possible. They would try they would try to find as many errors to try to recover. Find as many errors as possible. Give you as good feedback as possible so you could fix as many errors avoid as many retry cycles as possible. And and even possibly automatically correct the program. So that you could see if the correction were right and and then possibly the the results you got back were useful on the [inaudible] to do even more debugging before the next round. Now today were in a completely different situation. We were very fast almost interactive recompilation cycle for many projects and as a result users generally arent interested in finding many errors. They ten d to correct only one error per cycle. Compilers still report many errors Ill give you lots and lots of errors but my observation certainly might have it. Personally what I see many other people do is they only fix the first one because its the most reliable and the one that definitely needs to be fixed before before you can try to compile again. If the compilation is fast enough thats probably the most proactive thing to do. And as result complex error recovery today is just less compelling than it was a few decades ago. In this video were gonna talk about the core data structure used in compilers the abstract syntax tree. To briefly review a parser traces the derivation of a sequence of tokens but this by itself Is not all that useful to the compiler because the rest of the compiler needs some representation of the program. It needs an actual data structure that tells it what the operations are in the program and how theyre put together. Well we know one such data structure is called a Parse Tree but it turns out that a Parse Tree really isnt what we wanted to work on. Instead we wanted to work on something called an Abstract Syntax Tree. And the Abstract Syntax Tree is really just the Parse Tree but with some details ignored. We have abstracted a way From some of the details of the Parse Tree. And heres an abbreviation that youll see ASTs stand for Abstract Syntax Tree. So lets look at the grammar. Heres the grammar for plus expressions over the integers and we also parenthesize expressions. And heres a string and after lexical analysis what do we have? Well weve got a sequence of tokens again with their associated lexemes telling us what the actual strings were. And that gets past into the parser and then we build a parse tree. And heres a parse tree for that expression. Now if its expressed that this representation the parse tree is actually perfectly adequate for compilation. We could do our compiler using the parse tree. This is a faithful representation of the program. The problem is that it would be quite inconvenient to do that and to see this it only point out some features of the parse tree. First of all you can see if the parse tree is quite robust so for example we have here a node e and it has only one child. So when theres only one successor of the of the node what is that really doing for us? Well we dont really need the e at all we could just put the The five right here and and make the tree smaller and similarly for the other single successor nodes. Furthermore these parentheses h ere well these are very important in parsing because they show the association of of this of the arguments with respect to these two plus operations. It shows that this plus is nested; this plus down here is nested inside. Of this plus up here But once weve done the parsing the tree structure shows us the same thing. We dont need to know that these were inside a parenthesis that the fact that these two expressions or the argument of this plus already tells us all we need to know. And so you know? All of these nodes in here are also in a sense redundant. We dont really need that information anymore. And so we prefer to do is to use something called an Abstract Syntax Tree that just compresses out all the junk in the Parse Tree. So here is a. Abstract syntax tree or a hypothetical abstract syntax tree that would represent the same thing as the parse tree on the previous slide and you can see here weve really just cut it down to the essential items. We have the two+ nodes. We have the three. Arguments and the association is just shown by which plus node is nested inside the other. We dont have any of the extraneous nonterminals. We dont have the parenthesis. Everything is much simpler and you can imagine that itll be easier to write algorithms that walk over a structure like this rather than the the rather elaborate structure we had on the previous slide. Of course again is called an abstract syntax tree because it abstracts away from the concrete syntax. We suppress details of the concrete syntax and just keep enough information to be able to faithfully represent the program and compile it. In this video were going to talk about our first parsing algorithm recursive descent parsing. So Recursive Descent is what is called a top-down parsing algorithm and you might suspect that there are also bottom-up algorithms and they are indeed such things but we will be talking about them later but in a top-down parsing algorithm the parse tree is constructed from the top so starting with the root node and from left to right. And so the terminals then will be seen in the order that they appear in the token string. So for example if I have this token string here this is a hypothetical parse tree that I could construct and the numbers here correspond to the order in which the nodes of this parse tree are constructed. So we have to begin at the roots thats the first thing that happens and then if T2 is a. Belongs here in the parse tree. That would be next thing that happened but then if we have a nonterminal of the next position that will be number three and then if it has children well the left most one should be going left to right will be the fourth thing to be generated. And then lets say the two children of number four are both terminals that would be the next two terminals in the input and so on. The next thing thatll happen is the second child of number three and then the last two terminals appearing in left to right order. So lets consider this grammar for integer expressions and lets look at a particular input a very simple one just open paren five close paren. And now what were going to do is were going to parse this using a recursive descent strategy. Im not gonna actually show you any pseudocode or anything like that. Im just going to walk through how this how this input string would be parsed. But using this grammar and the Recursive Descent Algorithm and the basic idea is that we begin with a nonterminal we begin with the root node and we always try the rules for nonterminal in order. So we will begin by starting with e goes to t and if that doesnt work well try e goes to t + e. So this is gonna be a top down algorithm beginning at the root. Were gonna work from left to right we try the productions in order and when the productions fail we may have to do some back tracking in order to try alternative productions. There are three parts. Theres the grammar that were using. There is the parse tree that were building and initially thats just the root of the parse tree 3e and finally theres the input that were processing and well indicate our position in the input how much of the input we have read by this big fat red arrow and it always points to the next terminal symbol to be read The next token to be read. So in this case were starting with an open paren. Okay? And also in the grammar you can see the highlighting here the brighter red color indicates which production were going to try. So were going to begin to build our Parse Tree by trying production e goes to t and what does that mean? Well that means we make t the child of e and then we continue trying to build the Parse Tree. Well so remember were going left to right and top-down so now t is an unexpanded nonterminal is the only unexpanded nonterminal so we have to work on it. And what are we going to do well were going to try a production for t and since we havent tried any yet well just try the first one t goes to it. So the next step is to make nth a child with t and thats what our parse tree looks like. And now we actually have something that we can check. We can check whether were making progress. So observe that as long as were generating nonterminals we dont really know whether were on the right track or not. We have no way to check whether the nonterminals that were generating are gonna produce the the input string. But once we generate a terminal symbol then we can compare that with the next input token to see if theyre the same and in this case unfortunately theyre not. So the nth that we generated here doesnt match the open paren in the input and so clearly this parse th is parsing strategy or this. Parse Tree that were building isnt going to work out. So what were going to have to do is were gonna have to back track. That means were gonna undo one or more of our decisions. Were gonna go back to our last decision point and see if theres another alternative to try. So whats the last decision we made well we decide to use t goes to nth so we can undo that and then we could try the next production for t. And that happens to be t goes to n t so expand t using that production and now once again we generated a terminal in the left most position and so now were able to compare that with the input and once again unfortunately the nth token does not match the open paren so we have to back track again. So we undo that decision. And this takes us back to trying alternatives for t. Theres one more possibility and thats the t goes to (e). So we expand t using that production. And now we can compare the token open paren. With is this open paren? With the open paren in the input and they match. And so thats good. That means that were we might be on the right track. And since they match anything that we do in the future is going to have to match the different input and so well advance the input pointer. So now where were gonna work on next? Well we have to expand this non-terminal e and were gonna do the same thing we did before. Were just gonna start with the first production. So we have e goes to t and then we have to work on t so were gonna pick the first production for t and we have t goes to int. So now we can compare. Is int matching int in the input? And if it does and so we advance the input pointer again And now were here and whats left well we progressed to this point. Were looking at that open paren and that also matches. So that matches the input and now weve matched everything in the parse tree and our input pointer is at the end of the string and so this is actually a successful parse of the input of the input string. And so that means th at we accept and the parser terminates successfully. In this video Im going to cover a limitation of the Recursive Descent Algorithm that I presented last time. Heres the grammar from our last presentation and heres its implementation again as a set of mutually recursive function that together implement this simple recursive descent strategy. And now lets think about what happens. When we go to parse the input int simplest possible input strength. Well lets work through it. So remember we start with the function that implements all the productions for the non-terminal e. And so what were going to do here were going to call e. And that will try calling E1. All right? And what is E1 going to do? E1 is going to call T. Because of course the first production is E goes to T. So lets take a look at what T does. T is going to try out the production of T1 all right? And what does T1 do? Well T1 recognizes an int. Okay so thats good. And it will match it and return okay and then E will return and we will succeed in parsing. And I forgot to mention it also the process the input point will be moved across the int and so when were done you will return and we will have succeeded in parsing the string int because E return true the production for E return true and we consumed all of the input. All right? So now lets consider a slightly more complicated example okay? So lets try the input string Int times int. All right? So again we start with the production E. Okay? And the first thing well do is well try the production E1. Same thing we did last time. E1 is going to call the function T. And T is going to try the first production for T. Which again is the production int. Okay? And the input pointer of course is here and then it will try to match that against an int. Okay? If I match the first token in the input stream against the the terminal int. And it will succeed. Okay? So the input pointer will be moved over. So T1 will return true. All right? And as a result. This right hand side here of the function T will also succeed because T1 returns true so T will return true. Okay? Therefore E1 will return true and E E1 returning true will cause E to return true. And in fact that will be the end of the execution of the program will terminate. E will return true and the input player will only have advanced as far as int and so we will reject the parse. This is actually ends up getting rejected. And the question of course is what happened? All right. Why didnt we succeed in parsing this input? Which is clearly in the language of this grammar. Well the story here is actually a little bit interesting. What happened is down here when we discovered that Int matched the first production for T we said that T was done. Okay T had succeeded had matched its input. And then when E ultimately returns and the whole parse fails because we havent consumed the input we dont have a way to back track and try another alternative for T. If we were going to succeed we would have to say oh well even though we found a production for T that matched part of the input. Since the overall parts fail that must not have been the right production to choose for T. Maybe we should try some other productions for T. And in fact if wed tried the second production for T T2. We would have matched Int times T and then we probably would of succeeded. We would have been able to manage int times int. Okay? And so the problem here is that even though there is backtracking within a production; while were trying to find a production that works for a given non-terminals. So while there is backtracking For a non-terminal during the time that were trying to find a production that works for that non-terminal but there is no backtracking once we have found a production that succeeds for non-terminals. So once a non-terminal commits and returns and says I have found a way to parse part of the input using one of my productions. Theres no way in this particular structure this particular algorithm to go back and revisit that decision and try a different production. All right? So the problem is that if a production for non-terminal x succeeds theres no way to backtrack to try a different production for x later. So once x once the function for x has returned. And were really committed to that production. Now that means that the particularly Recursive Descent Algorithm that I should in the last video is not completely general and Recursive Descent is a general technique. There are algorithms for Resursive Descent parsing that can parse any grammar. That can implement the full language of any grammar. And they have more sophisticated backtracking than what I showed in the algorithm that I presented last time. Now the reason for showing this particular algorithm is that its easy to implement by hand. So this is actually an algorithm or approach to Recursive Descent that while it has this limitation as you can see its very mechanical and very straightforward to design a parser for a given for a given grammar. And it will work for a rather large class class of grammar. So in particular itll work for any grammar where for any non-terminal at most one production can succeed. So if you know from the way that youve built your grammar that in any situation that the grammar can get into or the Recursive Descent Algorithm can get into during parsing that at most one production can succeed. Then it this this parsing is gradually will be sufficient because there will never be once you find a production that succeeds there will never be a need to go back and revisit that decision because it must be the case that none of the other productions could have succeeded. And it turns out that the example grammar that were working with in the last couple of videos could actually be written to work with this algorithm. All right. And we would have to left factor the grammar. Well actually theres more than one way to rewrite the grammar to work with this Recursive Decent Algorithm but one way to do it Is to left factor it. Im not going to say any more about left factoring in this video because thats going to be a topic of a video thats coming up shortly. Welcome back. In this video Im going to outline a general algorithm for recursive descent parsing. Before I dive into the details of the recursive descent parsing algorithm let me justify a couple of small things that were going to use throughout this video. Token is going to be a type and were gonna be writing codes and so token would be the type of all the tokens. And the particular tokens that well use in the example are things like int open-pare close-paren + and and so token is a type and these things are instances values of that type. And then were going to need a global variable called next that points to the next token in the input string. And if you recall from the previous video we used a big arrow to point into the input to indicate our current position. The global variable next is going to play the same role in our code. So lets begin. The first thing were going to do is define a number of Boolean functions and one function we have to define is one that matches the given token in the input. So how does this work? Well it takes this argument a token okay this is a type token again. And and then it just checks whether that matches whats currently pointed to in the input string so is t okay equal to the thing pointed by next and notice theres a side effect we increment the next pointer. And whats returned then is a Boolean. This is either true or false. So yes the token that we passed in matches the input or no it doesnt. And again just to stress this those at the next pointer is incremented regardless of whether the match succeeded or failed. Now another thing we need to check for a matchup is the int production of asset. This is a particular production of a particular nonterminal s. And well denote that by a function that returns a Boolean and is written as s sub n. So this is this is a function that only checks for the success of one production of s. And when that I wont write out the code for that now well see that in a minute. And then were gonna need another func tion that tries all the productions of s so this one is going to be called just s with no subscript no subscript and so with this one well succeed if any production of s can match the input alright. So were going to have two classes of functions for each nonterminal. One class that where theres one function per production and it checks it checks just whether that production can match the input and then one that combines all the productions for that particular nonterminal together and checks whether any of them can match the input. Okay thats the general plan. Now lets see how this works for some specific productions and well just use the same grammar that we used in the last video. The first production of that grammar is e goes to t and now we wanted to do is we want to write the functions that are needed to decide whether this production matches some input. And this one happens to be simplicity itself and its easy to see why. So were first of all were writing the function e1 this is the function that deals with the first production for e and succeeds returns true only if this production succeeds in matching some input. Well. How would this production match any input? Well it can only match some input if some production of t matches the input and we have name for that function thats the function t which tries all the different productions for t. So e1 succeeds returns true exactly when t succeeds returns true and thats all there is to this first production. For the second production we have a little more work to do. Now e will succeed if t some of the input so some production of t has to match a portion of the input and after that we have to find a + in the input following whatever t matched and if notice the use of the short circuiting double end here. So this is actually important where youre exploiting the semantics of do uble end and C and C++ which evaluates. The arguments to the double end in left to right orders. So first t will execute and notice that t has embedded within its side effects on the pointer into the input. So its incrementing the next pointer and incrementing exactly however far t makes it. So whatever t manages to match the next pointer will advance that far. When this function returns its left pointing to the next terminal that t did not match and that needs to be a plus. And the call of term will increment the next pointer again which is exactly where e should pick up and whatever e can match it will increment the next pointer just beyond that. So that the rest of the grammar outside of this particular call can match it And then notice that this particular function is called e2 because this is the function for the second production for e. Well we have one more thing to deal with for e and that is the function e itself. We need to write the function that will match any alternative for e and since its only these two productions it just has to match one of these two productions And that; this is where the backtracking is dealt with. Now the only bit of state that we have to worry about in the backtracking Is this next pointer so that needs to be restored if we ever have to undo our decisions. And so the way we accomplish that is we just have a local variable to this function called save that records the position of the next pointer before we do anything. So before we try to match any input we just remember where the next point started when this function was called. Okay? And now to do to to do the alternative matching we first try e1. And we see if it succeeds and if it if it doesnt succeed actually lets do the succeeds case first. If this succeeds if this returns true then The semantics of double or here it means we dont evaluate e2 so this will not be evaluated. The second component here will not be evaluated if e1 if e1 returns true. Itll short circuit cuz it knows that its going be tru e no matter what and itll just stop there. And notice that whatever side effects e1 has on the next pointer will be retained and will remember and when we return true the next pointer will be left pointing to the next piece of unconsumed input. Now lets consider what happens if e1 returns false. Well if e1 returns false well then the only way this or can be true is if the second component is true. And whats the first thing we do? The first thing we do is restore the next pointer. Okay before we try e2. And if each returns true then the whole thing returns true and and the e function succeeds. If the e function fails well they were out of alternatives for e and the failure is gonna be returned to the next higher level production in our derivation and it will have to backtrack and try another alternative. Now finally what about this particular statement next pointer in the save variable and then the first thing the very first thing we do if we copy it back over the next again. This is just for uniformity to make all the productions look the same but since this is the very first production we actually dont need this assignment statement if we dont want to have it. So lets turn our attention to the non-terminal t. There are three productions. The first one is the t goes to int. And thats a simple one to write. We just have to match the terminal int so the next thing in the input has to be an integer and if it is then t1 succeeds. T2 is slightly more complex. Thats the production int t t goes to int t so we have to match an int in the input followed by a followed by something that matches any production of t. The third production is t goes to (e). So what has to happen? We have to match an open-paren first and then. Something that matches one of the productions of e we call the function e there and then finally a close-paren. And then putting all three of these together in the function t that tries all three alternatives we just have exactly the same structure we had for e. So we saved the current input pointer and then we try the alternatives t1 t2 and t3 in order and each step we restore the input point before we try the next alternative. Start the parser up we have to initialize the next pointer to point to the first token in the input stream and we have to invoke the function that matches anything derivable from the start symbols. So in this case thats just the function e. And recursive descent parsers are really easy to implement by hand. In fact people often do implement them by hand and just following the discipline that I showed the previous slides. To wrap up this video lets work through a complete example. So heres our grammar and here is all the code for the recursive descent parser for this grammar and here is the input that well be looking at and were gonna just mark the next pointer pointing to the initial token of the input all right? And Ill also draw the Parse Tree that were constructing at the same time. So well begin by invoking the start symbol so were gonna be trying to derive something from e. And the first thing well do is well try the first production. So well try e1 and what does e1 do? E1 is going to try t. Its gonna try to derive something from t. So our possible Parse Tree looks like this. And so we invoke t and what is t going to do is were going to try all three productions for t in order and so I was gonna call t1 and well see that t1 is going to fail because its going try it an int so I wont put it in the parse tree since it isnt going to work but the int is not going to match the open-paren. So thats going to return false which will cause us to backtrack. It will reset The the input pointer okay? And to the beginning of the string and then itll try t2. And t2 is also going to ask well is the input pointer = an int? And recall that the term function here always increments the input pointer. So in fact this pointer is going to move over one one tok en but this is going to return false because int doesnt match open-paren. So well come back here. The input point will be restored back to the beginning of the string and then its gonna try the alternative t3. Now when we finally get the t3 something good is going to happen. First thing its going to do is going to ask is the first thing in input an open-paren And in fact it is. And so the input pointer will advance to point to the int. And then its going to try to match something derivable from e so now we have our first recursive call to e. Were back here at e and its going to try e1 first and then e2. And so it calls e1 and e1 will only match something if it can match t. Okay so this is were down here inside of e now and now were going to call t. And whats t going to do was going to try all three productions for t in order. The first one of which happens to be the single token int and that is going to match. Its going to call term int; t1 is calling term int so that matches the next token in the input stream. So were happy about that. The input pointer advances again. And now we return through all these levels of calls. T1 succeeds which means that t succeeds which means that e succeeds. Okay. And now were back here in the production for t3 and were going to ask well is the next thing that we see in the input a close-paren? And it did it is and so a close-paren well be recorded. And now t3 will succeed which means that t succeeds this t succeeds and finally well return to the root call e and that returns true which means that the Parse succeeded. That plus the fact that we are now at the end of the input there is no more input to consume and we have returned from the start symbol with true and so we have successfully parse the input string. In this video were gonna talk about the main difficulty with Recursive Descent Parsing a problem known as Left Recursion. Lets consider a very simple grammar that consist of only one production s goes to s followed by a. So the Recursive Descent Algorithm for this production is the following. So we just have a function called s1 for the first production of s. And its going to succeed if the function s succeeds and then after that succeeds we see a terminal a in the input stream. And then we have to write a function for the symbol s itself and since theres only one alternative theres only one production for s we dont need to worry about backtracking or anything. So as well succeed exactly when as one succeed. Theres only one possibility in this case and now I think you can see the problem whats going to happen. Well when we go to parse an input string were going to call the function s which is going to call the function s1. And then what the function as one gonna do well the very first thing its going to do is to call the function s again. And as a result the function s is going to go into an infinite loop and were never going to succeed in parsing any input. This will always go into an infinite loop. So The reason that this this grammar doesnt behave very well is because it is left recursive. And a left recursive grammar is any grammar that has a non-terminal where if you start with that non-terminal and you do some non-empty sequence of re-writes. Notice the plus there. You have to do more than one re-write. So if youre actually doing a sequence of replacements you get back to a situation where you have the same symbol still in the left most position. And you can see this is not going to be good for parsing. So in the case of this grammar up here what happens while we get s goes to sa it goes to saa goes to saaa. And so on and we can always get to a situation where we have a long string of as and an s on the left end of the string. And if we always have an s on the left end of the string we can never manage any input because the only way we manage input is if the first thing we generate is a terminal symbol. But if the first thing is always a non-terminal we will never make any progress. And it just doesnt work. I mean Recursive Descent does not work with Left-Recursive Grammars. Well this seems like a major problem with recursive to same parsing. It is a problem but as well see shortly its really not so major. So lets consider a left-recursive grammar that slightly more general form. So here we have two productions now for s s goes to s followed by something alpha or it goes to something else that doesnt mention s and lets call that Beta. And if you think about the language that this generates its gonna join all strings that start with a beta and then follow and followed by any number of alphas. And but it does it in a very particular way. So if I write out some a derivation here where I used a few where I used the first production a few times. You can see whats going on. So again s goes to s followed by alpha. And then s goes to s followed by alpha alpha and then s goes to s followed by alpha alpha alpha and if I repeat this I get. S followed by any number of alphas and then in one more step I can. Put in beta and I get beta followed by any number of alpha. So thats the proof that it generates that language. That language that begins with a beta and has some sequence of alphas but you can see that it does it right to left it produces the right under the string first and in fact the very last thing it produces if the first thing that appears in the input and thats why. It doesnt work with Recursive Descent Parsing because Recursive Descent Parsing wants to see the first part of the input first and then work left to right. And this grammar is built to produce the string right to left. And therein lies the idea that allow us to fix the problem so we can generate exactly the same language producing the strings from left to right instead of right to left and th e way we do that is to replace left-recursion by right-recursion. And this requires us to add one more symbol in this case to the grammar so instead of having s go to something involving s on the left well have s go to beta so the first thing notice in the very first position and then it goes to s prime and what does s prime do well s prime produce what you would expect a sequence of alphas and it could be the empty sequence. And if you write out some you know? Example derivation here well have s goes to beta s prime. Which goes to now using the rules for s prime goes to beta alpha s prime Goes to beta alpha. Alpha s prime goes to and after any number of sequent any number of rewrites we get beta followed by sub sequence of alphas followed by s prime. And then in one more step we use the Epsilon Rule here and we wind up with beta followed by some number of alphas. And so you can see it generates exactly the same string as the first grammar but it does so in a right-recursive way instead of a left-recursive way. So in general we may have many productions some of which are left-recursive and some of which are not. And the language produced by this particular form of grammar here is gonna be all the strings. They are derived from asst start with one of the betas. So one of the things here that doesnt involve s and it continues with zero or more instances of the alphas. And we can do exactly the same trick. This is just generalizing the idea that we had before where we only have one beta and one alpha to many betas and many alphas and so the general form of rewriting this left-recursive grammar in using right-recursion is given here. So here each of the betas appears as an alternative in the first position. We only need one additional symbol s prime and then the s prime rules is take care of generating any sequence of the alpha i. Now it turns out that that isnt the most general form of left recursion. There are even other ways to encode left recursion in a grammar and heres another way thats important. So we may have a grammar that where nothing is obviously left-recursive. So if you look here you see that the s doesnt even appear on the right hand side here. And if you look at this production here a doesnt appear anywhere on the right hand side so theres no whats called Immediate Left-Recursion in this grammar. But on the other hand there is left-recursion because s goes to a alpha and then a can go to s beta. And so there we have in in two steps produce another string with s at the left end and so this is still a Left-Recursive Grammar. We just delayed it by inserting other non-terminals at the left most position before we got back to s. So this left recursion can also be eliminated. In fact this can be eliminated automatically it doesnt even require human intervention. And if you look at any of the text pretty quickly in the Dragon Book youll find algorithms were doing that. In this video were going to continue our discussion of top-down parsing algorithms with another strategy called predictive parsing. So predictive parsing is a lot like recursive descent. Its still a top-down parser. But the parser is able to predict which production to use. And its never wrong. [inaudible] parser is always able to guess correctly which production will yield to will lead to a successful parse if any production. Well it lead to a successful parse. And it does have some two ways; first of all it looks at the next few tokens so it uses look-ahead to try to figure out which production should be used. So based on whats coming up in the input string but also it restricts the grammars. So this this is only works for a restricted form of grammars. And theres the advantage is that theres no back tracking involved and so the parser is completely deterministic if you were to try alternatives. The predictive parsers accept what are called the LLK grammars. And this is a really cryptic name and so let me explain it. The first L stands for left-to-right scan. So that means were starting at the left end of the input and reading left to right. And in fact thats what we always do so all the techniques that we looked at look at will have an L in the first position. The second L stands for a leftmost derivation. So we are constructing a leftmost derivation. That means were always working on the leftmost non-terminal in the parse tree. And K here stands for K tokens of look ahead. And in practice while the theory is developed for arbitrary k in practice k is always equal to one. And so in fact well only discuss the ks k equals to one in these videos. To review in recursive descent parsing in each step there may be many choices of production to use and so we need to use backtracking to undo bad choices. In an LL-1 parser in every step theres only going to be one choice of productions of possible production to use. And and what does that mean? Well it means that if I have an input string if I have a configuration of the parser where I have some terminal symbols omega and a non terminal a you know possibly now followed by some other stuff there could be terminals and nonterminals but again a here is the leftmost nonterminal. And the next input. Is a token T Well then there is exactly one production A goes to alpha on input T. Okay theres only one possible production that we can use. And any other production is guaranteed to be incorrect. Now it can be that that even A goes to Alpha wont succeed. It could be that we will be in a situation where theres no production we could use. But in [inaudible] parser there will always be at most one that we could use. So in this case we would chose to rewrite the string to Omega Alpha Beta. Lets take a look at our favorite grammar the one weve been using for the last couple of videos. We can see an issue here with using this grammar for a predictive parser. Take a look at the first two productions for T. They both begin with Ns. And so if I tell you that the next terminal in the input stream as were parsing along is an integer that doesnt really help you in trying to distinguish between these two productions in deciding deciding which one to use. So in fact with only one token of look ahead I cant choose between these two productions. And that is not the only problem actually so we have a problem with T but the same problem exist with E. We can see that here both production for E begin with the non-terminal T and it is really clear what were to make of that because a T against a non-terminal terminal so how we even do the prediction but the fact that they begin with the same thing suggest that its not going to be easy for us to predict which production to use based of only a single token of look ahead. So what we need to do here is we need to change the grammar. This grammar is actually unacceptable for predictive parsing or at least for LL1 parsing. And we need to do something thats called left factoring the grammar. So the idea behind left factoring is to eliminate the common prefixes of multiple productions for one non terminal. So thats a mouthful. Lets do an example. Lets begin with the productions for E. And we can see again that E that both productions for E begin with the same the same prefix. What were going to do is just factor out that common prefix into a single production. So were going to have one production where E goes to T. And then were going to have multiple suffixes. So lets introduce a new non terminal X that will handle the rest. So here we have E goes to TX. So it says that everything that E produces begins with T and thats consistent with these two productions. And now we have to write another production for X that handles the rest. And what would that be? Well one possibility is if were in this production we need to have a Plus E and then in this production theres nothing. So thats easy to handle right. One possibility for X as it goes to Plus E and the other possibility as it goes to Epsilon. And now you can see the general idea. We factor other common prefix we have one production that deals with the prefix and then we write and then we introduce a non terminal or the different suffixes. And then we just have multiple productions one for each possible suffix. And you can see what this is going to do. This is effectively going to delay the decision about which production were using. So instead of having to decide immediately which production were going to use for E. Here in this grammar we wait until weve already seen the T whatever is derived from the T. And then we have to decide whether the rest of the production is a plus E or the empty string. Lets do the other set of productions. So we have tea goes to and now the common prefix is int that we want to eliminate So were going to have just one production that begins with int and then well have a new a non-terminal to stand for the various possible suffixes. And now we also have another production that doesnt h ave anything to do with int and so well just leave that one alone that production just stays here. Because it already begins with something different we wont have any trouble predicting between these two possible productions these two possible productions. And now we have to write. The productions for Y And again we just take the suffixes of the productions that we left factored and write them down as alternatives. So one is empty and the other one is times T. So we wind up with times T or epsilon. In the next few videos were gonna talk about how to construct LL1 parsing tables. And in this particular video were gonna begin by looking at how be build something called first sets. Before we get into the main topic of this video which is something called First Sets we need to say a little bit about how were going to construct parsing tables or what the conditions are for constructing LL1 parsing tables. And so what were interested in knowing were building a were building a parsing table. And we want to know for a given non terminal A. Kay this is the leftmost non terminal. And a given input symbol the next input symbol T [cough]. We want to know what con- under what conditions we will make the move A goes to alpha. Well replace A the non terminal by the right hand side alpha. Alright and that means that the entry in th the AT entry in the table would be Alpha and there are two situations in which we would like to do this. Alright? So the first one is if alpha can derive T in the first position That means that beginning with alpha there is some derivation some sequence of moves could be zero or more moves that will result in a T appearing in the first position of the string thats derived. And if there is such a derivation then using the move A goes to alpha at this point when T is the next input symbol would be a good idea. Because then we would be able to match the T. Eventually alpha could generate the T and then wed be able to match the T and then continue with our parsing of the rest of the input. Alright so in this situation when alpha can generate a T in the first position we say that T is an element of the first of alpha. T is one of the things there may be more things. But T is at least one of the things that alpha can produce in the very first position. One of the terminals I should say that alpha can produce in the very first position. Now theres another situation a slightly more complicated situation in which we might want to make the move or we wou ld want to make the move That if we see A as the leftmost non terminal and T as the next input that wed like to replace A by A goes to alpha. Alright? And the situation here that were going to consider is what if alpha cannot derive T? So alpha cannot in any sequence of moves derive T. So in fact what does that mean? That means T is not. Gonna be in the first of alpha okay? So and our next input symbol is T. Were still looking at the situation where we have A as the leftmost non terminal and T as the next input symbol. Now This doesnt sound very promising. Because we have an input symbol T that we want to match And the leftmost non terminal that weve got up next that we have to do a derivation for cant generate the T. And so but it turns out that this that its not hopeless. That we actually may still be able to parse the string even in that situation provided that alpha can go to epsilon. So if alpha can derive epsilon if alpha can go away completely and we can basically erase the alpha then it could be that some other part of the grammar can come in and match the T. Alright and so in what situation would that be? Well here are the conditions. So if A goes to Alpha as a production and alpha can go to epsilon via zero or more moves. Alright so alpha can eventually be completely wiped out. Alright and. If T can come immediately after A in the grammar so there has to be a derivation for this to make sense there should be a derivation where we are using the A okay? With the A as an important piece of the derivation you know from the start symbol. And what comes immediately after the A is the next input symbol that we are expecting. So in this situation if we could get rid of the A Then by having a go at the epsilon then well still be on track cuz potentially some other piece of grammar could come in and match the T. Alright. So in that case we would say what what do we have to test for? What under what conditions can we do it? Well we want to be able to do this if T can come after A in the grammar and we say that T is in the follow of A. T is one of the things that can come after A in the grammar. Now this is an important point and a place where people sometimes get confused and so I want to to emphasize this notice that. We are not talking about A deriving T. A does not produce T. T appears in a derivation After A okay? So the A and the T here it it doesnt have anything to do with what A produces. This has to do with where A can appear in derivations alright? So if the T can come after the A in a derivation then we say the T is in the follow of A. Right. So in this video were gonna focus on only this first part the first sets. In the next video well look at the follow sets and then the video after that well talk about how to put it all together to build this parcing table. [sound] All right lets focus now on our main topic for this video the computation of first sets. So here first of all we have to have a definition of what a first set is. And so were going to say for an arbitrary string. This is actually x here is a string. Could be a ter- could be a single terminal it could be a single non-terminal or it could be a string of grammar symbol. All right and if that If that X can derive T in the first position through some sequence of moves then we say that T T is a terminal here is in the first of X okay? So all the possible terminals that can be derived in the first position will be in the first of X Now For technical reasons that will become clear in a minute we also need to keep track of whether x can produce epsilon. Now so even though epsilon is not a terminal symbol if x can go to epsilon be a zero or more steps then well say that epsilon is in the first of x and this turns out to be needed. We need to keep track of whether x whether things can produce epsilon in order to compute all the terminals that are in the first set of a given grammar symbol. Alright so now heres a sketch of the algorithm. So first of all for any terminal symb ol the only thing the terminals can produce are themselves. So every terminal symbol in here I should just say T is the terminal. So for every terminal symbol it is in its first set just consists of a the site containing only that terminal. All right so now lets consider a non terminal X okay so here X is a non terminal and what it would be in the conditions when epsilon is in the first of X well if theres a epsilon production if X goes immediately to epsilon then obviously X can produce epsilon epsilon should be in the first of X But also if X can produce any other right hand side Alright Where everything on the right hand side can go to epsilon. Well then the whole right hand side can go to epsilon. So in that case also epsilon is in the first of X. I noticed that this will only happen if this it can only it can only potentially happen if all the EIS here are non-terminal symbols themselves. Obviously if theres any terminal symbols on the right hand side then that production can never go completely to the empty string. Okay. It will always produce at least that that terminal But if every non-terminal. On the right-hand side can produce epsilon. Meaning epsilons in the first of all those non-terminals. And there are no terminals on the right hand side. Then Then epsilon will be in the first of X. Alright theres one other situation and heres where we make use of the fact that we are keeping track of where epsilon can be produced alright. So lets say that we have a production like this okay and lets say the first N symbols A1 through AN here can all go to epsilon. So this can all disappear and can be replaced by the empty string. What does that mean so if we have derivation like this? Okay were to some number of moves it goes to Alpha well that means that X can through a bunch of moves here to derive Alpha itself okay. So X will go to Alpha by wiping out all of the AIs and I forgot to put the alpha here on the end there should be an Alpha after As have been there. Okay? And wh at does this mean? Well this means that anything that is in the first of alpha is going to also be in the first of X. All right? So if any prefix of the right-hand side can disappear then the remaining suffix the alpha it doesnt matter what the alpha is is left. Then the first of alpha will be a subset of the non-terminal on the right left hand side of X in this case. All right? Okay? Alright. So that is a definition of a first sets and how you compute them. Okay. And we have to we have to compute them for the terminals and for the non-terminals alright? Thats what these; these second two rolls here cover the non-terminals. I just noticed as I mentioned here at the beginning that this is well defined for any other Grammar sequence as well. I mean excuse me any other string in the grammar as well. It doesnt if I if I know how to compute it for terminals I know how to compute it for non terminals. Then I can compute it for arbitrary strings in the grammar as well. Lets analyze do an example. Lets take a look at this grammar and lets see if we can compute the first sets. Lets start with the easy stuff. Lets do the terminal symbols. Alright So for the terminals its really you know extremely straightforward. The first of plus is plus. The first of times Is just times every terminal is in a has its first set the first set of every terminal is just the second term in that terminal and so on for the others and this is not worth writing out. So itll be the first of open paren will just be open paren the first of close paren will be just close paren and I think that is all. Now we have to do ants as well okay? Alright so these are the first sets for the terminal symbols. And now lets look at something more interesting; lets talk about the first of the non terminal symbols. So What about the first of E? Well if we look at the production for E lets remember our rules. So we know that anything thats in the first of T will also be in the first of E. So the first of T Is a subset of th e first of E. Okay so in order to know what the first of E is we have to know what the first of T is. At least to know part of the first of E we have to know the first of T. So lets move on then to first computing. The first of T Lets lets try to get that set. Now the first of T is actually pretty easy because if we look at the productions for T we can see that they produce terminals in the first position. All right? So the only possibility in the the only possibilities in the first of T are open per en and int. And since there are only two productions for T and both of them have a terminal in the very first position theres no other terminal symbols that could be produced in the first position by T. So we can just read off the first of T directly from the grammar. And its the open paren in int. Okay? Now lets return to thinking about the first of E. So remember there was another case that we need to keep track of. Or sorry that we have to consider. So it could be or clearly everything in the first of T is in the first of E and weve already noted that down. But if T can go to epsilon then things that are in the first of X Could also be in the first of E. And now weve computed the first of T and we see that epsilon is not in there. The first of T always generates at least one terminal symbol and so therell never be a situation in which X can contribute to the first of E because T is always guaranteed to generate at least one terminal. So in fact this subset that we wrote up here is not a subset at all its an inequality. The first of T and the first of E are equal. So the first of E is also open per rand and Nth. All right? So now lets take a look at the first of X. Okay? So the first effects well clearly pluses in the first of X because one production per X plus immediately in the first positions so we must add plus to the first of X. And then X has an epsilon production so it can also go to epsilon so that means epsilon is also in the first of X And what about the first of Y. Well the fir st of Y its a similar structure to the productions request we see we have one production here in the [inaudible] of the terminal in the first position and thats times. So the first of y has times in it. And then y also has an epsilon production. Y can go directly to epsilon so epsilon is also in the first of y. Alright? And thats actually it for this grammar. These are the complete first sets for all of the symbols of the grammar. The terminals just have themselves in their first sets and then the non-terminals we computed have these sets. So that concludes our discussion of first sets and in the next video were going to take a look at computing follow sets. In this video were going to continue our discussion of the construction of the definition of follow of x and recall that the follow set for a given symbol in the grammar isnt really about what that symbol can generate really doesnt depend necessarily at all on what the symbol can generate. It depends on where that symbol can appear where that symbol is used in the grammar. And we say that t is in the follow of x if there is some place in the grammar some derivation where that terminal t can appear immediately after the symbol x. Okay so for all such. E. They make up the follow set of x. And heres some intuition about how we would compute follow sets. Lets say we have a situation where X goes to two symbols A B right? And then anything that B can produce in the first position will clearly be in the follow of A. So if we have X goes to AB. And then through some more steps we can get something like this A goes B goes to T beta then we have a situation where the T comes immediately after the A and so clearly something that was in the first of B is in the follow of A. So so the basic rule is that you have two symbols that are adjunct somewhere the first of the second one is in the follow of the first one. Hm now Another interesting effect here is that if we have a symbol at the end of a production. Lets take a look at the B here for a moment okay? And a claim here that anything thats in the follow of the left hand side is gonna be in the follow of B. In this case that the follow of X is a subset of the follow of B. And lets take a look at that. Lets say that we have a situation where we have a derivation from the start symbol okay? We wind up with X followed by T. It can it can be other stuff around the [inaudible] lets ignore that for the moment lets just focus on the XT. Then we can use this production X goes to AB and in one step we can get to ABT. And then we see T was in the follow of X a nd also T is in the follow of B as a result Okay. So anything in the follow of X would also be in the follow of B. And we can generalize this observation about what occurs at the end of a production. So anything that occurs at the end of the production it its follow set will include the follow set of the symbol on the left hand side of the production. Well what is the end of the production? If if B can go to epsilon if B can disappear then A will appear at the end of the production. Okay so if B can go to epsilon then it will also happen that the follow of X would be in the follow of A. And following up here in our example so we. Or up here we start with the start symbol. We got to XT. In one step we got to ABT and so T was in the follow of B. But now B can go into epsilon and so we can also get to AT and therefore T is also in the follow of A. And finally theres one special case. Remember that we have our special symbol marking the end of the input and what can that follow? Well the end of the input is in the follow of the start symbol alright? And this is just a way again of keeping track of what were going to do when we run out of input. And well see how thats used when we built the parsing tables but we always add as an initial condition that dollar sign is in the follow of the start symbol. So now lets take a look at and sketch of the algorithm for computing follows. So thats as we just said the dollar sign is in the follow of the start symbol. And now we take a look at each production. Okay A goes to alpha X beta were were focusing here on the X. Okay if we look at every production and we look at every symbol on the right hand side of that production. And the first of beta okay the thing that can follow x in this in this production the first of that will be in the follow of x and also we just subtract out epsilon if it was in the first sub-beta. Were not interested anymore in the epsilons for the purposes of follow sites epsilon never appears in follow sites so follow sites are always just sets of terminals. And now the second part of the algorithm is it if we have some suffix of a production beta that can go to Epsilon so Epsilon is in the first beta. Alright this suffix of the production can completely disappear then as we saw on the previous slide the follow of left hand side symbol will be in the follow of X. And thats it in terms of the rules for computing follow sets. So now lets work through an example. So heres our grammar again. And were going to compute the follow sets for each of the symbols of the grammar. So lets begin with the with the start symbol. Well start with the follow of E. And by definition we know that dollar is in the follow of E. So we get that one easily. And now the question is what else could be in the follow of E Alright? So in order to figure that out we have to look at where E is used in the grammar Alright? So remember always at follow sets are about where the symbol is used. Not what it produces. Alright? So here. Is a place where E is used and we can see that it is merely followed by a terminal symbol so certainly close paren is in the follow of E right? And theres one other place where E is used and thats over here. And it appears that the right end of the production and so then we know that anything thats in the follow of X is also gonna be in the follow of E. And thats a constraint and so well right that down over here coz this is just a property of the relationship. That the follow sets will have when were done computing them. This doesnt immediately tell us anything new thats in the follow of E. But we know that as we go along and we learn about things that are in the follow of X well have to add them to the follow of E. And let me just divide up. The slide here so we will put our properties that we know about relationships between fallocates over on the left hand side and well put the actual fallocates over here on the right side. So now to make thats the only two places those are the only two pl aces where E is used in the grammar and to make further progress we need to know something about the follow of X. Okay if we want to make further progress on the follow of E then we need to figure out whats in the follow of X. So lets focus on that for a minute. So wheres X used in the grammar well its used in only one place and thats here Okay? Where it appears at the right end of a production. And what and so therefore the symbol on the left hand side will be a subset of the follow set of X. So were gonna know that the follow of E is a subset of the follow of X. Alright? And what does that mean? Well so follow of X is a subset of follow of E. And follow of E is a subset of the follow of X. So that really means that these two sets are equal. The follow of X and the follow of E whatever they wind up being are gonna have to be the same set. And now weve looked at all the places where E is used in the grammar. Weve looked at all the places where X is used in the grammar. We cant learn anything more about what is in the sets the follow sets of E and X. Were not forced to add anything else to either set and so were done. And so we can close off this set. And we know the follow of E consists of dollar sign and closed paren. And we also know that X. Has the same set the same follow set. Alright so now lets move on to the follow of T All right. So whats going to be in the follow of T? Well we again we have to look at where T is used in the grammar. So T is used in two places. The first one is here in the first production. And so whats going to be in the follow of T? Well it could be anything that is in the first of X. Okay? Cuz X comes immediately after T. And if you recall from the previous video there were only two things in the first of X. One was plus. So this plus is definitely in the follow of T and lets just review. [cough] Excuse me. How that can happen so we can go from E. To TX okay? Im using the first production. And now we see the X comes after the T. And then in one more step we can go to T plus E. And now we have a derivation where plus follows T. And thats how we thats thats Y pluses in the follow of T. Alright? So the other thing that was in the first of X was epsilon because theres an epsilon production for X over here. But remember that were not interested in we dont include epsilon in follow sets. And so X doesnt contribute anything else to the to the follow of T. But since X can go to epsilon remember what that means. That means that over here looking back at this first use of T again in the grammar this X can disappear. Right and that means that anything it is in the follow of E is also in the follow of T. Now we already know the follow of E. So we can just add those things in. Okay? And let me write that down over here just so that we dont forget it. So to follow. Of of E is a subset of the follow of T. We wont really need this fact again but useful to write it down perhaps. Alright and now we are done with this use of x. Weve included everything implied by this production that we can in the follow of T and so now have to look at the other place where T is used and thats over here. Okay and so here were going to see that T is in the right end of a production so anything that is in the follow Y can also be in the follow of T alright? So the follow of Y. Is going to be a subset of the follow of T alright. So now we can go off and work on the follow of Y. We have to in order to figure out what the follow of T is going to be were gonna have to know the follow of Y. So where is Y used in the grammar? Well there is only one place and thats over here. And also Y appears in the right hand of production which means that the left hand side symbol its follow set will be included in the follow of Y. &gt;&gt; And so the follow of t will be a subset of the follow of y. All right? And now again we have two follow sets that are subsets of each other. Follow of y is a subset of follow of t and follow of t is a subset of follow of y. And so these two sets we know are going to have to be equal. Okay? So we can write down here. At the follow of Y includes plus dollar enclose parette. Just like the follow of T. And now were done. Weve weve follow of T and follow of Y. Weve followed all the implications of how the follow of T gets things into what can be included in the follow of T. Weve worked out all the places where Y is used in the grammar and added all the things that we can based on its context. And theres nothing more that were forced to add either said. So we can go ahead and close these sets off. Theyre finished Alright. So now Weve done the follow of E X T and Y. So weve taken care of all the terminal symbols. But Im sorry All the non-terminal symbols. But we still need to compute the follow sets for the terminal symbols. And unlike the case with first sets the follow sets for terminal symbols can actually be interesting. So lets take a look at the follow of open paren. Okay what can follow an open paren in a derivation? Well open paren is only used in one places. Its here. Okay. And so what can follow in open parens is whatever is in the first of E. And remember that the first of E was the same as the first of T because T always produces something in the first position. And the first of T was what? It was open paren. An int kay? And if you think about this for a minute this makes complete sense. What can come after an at any valid at any valid string in this grammar while its going to be another nested parenthize expression or is it going to be an integer. In particular you couldnt have a times or a plus immediately after an and you couldnt absolutely have the end of the input you couldnt have the input stop after an and have a valid string. So now lets take a look at the follow of. Okay? So whats in that set? Again we look at where the symbol is used. Its only used here in this one production. So and because it appears at the right end of the production we know that whatever is in the follow of T is going to be in the follow of ) all right. And so what was in the follow of T [cough] that was +$) Okay. So now lets move on and take a look at the operators. Lets look at the follow of plus. So wheres plus used? Well its only used here. So whatevers in the first of E is going to be in the follow of plus. And we already know what the first of E was. That was an open cannot ever disappear completely because T always produces at least one terminal. Therefore only the things that are in the first of year in the follow of plus. Because E cant go to Epsilon we only have to include the things that are in the first of E in the follow of plus. Again if you think about it for a minute this makes complete sense. What could come after a plus? Well it could be an integer the second argument to an addition or it could be the beginning of another nested expression. And it couldnt be a times. It certainly couldnt be the end of the input cuz you always have to have an argument after the plus. And &gt;&gt; And I think thats it. I think thats all the other possibilities. &gt;&gt; Okay Alright. Now lets take a look at the follow of times. What can come after a times. Where is times used its used here. So things that are in the first of t. Are gonna be in the follow of times again alright? [inaudible] we already know what that is. Thats the same as the first of E. Thats open paren and ints. And again this makes complete sense. What can come after a times? Its either the beginning of another [inaudible] expression or an integer. Its certainly not a plus or the end of the input okay? And again T cannot go to epsilon and so thats the only thing. Those are the only things that can be in the follow of times. And now we just have one more symbol to go. We have to look at the follow of an integer of an int. Okay where is that used in the grammar? Well its right here Alright. So the whats gonna be in the follow [inaudible] what s going to include everything in the first of Y. Okay. Whats in the first of Y well times was in the first of Y and epsilon was in the first of Y but remember we dont include epsilons in follow sets. So Y contributes times to the follow of int. But now because Y could go to the epsilon and epsilon could [inaudible] that means that this int could wind up being at the right end of this production. Okay it could the Y could disappear and then whatever could follow the T could also follow the int. Right so we have to include the things in the follow of T. In the follow of it and what was in the follow of T where that was a plus. It was a dollar. And it was a close paren okay? And what does that tell us? Well it tells us okay for most anything to follow an int but as an open paren cannot follow an int. So you cant have another nesit expression with a begin right after an int without an intervening operator Alright? And that completes the computation of the follow sets for this example. In this video were gonna put together what weve learned about first and follow sets to construct LL1 parsing tables. Our goal is to construct a parsing table T for a context free grammar G. And this is done by production. So were gonna do this one production at at time. And were going to in turn consider each production A goes to alpha in the grammar G. And so the first case Is if we are trying to figure out whether we can use A goes to alpha and T happens to be in the first of alpha. Alright so if we know that some terminal T is in the first of the right hand side then. If we were in a situation where A was the leftmost non terminal and T was the next token of input then then expanding by A goes to alpha would be a good move because the alpha could potentially through more productions match the T. And so well add to the parsing table at the A T entry The right hand side alpha. Alright the other situation that were interested in is what if we need to get rid of the A okay? So if the A cannot possibly match the T alright? So lets say the T is not in the first of alpha or we have some other situation where we want to erase the A. Well then it would be okay to use production A goes to alpha provided that. Alpha can actually go to Epsilon so alpha can go away completely Alright? So we can eliminate all trace of the A. And T follows A in the grammar. So T is able to come after excuse me T is able to come after A in some derivation. So if T is in the follow of A and the right hand side of the [inaudible] code epsilon then we add the move that when A is the leftmost non terminal and T is the next input we can expand A by A goes to alpha. And finally a special case for dollar because dollars technically not a terminal symbol. If were at the end of the input okay so we have some stuff left on the stack particularly we have if nonterminal a is still our leftmost nonterminal but weve run out of input well then our only hope is to get rid of the a completely. And so we want to pick a production for a that can go to epsilon so we look for a production a goes to alpha where epsilon is in the first of alpha and dollar can follow a in a derivation. So that is the procedure or those are the rules for constructing a parsing table. And now lets work through an example. So heres our favorite grammar that weve been looking at for the last few videos. And now lets take a look at what the parsing table will look like alright? And the parsing table will consist of columns that are labeled by the terminal symbols of the grammar. All right so here well have open paren closed paren plus times and inch. And then the rose will be labeled by the terminal symbol so well have E T X and Y. All right and now were just going to take each production and apply our rules and see what entries in the table we create. All right so when would we use E goes to TX? Alright well so the first thing to observe about this production is that it cannot produce epsilon on the right hand side. So TX always produces at least one terminal. And so the second case where were interested in whether the production can go to zero as it could go to epsilon excuse me is not going to apply. All right So we just have to consider what it can generate in the first position. So the only things that this of T which are open paren and int. So there are two situations in which we will use the production E goes to TX that is if E is the leftmost nonterminal and the next input is an open paren. Okay? And the other one is that if the next input is a when are we going to use T goes to open paren E closed paren. Well if T is the leftmost non-terminal alright? Thats this one on the left hand side and an open paren is the next symbol in the input kay. Thats the only thing in the first of this right hand side. Then it would be a good move to expa nd by open paren E closed paren. So theres only one situation. Where we use that production. Alright? And for the other production the other T production were going to use that when T is the leftmost nonterminal and theres an INT in the input. So over here well have [inaudible]. And I forgot one column over here for dollar. So well stick dollar in there at the very end. Okay? So now weve covered these first three productions. Lets take a look at this production. So when would we use X goes to plus E. Well clearly the only thing on the first of the right hand side is plus and the terminal symbol on the right hand side is X so the X plus entry we would want to expand by X goes to plus E. And similarly for Y the production the first production involving Y when Y is the terminal non terminal were trying to expand and theres a times in the input we would use the production Y goes to times T. Okay? And now we just have the two epsilon productions left. And these are the only productions actually that can go to epsilon. And so when would we use when would we use X goes to epsilon or Y goes to epsilon. Alright so recall that we need to know. What is in the follow of X in order to know when to use X goes to epsilon. And we computed that in the last lecture. But lets just write it down again here. Okay. And so what was in the follow of x well we had to look at where x was used in the grammar x was used there. I it appears at the right hand side of the production. So it would be things that were in the follow of e. What was in the follow of e well e is the start symbol so [inaudible] is in the follow of e close paren is in the follow of e. Alright? And then what was in the follow of y. Thats the other one where well need to know the follow set. Again we have to look at where Y is used so Y is used there. That means everything thats in the follow of T is in the follow of Y. The follow of y will therefore include the first of x because x can come after t. So plus will be in the follow of y. Alright but then x can go to epsilon and so everything is in the follow of E will be in the follow of t and therefore also in the follow of y. So the other two things in the follow of y were the dollar sign and close paren. Alright and so this is saying okay is that if we are in a situation. Where we have an X. Okay? Lets just focus on the X goes to epsilon production for a moment. Lets say that we have X on the stack okay on top of the stack and $ is our next input. Well what can we do? At the end of the input we have to get rid of the X so obviously we want to use the X goes to epsilon move okay so that makes sense. And the other situation that follows it tell us to use X goes to epsilon as if there is a) on the stack because the X cannot generate a) by itself. But hopefully some other symbol thats on the stack will be able to generate once we get rid of the x okay so we also use x goes to epsilon In this situation. And then [inaudible] for follow of Y or for Y goes to epsilon that production. There are three things in the follow three terminals in the follow of Y. And we should use Y goes to epsilon if they are the next thing in the input. So so if we see a plus and were trying to expand a Y well use Y goes to epsilon. If we see a closed paren and we see and were trying to expand a Y well use Y goes to epsilon. And finally if were completely out of input and we still have a Y left over well use Y goes to epsilon. And that is the complete parsing table all right? And now you can see. How this will work in every situation Okay. For our leftmost on terminal and for every possible input or lack of input we have a production that we can use. And now there are a lot of blank entries in this table. And what do those correspond to? Lets say that we were trying to expand x. And the next input symbol was a open paren. Well theres no entry here. Okay so thats an error. Thats a parsing error. So whenever you encounter a blank entry in a table you try to view the blank entry when youre parsing thats when you [inaudible] a parsing error because what this tells us the fact that theres a blank entry it tells us that there is no valid move. There is no way that we could parse that string. And we discovered that at the point where we tried to access an error or blank entry in the table. So now lets consider what happens when we try to build an LL1 parsing table for a grammar that is not LL1. And lets take a look at the simple left recursive grammar that we have looked at before. So S goes to SA is one production and S goes to B is the other production. And to build a parsing table for this well need to know the first and follow sets. So lets take a look at the first of S. Alright. So what can S produce in the first position? Well it can clearly produce a B. And theres no epsilon. Theres no possibility of generating epsilon from S. As a matter of fact thats the only thing thats going to be in the first of S. And what about the follow of S well what can follow an S? Well thats the start symbol so clearly dollar isnt the follow of S. And then the sub-terminal the terminal A appears right after S in the first production so A is also in the follow of S. And now were ready to build our table. And its going to be a very small table because we only have one non-terminal. And then we have two terminals A and B. And we have the end of input symbol. So its just three entries potentially in this table Alright? And so now lets take each production and see where we should put it. So lets just take a look at the second production first. Cuz thats [inaudible] for no particular reason. So if S goes to B when should we use that? Well clearly if we see a B in the input this would be a good one to use. Cuz cause the because that the first of the right hand side includes B alright? So so S goes to B. Would be used if we see a B in the input. And now what about S goes to SA? Well here again this cant generate epsilons so were only interested in what it can produc e in the first position. And once again the first of S is just B. And so we also have the move in the SB entry we would have the move as goes to SA. And now we see the problem right? Here we have an entry that has multiple moves. This is a multiply defined entry. Okay and what does that mean? Well that means if we see an s in if we have an s and we want to expand okay were trying to if s is our leftmost non terminal so its at the top of the stack and b is our next input symbol alright this table doesnt tell us exactly what move to make. Its not deterministic. It says theres two possible moves. That we can make. And so this is how you know a grammar is not have more than one entry. More than one possible move in some position in the table some entry excuse me. I used the word entry incorrectly. So if you wind up let me say that again. If you wind if you build the table and some entry in the table has more than one move in it then theres not an unique move For every situation for the parser. And that grammar is not [laugh](1) So we just said if any entry is multiply defined in the parsing table then the grammar is not LO1. And in fact this is the definition of an LO1 grammar so the only way to be sure that the grammar is LO1 or the mechanical way to check that the grammar is LO1 is to build the LO1 parsing table and see if all the entries in the table is unique. Now that we do know however that there are certain classes of grammars that are guaranteed not to be L1 not to be L1. And what are some of those? Well any grammar that is not left factored. Will not be lo1 okay. Any grammar that is left recursive will not be lo1. Okay any grammar that is ambiguous Is also guaranteed to not be L1. But this is not an exhaustive list. Other grammars are not L1 too. So in particular If the grammar required more than one token of look ahead it would not be all one. But even that isnt a complete list. So e ven grammar is beyond that that are not going to be all one. So what this amounts to these three things here amount to quick checks that you can do. To test whether a grammar is guaranteed not to be L1. But if just because a grammar is left factored and it is not left recursive and is ambig- unambiguous that doesnt guarantee that its L1. And the only way to know for sure is to construct the parsing table and see if all of the entries in it are unique. And unfortunately it turns out. That most programming languages theyre context free grammar. So the grammars that describe most programming languages are not LL1. And the L1 grammars are too weak to actually capture all of the interesting and important constructs in commonly using programming languages. And there are more powerful. Formalism for describing grammars and or practical grammars and were going to be looking at those in future videos. It turns out that they build on everything that weve learned here over the last few videos for Elmer grammar so none of that will be wasted but they assembled those ideas in a more sophisticated way to build more powerful parts. This is the first of what will be a considerable sequence of videos on bottom up parsing. The first thing to know is that bottom up parsing is more general than deterministic top down parsing. So recall we talked about recursive descent which is a completely general parsing algorithm but requires backtracking. And now were focused on deterministic techniques and we talked about L-L one or predictive parsing last time. And now were gonna switch shift gears and talk about bottom up parsing. And it turns out though even the bottom up parsing is more general its just as efficient and it uses all of the ideas that we learned in top down parsing. And in fact bottom up. Parsing is the preferred method thats using most of the parser generator tools. So one good thing about bottom up parcers is they dont need left factored grammars so we can revert to the natural grammar for our example and natural here is in quotes because we still have to encode the precedence of plus and times so bottom up parcers arent going to deal with ambiguous grammars. And lets just as an example consider how a bottom up parcer would work on the following typical input string. So the first thing to know about bottom up parsing is that it reduces what we call reduces a string into the start symbol by inverting productions by running productions backwards. So heres an example. On the left hand side is the sequence of states of the string. On the right hand side are the productions that were used And the thing to observe lets just look at the very first step. Is that we began with the entire string. We began with the the the string of terminals. And we picked some of those terminals. In this case just one this particular Int right here. And we ran a production backwards. We replaced the Int here by the left side of the production. We began with we matched the right side of the production Int and we replaced it by the left side. So Int went backwards here to T. And then in the next step we took Int times T this substr ing of The string that were working on And we replace it by the left-hand side of this production. N times T was replaced by T and so on. At each step here were matching some portion of the string. And Im underlining the portion thats being replaced at each step. And were running and that matches the right hand side of sum production. And then were replacing that substring by the left hand side. And finally this entire string here is replaced by E. And we wind up at the start symbol. So we began with an input string. This is our input string up here. Alright put string of tokens and we end with the start symbol down here And if you read the moves in this direction If you start at the bottom and read towards the top. Well these are just productions. And in fact this whole thing is a derivation. This is just a normal derivation going from bottom to top. But in this direction when we run it backwards beginning with the string towards the start symbol we call these reductions. And I havent told you exactly how we decided what reductions to do and you might wonder well how I knew to do this particular sequence of reductions. Well heres another interesting property of bottom-up parsing. So if you read the productions backwards they trace a rightmost derivation so if we begin here with e so were gonna--so remember the parser is actually going in this direction so this is the direction of parsing here. But now were gonna look at the steps the parser took in reverse and were going to see that it was in fact a rightmost derivation. So here E went to T+E. Well E was the only non terminal. But then E here is the one thats expanded its the rightmost non terminal. And then this T is expanded its also the rightmost non terminal to get int. And now this T is the rightmost tom- non terminal. Its expanded to get Int times T. And then this is the only end right most non terminal and so we wind up with the whole input string Int times Int plus int. And this leads us to The first important f act about bottom up parsing which is that a bottom up parser traces a rightmost derivation in reverse all right? So if youre ever having trouble with bottom up parsing its always helpful to go back to this basic fact. Bottom up parser traces a rightmost derivation but it does so in reverse by using reductions instead of productions. So heres the series of reductions again. Shown on the left And here is the parse tree that is constructed from those reductions. And this is actually I think a very helpful picture if we animate it to see the sequence of steps and to see what a bottom up parser is really doing. So here we begin with the input string. Over here And we show the same input string here. And now were just going to walk through the sequence of steps that the bottom up parser takes A series of reductions. And show how it builds an entire parse tree. And the basic idea is that in each step were performing a reduction. And remember when we do a reduction we replace the children of the right hand side of sum production by its left hand side. And just like when we were doing top down parson well we will do the same thing here. In the input and then we make T its parent. And now you can see whats going to happen. A top down parser begins with the start symbol and produces the tree incrementally by expanding some non terminal at the frontier. At the current at a current leaf of the partially constructed parse tree. The bottom up parsers is going to begin with all the leaves of the eventual pars tree The entire input. And its going to build little trees on top of those. And its going to be pasting together all the sub-trees that its put together so far to build the complete tree. Lets walk a few more steps and see how that happens. So in the next step we go from Int times T to T so Int times and the sub tree rooted at the other T become children of this non terminal T and you can see weve taken these three sub trees here and pasted them together into a larger tree. So as we go throug h the parcer bigger and bigger portions of the original input are gonna be pasted together into larger and larger trees. And the next reduction takes the Int to the far into the input and reduces it to T. And that gets reduced to E and then. At the very end the three remaining sub trees are all pasted together into one parse tree for the whole thing with a start symbol as the root. In this video were gonna continue our discussion of bottom-up parsing with the main strategy used by all bottom-up parsers so-called shift-reduce parsing. Here is a quick review of the most important thing that we learned last time that a bottom up parser traces a right most innervations in reverse. Now this particular fact has an important consequence. So lets think about a state of a shift reduced parse where we have string alpha beta and omega and lets assume the next reduction is going to be the replaced beta by X. Okay so remember were running productions backwards. Then I claim that Omega has to be a string of terminals. And why is that? Well if you think about it if this is a rightmost innervations in reverse then when X is replaced when we take this if we look at the forward step is the the backward step. So remember the parser is running this way replacing data by X. But if we think about the rightmost innervations in the other direction then X has to be the rightmost non-terminal which means there are no non-terminals to the right of X and so all the Character all the tokens whatever it is in this string have to be terminal symbols. Now it turns out that those terminal symbols to the right of the right most non-terminal are exactly the unexamined input in bottom of parsley implementations. That is if I have alpha X omega and Im and X is my right most to non-terminal then this is the input that we havent read yet. This is unexamined Input And its gonna be useful to mark where we are in the parse where our where our input focus is. And were gonna do that by using a vertical bar. So were gonna just place drop a vertical bar. Between the place where we read everything to the left and weve actually been working on this. So this stuff to the left here can be terminals and non-terminals and we the parts that weve seen all of that stuff. And the stuff to the right is after the parts hasnt seen. Now we dont know whats out there although we do know its all terminal symbols. An d the vertical bar is just gonna mark the dividing line between the two sub-strings. To implement bottom up parsing it turns out we only needs two kinds of actions. Shift moves and reduce moves. And weve already talked somewhat about reduce moves and so we have to introduce shift moves. So lets do that now. So a shift move reads one token of input. And we can explain that or represent that by moving the vertical bar one token to the right. So if our input focus is here and we want to read one more token of input then we just move the vertical bar over. And this signifies that now the parser knows about that next terminal symbol. And now we can start working on it. It can do things. We can match against it for the purposes of performing reductions. Again the stuff out here to the right of the vertical bar the parser hasnt seen yet. And then a reduce move is to apply an inverse production at the right end of the left string. So if in production a goes to xy and we have x and y here immediately to the left of the vertical bar. So this is our focus point okay and x and y the right hand side of the reduction is right there. Then we can do a reduction we can replace that right hand side by the left hand side and this is a reduce move. Here is the example from the last video and this is exactly the example just showing the reduced moves now with the vertical bar also shown. So this shows where the input focus was at the point where each of the reductions was performed. And whats missing of course now we know is the sequence of shift moves. So here is the sequence of shift moves and reduce moves that take the initial input string to the start symbol. So lets walk through this in more detail. So were going to go step by step. And were going to show each shift and each reduce move. And now in addition to our input string down here we also have a pointer showing where the where in the input we are. So initially we havent seen any of the input and our input pointer is to the left of the entire str ing. So the first move is to do a shift. And then we do another shift and then we do another shift. And now just look at the example from before if you look back at that at that example you know the next thing we need to do is to reduce. So remember were only allowed to reduce to the left of the arrows. So we can only reduce over on this side of the arrow. So we always have to read enough of the input before we can perform a reduced move. And then we perform another reduce move okay? And then it turns out the next thing to do is two shift moves and we havent explained yet how we know whether to shift or reduce were going to get there. Im just showing that there exists a sequence of shift and reduce moves that succeed in parsing this example. Now weve shifted the entire input onto this sorry weve weve. We shifted over the entire input so theres no more input to read. And now all we can do is reduce moves. But fortunately there is a sequence of reduce moves from this point that we can we can perform. So here we reduce int and then we reduce T plus T. Oh forgot we first reduce T to E and then we reduce T plus E back to the start symbol. In this video were going to introduce another important concept in bottom-up parsing the notion of a handle. To review bottom up parsing is these two kinds of actions: we have shift moves which just read one token of input and move the vertical bar one to the right And reduced moves which replace the right hand side of a production [inaudible] to the left of the vertical bar by a production left hand side. So in this case the production must have been A goes to XY. And also reviewing what we did in the last video the left string can be implemented by a stack where the top of the stack is marked by the vertical bar. So shift pushes the terminal on to the stack and reduce pops zero or more symbols of the stack and thats gonna be the right hand stack of some production. And then its going to push one non-terminal on to the stack which is the left hand side of that same production. And the key question in bottom of parsing and the one we havent addressed at all yet is how do we decide when to shift and when to reduce. So lets take a look at this example grammar. And lets think about a step of a parse where weve shifted one token onto the stack. We have Nth on the stack and then we have times N plus N still to go that we havent seen yet. Now at this point we could decide to reduce by T goes to N because we have the production T goes to Nth right here. And so we could then get into this particul- potential state or this particular state where we have T on the stack and then the rest of the input that looks like that. A- but you can see that this would be a mistake. There is no production in the grammar that begins Hence T times. Theres no production up here that looks like T times. And therefore if we were to to to make this move we would get stuck. We could continue to do reductions to rummage around in the string. But we would never be able to get back to the start symbol. Because there is no way to deal a sub string that has t times something in it. So what that shows us is that we dont always want to reduce just because we have the right-hand side of a production on top of the stack. To repeat that even if theres the right-hand side of some production sitting right there on top of the stack it might be a mistake to do a reduction. We might want to wait and do our reduction someplace else. And the idea about how we decide is that we only want to reduce if the result can still be reduced to the start symbol. So lets take a look at a right most innervations. So beginning with the start symbol we get to some state after after some number of steps where that means just an arbitrary number of steps. We get to some state X is the right most non-terminal and then the next step is to replace X with by the right hand side of some production. And remember again with bottom up parsing the parsers are actually going in this direction okay. So this is the reduction direction. The derivation direction the production direction Because thats the easiest way to talk about what strings are derived. We wanna begin with a start symbol. But the [inaudible] but the parsers actually going against the flow of these arrows. Anyway if this is a rightmost derivation Then we say that alpha beta is a handle of alpha beta omega. And that just means that yes it would be okay in this situation to reduce beta to X. And we could replace beta by X because its not a mistake. We can still by some sequence of moves get back to the start symbol. You know by by doing more reductions. So handles formulize the intuition about where it is okay to do a reduction. A handle is just a reduction that also allows further reduction back to the start symbol And we clearly only want to do reduction at handles. If we do a reduction at a place that is not a handle even though it looks like its the right hand side or maybe actually be the right hand side of some production that does not mean. That its actually a handle and we might if we could reduce there we may get stuck. So all we said so far is what a handle is. Weve defined a handle We havent said anything about how to find the handles. And actually how we find the handles is gonna consume much of the rest of our discussion of parsing. Welcome back in this video were gonna talk about the key ideas behind techniques for recognizing handles. There is good news and bad news when it comes to recognizing handles. The bad news is that there is no known efficient algorithm that recognizes handles in general. So for an arbitrary grammar we dont have a fast way to find the handles when were parsing. The good news is that there are heuristics for guessing handles and for some context free grammars for some fairly large classes of context free grammars these heuristics always identify the handles correctly. We can illustrate the situation with a Venn diagram. If we start with a set of all context free grammars then the unambiguous context free grammars are a sub-set of those and then an even smaller set are called the LR(k) grammars. And here just to remind you l stands for left to right scan look ahead. Now the LRK grammars are one of the most general deterministic families of deterministic grammars that we know of. But those arent the ones that are actually used in practice. Most of the bottom up tools that are practical use what are called the LALRK grammars which are a subset of the LRK grammars. And then what were gonna talk mostly about is a simplification of those called the simple LR grammars or the SLRK context free grammars. And these containment relationships or [inaudible] that is there are grammars that are [inaudible]. R k but not s l r k for every k and similarly there are grammars that are l r k for every k that are not l a l r k. As weve already said its not obvious how to detect handles. So what does the parser know? Well it sees the stack. At each step it knows the stack that it has already constructed. And so lets see how much progress we can make just thinking about what information we can get from the stack. So heres a definition. Were going to say that alpha is a viable prefix. If there is some omega such that alpha bar omega is a configuration a valid configuration of a shift reduce parse. Now keep in mind that the alpha here. This is the stack. And the omega here is the rest of the input. And what does that means? That means the parser knows this part. The parser knows alpha it doesnt know much of omega. It can do some look-ahead it can look at a small prefix of omega usually just one token but it certainly doesnt know the whole thing. So what does a viable prefix mean? Well a viable prefix is a string that does not extend past the right end of the handle. And the reason we call it a viable prefix is because it is a prefix of the handle. So as long as the parser has viable prefixes on the stack no parsing error has been detected. And really the definition is just giving a name to something its not anything very deep the fact that alpha bar omega is is viable thats just saying we havent encountered an error. That this is some state of a shift reduce parse. It hasnt said yet how were going to identity it or anything like that; its just saying that these are the valid states of shift reduced parse. Now the definition is useful in one way if it bring us to the last important fact important fact number three about bottom up parsing. In this effort any grammar the set of viable prefixes is a regular language and this is really an amazing fact and one thats going to take us a little while to demonstrate but this is the key to bottom up parsing. At least all the bottom up parsing tools are based on this fact that the set of viable prefix can be recognized by a finite automaton. So were going to show how to compute this automaton that accepts the viable prefixes but first were going to need a number of additional definitions. The first definition we need is the idea of an item. Now an item is a production that just has a dot somewhere on the right hand side. So heres an example. Lets take the production T goes to open paren E closed paren. What were going to do is were just gonna put the dot in eve ry possible position on the right hand side. So well have one item where the dot is all the way at the left end. Well have one where the dot is all the way at the right end. And then well have items where the dot is between every pair of consecutive symbols. So in this case there are four items for the production. One special case is what do we do with epsilon productions? Well for an epsilon production there is no there are no symbols on the right hand side. Well just say there is one item X goes to dot. And these items youll see them referred to if you if you look in help pages and in the literature as the LR zero items. Now were ready to discuss how we recognize viable prefixes. And the problem is that the stack has only bits and pieces of the right hand side of productions. In general most of the time we dont have a complete right hand side on top of the stack. Most of the time we only have a part of the right hand side. And. It turns out that what is on the stack is actually not just random its its it actually has a very special structure. In in these bits and pieces are always prefixes of right hand sides of productions. That is in any successful parse what is on the stack always has to be a prefix of the right hand side of some production or productions. Lets take a look at an example. Lets consider the input open paren [inaudible] closed paren. And heres one of our favorite grammars. Now this configurations where I have open paren E [inaudible] on the stack. Remember that this is our stack. And we have the close [inaudible] in the input. This is actually a state or a valid state of a shift [inaudible]. And you can see here that open paren E is a prefix of the production. T goes to open paren E close paren. And after we shift the remaining close paren onto the stack then well have the complete right hand side and it will be ready to reduce. So this is where the items come in. The item T goes to open paren E. Dot closed paren. This describes this state of affairs. I t says that so far we have seen open paren E of this production. And were hoping in the future to see the closed paren. So another way of thinking about it is that this item records the fact that were working on this production. And then so far weve seen this much. Everything to the left of the dot is what weve already seen and is what is on the stack and. What is to the right of the dot is what were waiting to see before we could possibly reduce. And we may or may not see that remember the parser doesnt know the input. In this case of course its the very next next symbol and so it can see in the look-ahead but you know at this point in time the parser doesnt know for sure whats coming up and you know and and if this dot were further to the left there might be many many more symbols that we had to go before we could perform the reduction. So anyway whats to the left of that records what weve already seen. And what is to the right of the dot says that what we are waiting to see on the stack before we can perform a reduction. And now we could talk about the structure of the stack. So you see its not just arbitrary collections of symbols. In fact it has this very particular structure. So the stack is actually a stack of prefixes of right hand sides. So the stack always has this organization where theres a bunch of prefixes stacked up literally stacked up on the stack. And whats going to happen is that the ice prefix if you were to pick a prefix out of this stack of prefixes While that must be the prefix of some production. Okay. The right hand side of sum production And what that means is that that prefix that [inaudible] prefix on the stack will eventually reduce to the left hand side of that production. So it will eventually reduce to XI in this case. And then that XI has to be Part of the missing suffix of the prefix that is below it on the stack. So if I look at the previous prefix the one thats right below prefix [inaudible] on the stack Then when I perform this reducti on that XI needs to extend that prefix to be closer to a complete right hand side of that particular reduction. Okay so in particular theres going to be some production. That is going to; already have a portion of its right hand side on the stack. So prefix of I minus one. And X I is going to extend that prefix and then theres gonna be some more stuff possibly that were waiting to see even after the X I is put there. And recursively all the prefixes above prefix K eventually have to reduce to the missing part of the right hand side of prefix K the alpha K that goes on the right hand side. [inaudible] This image you have a stack of prefixes were always working on the top-most prefix on the stack so you will be always working here on the right and shifting and reducing but every time we perform a reduction. That has to extend the prefix immediately below it on the stack. And when these when a bunch of prefixes have been removed from the stack through reductions then we when we get to work on the prefixes that are lower in the stack. So lets illustrate this idea with an example. So here is another input string and were gonna use the same grammar. You can you can rewind if you want to see the grammar again. But lets consider this state where we have open paren [inaudible] star on the stack. And we have int close paren remaining in the input kay? And so what items would record what is the what is the stack structure here and how do the items record it? Well lets start here at the bottom lets actually work from the bottom up. So we have in start the top of our stack so we this is the right hand side that were currently working on and that would be a prefix to this production T goes to int star T. Okay? So what this says is that were looking you know we weve seen in stars so far and were waiting to see [inaudible]. Im not showing the items but Im just showing the productions that this is eventually going to use. Now the one thats below it here the the prefix thats below it o n the stack is right here in between the open paren and the int. This ones an interesting case. Its actually epsilon. So theres nothing there now on the stack. But eventually once the int star has reduced to T. Okay? Then that T is going to reduce to E. And currently of course theres not a T there at all. So weve only seen epsilon. Weve seen none of the prefix of this production on the stack. And then for the last production the one deepest in the stack were currently weve currently seen an open paren. And were w- and we think were working on this production. T goes to open paren E closed paren alright? So when this E is produced that will extend this right hand side. And now we can record all of this with the stack of items T goes to open paren dot E E goes to dot T and T goes to N star dot T. Okay and we just record what we said on the previous slide that so far we see the open paren of this production. Weve seen nothing out of the right hand side of this production and weve seen N star so far of this production. And just notice how the left hand side of each of these productions is going to eventually become part of the right hand side of the. Of the right part of the right hand side of the production we are working on just below it in the stack. So when weve reduced this instar T to T that will extend this production when it reaches E that will extend this production To summarize this video we can say a little more precisely how we go about recognizing viable prefixes. The crux of the problem is going to be to recognize a sequence of partial right had sides of production. Where each of those partial right hand sides can eventually reduce to part of the missing suffix of its predecessor Next time in the next video were going to actually give the algorithm for implementing this idea. In this video were finally gonna come to the technical highlight of bottom up parsings After all the definitions of the previous videos now were actually gonna be able to give the algorithm for recognizing viable prefixes. So lets dive straight into the algorithm. The first step is really just a very technical point and its not not that important. But were going to do it anyway because it makes things simpler Is to add a dummy production as prime goes to S to our grammar of interest G. So just to set the stage we are trying to compute the viable prefixes of G. Were trying to come up with a algorithm for recognizing the viable prefixes of G. If S is the start symbol were going to make up a new start symbol as prime so as prime would be the new start symbol of augmented grammar and its just one production for as prime as prime goes to S. Right. So this just allow us to know exactly where our start symbol is used in particular our new start symbol as prime is only used in one place and. Left hand side of this one production and that just makes things a little bit simpler. Now recall what we are trying to do. We claim that the set of viable prefixes for a given grammar is regular and so what were going to do is were going to construct a non-deterministic final automaton that recognizes the viable prefixes okay? And the states of this NFA are going to be the items of the grammar. Now the input to the NFA is the stack. So the NFA reads the stack okay? And then it So lets indicate this so the NFA is gonna take the stack as an argument and its either gonna say yes thats a viable prefix or no. And its gonna read the stack from bottom to top. So its gonna start at the bottom of the stack and read towards the top of the stack. And our goal here is to write a non entromystic finite automaton that recognizes the valid stacks of the purser. So that is how well know. That our parser hasnt really counted any parse errors. Because this automaton that were going to construct will always output e ither yes this stack is okay meaning it could wind up parsing the input. Or no what weve got on the stack now doesnt resemble any valid stack for any possible parse of any input string for this grammar. Okay so lets think about what we what we need the moves of this machine to be. So lets say that were in the state E arrow alpha dot X beta. Now what does that say? So that says that so far weve seen alpha on the stack. Okay so remember the machine is reading the stack from bottom to top. This records the fact that the machine has already seen &quot;alpha&quot; on the stack. So what would be an okay thing to see next on the stack? Well if this is a valid stack if having okay if the next thing on the stack was an &quot;X.&quot; So we have a transition that if were in this state Where we are working on this production and weve seen alpha on the stack. If the next thing is an X on input X then we can go to this state. Where now we record the fact that weve seen alpha X on the stack and were waiting to see the remaining portion beta of that production. Okay so this is one kind of move that the non triamistic phymotine can make and again we do we add this kind of a move for every item. So for every item in the grammar I if it if the dot is not all the way at the right end then there will be a move like this where the dot moves over for whatever symbol happens to come to the right of the dot. The other class of transitions are the following And these are the the more interesting ones. So lets say that were in this configuration here. Where again weve seen alpha. And then the next thing on the stack is X. And here X is a non terminal. [sound]. And I should have said that in the previous case. X was either a terminal or a non terminal. So this X here is any grammar symbol not just a non terminal. But this #four here the the moves here in part four are specifically for non terminals. Okay so anyway if X is not on the stack. Okay lets assume that weve seen alpha and then the next thing on the stack is not S. Well is it possible that there could be a valid configuration of the parser where we saw alpha but then X didnt appear next? And the answer is yes because as we said before the stack is a sequence of partial right hand sides. So it could be that all thats on the stack right now for this production. Is alpha and if the next thing on the stack is eventually going to reduce to X. It might not be X itself it might be something that will eventually reduce to X. Well what does that mean? Well that means that whatever is there on the stack has to be derived from X it has to be something that can be Generated by using a sequence of X productions cause eventually its going to reduce the X. So for every item that looks like this and for every production for X now were going to add the following move were going to say that if theres no X on the stack well then we can make an epsilon move we can just shift to a state where we try to recognize the right hand side plus something derived from X. And these are the only two kinds of moves Either the items eith sorry either the grammar symbols were looking for are there on the stack and we extend. &gt;&gt; The prefix of a right hand side. So this rule here extends a prefix. So as we see more of that production on the stack or it tries to guess or tries to discover where the ends of the prefixes are. So if if alphas as much of the production that is on the stack currently well then this must be this x here must this this point here must mark the start of another right hand side in our stack of right hand sides. So we would expect to see something derived from some production for x. Two more rules. Every state in this automaton is going to be an accepting state. That means that if the automaton manages to successfully consume the entire stack then the then the stack is viable. And just notice that not every state is going to have transition on every possibl e symbol. So there will be plenty of possible stacks that that are rejected simply because the automaton gets stuck. And finally the start state of this automaton is the item as prime goes to dot S. So remember the states of the machine are the items of the grammar. And this is why we added this dummy production is just so that we could conveniently name the start state. So now lets consider one of our grammars weve been using a lot so this is the grammar. And now were going to augment it with the extra production as prime goes to E. And lets take a look at the automaton for that recognizes the viable prefixes of this grammar. And here it is and as you can see its rather large it has a lot of states and a lot of transitions and I just want to show it to you here before we describe how we calculated it just so you can get an idea that these [inaudible] for recognizing viable prefixes for grammars are actually quite elaborate. But now lets break this down and see how it was produced. So lets begin with the start state of this machine so we have S prime goes to dot E. And remember what this says is we want to be able to reduced to the start symbol to the new start symbol. And so were reading the stack and were hoping to see an E on the stack and if we dont then were happy to see something derived from E. So what transition we make from the state. One possibility is that we do in fact see an eon a stack and in that case the dot simply moves over saying yes weve read the first item on the stack or the weve read the E on the stack and so weve seen the full right hand side of this production. Now that would indicate that we were probably done with parsing. This is the state that you would reach have youd read the entire input and successfully parsed it you would have reduced the old start symbol and be about to reduce to the augmented the the new start symbol. But if youre not so fortunate as to see an E on the stack then you need to hope that youll see something derive fro m E. And there are a couple of possibilities there. One is that we could see something that would eventually use this production E goes to T. And since we havent seen any of it yet we put the dot all the way at the left indicating that were hoping to see a T which could then reduce to E and which could then reduce to S prime. Now if we dont see a T on the stack by itself the other possibility is that we could be working on this production. E goes to T plus E. And again we havent seen any of it so the dot Goes on the left hand side. And then notice how were crucially using the power of nondeterministic automata. So here we dont know which production is going to which right hand side of a production is going to appear on the stack. And in fact I notice that these productions are not even left factored so we dont know whether its going to be just a T there or a T plus E but. We just use the guessing power of the [inaudible] chromatin you chose which one to to use. Remember the [inaudible] sepse is any possible choice except. So you can always guess correctly. So intuitively you can. You will be able to pick the right one. Now of course we could compile this down to a deterministic machine that wont have to make any guesses. But at this level were writing [inaudible] its extremely useful not to have to figure out which of these two productions to use. We can just try both and see what happens. Now lets focus on this state E goes to dot T. What are the possibilities there? Well one possibility is that we see a T on the stack. And then we see in a complete right hand side. And notice how when the dot was all the way to the right hand side that is going to indicate that were ready to do a reduce. So well talk about that a little bit later but essentially thats how were going to recognize handles. When we finally reach a state where the dot is all the way to the right hand side thats going to say this could be a handle that you might want to reduce. Now if we dont see a T on the stack  then we just see something derived from T and theres a couple of possibilities a few possibilities there. One possibility is that its going to be the production T goes to int so since were just starting on this production again we just put the dot all the way at the left. Another possibility were working on T goes to (E). And the third possibility that were working on T goes to int x T. And each of the case here notice that the dots are all the way at the left indicating that were just getting started we have not actually seen any of the right hand side yet. Now lets shift our focus to this production E goes to dot T plus E. This item excuse me One possibility is that we see an E on the see a T on the stack okay in which case the dot just moves over. And the other possibility is that we see something derived from T in which case we will go to one of the states that begins a T production. And notice here that we already have all three of those items in our automaton. Were just going to it states that we went to from the item E goes to dot T. So this this item E goes to dot T plus E could also move to those three states. Now lets focus on this item here. T goes to dot open-paren E closed-paren. Well theres only one possible move here so this is only a a terminal its not its not a non-terminal so theres not going to be any possibility of having something derived from open-paren. We just have to see the open-paren in the input. So theres only one possible transition here which is that we see the open-paren excuse me on the stack and the dot just moves over. Now from this state once again we got is just next to a or just to the left of a non-terminal so we might see that non-terminal on the stack or we might see something derived from that non-terminal. Well if we see that non-terminal on the stack your dot just moves over and we get T(E.) indicating that weve seen an both an ( and E on the stack and were still waiting to see the ). Well we might also see somethin g derived from e okay. So we add these two transitions to the two items that begin productions for e. [sound]. Alright now lets focus on this state. T goes to open paren E dot closed paren. Again cause its a terminal that the dot its next to is only one possible move. We have to see that open paren if we see anything at all. And well wind up with the item T goes to open paren E closed paren dot. And now weve recognized the entire right hand side of that production on the stack. Lets take a look at this item. So were here because a terminal symbol the only possibility is to read that terminal symbol on the stack. So this would be the next item. E goes to T plus dot E. [sound]. Focusing on that item again have recognized the entire writing inside of this production We have E goes T+E. Or we can see something derived from here which case we make a transition back to one of those two states. Now where we got productions left to go or items left to go Here we haw T goes to.int. So we would have to see it next on to stack and that would be the full right hand side of that production. Down here we still have T goes to dot [inaudible] times T. So again theres a terminal symbol here for this production to remain viable. And once weve seen the [inaudible] we would like to see the times. So we wind up in this state and now weve got dot next to T. So again one possibility is that we see the T on the stack and then weve seen the full right-hand side of this production. But we might only see something derive from T. The might the the T might not be there yet. It might be in a state where were still waiting for the T to appear through some sequence of reductions. But then we would need to see something derive from T. And in this case we would make a transition to one of the three states that begin the productions for T. And that s the full automaton. That is those are all the states and all the transitions for the automaton that recognizes the viable prefixes of this grammar. In this video were going to use our example automaton for recognizing viable prefixes to introduce one more idea The idea of a valid item. To refresh your memory heres where we left off last time. This is the complete nondeterministic automaton for recognizing the viable prefixes of the example grammar deterministic automaton that is equivalent to the non-deterministic automaton. So heres the deterministic automaton that recognizes exactly the same language. This automa this deterministic automaton notices the viable prefixes of our example grammar. But now notice that each state is a set of items. So theres a set of non-deterministic automaton states in each of these states. And recall that what that means is that the non-deterministic automaton could be in any one of these states. And in particular this state here is the start state because it has the item S prime goes to dot E. The states of this deterministic automaton are called variously cananugal collections of items or the cananugal collections of LR zero items. If you look in the dragon book it gives another way of constructing the LR zero items than the one that I gave. Mine is somewhat simplified but I think also a little easier to understand if you are seeing this for the first time. Now we need another definition. Well say that a given item is valid for a viable prefix alpha beta. If the following is true that beginning from the start symbol this is our extra start symbol and by a series of right-most derivation steps we can get to a configuration alpha-x-omega and then in one step x can go to beta-gamma. And what this says is after parsing alpha and beta after seeing. Alpha and beta on the stack the valid items are the possible tops of the stack of items. That that we could that this item could be the determination state of the nondeterministic automaton. A simpler way of explaining the same idea is that for a given viable prefix alpha the items that are valid in that prefix are exactly the items that are in the final state of the DFA after it reads that prefix. So these are the items that describe the state after youve seen the stack alpha. Now an item is often valid for many many prefixes. So for example the item T goes to open paren .e closed paren is valid for all sequences of open parens. And to see that We can just look at our automaton and confirm that if we see an open paren remember this is the start state. So if we see an open paren we take this transition we wind up in this state here. And then every open paren we see we just go round and round in this state. So if I have a sequence of five open parens as my input then Ill have transitions one two three four five all looping in this state. And notice that this item. Is in is one of the items in that state. And that just says that this item is valid for any prefix or for excuse me any sequence of open parens. In this video were finally going to give an actual bottom up parsing algorithm. In particular well talk about SLR or simple LR parsing which will build on the ideas of valid items and viable prefixes that weve been discussing in our recent videos. The first thing were going to do is to define a very weak bottom up parsing algorithm called LR0 parsing. And the basic idea here is that were going to assume a stack contains a contents alpha and that the next input is token T And that the DFA this is the DFA that recognizes the viable prefixes. On input alpha that is when it reads the stack contents it terminates in some state S. algorithm needs to do. So if S if the final state of the DFA contains the item X goes to beta dot. Well what does that say? That says weve seen the complete right hand side of X goes to beta on the top of the stack and that furthermore everything thats below the stack still says that x goes to beta dot is a valid or a viable sorry is a valid item for this state. Meaning its okay to reduce by X goes to beta. So if we see a complete production dot all the way in the right hand side in the final state of the DFA then were just going to reduce by that production. The other possible move is a shift. If we wind up in a state where X goes to beta .t and then some other stuff is a valid item what does that say? That says that it would be okay at this point to add a T to the stack. And if T is our input well then we should do a shift move [sound]. Now when does LR0 parsing get into trouble? Well there are two possible problems it could have. It might not be able to decide. Between two possible reduced moves. So if any state of DFA has two possible reductions meaning it seem two complete productions and it could reduce by either one then theres not enough information to decide which reduction to perform and the parts wont be completely deterministic and this is called a reduced reduced co nflict. So again this happens if a particular state has two separate items indicating two separate reductions. The other possibility is that the final state of the DFA after reading the stack contents might have An item that says to reduce and another item that says to shift. And this is called a shift-reduce conflict. So in this case there would only be a conflict in a state where T was the next item in the input. But in that situation we wouldnt know whether to shift T onto the stack or to reduce by X goes to beta [sound]. Lets take a look at the DFA for recognizing viable prefixes that weve been using for the last couple of ideas and in fact this particular DFA does have some conflicts. So lets take a look at this state right here here we could either reduced by E goes to T you are in this state or if the next input is a plus we could do a shift and. In so in this particular situation if the next input is plus we could either shift and use this item or we can reduce and use that item. So this particular state has a shift reduced conflict. Now thats not the only conflict in this in this grammar though. In this state here we have a very similar problem. Here we could shift if the next input is a times. Or we could reduce by T goes to [inaudible]. And so this state also has a shift reduce conflict. It turns out that its not difficult to improve on LR0 parsing and well present one such improvement in this video called SLR or simple LR parsing. And this is going to improve on LR0 by adding some heuristics that will refine when we shift and when we reduce so that fewer states have conflicts. The modification to LR0 parsing that gives us SLR parsing is really quite small. We just add one new condition to the reduction case. So before if we saw it X goes to beta dot in the final state of our DFA recall what that means. That means beta is on the top of the stack and it is viable And so its fine to reduce. Now We do have a little bit more information. So so notice that the automaton her e doesnt take any advantage of whats coming up in the input. This is based entirely this decision here is based entirely on the stack contents. But it might be that it doesnt make sense to reduce based on what the next input symbol is. And how can we take advantage of that? Well if you think about it whats going to happen? We have our stack contents. And it ends in a beta and now were going to make a move where were going to replace that by X. Okay. And if the next input symbol is t so remember we have a vertical bar here and a t following what does that mean? Well that means that x has to come before t in the derivation. Or in another words t is gonna follow x. And if t cant follow x if t is a terminal symbol that cant come after the non-terminal x than it makes no sense to do this reduction. So we only do the reduction if t is in the follow of x. We just add that restriction and that is the only change to the parsing algorithm. So if there are any conflicts under these rules either shift reduce or reduce reduce then the grammar is not an slr grammar. Just notice that these rules amount to a heuristic for detecting the handles. So we take into account two pieces of information. The contents of the stack thats that the DFA does for us and it tells us what items are possible when we get to the top of the stack and also whats coming up in input and we can use that to define our reduction decisions. And for those grammars where there are no conflicts meaning there is a there is a unique move in every possible state under those rules. Then this heuristic is exact you know for for those grammars. And we just define those grammars to be the SLR grammars. Lets consider how things have changed for our running example. The deterministic automaton for recognizing the viable prefixes of the grammar weve been looking at for several videos now. Recall that we had shift reduced conflicts under LR zero rules in two states. So now lets look at this state first the upper state. So here we re going to shift if theres a plus in the input. Thats what this item tells us to do. It tells us theres if theres a plus then the right move is to shift. And so Now the question is when are we going to reduce? Well were only going to reduce if the input is in the follow of E. And what is the follow of E? We computed that a long time ago but just to remind you remember that E here is the original start symbol of the grammar so certainly dollar sign will wind up in the follow of E. And the other possibility for the follow of E is close paren because here at this point in the grammar close paren comes after E. And thats the only two possibilities. So what that says now what that means is that in this particular state we are going to reduce if either were out of input. Or if the next I the next token in the input is a closed paren and will shift if the next token in the input is a plus. And in any other situation we will report a parsing error. And so theres no longer any shift reduced conflict in this state and theres always a unique move for every possible input. The situation is similarly similarly improved for the other state. So here were going to shift in theres a times in the input and were going to reduce if the input is in the follow of T. And what is the follow of T? [sound]. Recall We computed this again a long time ago and I just happen to know what it is. And so Ill just tell you. Well it included everything in the follow of e. So a dollar sign in close paren are in the follow of T. But also a plus is in the follow of T because of this usage over here in the grammar where plus appears really after T. But those are the only things in the follow of T. And so now were going to reduce only if were out of input or if the next input item is a close paren or a plus and theres also a no shift reduce no longer any shift reduce conflict in this state. And so this grammar is an SLR1 grammar. Now many grammars are not SLR. To emphasize that SLR is an improvement on LR0 but it s still not a really very general class of grammars. So All ambiguous grammars for example are not SLR. We can improve a little bit on the SLR situation. We can make SLR parsers even more grammarous by using precedence declarations to tell it how to resolve conflicts. So lets revert to the most natural and also most ambiguous grammar for plus and times over the integers and weve looked at this grammar before. If you build the DFA for this grammar if you go through and build the DFA for the viable prefix of this grammar you will discover that there is a state. That has the following two items in it one says that if we see E times E that we have seen E times E on a stack and that we can now reduce by ecos E times E. The other one will say that if theres a plus coming up in the input we should shift. And notice that this is exactly the question. Of whether times has higher precedence than plus. When youre in this situation should you. Reduce thereby grouping the two Es together here Grouping the multiplication operation first. Or should you shift the plus in which case youll be working on that for a sentence at the top of the stack. So in this situation the declaration times has higher precedence than plus resolves the conflict in favor of the reduction. So we would not do the shift and we would wind up with no shift-reduce conflict. Note that the term precedence declaration is actually quite misleading. These declarations dont define precedence. They dont. Do that directly at all. What they really define are conflict resolution. They say make this move instead of that move. It happens that in this particular case. Because were dealing with a national grammar simple grammar for plus and times that the conflict resolution has exactly the effect of enforcing the precedence declaration that we want. But in more complicated grammars where there are more interactions between various pieces of the grammar these declarations might not do what you expect in terms of enforcing precedence fortuna tely you can always print out the automaton. The tools provide Usually a way for you to inspect the parsing automaton. And then you can see exactly how the conflicts are being resolved and whether those are the resolutions that you had intended. And I recommend when youre building parsers especially if its a a fairly complex parser that you do examine the parsing automaton to make sure that its doing what you expect. So now were ready to give the algorithm for SLR parsing. So The initial configuration is going to be with the vertical bar all the way to the left so the stack is empty. This is our full input and we [inaudible] dollar to indicate the end of the input. And now were going to repeat until the configuration has just the start symbol on the stack and dollar in the input. Meaning all the input is gone and weve reduced the entire input to the start symbol. So. An [inaudible] configuration will be written as alpha-omega; where alpha is the contents of the stack and omega is the remaining input and what were going to do is were going to run M run the machine on the current stack alpha and if M rejects alpha if M says that alpha is not a viable prefix then were going to report a parsing error. Were gonna stop right there. Now if M accepts alpha and it accepts it in a state if it ends in a state with items I then were gonna look at the next input call that A and what are we going to do? Well were going to shift. Yes theres an item. In I that says it would be okay to see the terminal A. Next. Okay? So thats just our shift move. And then were going to reduce if theres a reduction item in the in the set of valid items. And the next input can follow the non-terminal on the left hand side. So these are just the two rules that we discussed before. And then well report a parsing error if neither of these applies. Okay now one interesting thing about this algorithm if you read it carefully and you th ink about it for awhile. Youll realize that this step is actually not needed that we dont need to check here For whether M accepts the stack or not. Because this staff down here where we report a parsing error if neither of these steps applies this already implies that we will never form an invalid stack That our their stacks will always be viable. The parsing errors will be caught at this line and we wont pollute the stack with symbols that cant possibly result in viable prefixes. So in fact this error check here is not needed M is always going to accept the stack. If there are any conflicts in the last step meaning its not clear whether to shift or reduce in some state for some input symbol then the grammar is not SLRK. And K again is the amount of look ahead. In practice we just use one token of look ahead So typically just looking at the next token in the input stream. Welcome back In this video were going to do an extended example of SLR parsing. To review here is the parsing automaton for the grammar that weve been looking at in the last couple of videos. And this is just the deterministic version of the non deterministic automaton we built last time. And Ive just gone through and numbered all of the states. So lets take a look at what happens when we parse the input [inaudible] times [inaudible]. And just to review weve appended dollar sign here to the end to indicate where the end of the input occurs. Thats just an end of input marker. And because this is the beginning of the parse we havent seen any input yet. And so the vertical bar is all the way at the left hand side of the input. So the machine begins in state one and theres nothing on the stack. The vertical bar is all the way to the left again so the stack is empty. So it just terminates in state one. And these are the possible items that are valid for the initial state of the parser. So among those items we see that there are two that tell us that its okay to shift an integer in this state. And of course the first input is an integer and so there are no reduced moves. All the other items in here also have their jobs all the way at the left side of the item so theres no possible reduced move in this state. The only thing we could possibly do is shift and its okay to shift an integer. So to summarize on the initial configuration of the parser the DFA halts in state one it never even gets out of state one so it starts there and ends there without even reading any input because the stack is empty and the action that that state tells us to do is to shift. So that leads us in the following state theres an int on the stack and we have a times coming up on the input. So what happens in that situation? Well we begin. The automaton is going to read the stack. So starting from the bottom of the stack were in the start state. And then we read an int theres an int on the stack and we win d up in this state. And what does this state tell us we can do? Well it tells us one possibility is to reduce by T goes to int. But again we will only do that if the following input is in the follow of T And times which is the next input item is not in the follow of T. So times is not in the follow. Of T and so reducing here is not a possibility. That leaves only the other item to consider and here we see that this item says we can the time. So if the times the next thing in input which it is its okay to shift. So the DFA halts in state three and because theres a times in the input the move is to shift. And that puts us into this configuration where we have int and times on the stack. Times is at the top of the stack int is below it and we have an int coming up in the input. So what happens now again the DFA is going to read the entire stack. So beginning at the bottom of the stack the first thing it sees is an int and it moves to that state. And then it sees a times and so it moves to this state. And now in this particular state what are the possibilities? Well we can see first of all that there are no reduced moves. There are no items with the dot all the way at the right end. So the only possibility is a is a shift. And we could shift if the upcoming inputs a open paren which its not. More usefully we could shift if the upcoming input is an [inaudible] which is exactly what we see. So the DFA terminates in state eleven and the move in that state is to shift. And that puts us into this state where we have int times int on the stack and we are out of input. We are at the end of the input. So lets see what happens on the stack int times int. The automaton reads it int times int and it winds up back in state three. Sa3 tells us that we can shift if the next input item is a times and which it is not. Or we can reduce if whatever the next. Is in the next input is in the follow of T. And in fact dollar is in the follow of T. So in the end of the input come after a T on the stack. And that means its fine to reduce by T goes to int. So once we do that once we do the reduction T goes to int we wind up in the state times T. Thats our stack contents and of course were still at the end of the input. So once again the DFA is going to read the entire stack contents from the bottom to the top. First it reads the int at the bottom of the stack then it sees the times. And then it finally reads the t at the top of the stack. And it winds up in a new state state four. And the interesting thing about this particular step is that the DFA took a different path through the state graph than it did the previous time. And thats because the stack contents changed. We didnt just add stuff to the stack and so we didnt extend the previous path. We actually replaced some symbols or a symbol on the stack with a new symbol in this case the non-terminal T and that caused the DFA to take a different path. Now what does this item in state four tell us to do? Well it says that we can reduce by T goes to N times T if whatever. Follows in the input is in the follow of T. And once again dollar is in the follow of T. And so well do that reduction and now were left with the static contents just consisting of T. And of course were still at the end of the input. And lets see what happens now. So now of course the contents of the stack have changed even more radically and so the DFA just goes off in a completely different direction. It reads T winds up in this state and this state says we can either shift a plus if theres a plus in the input. And again theres no more input. Or we can reduce by E goes to T if dollar if the end of the input is in the follow of E Which it is. And so the reduction will be the one that we do. And now we have this stack contents consisting only of E. Lets see what happens in that situation. Now we make a transition to this state state two. And we only have one item S prime goes to E dot. And so this is a reduced move. And again dollar is in the follow of S prime  cause that is the start symbol. And since that is the start symbol we accept at this point. So once we get to that item as our reduce move we know that the input has been successfully parsed. In this video were going to wrap up our discussion of SLR parsing were going to give the full SLR parsing algorithm and also talk about some important improvements. The SLR parsing algorithm we discussed in the last video has one major inefficiency. And that is that most of the work that the automation does when it when it reads the stack is actually redundant. And to see this think about the stack. So we have our stack and this is the bottom over here. And this is the top of the stack over here. And what is going on in each step? In each step we might shift something onto the stacks we might add one symbol or we might pop some symbols and and push one symbol onto the stack. But basically theres going to be some small number of symbols that change at the top of the stack at each step. But most of the stack stays the same. And then we rerun the automaton on the entire stack. And so this work is all repeated. Everything that stayed the same From the previous stack is repeated work and then we do a little bit of new work just at the very top of the stack. And clearly if we could avoid this we could make the algorithm run much much more quickly. The way to exploit the observation that most of the work of the automaton is repeated at each step is to simply remember the state of the automaton on each stack prefix. So were going to change the representation of the stack were going to change what goes in the stack so before we just had symbols on the stack but now were going to have pairs. Each element of the stack will be a pair of a symbol and a DFA state. Thus the stack now is going to be a stack of pairs and whereas before a stack would have consisted just of the symbols sym1 up to sym n now were going to have the same symbols but each one of them is going to be paired with a DFA state and that DFA state is going to be the result of running the DFA and all the symbols to its left So all the symbols below it in the stack. So if I think about my stack and if I draw a little picture of the stack as a line then the DFA state here. Lets call this state I will be the result of running the DFA on the entire stack contents to the left of that point. And again if I look at some other point in the stack at the state stack state thats stored there. That would be running the results of running the DFA on the entire stack context contents up to that point. And one small detail here is that the bottom of the stack we have to get started. We need to have the start state stored at the bottom of the stack. And we just store that with any dummy symbol. It doesnt matter what symbol we pick. So now were ready to actually give the details of the parsing algorithm. And the first step is to define a table go to. And this maps a state and a symbol to another state. And this is just the transition function of the DFA. This is the graph of the DFA written out as an array. Our SLR parsing algorithm will have four possible moves. A shift X move will push a pair on the stack. X is a DFA state so thats named in the shift move now. And then the other element of the pair is the current input. And then well also have reduce moves which are just as before. So to recall a reduce move will pop the a number of elements from the stack equal to the length of the right hand side. And then it will push the left hand side onto the stack. And then finally accept an error moves for when weve successfully parsed the input and for when the parser gets stuck. The second parsing table is the action table which tells us which kind of move to make in every possible state. The action tables indexed by a state of the automaton and the next input symbol. And then the possible moves are things like shift reduce accept or error. So lets consider if we do shifts if the final state of the automaton at the top of the stack has an item that says it would be okay to shift an A. And go to that is from this state we can go to state J on input A. Then the move in state I on input A will be to shift AJ onto the stack And th ink about what that means for a second. What that says is that we have a stack. And then the next input is A. And then at this point its okay to shift an A onto the stack. And furthermore that the state of the automaton at this point is SI. Okay. So the state of Irarta [inaudible] the top of the stack is SI. The next input is A. Remember that the go to table is a transition function of the machine. So if we move the vertical bar over if we shift that A on to the stack well now we dont just put A on the stack we have to put a pair on the stack. And the question is what machine state should go there. Well its going to be state that we would reach from state I from state SI on input A which. The go to table tells us in this case is state SJ. And for that reason the action when we terminate in state I and the next input is A is to shift the pair A J onto the stack. The other three moves that go into the action table are things weve already seen. So if the final state of the automaton at the top of the stack has an item that says that we can reduce and the follow up condition requirement is satisfied. Mainly that the next input can follow the left hand side non terminal of the production. Then in the entry I for production x goes to alpha. And theres one exception here were not going to do that reduction if the left-hand side is the special start symbol the new start symbol that we add to the grammar is prime. Because in that case if the item that were going to reduce by is s-prime goes to s-dot and were at the end of the input then we want to accept. And any other Situation is an error. So in any other situation if were in state I and we have the next the next input is A well we dont know whether to shift reduce or accept. And so that is an error state. Finally here is the full SLR parsing algorithm. And Im just going to walk you through it so that we can see how all of the ideas weve been di scussing and all the various pieces fit together. Lets let our initial input be called I. And well just give it a name and its gonna be treated as an array that we can index. The index will be called J and initially its zero so that were pointing to the first token in the input string. Well just assume that the first state of the DFA is called state one. And that means our initial stack is going to have state one for the state of the automaton and some other dummy symbol that we dont care about In the in the first position. So the stack is just a pair with [inaudible] in the start state of the DFA. And now were going to repeat the following loop until weve either successfully pars the input or we detect an error. And at its steps what were going to do? Well were going to look at the next input symbol and were going to look at the final state of the automaton on the stack contents and thats always the state of the pair thats on the top of the stack and were gonna look those two things up in the action table and thats gonna tell us what kind of move to make. So lets just go through the moves in order. Lets consider the shift move first. So what happens? If were if it says were supposed to shift and going to state K then what were going to do is were going to shift the input that means were going to take the next input symbol and or the current input symbol excuse me and were going to push that on to the stack together with state K of the [inaudible]. That pair goes on to the stack and we also bumb the input pointer so that were looking at the next character of input. Now. Let me erase that so you can continue to read it. Now what about the reduce moves? So this ones a little bit interesting. First thing were going to do is were going to pop a number of pairs off the off the stack thats equal to the length of the right-hand side. So we pop a number of items off the stack thats going to the right thats equal to the right-hand side of the production and then what do w e push on to the stack? Well were gonna push the non-terminal on the left-hand side of the stack. And now the question is: what state goes on to the stack? What DFA state? Well. With that weve popped the stack. We can look at the new top state of the stack. So the DFA state was now the top state. After weve done the pops well tell us what the final state of the DFA was and what is left of the stack. And then now that were pushing X under the stack we want to know what state the DFA would go into on the transition labeled X. And so we use the Go To table to look that up The current top state of the stack. On symbol X where does the FA go? That is the state that gets pushed onto the stack. And then finally if if the move is accept we halt normally. And if the move is error we halt and report an error or execute our error recovery procedure. One interesting fact about this algorithm is that it only uses the DFA state and the input. The stack symbols are not used in really interesting way. And so we could actually get rid of the stack symbols and just do parsings with the DFA states on the stack. But that of course would be throwing away the program and we still actually need to program for the later stages of the compiler. And so to do the type checking and co-generation we need to keep the symbols around. Now simple LR parsing is called simple for a reason. And in fact in practice its a bit too simple. The widely used bottom-up parsing algorithms are based on a more powerful class of grammars called the LR grammars. And the basic difference between the LR grammars and the SLR grammars is that look ahead is built into the items. So what does that mean? Well a LR1 item is going to be a pair which consists of an item Just like we saw before. And this means exactly the same thing as before. And a look-ahead In case of an LR1 item theres just one token of look-ahead. If this was an LR2 item there could be two tokens of look-ahead in there. And the meaning of this pair is that if we ever get aroun d to state where we have seen all of this production all the right-hand side of this production. Then its going to be okay to reduce if the look-ahead at that point is Dollar thats the end of the input. And of course there could be any other token in there any other terminal symbol in there besides dollar. And this turns out to be more accurate than just using follow sense recall that the point where a reduction decision is made in SLR parsing we just look at the entire follow set for the symbol on the left hand side of the production. And this mechanism of encoding the look-ahead in to the items allow us to track and find the [inaudible] which look-aheads are actually possible in particular production sequences. And if you look at the automaton for your parser actually its not an LR1 automaton. Its an LALR1 automaton which is something very close to an LR automaton its a little bit of an optimization over an LR a pure LR automaton but anyway it uses exactly the same kinds of items with this pair of a of a standard LR0 item in a look ahead. If you look at that automaton you will see items that look like this and that will help you in reading the automaton and figuring out what it is doing. In this video were gonna to work through a couple of SLR parsing examples. So lets do a very simple example. Lets consider the grammar. S goes to SA or S goes to B. And what does this grammar do? It produces strings of As followed by a B. So any number of As followed by a single B.  And notice that the grammar is left recursive and recall that thats not a problem for a bottom up parser. Slr parsers LR parsers are perfectly happy with left recursive grammars. So lets begin by working out what the automaton for this grammar should be what the parsing automaton should be. And recall that the first step is to add a new production to the grammar. We have to add a new start symbol. That all it does it has one production that goes to the old start symbol. And thats again just for technical reasons. Now the start symbol or sorry the start state of the NFA of the parsing automaton is this item. S prime our new start symbol goes to dot S our old start symbol. And rather than build the NFA and then do the subset of states construction. Lets just go ahead and work out what items must be in the first state of the DFA. So remember that all the epsilon moves in the in the DF- in the NFA are due to moves that happen because we dont see a non terminal on the stack. But instead see something derived from that non terminal. So if we have a dot Right next to a non terminal. That means that theres an epsilon move in the NFA to all the items that have for all the productions all the all the first items for the productions of that non terminal. What do I mean by that? I mean that this state I mean epsilon production to S goes to dot SA. So this is the first item in recognizing this production. So the dots all the way at the left And there would also be an item for the other production for S S goes to dot B. Alright so thats the epsilon closure in the NFA of this start item. So thisll be the first state. These three things these three items would be the first state of the DFA. And now we have to consider what would happen on each of the possible transitions for each of the symbols that we might see on the stack. So lets think about what happens if we see a B. So if we see a B on the stack then the only item thats going to be in that state is S goes to B dot okay? So itll be fine to see a B and this would be the only item that was valid for the stack contents. Now another possibility is that well see an S. So if we see an S on the stack what will happen? Well were going to go to a state that has two items. S prime goes to S dot so that weve seen S on the stack and were ready to reduce by by this production possibly. And also S goes to S. A. And now Clearly in this state lets talk about his state down here. There are no more transitions possible. In all there is only one item in the state dots all the way at the right hand side so that state is completely done. In this state the one over here on the right side. While one of these items is complete the dots all the way at the right. But the other item still has an A so there could be one more transition out of this state. To the item S goes to SA dot Alright? And now if we look at this we see that for the most part these states are in pretty good shape. So these two states this one down here and this one over here they only have a single item and so theres no possibility of a shift reduce conflict in those states. Theres only one item theres only one thing to do. The only possibility here in both of these states is to reduce. This state the initial start state has no reduce moves. So its only shift moves here so there cant be a shift reduce conflict because there are no reduce items No possible reduce actions. And there is to reduce reduce conflicts for the same reason. The only state of interest really for the point of view for what who the grammar is SLR1 is this middle state. And here we can either reduce by s prime goes to s dot or we could shift and A onto the stack. And the question is what is in the follow of S prime? So what can follow S prime in the grammar? And if we look back up at our grammar well see that nothing can follow S prime. S prime is the start symbol and so in fact the only thing in the follow of S prime is the And to the input. And so what that tells us is that well reduce by s prime goes to s if if were out of input. And otherwise if there is an A on the stack sorry if theres an a in the input then well shift it onto the stack. And so this grammar is SLR1. There are no shift reduce or reduce reduce conflicts implied by this parsing automaton. Lets do another example slightly more complex. In fact lets just extend the previous grammar. Well have a a production. S goes to SAS okay? So now we have the non terminal twice with an A in between Or S can go to B just like before. And now lets work out the parsing automaton for this grammar. And once again Well need to add a dummy start symbol To the grammar And it will go. Its only production will be to generate the old start symbol. And now lets begin working out whats in the parsing automaton for this particular grammar. And and just like before were not going to go through the effort of constructing the NFA. That would be a systematic way to do it. One way to it is is the way we sketched. Was just to construct the NFA first and then do the subset of states construction. But this grammar is small enough. And simple enough that we can work out directly what is in what are in the states what items are in the states of DFA. So just like before because the dart here is immediately next to the S we know that we can without consuming any input at all make an epsilon transition in the interface to the items that start the productions for S. So these will be in the also in the DFA state. And thats it. We cant add any other productions here. So S is the only non terminal. And weve added all the first items initial items for S. And so that is the complete state. Okay? So just like before one possibility is that well see a B on the stack. And so that would give us the item S goes to B dot. And thats the only item valid for that state. Another possibility is that well see an S on the stack. Okay? In which case well make a transition to the state S prime goes to S dot. And S goes to S dot AS alright? So we saw that same state before in the in the other automaton. Now we could also see an A. Now what state would that take us to? And this is going to be a little different. In this state we could have the item or will have the item SA dot S and I notice that the dot is right next to S so instead of seeing an S on the set we could also see something derived from S in the next position on this stack. And so we have to throw in all the productions for S. Theres only two of them. But that means we could have the item S goes to dot SAS and S goes to dot P. Alright and then out of this state now there are a couple of different possible transitions we could see an S or we could see a B. Well if we see a B then we wind up in this state over here. And if we see an S Well whats going to happen? If we see an S then well wind up in another new state. Where we have S goes to SAS dot. Weve seen the complete right hand side of that production. Or S goes to SA.S. Actually that dots in the wrong place so lets erase that and lets put it in the right place. Its right here. Before the A not after the A. Alright and now we have to think about what happens in this state. So in this state the only possible input is an A and if it isnt A whats we going to have were going to have S goes to SA.S and then were gonna have to add the initial productions for S again. And so that would just take us back to this state and like other transition labels too we go to this state on an S and we come back to that state the bottom state here for the top state on an A. And I think if we hadnt made any mistakes that that is the complete transition system and all the states for this DFA. Now the question is is this  is this parsing automaton is it this is is this the parsing automaton of a a solar one grammar. And in order to answer that question we have to look for possible reduce reduce and shift reduce conflict. Well a quick scan of all the states here will show you or convince you that there are not. Any states where there are two possible reduce-moves. So there cant be any reduce reuse conflicts in this in this automaton. We can ignore states that only have a single item or states that have no possible reduce-moves at all. Because those are states in which there cannot be a shift-reduce conflict and that means we can ignore these two states. The two states over here at the extreme left. So now were left with these three states to think about. Alright so we look at this state last time. As before the follow of S prime Is just equal to the dollar sign. And so theres no shift reduce conflict in this state Because on on input A we can only shift. We cant reduce by S prime goes to S. All right and now were down looking at these two states. And lets just consider this bottom state first. Alright so what does this state say to do? Well this state says that well first of all observe. That the only transitions out of this state are on B and S and there are no reduced moves in this state at all so theres no possibility of a shift reduce conflict in this state either. That leaves us with just this state to think about. Now this state does have a reduced move the first item here is a is a reduction and that says that we should reduce by S goes to S A S if whatever comes next is in the follow of S so were gonna need to know whats in the follow of S. Well from S prime goes to S we know that anything thats in the follow of S prime is in the follow of S. So clearly dollar is in the follow of S. And then from this part of the grammar here we can see that A is in the follow of S. And then from this occurrence here of S we know that since it occurs at the the far right side of the production that an ything in the follow of the right hand side the left hand side non terminal is also gonna be in follow of S. Well in this case theyre the same. It just says that the follow of S is a subset of the follow of S which is trivially always true and so it doesnt add anything new. And so we wind up with just the follow of S being just two things dollar sign and A. But that poses a problem because this says that if we see an A in the input we should reduce. And this move here says that if we see an A in the input we should shift. And so this state does have a shift-reduce conflict. Alright and so this grammar is not SLR what. Welcome back. In this video were going to give a very brief introduction an overview of what were going to be talking about in semantic analysis. Lets take a moment to review where we are in our discussion with compilers. So we talked about lexical analysis and from the point of view of enforcing the language definition the main job that lexical analyses does is detect input parsing. We finished talking about that too. And again from the point of view of trying to determine whether a program is well-formed or not or whether its a valid program the job of parsing is to detect all the sentences in the language that are ill-formed or that dont have a parse string. And finally What were going to talk about now whats going to occupy us for a while is semantic analysis. And this is the last of what are called the front end phases. So if you think of lexical analysis parsing and semantic analysis as filters that progressively reject more and more input strings until finally youre left after all three phases have run with only valid programs to compile well semantic analysis is the last line of defense. Its the last one in that pipeline and its job is to catch all potential remaining errors in a program. Now you might ask yourself why do we even need a separate semantic analysis phase? And the answer to thats very simple There are there are some features of programming languages some kinds of mistakes you can make that parsings simply cant catch. Parsing well use in context free grammars is not expressive enough to describe everything that were interested in in a language definition. So some of these language constructs are not context free. And the situation here is very very similar to what it was when we switched from lexical analysis to parsing. Just like not everything could be done with a finite [inaudible]. And we wanted to have something more. Our context free grammar to describe additional features of our programming languages [inaudible] Grammars by themselves are also not enough and there some additional features beyond those that cant be easily expressed using context free constructs. So what does semantic analysis actually do? In a case of cool C it does checks of many different kinds and thats pretty typical. So heres a list of six classes of checks that are done by Cool C and lets just run through them quickly. First we want to check that all identifiers are declared and we also have to check that any scope restrictions on those identifiers are observed. Cool C compiler has to do type checking and this is actually a major function of the semantic analyzer in Cool. There are a number of restrictions that come from the object oriented nature of Cool. We have to check that the inheritance relationships between classes make sense. We dont want classes to be redefined; we only want one class definition per class. Similarly methods should only be defined once within a class. Cool has a number of reserved identifiers and we have to be careful that those arent misused. And this is pretty typical; lots of languages have some reserved identifiers with special rules that have to be followed for those identifiers. And actually this list is not even complete. There are a number of other restrictions. And well be talking about all of those in future videos. The main message here is that its medic analyzer needs to do quite a few different kinds of checks. These checks will vary with the language. The kinds of checks that cool C does are pretty typical of statically typed checked object oriented languages. But other families of languages will have different kinds of checks. Welcome back. In this video were gonna begin our discussion of semantic analysis with the topic of scope. The motivating problem for talking about scope is that we want to be able to match identifier declarations with the uses of those identifiers. We need to know which variable were talking about when we see variable X if variable X might have more than one definition in the program. And this is an important aesthetic analysis step in most programming languages including [inaudible]. So here are a couple of examples taken from cool. This definition Y this declaration Y that its a string will be matched with this used and so well know at this point here that Y is supposed to be a string and youll get some kind of an air for a compiler because youre trying to add a string and a number. In the second example Heres a declaration of Y And then in the body of the [inaudible] we we dont see any use of Y And that by itself is not an error. Its perfectly fine to declare a variable that you dont use. Although you could imagine generating a warning for that that doesnt actually cause the program to behave badly But instead what we see here is a use of X and theres no matching definitions. So the question is where is the definition of X? We cant see it And if there is no outer definition of X then well get and undefined or undeclared variable error here at this point. So these two examples illustrate the idea of scope. The scope of an identifier is that portion of a program in which the identifier is accessible. And just know that the same identifier may refer to different things and different parts of the program. And different scopes for the same name cant overlap. So whenever the variable x for example means it can only refer to one thing in an given part of the program. And identifiers can have restricted scope. There are lots of examples Im sure youre familiar with them of identifiers whose scope is less than the entire program. Most programming languages today have what is called static s cope. And cool is an example of a statically scoped language. The characteristic of static scoping is that the scope of the variable depends only on the program text not on any kind of runtime behavior. So what the program actually does at runtime doesnt matter. The scope is defined purely syntactically from the way you wrote the program. Now it may come as a surprise that there is any alternative to static scoping. In fact probably every language that you have used up to now has had static scoping But there are a few languages that are what are called dynamically scoped. And for a long time actually there was an argument about whether static scoping was better than dynamic scoping. Although today I think it is pretty clear that static scoping camp has has won this discussion But historically at least LISP was an example of a dynamically scoped language. And it has switched in the meantime. This is actually a long time ago now that it changed to static scoping. Another language which is now mostly of historical interest it isnt really used anymore called Snowball also had dynamic scoping. And the characteristic of dynamic scoping is that the scope of a variable depends on the execution behavior of the program. So lets take a look at an example of [inaudible]. So here we have some [inaudible] code and a couple of different declarations of X and also some different uses of X. Let me erase these. [inaudible] underline so I can use the color to indicate binding. So lets take a look at this definition. The question is which of these uses of x we have three uses of x actually refer to that definition. So it is in fact these two the ones that are outside of the inner let. These actually refer to this definition. So here if you refer to x you get the value zero But this other definition here. The inner definition of x is is used by this use of x. So this use of x gets this va- this meaning of x which in this case returns the value one. And whats going on here is that were using the most closely whats called the most closely nested rule. So a variable binds to the definition that is most closely enclosing it of the same name. So this x the closest enclosing definition of x is this one but for these two xs the closest and only enclosing definition of x is this outer one. So in dynamically scoped language a variable would refer to the closest binding in the execution of the program meaning the most recent binding of the variables so heres an example lets say we have a function G and G defines a variable A and heres it initialized say to four and then it calls another function Another function that isnt in the same syntactic scope. So here Ive written F right next to G but actually F could be in some completely other part of the code and F refers to A. And the question is what is the value of A here? Well. If its if we dynamically scoped then its going to be the value that was defined in G and here F of X will actually return four that will be the result of this call because this reference to A. Well refer to this binding or this definition of A and G. And we cant say much more about how dynamics how dynamics scope works until we talk in a little more detail about how languages are implemented. So well talk about dynamic scope again a little later on in the course. In Cool identifier bindings are introduced by a variety of mechanisms. Now there are class declarations which introduce class names. Method definitions which introduce method names And then there is several different ways to introduce object object identifiers. And these are the [inaudible] expressions [inaudible] parameters of functions attribute definitions in classes and finally in the branches of case expressions. Its important to understand that not all identifiers follow the most closely nested rule that we outlined before. So for example a rather rather large exception to this rule is class definitions in Cool. So class definitions cannot be nested. And in fact they are globally visibl e throughout the program. And what does that mean? That means that a class name is defined everywhere If its defined anywhere in the program that class name is available for use anywhere in the program or everywhere in the program. And in particular a class name can be used before it is defined. So as an example take a look at this fragment of cool code here. And here we see that in class foo we declare y to be of type var and then later on we declare class var. This is perfectly fine cool code. The fact that var is used before it is defined has no effect on whether the program is correct. This is a completely legal cool code Similarly with attribute names. Attribute names are global within the class in which they are defined so I that means they can be used again before they are defined. So for example I can define a class foo and I can define a method that uses attribute a and then later on only later on do I define what attribute a is and that is perfectly legal. So normally The list attribute definitions before method definitions but thats not required. A actually the method and attributory definitions can come in any order we like within a class and in-particular an attribute can be used before it is defined. Finally method names have quite complex rules. For example a method doesnt have to be defined in the class in which it is used. It could just be defined in some pairing class. And also methods can be redefined. So its possible to whats called overwriting of a method and give a method a new definition Even though it has been defined before. We dont actually have the language yet to talk about these rules with any precision but well be going into this in future videos. In this video were going to talk about simple tables an important data structure in many filers before we talk about what a simple table is I want to talk about a generic algorithm that were going to be seeing instances of over and over again for the rest of the course. So a lot of semantic analysis and in fact a lot of code generation can be expressed as a recursive descent of an abstract syntax tree. And the basic idea is that in each step we do the following three things: were always processing a particular node in the tree so if I draw a picture of the abstract syntax tree it might have a node and some sub-trees hanging off of it. And we may do some processing of the node before we do anything else. We arrive at the node say from the parents we come to here for the parent we do some processing in the node and Im just indicating that by coloring it blue to indicate that we did something here. And then we go off and we process the children. Okay. And after we process the children after we come back to the node we do something else. We may do some post processing of the node and then we return. And of course at the same time when weve gone off and processed the children then were processing all their nodes in the same pre imposed fashion so theyre getting the same treatment with some stuff being done before each node is touched and some stuff being done after all their children have been processed. Okay. And. There are many many examples of this kind of an algorithm. This is called a recursive descent traversal of a tree. There are some instances in which well only process each note before we process the children. Some where we only process each note after we process all the children. Some where we do both as illustrated here in this little diagram And returning to the main topic of this particular video. When were performing semantic analysis on a portion of the abstract syntax tree were going to know need to know which identifiers are defined. Which identifiers are in scope? An exam ple of this kind of recursive descent strategy is how we can process let bindings to track the set of variables that are in the scope. So we have our let node in the fx syntax tree and in one sub tree we have the initialization and in the other sub tree we have e the body of the let and then this is a let for some particular variable and lets just write that variable inside the parent node here. And so when we begin processing of this O just imagine that were coming from above. So were doing this. Were processing the abstract syntax tree recursively. And so we reach this point from some parent and. Theres going to be a set of symbols that are currently in scope. That thats some data structure that lives off to the side And in fact thats going to be our symbol table And what is going to happen here? Well the first were going to have to do is were going to have to process the initializer. Were going to need to know whether thats what whatever function were doing on this like type checking or whatever. We might get on and process that first And well pass the symbol table in. Okay. And then were going to process the body of the let But when we do that were going to pass in a set of symbols that are in scope. But now also X is now going to be in scope. So X is going to be added before we process E to the set of symbols. And then when we return from some expression E its going to be removed. So itll restore the symbol table to its previous state. So that after we leave this sub tree of the abstract syntax tree we only have the same set of symbols to find that we had before we entered it So in the terminology of the three part algorithm For recursive descent that we had on the first slide. What are we doing here? Well before we process E we are going to add the definition of X to our list of current definitions. Already any other definition of X that might have been visible outside of that expression. Then we are going to recurse we going to process all. Of the abstract syntax tree no des in the body of the [inaudible] inside of E and after we finish processing E we are gonna remove the definition of X and restore whatever old definition of X we had. And a symbol table is just a data structure that accomplishes these things. It tracks the current bindings of identifiers at each point in the abstract syntax tree. For a very simple simple table we could just use a stack and it would have just say the following three operations we could add a symbol. To the symbol table and that will just push the symbol push the variable onto the stack and whatever other information we want like its type. Well have a find symbol operation that will look up the current definition for a symbol. And that can be done by simply searching the the stack. Starting from the top for the first occurrence of the variable name And this will automatically take care of the hiding of all definitions. So for example If we have a stack lets say has X Y and Z on it and then we come into a scope that introduces a new Y. Y on top and now if we search the stack we find this y first effectively hiding the old definition of y and then. When we leave a scope we can remove a symbol simple popping a stack. Well just pop the current variable off of this stack. That will get rid of the most recent definition. And and leave the stack leave the set of definitions in the same state it was before we entered the node at all. So this example if we left the scope where the Y is defined and that was popped off the stack So that was gone. Now when we search for Y well find the outer definition. The one that was defined outside of that inner scope So this simple symbol table works well for let because the symbols rate at one at a time and because declarations are perfectly nested. And in fact the fact that declarations were perfectly nested is really the whole reason that we can use a stack/ So take a look at this little example lets say we have three nested lets and here Im not showing the initializers in the less sub trees and they they dont matter for what I want to illustrate. So if you think about it as we walk from the root here down to the inner bindings were pushing things on the stack well push things on the stack in the order X Y and then Z. And then as we leave after weve processed this sub tree and were leaving it walking back out were going to encounter these left scopes in exactly the reverse order. And popping them off the stack is exactly the order in which we want to remove them and thats why a stack works well. So Structure works fine for lets but for some other constructs its not quite as good as it could be so for example consider the following piece of code. Illegal piece of code I should add. Lets say were declaring a method and it has two arguments named X. Now thats not legal but in order to detect that its not legal you. Why is it not legal? Its not legal cause theyre both defined in the same scope. So Functions or methods have the property that they introduce multiple names at once into the same scope. And its not quite so easy to use a stack where we only add one thing at a time or one name at a time to model simultaneous definition in a scope. So this problem is easily solved with just a slightly fancier simple table. Here is the revised interface now with five methods instead of three. The biggest change is that now we have explicit enter and exit scope functions and so these functions start in the nested scope and exit the current scope. And the way you think about this is that our new structure is a stack of scopes so [inaudible] is the entire scope and the inner scope. Is are all the variables that are defined at the same level within that single scope. So just like before we have a find symbol operation that will look up a variable name and it will return the current definition or null if there is no definition in any scope thats currently available. Well have an add symbol Operation that adds a new symbol to the table and adds it in the curren t scope so whatever scope is at the top of our scope stack. And then one more new operation check scope. Will return true if X is already defined in the current scope So this just to be clear what this does this returns true if X is defined in exactly the top scope. It doesnt return true unless X is defined in the scope at the very very top of the stack. And this allows you to check for double definitions So for example in the code that I had before on the previous slide if we had two declarations of X. How would we check this? Well we would add X to the symbol table in the current scope. And then we would ask well is X already defined in this scope for the second one? And this interface would be return true and we would know to raise an error saying that X had been multiply defined. Finally let me just add that this is the simple table interface or something very close to this is the simple table interface that is supplied with the cool project. And theres already implementation of this interface provided if you dont want to write your own. So lets wrap up this video by talking a little bit about class names which behave quite differently from the variables introduced in let bindings and in function parameters. In particular class names can be used before they are defined as we discussed a few videos ago. And what that means is that we cant check class names in a single pass. We cant just walk over the program once. And check that every class that is used is defined because we dont know that weve seen all the definitions of the classes until we reach the very end of the program. And so there is a solution to this we have to make two passes over the program. In the first pass we gather all the class definitions we go through and we find every place where a class is defined record all of those names. And in the second pass we go through and look at the bodies of the classes and make sure they only use classes that were defined. And the lesson here this is actually not complicated to implement I think its quite clear. Should be quite clear how this will work. But the message here is that semantic analysis is going to require multiple passes and probably more than two. And in fact you should not be afraid when structuring your compiler to add lots and lots of simple passes if that makes your life easier so its better to break something up into three or four simple passes rather than to have one very very complicated pass where all the code is entangled. I think youll find it much easier to debug your compilers if youre willing to make multiple passes over the input. Welcome back in this video were going to have an introductions to types. So a very basic question to ask is what is a type anyway. And this question is worth asking because the notion of type what a type is does vary from programming language to programming language. Now roughly speaking the consensus is that a type is a set of values and also perhaps more importantly a set of operations that are unique to those values a set of operations that are defined on those values. So for example if I looked at the type of integers there are some operations that you can do on integers. You can do things like you can add. And you can subtract integers and you can compare integers whether they are greater than or equal or less than these operations are you know about numbers and then there are operations on strings And strings are a different type. They have. Operations like incantation and testing whether a string is is an empty string or not And the other various variety of functions that are defined on strings. And the important thing that these operations are different from the operations defined on integers and we dont want to mix them up. It would be bad if we started doing string operations on integers for example. We would just get nonsense. So in modern programming languages types are expressed in a number of different ways. In object orient languages. Often we see classes being the notion of type. So [inaudible] in cool the class names are the types theyre all the with one exception called self type. The class names are exactly the types. And I just wanted to point out that this need not be the case. It happens that its often convenient in designs that where the classes are not the only kinds of types or whether theyre. And in some languages [inaudible] where theres no notion of class the types are completely different things. So classes and types are really two different things that happen to be identif ied in a lot of object oriented designs. And I just want you to be aware that thats not necessarily the only way to do it. So consider the assembly language fragment add R1 R2 R3 and what does this actually do. Well it takes the contents of register R2 and the contents of register R3 it adds them together and it puts the results in register R1. And the question is what are the types of R1 R2 and R3 And you might hope that theyre integers but in fact this is a this is a trick question Because at the assembly language level I cant tell. Theres nothing that prevents R1 R2 and R3 from having arbitrary types. They could be they could be representatives of any kind of type and because theyre just a bunch of registers with zero and 1s in them the add operation will be happy to take them and add them up even if it doesnt make sense And produce a bit pattern that then stores into R1. So to make this a little clear perhaps its useful to think about a a certain operations that are legal for values of each type. So for example it make perfect example to add two integers if I have two bit patterns that represent integers then when I sum them up I would get a bit pattern that represents the sum of those two integers. But on the other hand if I take a function pointer and integer and I add them together I really dont get anything. Okay this is another the function pointer is a bit pattern. The imaginer s a bit pattern I can take those two bit patterns. I could run them through and I do get out a new set of bits. But theres no useful interpretation of that results. The resulting things I get doesnt mean anything but the problem is that these both have the same assembly language implementation. Okay nothing at the assembly language level these two operations look exactly the same. So I cant tell at the assembly language level which one of these Im doing. If I want there to be types if I want to make sure that I only do operations on the correct that I only do certain operations on on their correc t types then I need some sort of type description some sort of type system to enforce those distinctions. Perhaps Im belaboring this point but I think its important so one more time. A languages type system specifies which operations are valid for which types. Then the goal for type checking is to ensure that operations are used only only with the correct types. And by doing this type checking enforces the intended interpretation of values because nothing else is going to check. Once we get to the machine code level its all just a lot of 0s and 1s and the machine will be happy to do whatever operations we tell it to on those 0s and 1s whether or not those operations make sense. So the purpose of type systems is to enforce the intended interpretations of those bit patterns and make sure that if I have a bit pattern for integers that I dont do any non-integer operations on that and get something that is meaningless. Today programming languages fall into three different categories with respect to how they treat types. There are the statically types languages where all or almost all of the checking of types is done as part of compilation and Cool is one of these And other languages that youve probably seen like C and Java are also statically typed. Then there are the dynamically typed languages where almost all of the checking of types is done as part of program execution. And the Lisp family of languages like Scheme and Lisp itself are in this category as are languages like Python And pearl So youve probably used or heard of at least some of those languages And finally there are the un-typed languages where no type checking is done at all either at compile time or at run time. And this is basically what machine code does. So machine code has no notion of types and it forces no extraction boundaries when it executes. For decades there has been debate about the relative merits of static versus dynamic typing and without taking sides let me lay out for you what the various proponents on each side say. So the people who believe in static typing say that static checking catches many programming errors in compile time and it also avoids the overhead of runtime type checks. If I hooked on all the type checking and compiled time well I dont have to check the types at runtime. I dont have to check when I go do an operation that the arguments are of the correct type because I already that check once and for all in compile time. And these things are both definitely true. These are the two big advantages of static checking. First of all Proves that some errors can never happen those are caught at compile time so I never have to worry about those errors at run time and its faster Dynamic typing proponents counter that aesthetic type systems are restrictive. So essentially aesthetic type system has to prove that the program is well typed that all the types makes sense And it does this by restricting what kinds of programs you can write. There are some programs that are more difficult to write in an aesthetic type language because the compiler has a hard time proving them correct. And theres also a belief that I I see commonly stated that rapid prototyping is more difficult with ecstatic type system. I think the idea here is that if youre prototyping something if youre exploring some idea you may not actually know exactly what all the types are at that point in time And having to commit to something that is going to work in all cases. You know to having a type correct program when youre just trying to fiddle around and figure out what it is youre trying to do. Thats very constraining and makes the work go quite a bit slower. So whats the actual situation and practice today? Well an awful lot of code is written in [inaudible] type languages. And the practical [inaudible] type languages that people use a lot have always have some kind of escape mechanism. So in C in Java in C++ you have some notion of unsafe cast. In C an unsafe cast can just results in a runtime crash. In Java it r esults in an [inaudible] runtime when you have an unsafe or failed downcast. But the the effect is that you can get run time errors for type reasons. [sound] Now on the dynamic typing site the people who programming dynamic languages they always end up or seemed end up record fitting static typing to these dynamically typed languages. So typically if a dynamically typed language because popular enough than people trying to write optimizing compilers for them and the first thing that people want to have on an optimizing compiler. Is some insta type information because it helps to generate much better code? And so people wind up going back and trying to figure out how to get as many types as they can from these dynamically types languages as soon as they start trying to build serious tools to improve the programs written in these languages. And in my opinion its really debatable whether either compromise because both of these are compromises on the either strict static or strict dynamic point of view. But if either one of these represents the best or the worst of both worlds. But this is certainly where we are today in practice. Now Cool is a statically typed language and the types that are available in Cool are the class names so every time you define a class you define a new type and the special reserve symbol SELF&lt;u&gt;TYPE which well be talking about in a separate&lt;/u&gt; video all of its own. And the way cool works is that the user declares the types for identifiers. For every identifier you have to say what its type is But then the compiler does the rest of the work. The compiler refers the type for expressions. And in particular the compiler assigns a type to every single expression in in the program. So it will go through the entire abstract syntax string and using the declared types for identifiers it will calculate a type for every expression and sub-expression. To wrap up its worth mentioning that theres a couple of different terms people use for the process of computer types and that they mean slight ly different things. So the simpler problem is what is known as type checking. Here we have a fully typed program meaning we have an abstract syntax free with all the types filled in on every node and our only job is to check. That the types are correct so we can just look at each note and its neighbors and confirm that the types are correct in that part of the tree. And we can do this for every part of the tree and check that the Program is type correct. Type inference on the other hand is the process of filling in missing type information. So here the view is that we have an abstract syntax tree with no types on it or perhaps just a few types in key locations on say on the declared variables and then we want to fill in missing types. We have some nodes in there there with absolutely no type information at all and its not just a question of confirming or checking that the types are correct we actually have to fill in the missing type information. And these two things are different. Actually there are many languages that are actually very very different but people often use the terms interchangeably and will not be particularly careful in my videos about which term I am using either. In this video were going to talk about type checking in cool. Thus far weve seen two examples of formal notation used to specify parts of a compiler. Regular expressions were used in lexical analysis and context free grammars which we used in parsing. It turns out that theres another formalism which has gained widespread acceptance in type checking and thats logical rules of inference. If-else rules are logical statements that have the form; if some hypothesis is true then some conclusion is true. So if-else rules are implication statements that some hypothesis implies some conclusion And in the particular case of type checking an example or typical kind of reasoning that we see in type checking is that if a couple of expressions have certain types then some other expression is guaranteed to have a certain type. And so clearly that the type checking statement here is an example of an inference rule. An inference rule notation is just a compact way of encoding these kinds of if then statements. Now if you havent seen this notation before it will be unfamiliar but actually its quite easy to read with practice And well start with a very simple system and gradually add features. So well use a logical conjunction for the English word and and implication for the English word if and then. And now one special thing the string x colon t is read that x has type t. So this is logical assertion saying that x has a particular type. So now consider the following very simple type rule. If E one has type int and E two has type int then E one plus E two also has type int. And we could just take the definitions we gave on the previous slide and just gradually reduce this to a mathematical statement. So for example we can replace the if then with an implication. And we can replace the word and with a conjunction. And now we just have these has type statements alright? And we had a notation for that and we wind up with this purely mathematical statement that which says exactly the same thing. That if E1 has type int and E2 has type int that implies that E1 + E2 has type int. And notice that that statement that we just wrote out is a special case of an inference rule. Its a bunch of hypothesis conjoined together and implies some conclusion. The traditional notation for inference rules is given here. The hypotheses are written about the horizontal line and the conclusion is written below. And it means exactly the same thing as what we had on the previous slide. Mainly that if all the things above the horizontal line are true. These are all the hypotheses then the thing below. The horizontal line can be concluded to be true. And theres one piece of new notation here. This is the turnstiles that are used for the hypotheses and the conclusion. And the turnstile is read it is provable that. And what this means is that were just going to say explicitly. That something is provable in the system of rules that were defining. So the way you would read this is that if its provable that all these hypotheses are true. So if its provable the first hypothesis is true all the middle hypotheses and if its improvable if its provable the last hypothesis is true. Then it is provable that the conclusion is true And cool type rules are going to have. The following kinds of hypothesis and conclusions were going to prove within the system that some expression has a particular type. So with those definitions out of the way we actually have enough to write at least a few simple type rules. So if I integer literally if its an integer class interfering in my program then this rules says it is provable that I has type ENT. So every integer constant has type ENT. And heres the rule for add written out now in the [inaudible] rule notation. If its provable then that E1 has type int and is provable that E2 has type ENT. Then it is provable that E1 plus E2 has type ENT. So notice that these rules give templates for describing how to type integers and expressions. The rule for integer constants just use a generic integer i. It didnt give a separate rule for every possible integer and the rule for plus used expressions e one and e two. It didnt tell you what particular expressions they were. It just said give me any expression e one any expressions e one and e two that have type int. And so we can plug any expressions we want in that satisfy the hypotheses and then we can produce a complete typing for actual expressions. So as a concrete example lets show that one plus two has type ent. So we want to type the expression one plus two and since we know the rule for add that means we need to construct a proof of the type of the number one and a proof of the type of the number two. And we have a rule for dealing with integer classes mainly we can prove because one is an integer class that has type ent and we can prove that two is type ent and then now we have the two hypothesis we need for the sum expression and we can prove that one plus two has type ent. So an important property of any reasonable type system is that it be sound. And sound is here is a correctness condition. What we want is whatever the type system can prove that some expression has the type systems has a particular type T. The if I actually run that program. If I take E and I execute it on the computer the value that it returns the value that comes after running E in fact has the type predicted by the type system. So if the type system It is able to give types of things that actually reflect what kind of value you get when you run the program then we say that the type system is sound. Now clearly we only want sound rules but some sound rules are actually quite a bit better than others so for example. If I have an integer literal And I want to give it a type while we we I showed you the best possible rule before where we said that [inaudible] has type [inaudible] But it would also be correct just not very precise to say that has [inaudible] has type object. Certainly if I evaluate an integer I will get back an object because every integer in  is also an object But this isnt all that useful because now I cant do any of the integer operations And so there are lots of different sound rules theres not just one unique rule for any given [inaudible] expression that will be sound but some of them are better than others and in the case of integer literals the one we really want. Literal has type it because that is the most specific type that we can give to that type of program. In this video were gonna continue our development of Cool type checking with a discussion of type environments. Lets begin by doing some more type rules. So heres one for the constant false. So its provable that the constant false has the type [inaudible] and thats not surprising. If I have a string literal S then its provable that that has type string. And thats also not very surprising. The expression new T produces an object of type T. And the type rule for that is very straightforward. New T has type T. And were just going to ignore self type for now. As I mentioned in an earlier video well deal with self type later in a video all on its own. Here are a couple of more rules. If its provable that an expression e has type bool then an e Boolean complement of e not e also has type bool. And finally perhaps our most complex rule so far the rule for a while loop and we call that the e-1 here is the predicate of the loop this is what determines if we keep executing the loop or not and e2 is the body of the loop. And so type one is required to have type bool. It needs to be provable that e one had type bool and we allow e two the body of the loop to have an arbitrary type. It can have any type t. It has to have some type so it has to be type able under some. Rules but we dont care what the type is because the type of the entire expression is just object. We dont actually return the this expression doesnt return an interesting value doesnt produce an interesting value and to discourage people from trying to rely on it we just type the whole thing as object. And this is a little bit of a design decision. Now we could have designed a language for example where the type of a while loop is was type t. And that you would get the last value of the loop that was that was executed but the problems is that if E one the protocol loop is false and reaches the loop the first time Then you never evaluate e two and no value is produced and in that case you would get a a void value. Which if so mebody tried to dereference it would result in a run time error. Thats so to discourage programmers from lying On the loop producing a meaningful value. We could just type it as object. So far its been pretty straight forward to define reasonable type rules for every construct that weve looked at. But now we actually come to a problem. Lets say we have an expression which consists just of a single variable name and thats a perfectly valid cool expression and the question is what is the type of that variable call it X And as you can see. When were just looking at X by itself we dont have enough information to give X a type. This local structural rule does not carry any information about the type of X And stepping back one level inference rules have the property that all the information needs to be local. Everything we need to know. To carry out the function of the rule has to be present in the rule itself. There are no external data structures. Theres nothing were passing around here thats on the side. Everything has to be encoded in this rule and so far at least we just dont know Enough to say what the type of a variable should be. So the solution to this problem is just to put more information in the rules and thats what were going to do so a type environment gives types for free variables. So what is a free variable a variable is free in an expression if it is not defined within that expression. So for example in the expression X X is free. In the expression x plus y (x+y) well here this expression uses both x and y and theres no definition of either x or y in that expression so x and y are free And that expression. If I have let Y... So Im declaring a variable Y in X + Y. Well whats free in this expression well this expression uses X and Y but the use of Y is governed by a definition of Y that occurs within the expression itself. So we say here that Y is bound Y is a bound variable in this expression but X is still free so only X is free in that expression. And the ide a here is that if I have an expression with three variables and you want me to type check it you have to tell me what the types of those variables are. So I can type check X if you tell me what the type of X is. I can type check X plus Y if you tell me the types of X and Y. And I can type check this expression this line expression if you tell me the type of its one free variable X the type of Y. We will be given a declaration by the let but we still have to tell me what the type X is. So the free variables are just those variables where you have to give me the information and then I can carry out the type checking. The type environment encodes this information so a type environment is a function from object identifiers from variable names to types So let O be a type environment. One of these functions from object identifier names types. And now were going to extend the kinds of logical statements that we prove to look like this. And the way that this going to be read is that under the assumptions that variables have the types given by O. So the assumptions go over here on the left side of the turnstile. These are the assumptions that were making about the free variables in E. So the assumption that excuse me three variables. Have the types given by o is provable thats this turn style here that the expression e has type t. And so this notation very nicely separates what were assuming. This is input to our process of figuring out what the type is from what were proving. So if you tell me the types of the free variables as given by o then I can tell you the type e. The type environment has to be added to all the rules that weve gone through so far. So for example for intergal literals if I have some set of assumptions of all the types of variables that doesnt really change it doesnt in fact it doesnt change what the type is an intergal literal. Any intergal literal will still have type int. And so in this case for this particular kind of expression I we dont use any of our assumptio ns about the types of variables. Now its a little bit different with the case of sum expressions. So if I have the expression E one plus E two and I have some assumptions zero about the types of variables well then I want to prove that E one has type int and Im gonna do that using the types of the variables given by zero so E one might contain free variables and Ill need to look in zero to figure out what the types of those variables are. And similarly for E two I will type E two under the same set of assumptions. And if E1 has type int under the assumptions O and E2 has type int under the assumptions O. Well then I can conclude that E1 plus E2 has type int under the same set of assumptions O. And we can also write new rules so now our big problem with free variables becomes a very easy problem. If I want to know what the type of X is and theres a missing O here if I want to know what the type of X is I simply look it up in my object environment. So under the assumption that the variables have the types given by O what is the type of x? Well I look up in O what the type of X is assumed to be and I then can prove that X has that type T. So now lets take a look at a rule that actually does something interesting with the variables from the point of view of the environments. So here is a [inaudible] expression. And lets remind ourselves what this does. This is a [inaudible] expression that has no initialization. So it says that X is going to be a new variable. Its going to have type T0 and that variable is going to be visible in the sub expression E1. And so now how am I going to type check that? Well Im going to type check E1 in some kind of environment. And this is a new notation here so let me define what it means. So remember O is a function it maps a variable names to types and OT/X this notation here is also a function. And what this is is the function O modified at the single point X to return T. So in particular a this function this whole thing here is one function this wh ole thing Im underlining here is a function that applied to X is Returns t So that says that this sort of assumptions says that x has type t and for any other variable. So I apply it to some other variable y where x is different from y. Well then I just get whatever type y has in [inaudible]. Okay? So what this rule then says is that Im going to type check E1 in the same environment O except that at point X its going to have the type T0. So were gonna change just the type of X that have type T0 because thats the type of the new identifier thats bound at E1. And all the other types will be the same. And using those assumptions Ill try to prove that E1 has some type. I will get a type for E1. And then that will be the type of the entire let expression. Now notice something about the type environment. What this says is that before we type check E1 we need to modify Our set of assumption. Modify our type environment to include a new assumption about x then we type check e one and then of course when we leave type checking e one were going to remove that assumption about x that new assumption because outside of the let we just have the original set of assumptions though. And so I hope that that terminology and that description reminds you of something that we talked about earlier because this type environment is really implemented by the simple table. So in our rules The type environment that carries around the information that will be stored or is typically stored in the symbol table of a compiler. In this video were going to talk about sub typing another important idea in cool and other object oriented languages. Lets begin by taking a look at the typing rule for let with initialization. So last time we looked at the let rule but didnt have the initializers. Lets just see how adding the initializer right here changes things. So whats going to happen here? Well first of all notice that the body of the rule is almost the same. So We type check E1 in an environment where X has type T0. The type is declared to have in the let And all the other variables have whatever types O gives them And we can add some type T1 and thatll be the type of the whole thing. So this piece Right here is exactly the same as before. So whats new is this line of where we type check the initializer. And so how does that work? Well first of all under the assumptions o we type check e zero we get some type t zero. And this is really an aside from the main point but notice that we use the environment o in particular x. The new definition of X is not available in E0 so if E0 uses the name X that means it uses the name of some other X thats defined outside Of the lead because we didnt include a this definition of X in the environment for type checking E0. All right now but the main point a thing I want to point oh Im a sly is that easier or here has type zero which is exactly same type as X and thats a requirement of this rule it says that E0 has to have the same type as X and thats actually fairly weak of because its really a no problem if E0 has a type which is a subtype of T0 a. T zero can hold any sub-type of T zero that would be absolutely fine. But here weve limited ourselves to only allowing initializers that exactly match the type of X. So we can do better if we introduce the sub typing relation on classes. And the most obvious form of sub typing is that if X is a class and inherits directly from from Y meaning theres a line in the code that says X inherits from Y. Then x should be a sub type o f Y. And furthermore this relationship is transitive. So if x is a sub type of y and y is a sub type of z then x is a sub type of z. And finally as you might expect its also reflexive so every class is a sub type of itself. And using sub typing we can write out a better version of the let rule with initialization. So once again the body the the part of the rule that deals with the body of the let is exactly the same as before so lets not look at that and. Now what were going to do is were gonna type check E0 and we get some type T0 out and then T0 now is only required to be a sub-type of T so this here is another hypothesis. And it just says that T zero has to be a sub type of T and what is T well T is now the type that X is declared to be. So this allows E zero to have a type thats different from the type of X and the only issue here is that more programs will type check with this rule in the previous one. The previous rule that we had was certainly correct any program that compiled with that rule would run correctly but this is a more permissive and still correct rule. More programs will compile and type check correctly using this rule. Sub-typing shows up in a number of places in the cool type system. Heres the rule for assignment which is in many ways similar to the rule for let. So how does an assignment work well on the left hand side is a variable and the right hand side is an expression were gonna evaluate the expression and assign whatever value we get back. To the variable on the left-hand side And so what how is this type-checked? Well first of all we have to look up the type of X in the environment and we discovered it has some type T0 And then we type-check E1 in the same environment. So the set of variables here is not changing. And so we type-check E1 environment O and we get some type T1. And now what has to be true for this assignment to be correct? Well it has to possible for X to hold the value of type T1. So Xs type T0 has to be a super type has to be bigger than the type of T1. So if this [inaudible] Is satisfied then the assignment is correct. Another example that uses sub-typing is the rule for attribute initialization which except for the scope for identifiers is very very similar to the rule for normal assignments. So recall what a class looks like you can declare a class in Cool and it has at the top level some set of attributes and methods. And what does an attribute Definition look like. Well it looks like one of these things. Its a variable declared to some type and you can have an initializer on the right hand side. And so in what environment then is this initializer type checked? Well its type checked in this special environment O sub c which just consists of the types of all the attributes that are declared in class c. So this mean we have to make a pass over the class definition pull out all the attribute definitions all the names of the variables and their types build an environment. That [inaudible] all that information and then we can type check the initialize rs because remember the initializer for an attribute can refer to any of the initialize rs for the class. So lets take a look at how this works. First we look up the type of X in the environment. Thats sum type T O. Now we type check E1 in the same environment. Thats sum type T1. And then just as with assignment T1 needs to be a subset or a subtype of the type T O. Now we come to another interesting example how we type check If and Else. And the important thing about If and Else is that when were doing type checking we dont know which branch is going to be taken we dont know whether the program is going to execute E1 or E2 and in general actually this statement may or If this expression may execute multiple times doing a run of the program in sometimes it may execute only one other times it may execute it two. And so what that means that the resulting type of am If and Else is either the type of E1 or the type E2 and we dont. Know a compile time which one is going to be . So the best we can do. Is to say the type of entire if then else is the smallest super type larger than either e1 or e2 The need to compute an upper bound over two or more types comes up often enough that were going to give the operation a special name. Well call it the LUB or least upper bound of X and Y. And the least upper bound of X and Y is going to be Z if Z is an upper bound so meaning its bigger than both X and Y and also if it is the least among all possible upper bounds. So what this line here says is that there is some other Z prime thats bigger than X and Y. Well then z has to be smaller than z prime. So z is the least if z smallest of all the possible upper bounds of x and y. And in Cool and in most object oriented languages the least upper bound of two types is just their least common ancestor in the inheritance tree. So typically the inheritance tree is rooted at object or some similarly named class that incorporates that includes all possible classes of the program. And then theres some kind of a hierarchy which is a tree That descends from object and and if I want to find the least upper bound of two types say this type and this type I just have to walk back through the tree until I find their least common ancestor. And so in this case if I pick these two types out of my tree this would be the least upper bound of those two types. In this video were going to continue our discussion of type checking and cool with the rules for type checking methods and method calls. So heres the situation we want to type check a method call lets say that we have a dispatch on some expression easier and were calling some method named F and we have some arguments E one through E N. Well so clearly were gonna type check E zero its gonna have some type E zero and similarly were gonna type check all of the arguments and theyre gonna have some types and then the question is what is the return type of this method call what value what kind of value do we get back after we call this method? And as you can probably see were in a very similar situation here that we were in before when we were typing check the variable reference. We have this name F and we dont know anything about what it does we dont know the behavior of F is unless we have some information about Fs behavior we cant really say what kind of value it is going to return. An added wrinkle in cool is that method object identifiers live in different name spaces. That is it is possible in the same scope to have a method called foo and also an object called foo and we wont get them confused. They are different enough and used differently enough in the language that we can always tell when were talking about the object foo and when were talking about the method foo. But what this means in effect is that theres two different environments. One for objects and one for methods and so in the type rules this is going to be reflected by having a separate mapping a separate method environment thats going to record the signature of each of the methods. And a signature as is a standard name that youll probably hear used in other contexts but the signature of a function is just its input and output types. And so this table m is gonna take the name of a class. Its gonna take the name of a method in that class and is just gonna tell us what are the argument types of the methods. So all but the last type in the list here is one of the arguing types of the method and then the last type is the result type. Thats the type of the return value. So the way we are going to write the method signature is just as a tutor or a list of types the first all but the last one taken together are the are the types of the arguments in order. And then the very last one is the type of the result And so an entry like this in our method environment just means that f has a signature that looks like this. It takes in arguments with the respective types and it returns something of type t n plus one. So with the method environment added to our rules now we can write a rule for dispatch. So notice first of all that we have these two mappings one for object identifiers and one for method names on the left hand side of the turnstile. We have to propagate that method environment through all the typing for the sub expressions and for the case of method dispatch we just type The type of the expression were dispatching to e zero and all of the arguments and get types t one through t n and then we look up the type of f in the class t zero. So what class are we dispatching to? Well thats gonna be to the class of e zero And so where do we look up m in our environment. Where there better be a method called F to find in class T0 and it must have some signature with the right number of arguments. And then the actual arguments that were passing the E1 through E-N theyre types have to be sub-types of the declared formal parameter. So here the signature of F. Says that for example the first argument of f has type t one prime and so were going to require that the type of e one be some type t one such that t one is a sub type of t one prime. And similarly for all the other arguments of the method call. And if all of that checks out if that has a signature like this and all the sub type requirements on the actual arguments and the formal arguments match then were going to say that the entire expression. [inaudible] Return something of type t n plus one the return type of the method. The typing rule for static dispatch is very similar to the rule for regular dispatch. So recall that syntactically the only thing thats different is that the programmer writes the name of the class at which they wish to run the the method. So instead of running the method F as defined in the class E0 whatever that class happens to be were going to run whatever that method F happens to be in some ancestor class of the class of E0. And how is that expressed in the type rules? Well once again we type E0 and all of the arguments. And now we require that whatever the type was we discovered for E0 it has to be a sub-type of T. So T has to be an ancestor type in the class hierarchy of the type of E0. And moreover that class T had better have a method called F. That has the right number of our units with the right kind of types such that all the type constraints work out that the actual argument types are sub types of the corresponding formal argument types and then if all of that is true well be able to conclude that the entire dispatch expression has a type t n plus one which is the return type of the method. In this video were going to talk about how one takes the type checking rules and translates them into an implementation. The high level overview of cool type checking is that it can be implemented in a single traversal over the abstract syntax tree. And theres actually two phases here. Theres the top down phase in which the type environment is passed down the tree. And theres a bottom up phase in which the types are passed back up. So we start at the root of the tree with an initial type environment this type of environment is passed down recursively through the various nodes of the abstract syntax tree until we hit the leaves. And starting at the leaves we use the environment to compute the types of each sub-expression working our way back up the tree to the root. Lets start our discussion of the implementation of cool type checking with one of the simpler rules in the type system the rule for addition. And lets just briefly review what this rule says. It says that the type check E one plus E two we first have to type check E one and then we have to type check the sub expression E two. And both of those sub expressions have to have type end. And if they do then we can conclude that the overall expression the sum of the two sub expressions also has type A. And furthermore this type checking is carried out in some environment. In this case the environment is the same for the entire expression and both sub expressions. Just just to remind you theres always an object environment for the object names and scope a method environment for the methods of the various classes and we always need to know the current class. Now how will we implement this? Well we will have a recursive function called type check It takes two arguments it takes an [inaudible] environment and this will be a record Im not specifying exactly how this record is declared but it is essentially going to be three parts m o and c. And it also takes an expression and so here we are just doing the case here where the expression is E1 + E2. And what should the code look like? Well we can pretty much just read the rule and translate directly into code and this is one of the nice things about the notation for type systems is that it really tells you very very clearly how to write the implementation from the description. So whats the first thing we have to do? Well we have to type-check the sub expression E1. And we can see from the rule that the environment in which E1 is type checked is exactly the same as the environment of E1 plus E2. So we just pass whatever our original environment argument was for E1 plus E2. We pass an an argument on to a recursive call of the type check to type check the sub expression E1. And that type-checking will run and it will return some type T1 and we dont know that T1 is an integer at this point. Were gonna have to check that so we just remember what the type of E1 is. And furthermore we type check E2 okay? And that also happens in the same environment we can see that here in the rule. And again well get back some type for E2 so type T2. And then we confirm that both T1 and T2 are type integer. And we could have done a the track that T1 is is int a right away right after we had the type check T1 that would be a fine thing to do. Here just to save space on the slide I have to clip the checks for T1 and T2 a on one line. And if that check succeeds. If it doesnt succeed presumably there should be some code in here to print out an error message But if that if both T1 and T2 are in fact integers than the type of the whole expression is also an integer. So thats whats returned by this call by the outermost call here to the type check function. So now lets take a look at a somewhat more complex type checking rule and its implementation. Heres the rule for a net with initialization. So were declaring a variable x of type t. And thats going to be visible in the expression E1. But before we execute E1 were going to initialize X to the value of E0. And then after weve evaluated the entire let expression we expect to get back something of type T1 And now for all of that to work out. A few things have to be satisfied and those are listed as premises here of the rule. First of all E0 has to have some type T0 which is a subtype of T. And thats to guarantee that this initialization is correct that X can actually hold something of E0s type. And for the entire expression to have type T1 well then E1 has to have type T1. But that type checking is carried out in an environment thats extended with the declaration for X. So we so we also know within E1 that X has type T. So now lets write the type-checking case for this. So the function type check is again is gonna take an environment as argument and now were doing a case for a led with initialization. So just reading of the the rules and what the conditions are that we have to check we can see that one of the first things we have to do or one of the things we have to do is to check that E zero has some type T zero. So we just have a recursive call to type check here. This is carried out in the same environment as the overall expression. So we just pass the environment on to the recursive call And now were just type checking E zero and we record its type T zero. So the second premise is implemented like this. Now were type checking E1 and we expect it to have some type T1 but now the environment is different so were taking the original environment the overall environment of the expression and were adding a declaration that X has type T to that environment. So were extending the environment with an additional variable declaration. Okay? And so we do that type checking call and we get back a type T1. Now we have to check that T0 is a sub-type of T. So thats a thats a call to some function that implements the sub-typing relationship and if if that passes if that check passes well then were done. And we can return the type T1. And theres a little mistake here on the slide there shouldnt be a semicolon there. S o we just return T1 as the type of the entire expression. In this video were gonna talk about static typing versus dynamic typing. One way to think about the purpose of static types system is to prevent common programming errors and they do this at compile times. So they do this when the program is compiled. And in particular they do it without knowing any input to the program. So the only thing that is available is the program text and thats why we call them static Because they dont involve any of the dynamic behavior the actual execution behavior of the program. Now any type system that is correct any static type system that actually does the right thing is going to have to disallow some correct programs. It cant reason completely precisely at compile time about everything that could happen as the program runs. Now what this means is that some correct programs by that I mean some programs that would actually run correctly if you executed them are going to have to be disallowed by the type checker. And so for this reason some people argue for a dynamic type checking instead and this is type checking thats done solely when the program runs. So at run time we check whether the actual operations were executing are appropriate for the actual data that arises when the program executes. Other people say well the problem is really just that the type systems just arent expressive enough and we should work on fancier static type checking systems. And. Over time theres been a considerable development in both camps. We see a lot of new dynamically type checked languages coming out so a lot of the modern scripting like languages and domain specific languages have some form of dynamic type checking. Other people have been working on fancier and fancier type systems and actually theres been a lot of progress in static checking The disadvantage of the more expressive text. Time checking systems they do tend to get more complicated though and not all of these features that these people have develop have actually found their way yet into main stream languages. Now  one important idea that this discussion suggests is that there are two different notions of type. There is the dynamic type. That is the type that the object or the value that were talking about actually has at run time And then there is the static type which is the compile time notion what the type checker knows about the object. And there is some relationship that has to exist between the static type and the dynamic type if the static type checker is to be correct. In this relationship can be formalized by some kind of a theorem that proves something like the following what wed like to know is that for every expression E for every program expression E that you can write in the programming language the static type that the compiler says that the the expression is going to have is equal to the dynamic type of that expression. Another way is saying that if you actually run the program. Then you get something that is consistent with what you expected to get from the static type checker. That the static type checker is actually able to correctly predict what values will hap will will arise at run time. And in fact in the early days of programming languages these were exactly the kinds of terms we had for the very simple type systems in the languages at that time. Now the situations a little more complicated for a language like COOL. So lets take a look at the execution of a a typical COOL program. So heres a couple of classes class A and a class B that inherits from A. So B is going to be a subtype of A which well write like that. And now we have a declaration here of X having type A and this is the static type of X. So the static type of X is A. And thats what the compiler knows about Xs value. And then here when we execute this line of code we can see that we assign a new A object to X. And the fact that its new is not important. All thats important is the fact that its an A object. And so at this point the dynamic type of X is also A. Okay. So if this line of code when it actually execu tes A which was declared to have static type A actually holds an object of class A. But a little bit later on down at this line of code the dynamic type is actually different. The dynamic type here of X Is going to be B. K line of code executes x holds a b object even though its declared Have a different type. And this is a very very important distinction to keep in mind. So theres a static type theres a type that the compiler knows about and thats invariant. X has type A It always has type a All the uses of x for the entire scope of our typed with class A by the compiler. But at run time because we have assignments and we can assign different objects to x x can actually take on objects of different types different run time types. Type b thats assigned x when the program executes. In the last video we talked about the difference between static and dynamic typing and how one trend in static typing is towards increasingly expressive type systems. In this lecture were gonna talk about self type which will give you a taste or what those more expressive type systems can look like. To begin with lets motivate the problem that self types solves by looking at a simple class definition so here we have class counts and it has a single field I which is an integer initialized to zero and it has one method increment. And essentially the class count just increments a counter. So you initially when you allocate a new count object the counter is zero and then every time you call ink the counters value is increased by one. And notice that this can be thought of as a base class that provides counter functionality so whenever I wanted a counter for some specific purpose I get to find a new sub-class and that of count and that sub-class would automatically inherit the inc-method thereby allowing me to have counter without having to re-implement the code. In this case the code is very very small but in general you can imagine having a class that implements something tricky or requiring a lot of code and its useful to be able to reuse that in sub-classes. Now consider a sub class account that we might want to define called stock. Say were implementing a warehouse accounting program and we want to keep track of the number of items that are in stock of certain different kinds. So we define a new class stock that inherits from count and now well have a new field in here to make this Object thats classed different from its parent will just have a name that corresponds to the name of the the item thats in stock. And now down here we can actually use this. We can decla- allocate a new stock object we create a new object. We increment it To indicate that we have one thing in stock And then we assign it to some variable that weve declared of typed stock And then later on we can use this A object as we like. Now the problem is that this code actually will not type-check. There is a type error in this code. And why is that? Well lets think about it for a minute. So what is the signature of inc? So inc remember was declared to return things of type count. Right and when the inc method is inherited by the stock class this signature doesnt change it still returns things of type count. So here we have a new stock object we call the increment method But the type of this whole thing is a count and then we try to assign that to a stock but that doesnt work because count. Is not a subtype of stock a variable of type stock cant hold a value of type count and so the type system will report an error right here at the assignment statement. And you can see that this is actually a serious problem because it has made the inheritance of the increment method pretty useless. I can define new sub-classes of stock but I can never use the increment method on them at least not without getting back something of the parent type. And so its not as the inheritance of the incremental method is not as useful as one might have hoped. So just to review new stock the the incremented new stock will have dynamic type stock. Thatll actually be a stock object that is returned okay. So dont get confused here. This is the dynamic type Im talking about. So when we allocate the new stock object and then we call the increment method remember the increment method returns self so the increment method was implemented something like this. Leave out the types but it was I gets I + one and it returned the self object alright so its definitely returning whatever object is passed in here at the dispatch point so its returning something of dynamic type stock. And so this program will actually run if we didnt have type-checking. If we actually run this and it would work just fine. This would produce a dynamic stock object and would store it into the stock variable. But its not well typed because the ty pe checker loses track of the fact that this is a stock object. All it knows is that increment is declared to have return type count and which is certainly correct because every stock object is also a count object its just not useful in the context of this piece of code. And so the type checker loses information. Which make it not very pleasant to try to put the increment method in the count class to begin with. So to solve this problem were going to look at extending the type system. The insight is going to be that the increment method returns self. In this case the increment method actually returns the self object and therefore the return value is going to have the same type as self. Whatever self happens to be which could be count or it could be any sub-type of count. So the self object only has to be dynamically something that holds a value thats a sub-type of the declared type of the self parameter and so it could be any one of the sub-types in this case of the count class. And to do this were actually gonna have to introduce a new key word called self type that is gonna be used as the return of the type of the return value of such functions. And were gonna have to modify our typing rules to handle this new kind of type. So the idea behind self type is its going to allow the type to change when inc is inherited or allow us to reason about how the actual return type dynamically of increment method changes when the increment method is inherited. So we change the declaration of inc to read as follows. Weve declared the return type now to be self type meaning the return value Of the increment method has whatever type. Is the type of the original self parameter? And when we do that now we can see. That its possible. We dont we havent said how we do it but you should be able to see that it intuitively makes sense that we could prove facts of the following forms. So when the self parameter has the type count remember that the thing we dispatch to the thing we call inc on is the self para meter. So when we dispatch to account object we get something back of type count. And we call when we when we dispatch on a stock object when we call increment on a stock object well whats the type of self? The type is stock and we get back something of type stock. And now the program that we had before with this one change is well typed and would be accepted by the Cool type system. Now its very important to remember that self type is not a dynamic type it is very much a static type and part of the static type system. Its also important to realize that self type is not a class name. So unlike all the other static types in cool it is not a name of a class its its own special thing and well say more about exactly what it is in future videos but the purpose of self type as weve seen is to enable to type checker to accept more correct programs and effectively. What self type does is to increase the expressive power of the type system. In this video were gonna continue our discussion on self-type by talking about the type-level operations that are available on self-type. And this will help to clarify what self-type actually is and its role in the type system. Lets begin by thinking about the example that we discussed last time and if youve forgotten what that is let me just write it down quickly. We had a class called count and count had one. Field an integer I that was initialized to zero and it had one method called INC that returned something itself type and all it did was to increment the counter field and return the self-objects. And Ive probably made some syntax errors here but thats not really important. Thats basic code for the CAL class. And the question is what can be the dynamic type of the object thats actually returned by INC. And the answer here is it could be whatever is the type of this self-object. Whatever is the dynamic type of the self-object. And if we think about a big program where there is multiple classes that inherit from count. Then the answer is that INC could return count. Or any subclass of count. So its going to return something thats at least at most count and but it could return something more specific. The dynamic type could be something more specific it could be a sub classic count or a sub class of a sub class of count. Anything that inherits directly or indirectly from count is a possibility. So whats the general case? Well lets think about a class c. And in this class C theres some expression buried somewhere inside of it that has the type self-type. It doesnt really matter how that expression got the type self-type or what it is. Lets just say that it has that type somehow. Well what are the possible dynamic types of the expression e. And from our discussion on the previous slide it seems clear the dynamic type of e when you run e youre going to get back something thats a subtype of the class c the enclosing class in which the self-type appears. And thats interestin g because it shows us that the the meaning of self-type actually depends on the context. So what this self-type means this self-type means a subtype of the class C. If Id written self-type in a class D in the de somewhere in the definition of class D there it would mean a subtype of the class D. And so to remind ourselves what class were talking about what enclosing class were talking about were gonna subscript occurrences of self-type with a class name. So self-types of c here is going to refer to a syntactic occurrence of the keyword self-type in the body of the class c. And this also suggests a very simple typing rule. And really the first useful fact about self-type which is that self-types of c is a sub type of c. And this is really a key idea here that a self-typing class C is some sub-type of the class C because it also helps illustrate what self-type really is. The best way to think of an occurrence of self-type is that its a type variable that ranges over all the sub-classes of the class in which it appears. So self-types of C you should think of as a type variable its something that doesnt have a fixed type but is guaranteed to be some type founded by C so its gonna be only one of the class. That inherits directly or indirectly from the class c. Now that rule that self-types of c is a sub type of the class c has an important consequence. It means that when were doing type checking with self-type it is always safe always safe to replace self-types of c by c. So I say its okay to promote any self-types of c which could be c or a sub type of c to just say okay were just going to say its c. And that suggests one way to handle self-type which is just to replace all the occurrences of self-type sub C by C. Now unfortunately that turns out not to be very useful. Its sound its correct to do that but thats really just like not having self-type at all. Thats as if we went back to the example we did in the last video where we started out without self-type and we found out we couldnt use inheritance in the way we expected. So to do better than just throwing all the self-types away we need to incorporate self-type into the type system. And the way were going to do that is by looking at the operations that work on types in the type system and there are two of them. Theres the sub-type relationship that weve talked about before so when one type is a sub-type of another and theres the least upper bound operation that tells us what the smallest type is thats bigger than both of two argument types. And all we have to do and what were going to do now is extend these operations to handle the type self-type. So lets begin with a sub-type relationship and in our definition here were going to use sub-types T and T and these are just normal class names. They are class names but not self-types. So one possibility is that we have self on both sides of. Convince yourself of this. Think of self-type again as a variable. And we can plug in for that variable any sub-type of C. But just like variables in algebra if we plug in one particular class for an occurrence of this variable we have to pick the same one for every occurrence of the variable. So in particular now if we pick some sub-class A of C then we wind up with A. Is the sub type of A. If we plug in A for both sides we can see if their relationship holds similarly C is the sub type of C and for any sub type we might pick if we bind the variable to that sub type we can see that this relationship will be true. Now another thing you might think is what if the self-type sub-C is compared with self-type from another class? Say self-type sub-D. And it turns out in the cool type rules this will just never come up. The cool type rules are written in such a way that we never need to compare self-types from different classes. And I havent shown you that thats the case yet. But when we actually go through the type rules for self-type youll see that is true. Now another possibility is that we have self-type on one side and the n a regular type on the other side. So when is self-types of c a type of t? Well were going to say if thats true if C is the subtype of T. And here were using our rule that its always safe to replace self-type by the class that that index is in. So in this case C is a super-type of anything that self-type C could be. Clearly if C is a sub-type of T if T is at least C or possibly something higher in the class higher key then T would be a super-type of anything that self-types of C could stand for. Another case is when we have a regular class name on the left hand side of the sub typing relationship and self-type on the right hand side. And in this case it turns out we have to say that this relationship is false. That so T is never a regular class name is never. A sub type of self-type sub C. And to see this just think about the possibilities. So where can C and T be in the type hierarchy? So if T and C are unrelated. You know if they are inherent from object and they have nothing to do with each other. Well than clearly T cant be a sub type of self-type sub C. They are just two unregulated classes. So the only way that this could possibility work out is if they are related somehow. Now if. If T is a sub type of C well then you might think that could work out. But it turns out that we cant allow it even in that case. And heres the reason why think about a hierarchy where T has some subclass lets just say that it has a subclass A. And now because self-types of C ranges over all the possible subtypes of C we could plug in A here and T is not a subtype of A theyre in the wrong relationship. And so it doesnt work for all the possible values of subtypes of C we cant say that this is true we have to say that it is false. Now there is one very special case. Where one could argue that we should allow this to be true. And that is in the case where T is actually a leaf of the class hierarchy. And let me actually draw this a little bit differently just to emphasize this. Lets say t hat C. Is a class up here and then T you know is through some chain of inheritance relationships is a subtype of C. So its not immediate but there might be other classes in between. Just emphasize this isnt doesnt this relationship doesnt have to be immediate inheritance. It could be transitive inheritance. And now if T is a leaf. Of the hierarchy. If n is the only leaf of c if c has no other sub classes then in fact T is a subtype of SELF_TYPE sub C. Because it is the unique minimal type that is in the subtype hierarchy of C. But the problem is that this is extremely fragile and doesnt work if you modify the program. In particular a programmer might come along and add some class A over here thats unrelated to T but is also a subclass of C. And now this would no longer work. Because if I plug in A for SELF_TYPE sub C then I see that T is not a subtype of A. Right so we can allow it at a very special K. That C had only a chain of inheritance. Not a general tree under it. And that T was the least of that chain. But that is so fragile. To future program extensions. And we you know if you if you broke it by adding another class over here all of sudden you would get type errors and pieces of codes that had previously been typed check to work and hadnt changed at all. It just wouldnt be a very nice language design. So summarize T is never a sub type of self-types of C. And finally if were comparing two normal types with not self-type then we just use the rules that we gave before. So the self-typing rules we had for normal class names havent changed at all. And that covers all four cases we can have self-type on both sides we can have self-type just on the left side or just on the right side and finally we can have a subtyping relationship with no self-type at all. Now that weve seen some of the operations on self type in this video were going to talk about where self type can be used in Cool. The parser checks if self type appears only where types are permitted but thats in fact a little bit too permissive. There are places where some other types can appear but self type cannot and so the purpose of this particular video is to go over the various rules for the usage of self type. So lets begin with a very simple rule. So self type is not a class name so it cant appear in a class definition can neither be the name of the class nor the class that is inherited from. In attribute declarations the type of attribute in this case we have an attribute x and is declared to have type t it is okay for t to be self type so its fine to have attributes that are declared to be the self type of the class. Similarly its fine to have local let down variables that have type self type. And its fine to allocate a new object of type self type. And what this actually does is that it allocates an object that has the same dynamic type as the self object. So whatever the type of the self object happens to be which is not necessarily the type of the enclosing class at run time the u t operation will create a new object of that dynamic type. The type named in aesthetic dispatch cannot be self type again because it has to be an actual class name. Finally lets consider method definitions. So heres a very simple method definition. It has one formal parameter X of type T and the method returns something of type T prime. And it turns out that only T prime only the return type can be of type self-type. No argument type can be of type self-type. And to see why lets I can show it actually two different ways. Why why this has to be the case. And well do both because this is actually important. So lets think about a dispatch to this a method so lets say we have some expression e and we call method m and we have some argument e prime. And now lets say the argument e prime As the type t zero. So if you recall the rule for method calls t zero is gonna have to be a sub type of the type of the fall parameter. Were gonna be passing this in so whatever type x is declared to have here has to be a super type of the type of the actual argument. So that means that t zero is going to have to be a sub type of now lets assume that the argument can be of type self type. Some view that t zero has to be a subtype of self type this is in some class c wherever this is defined and remember that we said This was always false that you couldnt have self type on the right hand side and a regular type on the left hand side. Because that would lead to problems that would that we would never be able to prove that in general for a that that a type is actually a sub type of self type because self type can vary over all the sub types of the class C. So thats one way to see that we cant allow method parameters to be typed self type but its also helpful to just think about executing the code or some example code and see what can go wrong. So heres an example. And let me just walk you through what happens if we allow a parameter to have type self type in this example. So there are two class definitions. Class A has a method comp for comparison and it takes one argument of type self-type. And it returns a bull. So the idea here is that the comparison operation probably compares the this parameter with the argument and returns true or false. Then theres a second class B and B is a sub-type of A it inherits from A. And it has one new field B little b here of type int. And now the comparison function in class B is overridden has the same signature as the comparison function or the comp function in class A. But the the method body here accesses the field B. And now lets take a look at what happens with a piece of code that uses these two classes. So here X is going to be declared to be of type A. But were going to assign it something of type B. And here were notice that theres a gap between the static type which will be A and the dynamic type which will be B. And thats actually key to the problem. And now we Invoke the cup method on X and we pass it a new A object. And so what happens well this type checks just fine because X is in class A. X is of type A and this argument is also of type A. So if self-type if having an argument type self-type is ever going to work it has to work for this example where the two static types of the of the dispatched of this parameter and the former parameter are exactly the same. So that clearly has to be allowed if we allow self type as the type of the argument. And now lets think about what happens when it actually executes Is going to invoke the comp method in the b class okay Because X is of dynamic type B. And then its going to take the arguments and its going to access its B field. But the argument is of dynamic type A and it has no B field. And so this is actually going to cause a runtime crash. So and just to go over that one more time Just to make sure that it is clear. Here X has type A ut dynamic type B. The argument has static type A and dynamic type A and when this method gets invoked the argument That which is of dynamic type A does not have the operations all the fields and methods of the class B And results in a run time undefined behavior at run time. In this video we are going to use what we have learned so far about self-type to incorporate the self-type into the type checking rules for Cool. First lets remind ourselves what the type checking rules for Cool actually prove. So the sentences in the type logic look like this and they prove things are the form that some expression has some type and they do that under the assumption that object identifiers have some types given by O methods have signatures given by M and the enclosing class the current class in which E sits in in which we are doing our type checking is class C. And the whole reason for this additional piece here we havent actually discussed this before why we needed this C. It is because self-types meaning depends on the enclosing class. So if you recall we. Introduce this notation self type sub C to record and what class a particular occurrence of self-type sits. And this C in the environment is exactly that subscript it is tracking what class we are in. So when we see occurrences an occurrence of self-type we know what kind of self-type. Were talking about so now Im ready to actually give the type rules that use self-type. And for the most part this is really easy because the rules just remain the same. That is they look the same but theyre actually a little bit different because they use the new sub typing and least upper bound operations that we defined before. So for example here is the rule for assignment and this looks identical to the rule for assignment that we discussed several videos ago. But notice that this use of sub-typing here is now the extended definition of sub-typing to incorporate self-type. So now this rule works with self-type as well as with the normal class names. Now there are some rules that have change in the presents of self-type. And in particular the dispatch rules need to be updated. So here is the old rule for dynamic dispatch. And this rule this part of the rule actually doesnt change. It stays the same. But I just wanna point out the essential restriction in this rule is that the return type of the method could not be self-type. And thats actually the place where self-type buys us something. So the whole purpose of having self-type is to have methods whos return type is self-type. Because thats were we actually get the extra expressive power. And know we have to consider the case now that we have self typing we done all this work what if the methods return type is self type. How are we going to type check that? Well heres the rule. So as usual we type check the. The expression that were dispatching to thats E zero and all of the argument and we just get their type. And their just type checked in the same environmental as the entire expression. And now just like before we look up in class T0 the type of E0 the method F and we get its signature. Okay. And then we have to check that the arguments conform. That every actual argument E1 through En has a type thats compatible with the corresponding formal parameter in the method signature. And if all of that works out then we can say that this dispatch is going to have type oh look T0. So where did that come from? Well the return type is self-type. And so the result of this entire dispatch is going to be the type of whatever e zero was. E zero is the self-parameter. Whatever type we got for e zero that is a sound static type for the result of the entire expression. So we simply use the type of e zero as the type of the entire static the entire dynamic dispatch. Now recall the full parameters of a function cannot have type self type but the actual arguments can have type self type and the extended sub typing relationship will handle that case just fine. One interesting detail is that the dispatch expression itself could have a type self-type. And so what do I mean by that well lets think about E zero. Dispatching to method F and then what happens if E zero has type self type? What if we can prove that E zero has type self type. And the problem here is that we need to lo okay up in the in the M environment in the method environment in some class. The definition of or the signature of method F/ we have to get back that type signature so we can do the rest of that type checking. And if E0 has type soft type normally we use the type of E0 to do that to do that look up. What type do we use here? Well if this whole thing is occurring in class C. If we have if were type checking in class C. &gt;&gt; You just put the line there. &gt;&gt; And its safe and this is a soft type sub c and as always its safe to replace soft type sub c by c. So well just use the class c there. The correct class that we are type checking in to look up the method name test. We have to make similar changes to the rules for static dispatch. So here is the original rule for the static dispatch and... And again this part of the rule will not change. Uh... This... This handles the case where the return type of the method is not the self-type. But if the return type of the method is self-typed then the rule looks a little bit different. So we once again we type check the expression that were dispatching to and all the arguments in the same environment as that of the entire expression. We have to check that the class were dispatching to the type t zero is a sub type of the class named in the static dispatch. We have to look up the method. It has to exist in that class that were statically dispatching to. So we have to look up in class T the method F and get its signature. And then we have to check that the actual arguments conform to the formal parameters of their types. If the types of the arguments match the types the declared types of the formal parameters. And then the only thing thats kind of curious about this rule is that the result type here is again T zero. And why is that right? It could have been a a T. It could have been the type to which we statically dispatched. And its not because self-type is the type of the self-parameter. And even though were dispatching to a method in class T  the self-parameter still has type T0. And recall that T0 is a subtype of T. So we use the static dispatch to reach a method definition thats hidden potentially by overwrite overwritten methods in the subclasses. But that doesnt change the type of the self-parameter. The self-parameter still has type T0 even though were running a method. Of the in a superclass of G0. There two new rules for self- type. One involves the self-object. So the self-object has type self-type sub-C. And notice this is one of those places where we need to know the enclosing class. So we know what kind of self-type were referring to. And similarly theres a rule for allocating something of type self-type. So a the expression new self-type also produces something of type self-type sub-C. In this video were going to wrap up our series on type checking with a discussion of how to recover from type errors. So as with all the other front end phases like flexing and parsing its important to recover from errors that happen during type checking But unlike parsing its much much easier to recover from errors in the type checker because we already have the up stacks and text tree and so theres no need to skip over portions of the code as we did in parsing before we even knew what the structure of the program was. The problem though is what type should we assign to an expression that has no legitimate type? The type checker works by structural induction and it cant just get stuck. So if we find some sub expression that has no type that we can meaningfully give it we still have to do something with it so that we can type check all the expressions that surround it. One possibility is to simply assign the type object as the type of any ill typed expression. And the intuition here is that even if we couldnt figure out what the type or the expression was supposed to be certainly it was something that was a sub type of object. So it is certainly safe to assign any expression the type object. So lets consider what happens with this strategy in a simple piece of code. So here we have a little code fragment and we just assume here that X is undefined that actually theres a bug in this code and thats at X has no binding. So theres no type anywhere for X. So what happens when we type check this? Well were going to recursively walk down the abstract syntax tree eventually well get to the leaves and well try to type check X. And then well discover that there is no type for X anywhere and that will result in an error message Saying that X is undefined And then in order to proceed with type checking in order to recover well have to assign X a type And so well just assume that X has type object because thats our recovery strategy And now well continue to type check as we walk up the abstr act syntax tree and the next thing well try to do is to type check this plus operation. Well see that were adding something of type object to an integer. And of course plus doesnt work on things of type object so well get an error. Something like plus applied to an object. And then well have to decide now that we couldnt type check this plus what the type is of X+2 so this whole sub-expression and of course our recovery strategy is to say well that also has type object And now the next thing up in the abstract syntax tree is this initialization assignment. Here were assigning Y the result of this expression But we couldnt type check this expression so it has type object And now the type checker sees that were assigning something of type object to something of type int and we get yet a third error Saying that we have a bad assignment of some kind. So The nut of the the problem here is that this simple recovery strategy works. If we do recover we continue type checking. But a single error leads potentially to lots of other errors. So this is a workable solution. It it it achieves the goal of recovering But in general it will lead to cascading errors. Once you have on one type error that type error will just cause many more because not very many things can be done with something of type object And probably the code was written assuming some more specific type And these errors will just propagate up the abstract syntax tree until some point just result in multiple errors Another possibility is to introduce a new type a No type that is specifically for use with ill typed expressions And No type is very special. Its not a type that is available to the programmer. Its only available to the compiler and its just there for error recovery and type checking. And the special property of No type is that its going to be a sub type of every other type. So if you remember object was the opposite. Object is a super type of every type and that had the bad property that there are very few methods that defined on object and so if you plug in type object. Where you expected some other type probably its not going to type check. So we can fix that problem by introducing no type. And no type will have this special property that every operation every operation is defined for no type And furthermore well say that it produces no type as a result. So any operation in the language that takes an argument of type no type it will produce a result of type no type. So the no types will propagate. And now lets take a look at our same code fragment and lets work through what happens if we use no type. So again we walk down the abstract syntax tree we get to this leaf X we see that X is undefined we produce an error saying X is undefined. And then we have to assign a type to X so we say well X has type no type and now we consider the plus operation And now plus is taking an augmentative type no type integer And this is fine. Were not gonna produce any errors thats consider to be well type and the results is also of type no type. And now were doing an assignment And no type is compatible with N. No type is a subtype of N. So this assignment is also type correct And we dont produce any type of error in that stage either And so you can see what happens here is that no types propagate up the abstract syntax tree just like the object types did before But since no type is a special type its used only for error recovery. We can distinguish it from all the other regular types And we know that we shouldnt print out an error message after the first one is produced. So a real compiler a production compiler would use something like no type for error recovery But there is an implementation issue with no type. And in particular the fact that no type is a subtype of every other class means that the class hierarchy is no longer a tree. If you think about it you have object at the top and then you have this tree structure branching out. But then no type is a subtype of everything. So no type becomes a bottom ele ment And This is now a DAG and not a tree and that makes it just slightly harder to implement. Instead of being able to just have tree algorithms now you have to have either have a special case for no type or just do something more general. And this is just enough extra hassle that I personally dont think its worth doing for the course project and I recommend that you use the object solution and we just live with the propagating or compounding errors that that produces. In this video were going to begin our discussion of run time systems. Now at this point we have actually covered the entire front end of the compiler which consists of the three phases lexical analysis parsing and semantic analysis And these three passes or these three phases together their job is to really enforce the language semantics or the language definition. So we know. After these three phases are done that if no errors have been generated by anyone of those phases then the program is actually a valid program in the programming language that were compiling And at this point the compiler is going to be able to produce codes to produce a translation of the program that you can actually execute. And I should say that of course. Enforcing the language definition is just one purpose of the front-end. The front-end also builds the data structures that are needed to do co-generation as we seen but there is a real. Once we get through the front-end we no longer looking for errors in the program. Were no longer trying to figure out whether its a valid program. Now were really down to the point where were going to generate code And that is a job at the back end. So cogeneration is certainly part of it. The other big part of the back end is program optimization so doing transformations to improve the program. But before we can talk about either one of those things we need to talk about Runtime organization And why is that well because we need to understand what it is were trying to generate before we can talk about how we generated and have that makes sense. So first were gonna talk about what the the translator program looks like and how its organized and then well talk about algorithms and code generation algorithms were actually producing those things. And this is a well-understood area or at least some very standard techniques that are widely used and those are the ones we wanted to cover and and encourage you to use in your project. The main thing were going to cover in this sequence of videos is the management of Runtime resources and in particular Im going to be stressing the correspondence and the distinction between static and dynamic structures. So static structures are things that exist to compile time and dynamic structures those are the things that exist or happen at Runtime And this is probably the most important distinction for you to try to understand if you really want to understand how a compiler works. What happens to the compile time and what happens at run time. Having a clear separation in your mind between what is done by the compiler and what is deferred to when the target program or the generated program actually runs that is key to really understanding how compilers work. And well also be talking about storage organization. So how memory is used to store the data structures of the executing program. So lets begin at the beginning. So initially there is the operating system and the operating system is the only thing that is running on the machine and when a program is invoke. When the user says he wants to run a program what happens while the operating system is going to allocate space for the program the code for the program is going to be loaded into that space and then the operating system is going to execute a job to the entry point or the main function of the program and then your program will be off and running. So lets take a look at what the organization memory looks like very roughly when the Operating System began execution of the compiled program. So were gonna draw our pictures of memory like this. That would be just a big block and there will be a starting address at the a lower address and a higher address and this is all the memory that is allocated to your program. Now into some portion of that space goes to code for the program so the actual compiled code for the program is loaded usually at one end of the memory space allocated to the program. And then there is a bunch. Of other space that is going to be used for other things and well talk about that in a minute. Before going on I want to say a few words about this pictures of Run-time Organization because Im going to be drawing a lot of them over the next few videos. So its just traditional to have memory drawn as a rectangle with the low address at the top and the high address at the bottom. Theres nothing magic about that just a convention we could adjust it easily every verse or order of the address no big deal. And then well be drawing lines to the limit different regions of this memory showing different kinds of data and how theyre stored in the memory allocated to the program. And clearly these pictures are simplifications if this is a virtual memory system for example theres no guarantee that these data is actually laid out contiguously but it helps to understand you know what the different kinds of data are. And what the a compiler needs to do with them to have simple pictures like this. So coming back to our picture of run time organization we have some block memory and the first portion of that is occupied by the actual generated code for the program and then there was this other space and were what goes to that space? Well what goes to that space is the data for the program. So all the data is in the rest of the space and the tricky thing about code generation that the compiler is responsible for generating the code but its also responsible for orchestrating the data. So the compiler has to decide what the lay of the data is going to be and then generate code that correctly manipulates that data so there are references of course in the code. To the data and the code and data need to be designed the code and the layout of the data excuse me need to be designed together so that the generated program will function correctly. Now it turns out that this actually more than one kind of data that the compiler is going to be interested in and what well be talking about. In the next video is the different kinds of data and the different distinction between the kinds of data that go in this data area. In this video were going to being our discussions of run time structures with the notion of procedure activations. Before we begin the discussion of activations its worth being explicit that we have two overall goals in code generation. One needs to be correct to generate code that actually faithfully implements the programmers program And the second is to be efficient that that code should made good use of resources and in particular we often care that it run quickly And is very easy to solve These problems in isolation. If all we care about is correctness its not a hard problem to generate Code that is very simple but also very slow and correctly implements the program. If all we care about is speed we dont care about getting the right answer the problem is even easier. I can generate extremely fast programs that generate the wrong answer for any problem that you carry to me And so really all the complications in code generation arise from trying to solve These two problems simultaneously And what has grown up over time is fairly elaborate framework for how a code generator and the run and the corresponding run time structures should be done to achieve both of these goals okay? And the first step in talking about that is to talk about activations. Were going to make two assumptions about the kinds of programming languages for which were generating code. The first assumption is that execution is sequential. Given that we execute the statement the next statement that will be executed is easy to predict. In fact its just a function of the statement that we just executed. So controls is going to move from one point in a program to another in some well defined order. The second assumption is the one that procedure is called controllable always return to the point immediately after the call. That is if I execute a procedure f once f is done executing control will always return to the statement that followed Point where f was call And there are certainly programming languages and programming lan guage features that violate this assumption. So the most important class of programming language is it violate assumption one are ones that have concurrency. So the concurring program just because I execute one statement there is no easy way to predict what the next statement is to execute it because it might be in a completely different thread. And for assumption too Advanced control constructs things like exceptions And Calls [cough]. If you happen to know what call cc is its not important if you dont. These kinds of constructs that affect the flow of control in fairly dramatic ways can violate assumption to. So in particular if youre familiar with catch and throw style exceptions in Java and C++ when we throw an exception that exception might escape from multiple procedures before it is caught and so theres no guarantee when you call a procedure if that procedure can throw an exception that that it control whatever return to the point immediately after the procedure call. Now were gonna keep these assumptions for the rest of the class. We may later on in future videos briefly discuss how we would accommodate some of these more advanced features if the the material that were going to cover. Is basic to all implementation and even languages have concurrency and exception build upon the ideas that were going to discuss here. So first the definition When we invoke the procedure p. Were going to say that is an activation of the procedure p and the life time of an activation of p is gonna be all the steps are involved executing the procedure p and including all the steps in the procedures that p calls so its going to be all the steps in the procedures that p calls. So its going to be all the statements that are executed between the moment that p is called and the moment that p returns including all the functions and procedures that p itself calls. We could define an analogous notion of the lifetime of a variable. So the lifetime of a variable x is gonna be the portion of the execution in which x is defined That means that its all the step of execution from the time that x is first created until the time when x is destroyed or deal located and just note here that life time is a dynamic concept so this is that implies to the executing program. Were talking about the time when the variable first comes into existence until the moment in time when it goes out of existence And scope on the other hand is a static concept that go prefers to that portion of the program text in which the variable is visible. Okay so this is a very different idea from the life time of the variable and again. Its very important to keep these two times what happens at runtime and what happens in compiler time or what is associated with the static properties of the program distinct in your mind. From the assumptions that we gave a couple of slides ago we can make a simple observation and that is when a procedure P calls the procedure Q. Then Q is going to return before P returns. And what that means is that the lifetime of procedures are going to be properly nested and furthermore that means that we can illustrate or represent activation lifetimes as a tree. Lets illustrate activation with a simple example. So heres a little cool program and as usual it will begin running by executing the main method in the main class. So the first activation and the root for our activation tree for this program is the method main. And. Main is going to call the method g and so gs lifetime the set of instructions were g exist where a period of time of the execution where g existed is gonna be properly contain within the execution of this call to main. And so we can illustrate that by making g a child of main. So this indicates that effect of g is a direct child of main indicates that main calls g and also the gs lifetime is properly contained within the lifetime of main. After g returns main will call f and so f will also. The a child of of main And then F as itself is going to call G again And so its gonna have another activation of G And so G Will also be a child of f. And this tree that is actually the complete tree for this particular example illustrates the number of things. First of all as we already said it shows the containment of life time. So again for example gs life time is contained with a name but it also shows some other interesting lifetime relationships. For example the life time of this activation of g and the life time of that activation of f are completely distinct because their siblings in the tree their lifetimes do no overlap at all. And another thing to notice here is that there can be multiple occurrences of the same method in the activation tree. So every time the method is called that is a separate activation so in this particular activation tree there are two activations of g. So heres a somewhat more complicated example the involves a recursive function. Lets begin here at the at the first call. So The call to main And all main does is call F with the argument three. So there is an activation of F from Main. And then what does f do well f asks if its argument is zero and if it is that calls g while the initial argument is three so thats not going to be true on the first call to f. In otherwise it calls f with the argument minus one. So I was making note over here on the side about what the argument is because we need to keep track of that. So f is called with three clearly that is not zero and so then f is going to be called again with the argument two that will results in f being called yet another time with the argument one and finally f will be called. With the argument zero Which will then result in a call to G And so this is the activation tree for this particular program And again notice that there is gonna be multiple activation of the procedure on the same run of the program. It just indicates that the same procedure can be called multiple times and also note that the recursive procedure will result in nesting of activations of the same function within itself And so when f calls i tself and so the life time say of the second call to f is properly contained within the life time with the fist call to f. In the previous video we talked about activation but we never said what information we actually need to keep for an activation. Thats the topic of this video. An activation record is all the information thats needed to manage the execution of one procedure activation And often this is also called a frame that means exactly the same thing as activation record. These are just two names for the same thing. Now one interesting fact about procedure activations is that they have more information in them than you might expect. So in particular when a procedure f calls a procedure g the activation record for g will actually have information not only about g but very frequently also about the calling function f. So typically the activation record for a procedure will contain a mixture of information both about that procedure and about the procedure that called it. Now up this point we havent said why we need to keep information about activations at all And the reason is that there is some state associated with this procedure activation that is needed on order to properly execute the procedure and we have to track that somewhere and thats the activation record is gonna be forced. Its gonna be the whole the information needed to properly execute the procedure. So lets look at that in a little bit more detail. Lets consider this situation where a procedure F calls procedure G And what is going to happen so conceptually what happens when f calls g is that f is suspended. F is going to stop Executing while g is running. So g is going to be using the processor and all the resources of the machine. But when g completes we wanna start executing f again f is going to resume. And so in between while g is running we have to save the state of the procedure activation of f somewhere so that we can resume it properly and thats again what the activation record is for And so gs activation record Is going to have to have information in it that will help us to complete the execution of g so there will be some inform ation about g that we just need in order to run g But also gs activation record is going to have to store whatever we need to be able to resume the execution of procedure f. So lets work through an example. Heres one of the programs that we looked at in the last video and here is a design for a concrete activation record for the procedure f. So well have one position for the results of f that will hold the return value after we finished execution of f. There will be a position here for the argument to f so is it so fy takes one parameter so I only need one word here to hold the the argument to the function. There will be a control link so a pointer to the previous or the callers activation and well also have a slot for the return address so the address in memory or the address of the instruction that we are supposed to jump to after the execution of f completes. So now lets just execute this program by hand and work out what the activation records will look like down the stack. So when the front program is first invoked it will call main. There will be an activation record for main And we were not gonna worry about that. Were just gonna focus on that. So theres some stuff for main but were not going to do to to to talk about that And then main is going to call f all right And so when main calls f an activation record will be pushed onto the stack and well have four slots and or four fields for values. And what were going now while the first lines for the result well is just starting to run if its just beginning execution so there is nothing to put there at the moment. That gets filled in when f returns. The second position will hold the argument to f so that would be the number three. The third slot will hold the controlling so thats gonna point back. To the activation for main and the fourth position will hold the return address and this is actually not completely trivial because f is called in multiple places. So if you look at the program theres a called f in main and theres a call to f inside of f itself. And so Depending on where the function is called from after that function completes with one or return to a different address. In the case of the main when this called F completes we wanna return to the. Whatever instructions comes after the called f which is just gonna be something that wraps up the execution program since its the only exit point of main inside of f. Its going to be the conclusion of the conditionals. So this point double star here is going to be at whatever is left on the conditional then the return from F. And so depending on what F is called from we wanna return to one of two different places okay? So in this case F is being called from main and so well put the single star address in that position of the activation record All right? So then f is called the second time the body of f executes and the argument three is not zero thats way we wined up calling f again but that means that another activation record will be pushed on to the stack that will also help for slots as an activation record for f (I probably should label these) so thats an activation of f so its also an activation of f. And what goes in this one well again the result doesnt have anything initially in it. The argument in this case would be two. The controlling in this case will point back to the previous activation of f and the return address in this case will be the point double star. So after two calls to f this is what the stack will look like with this particular activation record design. So here is the same picture just running a bit more neatly and theres one additional we want point out which is at this stack of activation records and let me. Delineate the activation records here Is not as extract as the kinds of stacks who were probably taught about in a data structures class if youve had such a class. So here there are distinct activation records on the stack and we treat them as such in the Runtime system well treat them as such But this is also like one gigantic array. All o f this Data is just laid out in contiguous memory. These were all contiguous addresses and one is activation record here just follows on with the next address merely after the previous activation record. And compiler is compiler writer will often play tricks to exploit the facts that these activations are adjacent to each other in memory and well see one such potential trick in just a moment. To summarize some of the highlights of these examples so far I wanna repeat the main is not very interesting. So it has no argument or local variables and if results is never used And so while it does have an activation record were just not focusing on that and were not concerning ourselves with what goes in at activation record. Were just focusing on the activation record for f. Just be true this clear the start and double star that I use in the example these are addresses in memory. These are actual memory addresses and they refer to addresses of code. Those are the addresses of the instructions that come after they call f because thats the place where f would return to. And finally I want to stress that this really is only one of many possible activation record designs. You can design a different activation record for f that has had different information that would work just fine depending on the structure of the rest of the cogenerator in the runtime system. So in particular many compilers dont use the controlling because they dont need inexpensive link to be able to find the calling or the activation record of the calling procedure and in fact in your class project the Khul compiler you wont be using a control link Most. Activation records wont have the return value on the activation record because itd be more efficient and convenient to return it in the register. All right this is just one possible design and with and you could just design other activation records that will work just fine. The important thing about the activation record is that it just have to have sufficient information in it to e nable the generated code to properly execute the procedure thats being called And also to resume execution of the calling procedure. So far weve only looked at the procedure call for this activation record. We havent talked about what happens when activations return. So lets consider what happens in our example after the second called f that this one this activation down here returns. So whats going to happen is were going to make the caller the current activation so itll actually become the top of the stack so Ill have this big fat green error here indicated that this is now the current activation this one up here. Okay? So this is the call this is the what was the caller and is now going to resume executing. And the interesting thing here is to note that like I said before this isnt as abstract as a stack in a data structures course. So while we have restored this as the active procedure this data down here this this activation that was running is still there in memory. And in fact we can go and look at it if we want to. And the way I set this example up in fact we need to because the results of the procedure that we called is now stored here. In the first word of this activation All right So when f begins executing again is going to have to look up that result in order to know what the result was of the procedure levels called. So the advantage of placing the return value in the first position in the frame that the call can find it at a fix offset from its own frame. Lets back up and just see that so here when the second call to f has returned and the the first call here has resumed executing this call the code for this call will know that the science of this activation record is four. There are four words in this activation record and so they can find the result to the procedure that it called in the four + one position and five words passed the top of the frame. So in particular theyll be able to find this where in memory and even though this has been popped out of the stack as I said before that data is still there at least until another procedure is called. And so if we immediately read the result of the function call after we return from the function well be able to pick up that result and then use it in the continuation of the execution of the call in procedure. And once again I just wanna stress I know this is a couple of times but its very important that theres absolutely nothing magic about this organization. We could rearrange the order of elements in the frame. We could divide the responsibility between the caller and the calling differently And really the only metric here is that one organization is better than another if it results and faster code or in a simpler code generator And I know I also mentioned this before but its also an important point in a production compiler we would produce much of the frame contents as possible in registers And in particular there would be a effort to pass the method results and the method arguments in registers because those are excess so frequently. Finally to some up our discussion of activations and activation records the issue is that the compiler has to determine at compile time okay so this happens statically. The layout of the activation record and also has to generate code that correctly accesses the locations in that activation record. And what does this mean this mean that the activation record layout and the code generator have to be designed together. Okay. So you cant just assign your code generator and then figure out later what your activation record layout is gonna be or vice versa This two things needs to designed together because they depend on each other. In this video we are going to continue or discussion of run time organization by talking about how compilers handle global variables and heap data structures. Lets start by talking about global variables. The basic properties of the global variables that all the references point to the same object. That what it means to be global And for this reason we cant store all the variables in the activation record because the activation record if of course is the allocated. When the activation completes and that would be the allocated global variable. So. The way that little variables are implemented is that all global are signed the fix address once And these variables with fixed addresses are said to be statically allocated because theyre allocated essentially at compiled times. So the compiler decides where they going to live and then they will live there in all executions of the program And depending on the language there may be other statically allocated values and well actually see some later on but they behave just the same as global variables. So I think global variables changes our run time organization picture a little bit. First we have the code as before and then immediately after the code is typically all of the static data. So these are the global variables and other static object things that have fixed addresses for the duration of the execution of the program and then the stack comes after that. So the stack will start at the end of the static data area and grow towards the end of the programs allocated memory. Trying out to the heat any value that that outlives the procedure that creates it also cannot be stored in the activation record. Lets take a look at this example. So here we have a procedure foo and lets take a look at the activation record or frame for foo. Now lets say that a foo allocates a bar object and that were going to store that object in foo activation And now when this method returns of course the activation record would be de-allocated so the bar obj ect will also go away but that wont work here because notice that the dynamically allocated object - the object we allocated during the execution of foo - is also the results of foo so this has to be this has to be accessible to foos caller. After [inaudible] exits And so what that means is that this borrow object and all dynamically allocated data has to be stored some place other than the activation record and language is what dynamically allocated data generally use a heap for that purpose. At this point we can summarize the different kinds of data that the language of implementation has to deal with. First there is the code and in many languages I shouldnt say most. In many languages. The code is fixed size and read only. I mean that the compiler creates all the code that will be used in the execution of the program and that could be allocated once. It should say that there are many languages also were this is not true and code can be dynamically created at one time. The static area. Would contain data with six addresses and this would be things like global variables and this is also typically fixed size and it was maybe readable and writable as opposed to the code which I generally dont want to be able to write. And then a stack is used to contain an activation record for each currently active procedure and the activation record is generally fixed size so each activation record for each particular kind of procedure has a fixed size and this will contain all the local information the local variables contemporaries that needed to execute a particular activation. And finally the heap is for everything else. So the heap is just for all the data that doesnt fit into other categories. This includes all of the dynamically allocated data And if you are familiar with C then the heap in C is managed by the programmer using garbage collection actually takes care of reclaiming data from the heap that is no longer used. Now many lang uage implementations use both the heap and the stack and there is a little bit of an issue here because both the heap and the stack grow. And so we have to take care that they dont grow into each and step on each others data And there is a very nice and simple solution to this and as a start to heap and the stack at opposite ends of memory and let them grow towards each other. So lets take another look at our Runtime Organization picture And just for review first we have the code and then we have the static data. And then we have the stack which grows towards in this case the high address allocated to the program And notice that the stack doesnt necessarily just grow as procedure three terms stack will also shrink. So as the program runs the stack will get bigger or smaller depending on how many procedures are currently running. And the heap will start at the other end of memory and grow towards the lower address and so we allocate objects well be allocating from the back memory or the end of the memory allocated the program up towards the top of stack And If these two points have ever become equal and whether the two pointers. So we have a stack allocation pointer which says where we are going to allocate the next stack frame. And we have a heap allocation point where it says where will allocate the next object if we have another dynamically allocated object. As long as one of these two pointers dont cross as long as it never become equal then the program has memory to either add another stack frame or another dynamically allocated object and the program can continue away. If these programs ever become equal then the program is in fact out of memory and at that point the run time system will abort the program or try to get more memory from the operating or take some other course of action to deal with the fact. If there is no there is no more memory But as long as these two pointers dont cross notice that this design Allows the heap and the stack to share this this data area in whatever way suits the program best. So this same design without any changes will work for programs that needed a lot of heap and only a little stack and for programs that need a lot of stack and only a little heap and things will have a rough balance between stack and heap as long as they dont exceed the total memory allocated to the program. In this brief video were going to talk about alignment. A very low level but very important detail of machine architecture that every compiler writer needs to be aware of. First lets review a few properties of Contemporary machines. Currently most modern machines are either 32 or 64 bit meaning you have the 32 or 64 bits in a word and the word is actually subdivided into smaller units. We would say that there are eight bits in a bye and then four or eight byes in word depending whether its a 32 or 64 bit machine. And other important property is that machines can be either byte or word addressable. Meaning that in the native language of the machine in machine code it may be possible to either name only entire words or it may be possible to reference memory at the granule area of individual bytes. They say that data is word aligned if it begins at a word boundary. So if we think about. Data in memory or the organization in the memory and is laid out into bytes. And lets say. That this is a 32-bit machines so that four bytes make a word and one word begins here and the next word begins here and if data is allocated on a word boundary say it needs more bytes then that would be a word a line a piece of data. If a piece of data begins in the middle of the word so lets say for example that begins here and we have some data thats allocated here this data is not word aligned doesnt begin on a word boundary And the important property or the important issue is that most machines have some alignment restrictions. So these restrictions come in one of two forms. So on some machines if the data is not properly aligned meaning that you tried to reference data that isnt aligned the way the machines requires then the machine may just fail to execute that instruction. Your program may hang or even the machine may hang and its but the important thing is that program will not execute correctly. So theres a its incorrect to not have the data aligned properly. Now there are other machines that well actually al low you to put the data anywhere you like but at a significantly cause And maybe that accessing data that is aligned in word boundaries is cheaper than accessing thats on non-word boundaries And these performance penalties Are often dramatic so it can easily be ten times lower to access missile line data than to access data that has the alignment favored by that particular machine. So lets take a look at an example where data alignment issue tend to come up. One of the most common situations where we have to worry about the alignment is in the allocation of strings. So lets say we have this string the string Hello and then we want to put it in memory. So let me draw our memory as a linear sequence of bytes so Ill mark out some bytes here. And lets assume this is a 32-bit machine so let me make the word boundaries a little bit heavier boundaries. So one two three four. Okay. So there are the the word boundaries And now lets say there were we are trying to have aligned data a word aligned data and so allocate this string beginning in the word boundary. So the each character will go on the first byte when e then l then l then o. And now we may have terminating null depending on how strings are implemented. And lets assume that we do. And this is fine placement of the strings extremely begins in the word boundary and. That assess by presumably any alignment restrictions of the machine and now the question is where does the next data item go? So we could begin the next data item right in the next available byte and that would be good if we are very concerned about not wasting memory. But I noticed that that data item will then be were aligned. We may either run into correctness or performance problems if the machine has restrictions on the alignment. So the simple solution here is to simply skip to the next word boundary and allocate the next data item whenever it is on the next word beginning at the next word boundary. And what happens to this two bytes here well these bytes are just junks. T heyre not used at all they never reference by the program. It doesnt matter what theyre value is because the program should never refer to them. Its just unused memory. And note that if we didnt have the terminating zero then there would be the terminating no character then and then would be three unused bytes after the string. So to summarize this is the general strategy for dealing with alignment when you have alignment restrictions. Data begins on the boundaries typically word boundaries that are required and if the particular data that youre allocating has a none integral length. Meaning that it doesnt end directly on the next required boundary and you just skip over whenever bytes are in between to get the data the next data thats going to be allocated on the correct boundary. In this video we are going to move beyond our discussion of Run-time Organization and begin talking about code generation And in this first you know it was probably quite a long series of videos on code generation we are gonna talk about the simplest model for code generation which is called a stack machine. So in a stack machine you might guess that the primary storage is some kind of a stack and you would be right. In fact the only storage that the stack machine has is the stack And the way the stack machine works is that it executed an instruction and all instruction have the form. Theres some function of some arguments and they produce one result. And what that does is itll pop in upper hands for the stack so the arguments a1 through an are stored at the top of the stack. It will then compute the function f using those operands and it will push the result r back on top of the stack. Okay So lets take a look at a simple example. Lets see how we would compute seven plus five using a stack machine. So we would have our stack And initially the stack might have already have some stuff on it but we dont care what that stuff is and so it will execute seven plus five. What we would do well first we will have to get the seven and the five out of the stack so as we get pushed on stack and well see more about how that happens in a minute. And lets say that seven and five were both on the stack. And so now we wanted to compute the addition on seven and five so addition takes two arguments so we would pop the two arguments off the stack. And we wined up with the five and the seven Pop-up the stack. We will perform the operation plus and then the result will get push back under the stack. So this would be good to twelve and then twelve will get push back on to our stack. Okay. And I noticed that I did indicate that there might be some other stuff on this stack already. Let me give that stuff a name. And let me talk about one very important property of the stack machine. So those we have evaluated seve n+5 we round up in the situation where the results of that operation was on top of the stop of the stack. Okay and the initial stack contents was unchanged. This stack the stack that was below the arguments that we are interested in didnt get modified. Okay. So we have survived through all the operations unchanged. And this is an important property of the stack machine. That we will exploit and the general to say what the general property is when you evaluate an expression the result of the expression will be on top of the stack and the contents of the stack prior to the beginning evaluation of the expression will be preserved. So now lets take by how we could program a stack machine. So lets have a language with just two instructions in it. We can push an engine run to the stack and then we have the operation add which will add the two integers on the top of the stack. And now lets take a look at this program which pushes seven and then pushes five and then does an add. So lets think about how this program would work. Okay so we have our stack contents and now and the first instruction is to push seven. So wined up with the seven on the stack added to the stack and now we push five. Okay. And so the next step well have five and seven on top of the stack then well perform the add and then well pop these two elements off the stack and add them and push the result back on. And well wind up with twelve on the stack and again the original stack contents are preserved. Now what interesting property of stack machine code is that the location of the upper hands and result is not exquisitely stated in the instruction. And thats because these instructions always refer to the top of the stack. And this is in contrast were register machine or register instructions that explicitly name where they take their upper hands from and where they put the results. So for example you might be familiar from seeing some machine code or assembly code in the past or and add instruction by typically take three registers two for the arguments two for the registered arguments are gonna be added together and one for the destination for the result where in the stack machine we just have. A single word add and no explicit naming of the arguments because its fixed where the arguments will come from. The arguments will always be popped from the stack and the result will always be placed back on top of the stack. And. The interesting property here is that it leads to more compact programs because we have to say less in the instructions the programs themselves are actually quite a bit smaller than register machine programs. And this is one of the reason reasons that Java bytecode uses a stack evaluation model because it leads to more compact programs and especially in the early days of Java when it was very expensive to ship these programs around the Internet to download them having very small compact code was a good property. And by we might wonder why would we prefer register machine and the answer is that register machine code is generally faster because we can place the data exactly where we wanted to be. We will generally have fewer you know immediate operations and less manipulation of the stack pushing and popping stuff to get to the data that we want. And then it turns out that there isnt inter-media point between a pure stack machine and a pure register machine thats interesting. This is called an N register stack machine. And conceptually the idea of the N register stack machine is to keep the. Top end locations of the stack in registers. And the particular variant of the un-resourced stack machine that we particularly interested in is the one register stack machine because the terms that you get widely benefit by even having a single register thats dedicated to the top of the stack. This register is called the accumulator so the dedicated registry here is called the accumulator. Its called that because intuitively it accumulates the results of operations and then all the other data lives on the stack. So what is the advantage of a one register stack machine? Well lets think about the add instruction and how it works in a pure stack machine? So in the pure stack machine what is the add instruction going to do its going to pop two arguments from the stacks a five and seven. And its going to add them and then its gonna put the result back onto the stack. And lets just name the rest of the stack contents there. And that requires three memory operations. After load two arguments and then store one result. But in the one razor stack machine the add operation actually does a lot of its work out of the one register. So the one of the arguments is already stored in the register because thats the conceptually the top of the of the stack. And the result will be pushed back on the top of the stack which again is just the accumulated register. So here one of the arguments in the right are both taking from registers and theres only one memory reference to get the second argument from the portion of the stack thats stored in the memory. So in general lets think about how we would evaluate and arbitrary expression using a stack machine. So now this isnt I should say you know just stack machine called like were looking at it before. This is not just a sequence of bytecode level operations this is actually a full expression as you might find in Kuhl so there are other complex expressions nested inside of some operation. All right. And so forget the operation that takes N arguments and those arguments are expression that themselves needs to be evaluated so heres a general strategy for doing that with the stack machines. So for each of the sub-expression each of the arguments in order were going to evaluate it recursively using the same stack machine strategy and that will end up putting the result when we evaluate EI recursively the results will be in the accumulator. And so the results is in the accumulator alright. And then were going to push that results onto the memory stack. So wer e going to take that results and were gonna free up the accumulator and save it on the stack the portion of stack thats in memory okay. So we do this evaluating the sub-expressions for the first and -one arguments. So everything except the last one okay. Were gonna use the same strategy for the last one for en. We just evaluate. We dont push the result on the stack. That just means that the result is left in the accumulator okay so now we have one of the arguments of the accumulator. The last one we evaluated and the other in line as one are o the top of the portion of the stack thats in memory. So that what we all have to do is we pop in -one values from the stack and combine any compute up using the -one values plus the value of the accumulator and we store the result back into the accumulator okay. So thats the general strategy for evaluating an expression using a stack machine. So lets do this now for a simple example. Lets take our same example that weve been using and lets evaluate the expression seven plus five. So how were gonna do that? Well were evaluating a plus expression and that takes two arguments two expression as the way to evaluate each of those. So first we evaluate the expression seven. Let me actually let me draw our stack here. Okay so we have our initial content to the stack we have our initial accumulator. And so now were evaluating seven okay? And of course a constant loose evaluate to itself and the result is toward the accumulator okay? So thats the first step after evaluating seven. And now because thats the first argument to plus it has to get pushed on to the stack the portion of the stack in main memory. So. Now we have a situation that looks like this. All right in the course to seven is still in the accumulator but were now about to override it were not gonna use that value again. Because the next thing were gonna do is evaluate the second argument to plus and that happens to be in this case also a constant expression five and so that will get evaluated and then stored in the accumulator. Okay so I will override the seven. This will be five there all right? And now we have evaluated both arguments. Okay remember in the case of just having two arguments. The first argument gets evaluated and saved on the stack so it doesnt so we dont lose the value when we evaluate the second argument. And the second argument we uses is the last one we can just leave in the accumulator And that way actually evaluates the plus. Okay so we do the accumulator gets the accumulator plus the top of the memory stack. So in this case that results in adding seven and five. And we line up and of course we pop the argument from the memory stack right. So we have just the original contents there and now the value twelve in the accumulator. So as I think you would see from the example the invariant that were gonna maintain with the stack machine is that after we evaluate an expression e the accumulator holds the value of e so the result of evaluating e winds up in the accumulator and the stack is unchanged. And so the stack the memory portion of the stack is whatever it was before we start of evaluating e. And this is a very very important property expression evaluation preserves the stack. So now lets look at a more elaborated example just slightly more elaborate three+7+5. And the interesting thing about this example. Is that now one of the arguments to the other plus is itself a compound expression. So it would have to be that would have to be evaluated recursively as part of evaluating the entire expression so lets see how this works. So the first thing thats going to happen or evaluating the outer plus were gonna evaluate the first argument to that plus thats just the constant three so were gonna load it into the accumulator. And thats the result of evaluating three. And now because its the first argument to the plus we have to save it before we can get around to evaluating the addition itself. So that result is pushed on to the stack. And now were g onna evaluate the second argument to the outer plus and that itself has two arguments. And the first argument to that to the inner plus is seven. And so that winds up getting stored in the accumulator thats the result of evaluating seven. And then because the inner plus has two arguments we have to evaluate the second evaluate the second argument to the inner plus the seven has to get saved to the stack. So now the stack has seven three and whenever it had before we start it. Next were gonna evaluate the second argument to the inner plus And so evaluating a constant five will result in five being loaded in the accumulator and now we have evaluated all the arguments to the inner plus okay. And so we know from our stack discipline that the last arguments is in the accumulator and the first argument will be on top of the stack. So the next thing that will happen is that well pop that second argument from the stack added to the accumulator and store back into the accumulator and so now we have the results of the inner plus in the accumulator. We also have the pop the seven from the stack okay and finally now weve evaluated the second argument to the outer plus. So now we can perform the outer edition. And what is that involve that takes the stack contents then adds it to the value that is currently on the top of the stack which is the value three which is what we saved a long time ago now to to remember it from what it was to do the other addition and we wind up. After we pop the stack with fifteen in the accumulator thats the results of the entire expression and notice its the same stack that we started with. Okay? So evaluating this entire expression resulted in the result in any accumulator and the stack being unchanged And if you looked at that the sub-expression you can see that the same things happened. So lets take a look at the evaluation of seven plus five. So where that take place that started here. Okay. Started at this instruction. And it lasted down to here and you can see that the evaluation of seven + five which encompasses these five expressions resulted in twelve being put on top of the stack thats the result of seven + five and it didnt affect the contents Im sorry. It resulted in twelve being placed in the accumulator and it will left the stack unchanged to where it was when the evaluation of seven plus five began. So here is where it began and the value we had saved three was on the top of the stack and when were done evaluating seven plus five indeed again the value three and. All the other stuff that was there before are still on the stack. After numerous videos on run time organization and stack machines we are finally ready to begin our discussion of code generation. So as I mentioned in the previous video were going to focus on generating code for stack machines. This is probably the simplest code generation strategy. It doesnt generally yield extremely efficient code. Its an interesting strategy and certainly not totally not an unrealistic one. Its more than complex enough for our purposes. We want to run a real machine and were going to the mix processor. In particular were going to use a simulator from it which runs on about any kind of hardware so that will be very convenient for the course project And the basic idea the basic strategy is going to be to simulate a stack machine using Mipps instructions and registers. So the first decision in designing our simulation is deciding where to put the accumulator in. Well keep that in this register A0. Any register would have done but well just use A0 always for the accumulator And then the stack is going to be kept in memory And I should point out here that when we talk about a one register stack machine nominally that register in this case A0 is the top of the logical stack of the stack machine But just to avoid confusion in the terminology Im going to refer to A0 as the accumulator and the stack as all of the other data thats kept in a memory stack on the MISC processor so well just consider A0 the accumulator to be distinct from the stack which lives in memory And the stack on the MIPS will grow towards the lower addresses which is the standard convention on MIPS. The address of the next location on the stack is going to be kept in the MIPS register $sp and this register actually has a mnemonic name that stand for stack pointer. So normally on the MIPS machine compilers use SP to point to their stack and the top of the stack will always be at the address SP plus four. So remember the stack is growing towards low addresses and the address in the stack pointer is the ne xt unallocated location on the stack. So the stack pointer actually points to unused memory and the top of the stack therefore is at the next higher word address which would be SP plus four Now the MIPS architecture is quite an old architecture. It was designed in the machine. And the idea behind RISC machines was to have a relatively simple instruction set. Most of the operations used registers for operands and results. And then load and store instructions are used to move values to and from memory. So primarily all the computation takes place in registers and the memory operations are primarily are just loading and storing data. There are 32 purp- there are 32 general-purpose registers on the MITS its a 32 bit machine. Were only going to use three of those registers. We already talked about SP the stack pointer. A0 the accumulator and well need one more register for temporary values. So some operations that take two arguments like plus and times will have to have two registers to hold the arguments to the operation. So well use the accumulator for one of those and a temporary register for the other. And there is a lot more information on the MIPS architecture in the SPIM documentation. Spim is the simulator that we well use to execute MIPS code. Now of course to generate code for the mix. Well also need some mix instructions. And well be able to get away with just a very small number of instructions. Five in fact for our first example And here they are. So the first instruction we need is load or load word And the way this works is it takes the value of register two takes the contents that are in register two Adds a fixed offset. So this is a number thats directly in the code Adds a fixed offset to that to the contents of register two. Thats a memory address. It loads the value of that memory address into register one. The add instruction adds the contents of register two and register three together and stores the results in register one again. The store operation or store word operation takes the value in register one and stores it into memory. So thats stored at a memory address and with the memory address is the contents of register two plus a fixed offset thats in the code. And an add immediate unsigned takes is an unsigned add and it takes a value in register two an immediate value. So this is just a number thats a constant thats directly embedded in the code. It adds that to the value register two and stores the result in register one. And the unsigned aspect here just means that the overflow is not checked were not were not checking whether we generate a number thats beyond beyond what we could represent if we had sine numbers. Finally load immediate just takes a constant thats in the code and puts it into the register thats named as the first argument Alright? So those are the five instructions that we need to do a one very simple example. So now were ready to do our first program and not surprisingly its the same program that we looked at in previous videos when we were talking about stack machine code. So lets look heres the program for adding seven plus five written out in our little abstract stack machine language. Now our goal is to implement this program using MIPS instructions. So over here on the right Im going to layout the instructions we would use to simulate this program or implement this program on the MIPS machine Alright? So the first instruction is to load seven into the accumulator. And we can do that with a load immediate. Were going to load immediate the value seven. A0 is our accumulator register and so this instruction puts seven in the accumulator. Next instruction we want to push the value of the accumulator onto the stack. How do we do that? Well we have to store the value onto the stack and remember the stack pointer points to the next unused memory location. And so were just storing directly at what the stack pointer points to so thats at zero offset from the stack pointer. The value of the accumulator pushes the value onto the stack and now to restore the invariant. That the stack pointer points to the next unused location we have to subtract four from the stack pointer. Okay. So these two instructions together implement a push they push the data value onto the stack and they move the stack pointer to the next unused address. Alright now Im ready to do the next instruction loading five into the accumulator. Well we already know how to do that. Well be a load immediate into the accumulator register A0 the immediate value five Are now ready to do the add And how does that work? Well first we have to load the value of thats on the top of the stack alright. Because its like an argument is taken from the top of the stack. And since MIPS can only do operations out of registers that value has to go somewhere into a register. And this is where we use our temporary register. So now this value is now at offset four from the stack pointer because we subtracted four from the stack pointer And we load it into register T1. Okay And then we can actually perform the add. And so we add the accumulator to the value of T1 and we store the result back into the accumulator And finally were going to pop the stack so were done with the value thats on the stack And how do we pop? Well we just add four to the stack pointer and that moves the stack pointer back popping that value off of the stack. In the next two videos were going to be looking at code generation for a language thats higher level than a simple stack machine language weve been talking about so far. So heres a language with integers and integer operation and this was the grammar. So a program consists of a list of declarations and whats a declaration? A declaration is a function definition so it has a function name the function takes a list of arguments which are just identifiers and the function has an expression which is the body of the function And what in function bodies look like well they expressions can be integers identifiers if then else and the only predicate that were going to allow is an equality test between integers and then sums of expressions differences of expressions and function calls. Now well just say that the first function definition in the list is the entry point. This will be the main routine or the function that gets run when the program starts And this language is expressive enough to write a Fibonacci function And here it is And this is just a standard definition if X is one then the result is zero. If X is two the result is one. Otherwise its the sum of fib of X minus one and fib of X minus two. Now its a two code generation for this language. We need to generate code for each expression E; we need to produce MIPS code for each expression E that accomplishes two things. First of all that code is going to compute the value of E and leave it in the accumulator A zero. Right? So when the code for E is done the value of E will be stored in the accumulator And furthermore E is going to the code for E excuse me the generated code for E is going to preserve the stack pointer and the contents of the stack. That means whatever the stack is when we started executing E or the code for E the stack will be exactly the same after were done executing the code for E And were going to write a code generation function C-gen of E that produces code. Okay? So C-gen would be something that produces a program. It produces code that will accomplish these two things. Now our co-generation function is just going to work by cases And to begin with lets focus on the expressions and were just going to have different kind of code or a certain kind of code thats generated for each kind of expression in the language. So to evaluate an expression which is a just an integral constant all we have to do is load that constant into the accumulator. So the code generation for I for the constant I is the instruction load immediate into the accumulator the value of I And its easy to see that this preserves the stack as required so this doesnt modify the stack pointer or the contents of the stack at all so the stack is exactly the same before and after the execution of this instruction. &gt;&gt; And another thing I want to point out or I want to emphasize here is Im going to be following a convention that things that are in red are things that are done at compile time And things that are in blue are things that are going to be done in run time. So in this case at compile time we execute the function C gen of I And that produces code. Here that will run at run time okay. So C gen of I something that would execute a compile time and it produces a program that will be executed at run time And this is to help you separate in your mind and and to develop a very firm grip on the idea that we have a real division of time in these programs Theres stuff that happens inside the compiler and then theres computation thats deferred until the program that we are producing actually executes. All right now lets look at another example. Lets lets take on the addition of two expressions and think about the code that gets generated for that. So what are we going to do? Well the first thing that happens when we execute E1+E2 is that we have to compute the values of the sum expressions we have to know what integers were going to add. So we better generate code for E1. And thats going to happen at com pile time. Were definitely going to generate that code at compile time. And then once weve got the value of E1 well remember we only have one register stack machine so were going to have to save that value somewhere until we also know the value of E2 and where were going to put it. Well do what we always do; well put it on a stack. So E1 The the the code for E1 is guaranteed to leave the value of E1 and the accumulator. So what were going to store the value of E1 onto the stack. And we know how to do that. We store A0 onto the stack and then we have to bump the stack pointer. &gt;&gt; And then we can generate code for E2. Okay and again this stuff in blue is a part of the program that will be executed at at run time. These are calls to the co-generator that are happening at compile time. And so we generate the code for E2 and then that goes here after this code for pushing the value of E1 on the stack And once we have the value for E2 now we can perform the add So how do we do that? Well first we retrieve the value of E1. So we load the value of E1 Which is on the stack. And notice that. This works because E2 is guaranteed the code for E2 is guaranteed to preserve the stack. You know this code for E2 here and let me digress for a moment; this code for E2 can be arbitrarily complicated. This could be a whole program. It could go call functions. It could allocate data structures. It could print things out. It could do all kinds of complicated things But because we have this invariant that all code generation for all expressions will preserve the stack we know that no matter how complicated this is and how long it takes. When its done executing the stack will be in the same state. And thats what allows us to know. Where to find the value of E1 that we stored away Its going to be at the top of the stack Okay so we load the value B one back into a temporary register now we can do the add Okay so we add T1 and A0 together and store that back in the accumulator And now we have to pop the stack And now notice that this is all the code here for E1+E2 and when were done weve established our the value of E1+E2 is in the accumulator. That was established by this instruction. And this pop here restores the state of the stacks. Now the state of the stacks here is exactly what it was when we entered this block of code up here. Now to be completely precise I really should write this code generation function out a slightly different way And that would be like this. So what were really doing here is we are generating code for E1 and then were printing out into a file or something like that the code to do the push. Okay and then we generate the code for U2 And now these calls the code generation are also printing in to the same file okay. So here you know they just printed out the instructions whatever the instructions are like security one this is printing out the code to execute to do the push. You print out the code to do U2 And then we print out the code to do the ad and the pop Fence. Yes The add and the pop. Okay and this is just a this is much more verbose over here and so Im trying to go in and leave out the prints and just indicate in blue the instructions that are deferred but I hope you understand what this means. Everything in red here of course is being done in compile time so you know were calling these co-generation functions a compile time. The print statements are being executed in compile time and then were accumulating somewhere in some data structure or in a file all the instructions that will be executed at run time. So lets think about a possible optimization to this code. Instead of pushing the result of E1 on the stack what if we stored the result of E1 in a temporary register T1. What would the code for that look like? &gt;&gt; Well in that world to generate code for E1 plus E2 what would we do? Wed generate code for E1 and that would be followed now by instead of pushing the result on the stack we would take the result of E1 which of course is in the accumul ator A0 we would store it in a temporary register And then we would generate code for E2. Alright that we followed by the code for E2 and then we could just do the add. We would take the result of E2 which is in the accumulator A0 add it to the contents of T1 and store that into the accumulator A0 and of course theres no pushing and popping from the stack here so this code preserves the stack and it looks like anyway that it actually puts the value of E1 plus E2 into the accumulator. Unfortunately this code is incorrect so this is actually wrong and you dont want to do this And to see why lets consider what would happen. If E2 Was itself the actually lets do it for a concrete example. Lets do the example one plus two Plus three Parenthesize like that okay. So whats going to happen so E1 here so were doing one plus two plus three. So this will be a load immediate the first the code for E1 will be a load immediate into A0 of the number one. Okay and then well have the move. Well try to save that value I in temporary register T1. And now were going to generate code for E2. And whats E2? Well E2 is itself a plus expression. So were going to recursively call the code generator to generate code for two+3. So we generate code for the new first expression. So that will be a load immediate into A0 of the value two And now you should be able to see whats going to go wrong because. Since this uses the same co-generation strategy its also going to try to use T-1 to hold the temporary value. So its going to move the accumulator into T-1 thereby clobbering the value of the previous self expression that we had evaluated the number one. Okay so that values going to be overwritten and then were going to do and add  And oops I may have made a mistake were not going to do an ad let me erase that Forgot to generate the code for the three so now we load the value of three. I in to the accumulator And now we can do the add now comes the add And so we do A0 T1 A0 and when you execute this what do you get. You get two + three which is five and thats fine but now Now we have the value of this sub expression. In the accumulator and now ready to do the outer add. So that generates another add instruction. Which is exactly the same But unfortunately the first value of T1 the first temporary we tried to restore has been overwritten And so whats in that whats in T1 at this point is the value two instead of the value one and we get that one+2+3=7. Which is not what we wanted And so the problem here of course is that in the presence of nested expressions and particularly nested expressions of the same kind if the expressions try to use a fixed register for their temporary values then if you try to generate a code for two different expressions that are nested - sorry two expressions of the same kind that are nested beside each other they will step on each others temporary intermediate results And so thats why we have to use a stack to store intermediate values. So this example illustrates a couple of features of code generation that I just want to emphasize. First of all notice that the code for plus is really a template that has holes in it for the code for evaluating E1 E2 that is there are some fixed instructions that we admit And then there are places where we plug in the code for E one and the code for E two okay so thats what I mean by a template so theres some fixed stuff which are the instructions that actually do the ad and then theres a place where we can just plug directly in arbitrary code whatever it is for implementing E one and E two and well see the same pattern with all the other kinds of expressions. The other important point is that stack machine code generation is recursive. That is you know the code for E1 plus E2 is code for E1 and E2 glued together and recursively regenerate code E1 and E2 which will have their own templates and may even be other expressions of the same kind as we just saw And what this means is that code generation can be written as a recursive descent of the abstract syntax tree at least for the expressions. Alright so lets consider another new instruction. Lets add the subtraction instruction And this is just like addition instruction so sub just subtraction to register instead of adding them. And code generation then for subtraction expression as you might imagine look and awful like code generation for a plus expression. So what do we have first we have a place where we plug in the code for E1. &gt;&gt; Then we have to store the value of E1 on the stack. We have to remember that intermediate result And then we can go off and compute the value of E2. So this is where the code for E2 gets plugged in And then at the end we load the value of E1 back into a temporary register. I actually do the operation the subtraction and then pop the stack And the thing to do note about this code is that its exactly the same as the code for addition except for this instruction right here where we do a subtraction instead of an add. This video is a continuation of the previous video where well be finishing up co-generation for the simple language dealing with function calls function definitions and variable references. So just to remind you what were working on here is the simple language And again we have a bunch of different kinds of expressions And we dealt with all of these last time except for variable references and function calls And of course we also have function definitions. So as I said in the introduction these are the three constructs well be looking at in this video. &gt;&gt; The main issue in designing the co-generation for function calls and function definitions is that both of these will depend intimately on the layout of the activation record. So really co-generation for function calls co-generation for function definitions and the layout of the activation record all need to be designed together. Now for this particular language a very simple activation record will be sufficient. Because we are using a stack machine we are modeling a stack machine in our code generation. The results of a function call will always be in the accumulator and that means there is no need to store the results of the function call in the activation record And furthermore the activation record will hold the actual parameters. So when we go to computer function call with arguments X1 through XN we will push those arguments onto the stack And as it happens these are the only variables in this language that are no local or global variables other than the arguments to a function call And so those are the only variables that will need to be stored in the activation record. Now recall that the stack machine discipline guarantees that the stack pointer is preserved across function calls. So the stack pointer will be exactly the same when we exit from a function call as it was when we entered the function call And this means we wont need a control link in our activation record. The point of a control link is to help us find the previous activat ion and since the stack pointer is preserved it will have no trouble finding it when we return from our function call and well never need to look at another activation during a function call since there are no non-local variables in the language. We will however need the return address and that will need to be stored somewhere in the activation record And one more thing. It turns out that a pointer to the current activation will be useful. Now this is to the current activation not to the previous activation And this pointer will live in the register FP which stands for Frame Pointer. This is a conventional this is a this is the register name on the MIPS and the name is chosen to denote the frame pointer And by convention the compilers put the frame pointer there. What the frame pointer is good for well it points to the current frame so thats what the name comes from. But what its good for well see in a few minutes. Right so to summarize for this language an activation record that has the callers frame pointer The actual parameters and the return address will be sufficient. So lets consider a call to the function F and has two arguments X and Y. Then at the time the call is performed before we start executing the body of the function this is what the activation record will look like So well have the old frame pointer. So this is the frame pointer that points to the callers frame. Not to the frame of the function that were executing And the reason that it does that is that we have to save it somewhere because the frame pointer register will be overwritten with the frame pointer for the current activation so we have to save the old one so that we can restart the caller when we return to it from the current function. And then there the arguments of the function and those that are pushed on the stack in reverse order. So the last argument is pushed on first and the first argument is at the top of the stack And the reason for doing it this way is itll make the indexing to find the a rguments a little bit easier. A little bit simpler And then We have the stack pointer so theres a theres nothing here. What will go here is the callee the function that were calling will push on the return address. So this is where the return address will go And these elements the callers frame pointer the arguments to the function and the return address of the call function will make up the activation record of F. A bit of terminology the calling sequence is the sequence of instructions that both the caller and callee to set up a function invocation okay? So thats referred to in compiler lingo as the calling sequence And were going to need a new instruction to show the calling sequence for this for for function calls. And that will be the jump and link instruction. So jump and link what it does is it jumps to the label that its given as an argument And it saves the address of the next instruction after the jump in link in the register R.A. Which stands for return address. So what would happen in the jump in link instructions if I have jump in link to label L And then theres an add instruction that comes next. I dont know what it is. Its the address of this instruction the one after the jump in the link that will be stored in the ret- in the in the register RA. So this instruction will jump to L. It will store the address of this add instruction in RAb And it will execute whatever code is at L. And then the code thats at L can execute a jump back to the address in here to execute the return to the caller. So now were ready to actually generate code for a function call expression. So lets say we have the call F of E1 To EN Where of course E1 through EN are expressions. And let me change colors here. So these are expressions here not values. So how are we going to do that? &gt;&gt; Well the first thing were going to do is were going to start building the activation record And so we save the current frame pointer. This is the frame pointer for the collar. &gt;&gt; Okay. &gt;&gt; This is pointing to th e collars frame. &gt;&gt; Right &gt;&gt; And we store that at the stack pointer. We have to bump the stack pointer. And then we generate code for the last argument for EN right? And so that code gets inserted here And then we push it on the stack. So we store the results of EN which will be in the accumulator A0 on the stack and then we bump the stack pointer. Alright and well do that for all the arguments finishing up with E-1. So we generate code for E-1 and we push it onto the stack. So now all the arguments are on the stack and now we just do the jump in link. So weve done as much of the work or much of the calling sequence as we can do on the callers side. So this code is executing in the function in the caller. Okay so this is the caller side of the calling sequence and it builds up as much of the activation record as it can. In particular its evaluating the actual parameters and pushing them on to the stack to form part of the activation record for the called function and then we do the jump and link. And we jump to the entry point of the function that were calling. So were this is a call to to F and so we jump to Fs entry point. So a few more things to note First of all as we discussed on the previous slide When we execute the jump in link instruction that is going to save the return address in the register RA And that address will be this address here the one that comes after the the address of the next instruction after the jump in link instruction And youll notice also that the activation record weve built so far is four times N plus four bytes. So this is where N here is the number of arguments. Each argument takes up four bytes and then four bytes for the old frame pointer. Now were ready to talk about the callee side of the calling sequence And were going to need one new instruction for that. The JR instruction stands for jump register. And it just jumps to the address in its register argument. So now the callee side is the code for the function definition okay? So this is the co de that actually executes the body of the function. And how do we generate code for that? Well lets take a look. Now actually the very first thing that should be here is that this first instruction of the call side is the entry point. So were missing the label here So this would be labeled F entry. Okay So this is the target of the jump in link instruction. And then the very first thing we do is we set up the frame pointer. So we copy the current value of the stack pointer into the frame pointer. That sets that points to the end of the frame for the call-ee for the new function thats being executed. We also save the return address at the current position on the stacks. Remember there was one more thing to do one thing one thing that was missing. On the caller side on the caller side of the sequences which is the return address. We dont know the return address until after the jumping link instructions executes And so the callee is the one that has to save that value. Okay so after the jumping link the RA register contains the return address and that we save it into the frame. All right and then we push the stack pointer. this point the activation record is completely set up and now we can just generate code for the function body. And after the function body executes of course the stack pointer will be preserved and and that means that the return address will be at four offset from the stack pointer so we can load the return address back into the return address register And then we can pop the stack So here were going to pop off The current frame from the stack And thats going to be song size z. Which we I havent shown you what it is yet But well calculate The size of z in just a minute? This is going to be an immediate value. So its a constant that we plug in there And then we load the old frame pointer. Okay So once weve incremented the stacks we popped off the existing frame and so now were pointing at the frame pointer at the first were were were pointing at the first thing beyond the previous stack frame and what was that well that was the first thing that we saved in the stack frame for F and thats the old frame pointer. So now we restore the old frame pointer so that the call the function that called us well have its frame pointer back and then now were ready to return it resume execution of the calling function. We just do that by a jump register to the return address All right? So note here that the frame pointer points to the top of the frame not the bottom of the frame. Okay? So that will actually be important when we talk about how we use the frame pointer When we get to talking about the variable references next And the callee pops the return address The actual arguments in the saved value of the frame pointer from the stacks. So the callee pops off the entire activation record and also restores the callers frame pointer And whats the value of Z? Well there are N arguments. Each of which take up four bytes So theres at so the size of the activation record is four times N. Plus there are two other values. In the activation record One is the return address. And the other one is the old frame pointer. Okay and the space for two more words is eight bytes. So thats the size of the activation record. So thats how much we have to add to the stack pointer to pop the activation record for F off the stack. Just to give you a sketch of what this looks like before the call. We have the frame pointer for the caller and we have the The current value of the stack pointer And on entry to the function. Okay after the calling after the calling functions side of the calling sequence has completed whats on the stack well we have the old frame pointer and the two arguments and then the stack pointer points to the next unused location. Which is where the return address will go Alright then we do the jump and link. We jump over and the return address gets pushed on to the stack a nd the frame pointer gets moved to point two the current value of the frame. Okay youve got to point to the top of the frame. Okay? And then after the call what has happened? Well weve popped everything off the stack weve popped the entire. Your activation record for the call function off of the stack And so now notice that were back in the same state. So again function calls have to preserve the invariant that. The stack is preserved across the call so the stack should be exactly the same after the call as it was on entry to the call. So we are almost done with code generation for simple language. The last construct we need to talk about is how we generate code for variable references. Now the variables of a function again are just its arguments just the parameters to the function. There are no other kinds of variables in this simple language And these variables are all in the activation record. So we really all we have to do is generate code to look up a variable in its appropriate place in the activation record But there is one problem and thats that the stack does grow and shrink with intermediate values. So when you call a function and you begin executing its body values will be popped and pushed onto the stack beside the activation record. So think back to the code generation for plus and minus and if then else intermediate values were being pushed and popped from the stack And so what this means is that these variables that are in the activation record are not at a fixed offset From the stack pointer. So we cant use the stack point very easily to the side or to find those variables. So the solution is to use the frame pointer. The frame pointer always points to the return address in the activation record and because it doesnt move during the execution of the function body we can always find the variables at the same place relative to the frame pointer. So how do we do that? Well lets consider the i-th argument X of I and does the i-th argument to the to the function. So where is that going to be relative to the frame pointer? That will be at offset Z from the frame pointer And Z is just four times I. Right and this is actually the reason here for generating for pushing the arguments on the stack in reverse order starting with the last argument to the function because it just makes this index calculation simple. It wouldnt be that much more complicated if we pushed the arguments in the other order. It just makes it a little easier to see how the indexing works And anyway this index this offset is being calculated at compile time. So notice that this number this four times I is something that the compiler knows and what were putting in the code here is just a fixed offset. So we are not actually doing that multiplication at run time. See here is just a number as computed statically by the compiler. So anyway We just load and off send Z which is the four times I where I is the index the position of the variable in the list of parameters. At that offset from the frame pointer thats where XI is stored in the activation record And we just load it into the accumulator. So that is the entire code generation for a variable reference. Heres a little example. So for the function the hypothetical function that weve been looking at with two parameters x and y. X is going to be at the frame pointer +four and y will be at the frame pointer +eight. In this video were going to generate code for a small example program. The program well take a look at takes a positive imaginary X and sums all the numbers from O up to X. So if X is O then the result is O. Otherwise it is X plus the sum of all the numbers up to X minus one. So this isnt a interesting program but it does illustration all of the features that we discuss in the previous couple of videos. So lets dive right in and talk about how were going to generate code for sum two. So we begin by giving it a label for the entry point to the function so thatll be the sum two entries. Alright and now we have to generate code for the callers side call callee side excuse me of the calling sequence. So what was that? So the first thing we have to do is we have to set up the frame pointer which would just be the value of the stack pointer. So thats the frame pointer for this activation and. Then were going to have to store the return address at the current value of the stack pointer. And then were going to move the stack pointer into the - Whenever we store something on the stack we have to move the stack pointer to the next unused location.  Alright. Okay And so now we have to generate code for this if then else All right? And the very first thing if you go back and look at the code for if then else is to generate code for the first sub expression of the predicate. So were going to generate code for X and thats really easy And were generating code for a variable just looks up the variable in the current position of the frame. Sorry at the correct offset from the frame pointer alright? Alright so once we do that now we are generating code for the predicate And how do we do that? Well we generate code for this first sub expression and now we have to save that sub expression somewhere Because we are going to generate code for another sub expression. So the equality there is a binary operator so we have to save the value we just computed somewhere on the stack Alright? So well do that so well st ore the value of a zero on the stack. And that will involve as always moving the side pointer. Okay and now we generate code for the second sub-expression of the predicate. All right thats also easy. Thats just load the immediate of the immediate value into the accumulator alright. And now Im going to load the value that we said the first or we move the predicate back into a temporary register and actually do the comparison. So this is more code as actually part of the conditional alright so we do a load word Entity one Of the value that we saved before. Okay and now we need to pop the stack okay. Well do that here because were done with that value. Alright and now were going to do the branch. So now we test whether. The two sub-expressions of the predicate are equal or not and if they are then we jump to the true branch. And here Im going to give the true branch a unique label because this might be part of a larger program where there are many if-then-elses and so Im going to append some identifying number on the end. Instead of writing out true branch Ill call this true one Alright? Okay and then if we fall through then were on the false branch well call that false one And now were generating code for the false branch which is this summation here Alright? And how are we going to do that? Well this whole thing is a plus expression which means we have the generic code first. For the first sub-expression which is just X. Alright? So what do we do? Well we load. To generate code for x we look up x at its current offset. And that is the appropriate offset in the frame using the frame pointer. Okay? It is the only argument and so its at four from the frame pointer. Im sorry the only argument to the procedure and so thats stored at the first position for arguments which is always four from the frame pointer in our scheme. All right and now that weve loaded it we have to save it because it is part of a binary operation so were going to save that value on the stack. Kay. And now we will adjust the stack pointer. Okay. And what are we going to do next? Well now weve weve we computed this sub-expression this X. We cant do the plus yet until we compute the second sub-expression which is the function call Alright? So now we have the generate code for the function call and Im going to move up here to the other side of the screen here to to show the rest of the code. Okay And the first thing we do to generate code for the function call Is to start setting up our activation record Alright? This is even setting up the new activation record for the function call that were about to make Alright? So what do we do there? We store The frame pointer. Kay use this to our old frame pointer. Add the stack on the stack. [sound] Alright and now we have to compute the argument All right? We have to compute the x-1. So that code gets inserted here in the template for our function call. So whats going to happen there? Well were completing subtraction so the template for subtraction is to first generate code for the first sub-expression then generate code for the second sube-xpression and then subtract them. All right so lets do that. So first we generate code for x again. Okay and now since its the first argument of a binary operation were going to save it on stack. Alright now we generate code for the second argument of the subtraction. Okay and now we perform the subtractions so we have to load the first argument back into a temporary register. Have to actually do the subtraction. Excuse me here. Alright and then we can pop the temporary value off the stack. Okay now we have actually done subtraction. Let me see that. There is everything from about here to down there is computing x minus y. Okay... So this is computing x And this was computing one And then this whole thing is computing the subtraction Alright? So now we compute the argument. What are we going to do? Well we save it on the stack. So now we save the result on the stack. Were saving it into the new activat ion record that were building Alright? And then we have to advance this or move the stack pointers as always And now were ready we have to do the function calls And now we do the jump in the link to the entry point of sum two Okay? And now when this returns what its going to return with its going to return what the result of computing the sum to in the accumulator all right? And so then were ready to perform the addition And now weve computed the second argument to the addition and how do we do that? Well look back at the template for addition the next thing what happens is we reload the temporary value that we saved on the stack. Alright and now we got actually perform the edition. Okay? And then we could pop the temporary value of the stack Alright And that actually ends the the else branch the false branch of the entire if and else And theres now a branch around the rest of the if and else code And well call that label if and one And now comes the code for the true branch. And what we are going to put there well its not very complicated because all were doing true branch is loading or generating codes for zero which is a single load immediate load immediate Alright And thats the entire true branch and so now were at um there should not be a colon there excuse me and in fact I can just erase that a little bit Alright And now were at and actually I see it notice in the wrong place so lets fix that so this is a branch at the end of the false branch at the end of the else part of the if and were going to to branch around per quote for the two branch which is only one instruction. And so the very next instruction is the label end if. So now whats left to do weve generated code for the whole if then else so now it goes here is the rest of the template for the function definition so now we have to generate the code returns back to the caller and how do we do well we have to load the return address The on the stack okay? And now we pop the stacks so we pop the entire activation record off the stack and now because of the activation reco rd well remember theres always two words. One for the return address and one for the frame pointer and then a number of words equals to the number of arguments where theres only one argument here so we have three words so its twelve bytes. So we increment the stack pointer by twelve all right? And then we load the old frame pointer we store the frame pointer. Okay and then we return. So one more instruction well do a jump register to the return address And that is the entire code for this simple functions sum2 And theres a couple of things to point out. So first of all the the code is constructed as a bunch of templates pasted together and I try to point out as we go along how that works But we do lined up with one linear sequence of code. Alright and if if youre all confused as we work as to go back and look at those templates and look at this example and understand how the code all fits together and how it works. And the other thing I would point out is just that this is your extremely inefficient code so later here where we were generating code to check whether x=0. Notice here that we we load x so this is a load Of x And then we immediately store the x again into the stacks we just loaded it now from the frame then we immediately store it back in the memory and then we and load the immediate value then we reload the value of x here. So you know moving the value of x we you know all around. So we load it we store it we load it again and this was a lot of wasted motion here and thats a result of this very simple cogeneration strategy where we want to be able to compose code together. We will be able to compose these templates in a way that it will work properly. This code does not have to be this inefficient in a lot of the techniques of what we discussed in sub-sequential lectures we talked about in a smarter code generation techniques and also optimizations like even improve the code further. In the last couple of videos we have talked about code generation for simple programming language and I mentioned at the end of the last video that realistic compilers do things a bit differently and in particular they do a better job of keeping values and registers and of managing the temporaries that have to be stored in the activation record. Were actually going to talk about both of those problems. In this particular video were only going to talk about the second one and so were going to be covering a better ways for compilers to manage temporary values. So the biggest idea which weve already seen is to keep temporaries in the activation record. Now this is not as efficient as keeping temporaries in registers but thats the subject of a future video were not going to talk about that today. What were going to talk about is improving the language we manage temporaries that happened to be in the activation record for whatever reasons. So why it doesnt matter why we want them to be into activation record but given that its there thats the most efficient code that we can generate And the improvement that were going to make Is have the co-generator assign a fixed location In the activation record for each temporaries. Were going to pre-allocate memory or a spot in the activation record for each temporary and then we will be able to save and restore the temporary without having to do the stack pointer manipulations. So lets take a look at the [inaudible] program for a simple programming language. Here is the Fibonacci function again and let me change colors to something that says more contrast and lets think about how many temporaries we need to evaluate this functions. So this function body when it executes well need a certain number of temporaries and if we know how many temporaries that needs in advance then we could allocate the space for those in the activation record rather having to do push and pop pushing and popping from the stack at runtime. So lets take a look and if then else is going t o involve a temporary because it always do this predicate comparison here were going to have to evaluate the the first argument to the predicate and then save the result of that while we evaluate the second argument to the predicate. So this one involve one temporary well need one temporary for that predicate. Similarly for this predicate to evaluate it since its a two argument operation in comparison well also need one temporary for that. 1010. Theres this expression over here which is kind of complicated. How many temporaries will we need for these? Well remember how this works. So evaluate the first expression and then we save the results of that so this will require one temporary for the result of the called fib going to have to be saved and only evaluate the plus And while we are evaluating the call the fib though is actually before we evaluate to call the fib we have to evaluate the argument of fib and that involve the subtractions. We also need one temporary here for the subtraction. Okay And now we have about the second side of the this edition here. Well this also involves a subtraction. Okay So we got to have one temporary here to hold on to the value x while were evaluating the minus to compute the value of the argument before we call here for the predicate but notice that once the predicate is decided once we know the answer to whether this predicate is true or false we dont need that temporary anymore. So in fact that temporary can be reclaimed; we dont need the space for that temporary anymore by the time we get to the false branch. And again once this predicate is evaluated we dont need the space for that temporary anymore okay? So now were down to the plus. The first thing that happens is we evaluate the argument to this first call the fib. Once thats evaluated we dont need the temporary for it anymore. Now the results of fib has to be saved somewhere while we do the plus okay? And then wer e going to have to evaluate the argument to the second call of fib and then notice that this happens while we still need this temporary here so in fact we need both of these temporaries at the same time. Okay because while were evaluating this argument the second call of fib we still need to be holding on to the first argument to the plus. And so in fact this particular function can be evaluated with just two temporaries. So all the space we need to compute the value of this function body. So in general we can define a function nt of e that computes a number of temporaries  needed to evaluate e1 + e2. So thats going to need at least as many temporaries as e1. Okay so if we need a number of temporarys k to evaluate e1 lets have at least k temporaries to evaluate the whole expression And then well also need at least as many temporaries as its needed to evaluate the two+1 because we have to hold on to the value of e2 while we are evaluating so we have to hold on the value of e1 while were evaluating the two. Okay And its going to be the maximum. Over these two so itll be the maximum number with between the maximum number of temporaries need to evaluate a one and one + the number of temporaries to evaluate two. That would be the total number of temporaries the minimum number of temporaries needed to evaluate e1 + e2 And the reason is a max instead of a sum. Is that once weve evaluate e1 we dont need any of the space that was used to evaluate e1 anymore. All those temporaries are done. All we need is the answer. We dont need the immediate results and that means that the temporaries that were used to evaluate e1 can be reused to evaluate e2. So generalizing from that one example here is the system of equations that subscribes the number of temporaries needed to evaluate every kind of expression in our little language. So lets take a look. So we already talked about e1+e2 is just the max of over the number or temporaries to value of e1 and one + number of temporaries to value of e2. So e1-e2 is exactly the same thing because the same structure is a different computational operation but is a binary operation and we have to save the value of e1 while evaluated e2. So its the same formula. [inaudible] Now for if and else well what do we need? We need one Im sorry we need its going to max again. Its going to be max over some number of different quantities. How many temporaries might we need? Well we might need as many temporaries or as needed to evaluate the value of e1 and we certainly need at least as many alright. So if you want to take a certain number of temporaries the whole f and l is going to require at least as many temporaries. Now of course once e1 is done evaluating we dont need its temporaries anymore. And and we can evaluate e2 okay. And while we are evaluating e2 we have to hold on. To the results of e1 thats where the one plus comes from. So to that while were evaluating e2 we need one plus the number of temporaries to evaluating two to hold all the temporaries of the computation. And then once the predicate is done we dont need any of those temporaries anymore at all ad were going to evaluate either e3 or e4. And so then we just need however many temporaries each of those requires and whatever the maximum is over these four quantities thats the minimum number of temporaries we can get away with to evaluate the entire if then else. Lets take a look at a function call. So that the space needed for the function call is number of temporaries the max over the number of temporaries to evaluate anyone of the arguments and this is actually an interesting case because notice. That we dont need we dont have anywhere in this formula space for the results for the e1 through en Of course once weve evaluated the e1 then we need to save it somewhere and so you would think that we might see some numbers in here representing the temporary space needed to hold on to the results of the evaluating these expressions. And the reason that we dont have that in here is that. Even though those values are saved they are indeed saved; theyre not saved in the current activation record The space where the results of e1 and the results of all any of the arguments. Yeah again is saved in the new activation record that were building And so the space for the the results of e1 through en is that those values are stored in new activation record and that storage of current activation record and were trying to compute the number of temporaries needed to evaluate inside of the current activation And then for integer that doesnt take any space at all to require any temporaries I mean. So theres zero temporaries required for that and also for a variable reference so it requires no temporaries. So now lets go through our example and work out systematically using the equations. How many temporaries we will need? Okay? So here for this if then else remember it was going to be the max over the number required to evaluate e1 well that zero. One + the number to evaluate e2 which is the second expression in the predicate so that would be one because the number one requires zero temporaries and the one the we have one hold on to x all right? And then max over the branches. So to evaluate zero requires Zero temporaries and now. We have to compute. The number required here. Okay so once again to evaluate the first expression if and else requires zero temporaries to evaluate the second one we require one. One + the number required one + zero to evaluate that constant we got zero temporaries and now for the last expression how many will this one will require. Well this is going to require zero for this guy. One for the second argument so to evaluate fib is going to require one temporary okay and then its going to be one plus over here. We have to hold on to the results there. The value of x - two so how much that going to require? That is going to require the max of zero and one + zero okay so this would be one alright so we have over here we have one + one = two okay and now were taking the max over two and one. So thats two okay? And this is the last expression in the our if and else. So clearly this if then else here will require two temporaries okay? Because the max over the number required for either part of the predicate the then branch and the else branch And now this whole expression. Requires two temporaries and thatll be the max of the four components of the outer if then else And so then for for the entire expression we get two temporaries. Once it computed the number of temporaries required to evaluate the function value we can add that much space to the activation record. So now our activation record is going to require two frame pointer. The n is for the n argument of the function And then the rest of it is just the space required for the temporaries And now we can talk about how were going to layout the activation record. Well leave the first part of it the same so everything up to the return address is laid out just before. First the color string pointer then the and arguments in reverse order and then the return address. And then after the return address come the and locations or the nt(e) excuse me locations for the temporaries. In the last several videos we have discussed code generation for a various simple programming language. In this video we are going to take a look at code generation for more advanced feature objects. Fortunately this dated code generation strategy for objects is really just an extension of what weve already learned. So everything that you learn before were going to be using and then theres going to be some additional things that we do specifically for objects And the important thing to know about objects is slogan that you hear. When people talked about object oriented programming is this one. So if b is a subclass of a then an object of class b can be used wherever an object of class a as expected. So theres a substitutability property. If I have a piece of code that can work on as then it could also work on bs and any other subclass of a. What this means for the for the case of code generation is that the code that we generate for class a. So the code that we produced for method in class a has to work unmodified for an object to class b And to see this keep in mind that when we compile a when we compile class a I we may not even know all the subclasses of a. So those maybe not even have been defined yet. So in the future some programs may come along. To find a subclass of a then our compiled version of a will have to work with that new subclass. So there really only two questions that we have to answer to give a complete description of how to generate code for objects. The first one is how our object represented in memory. So we need to decide a layout and representation for objects And the second one is how is dynamic dispatch implemented so thats the characteristic feature of using objects just so we can dispatch in the method of an object and we need an implementation of that. So to be concrete were going to use this little example throughout this video and Ill just take a moment here to to point out some features of it. So we have three classes classes am b and c And notice that a is a base class and b and c both inherit from a And all three classes define some attributes some fields and also some methods. Now a couple of important features here is that notice that because b inherits from a and c inherits from a they all they both inherit both of those classes inherit the attributes a and d from class a. So these two attributes that are defined in class a are available in class b and in class c So even though theres no mention Of a and d in the definition say of class b. The methods in class b can still refer to those attributes. They are part of the attributes of class b. They are just copied over or inherited from a. Another feature of this example that I like to point out is that all of the methods refer to the attribute a so actually referred into this method and this one referred twice in this method and also in this method. And the significance of this is just what we discussed a couple of slides ago. For all of these methods to work attribute a is going to have to live in some place and some place where all of them can find it they generate a code run. Some particular less considered the method f. So the method f exists in all three classes. All three classes when it runs it will refer to attribute a and even though the objects would be different. In one case it might be running on an object and in another case on a c object. It would need to be able to find the attribute a and so therefore the attribute a has to be in the same place in each object And so how do we accomplish that? Well the first principle is the objects are laid out to in contiguous memory. So an object is just a block of memory. Okay with no gaps and all the data for the object is stored in the words of that lock of memory. And each attribute is stored at a fixed off set in the objects. So for example there may be a place in this object for attribute a On this case its at in the middle of the object is in the in the fourth position And no matter what kind of object it is whether its an a. B or c objects and are example as with a we always live with that position so that any piece of code that refers to a any method that refers a can find can find the a attribute. Now the other thing thats important to understand and this is you know slight digression from what were talking about but its a key aspect of code generation for object is that when a method is invoked the object itself is the self parameter. So the self parameter is the entire object so self. When a function is involved it will refer to the entire object so you think itself is going to be appointed to the entire object. Remember that self is like that this variable or this name in Java. And then the fields we refer to particular or the attributes of the object will refer to particular position within the objects. So for example the attributes we decided to leave it there. So here is the particular object layout used in Kuhl. So the first three words of a Kuhl object contain header information and every Kuhl object always has these three entries. The first position is a class tag and also at zero then the next word it also four is the size of the object and then something called the dispatch pointer and then all of the attributes. Now the class tag is an integer which just identifies the class of the object. So the compiler will number all of the classes. So in our example we have three classes a b and c and the compiler for example might assign them the numbers one two and three. It doesnt matter what these numbers are As long as they are different from each other. So it doesnt have these numbers consecutively or anything like that The important thing is of the class tag is a unique identifier for a class each class has its own unique bit pattern that tells you what kind of class the object is And the other fields here the object size is also an integer which is just a size of the object in words and the dispatch pointer. Is a pointer to a table of methods so the methods are stored off to the side and the dispatch pointer is a pointer to that table and well talk about this more later and then all the attributes are laid out in the sub-sequence slots in some order thats determined by the compiler so the compiler will fix and order for the attributes in the class and then all the objects of that class will have the attributes of that class in the same order. And again all of this is laid out in the continuous chunk of memory. Now Im ready to talk about how inheritance works. So the basic ideas like given a layout for class a a layout for a subclass b so this is a subclass of a can be defined by extending the layout of a. So we dont need to move any of the attribute of a we can just add more fields onto the end of as layout. And so thats going to leave the layout of a unchanged which is a great property because this is how the position of an attribute in the a object will always be the same for all the subclasses. So essentially we will never once we decide where an attribute lives in a class it will never change for any of the subclasses of that object. So b is just going to be an extension of the layout of a. So lets take a look at our example here and see how that that works. Let me just write down here a little bit about these classes because we dont have the example on the screen. So we have a class a and class a had two attributes a and d okay? And it doesnt matter what their types are or what the methods were here. Were just looking at the class names and the names of the attributes that are defined in the class. And then we have b. Which inherits from a and b added a attribute b and then we had c which also inherits from a but has no relationship to b. And class c define an attribute little c. Alright. So thats the structure of our example is relevant to the layout of the objects. Okay. So Lets talk about the layout of class a. So in position zero at all sub zero therell be a tag for a that will be some small integer at the compiler picks. Therell be a size of a well come back to that in just a se cond. There will be a dispatch pointer again which were going to talk about later. And then come the attributes of a and it just laid out the compiler the way its done in the the Kuhl c implementations is that they are laid out in the order in which they appear textually in the class. So in this case first the attribute a And then the attribute d all sets twelve and sixteen And now since the object there are two attributes and three header words that means the size of the object is five words and so its a five that goes in the size field for a objects. Now lets take a look at b. Okay? So b is going to have a different tag b objects will have a different tag so they to distinguish them from a objects. Theres going to be extra fields so the size will be one bigger But now the layout preserves the layout of a. So the attributes of a appears in the same position. So you can think of there being an a object Actually embedded inside of the b object. If I were to strip off the end here that were just you know cover up this last bit here b I would say that this object here has the same size and the same attributes as an a object so any piece of code that could work on an a object will also make sense running on a b object. Now Of course the tag is different because it actually is a subclass and you know and there is an extra field so the the size is different but the point is that any code that it refer is just to the fields here will still work just fine. So any a method that was compiled that refer to the methods of an a object will still find those attributes in the same place at the b object and afterwards there is also one more field here. Which is the new attribute b It just gets laid out after all of as fields. So after all of as fields come all of bs fields in the same order which they appear textually in the class because theres just only one theres just one new field there. And now looking with class c or the story with class c is very similar so c has its own distinct tag and also has one more attribute than a so it has size six. And now again the a attributes were on the same position and now the c attribute just comes after the a attribute. And so notice here that a methods again will work just fine on c objects because the attributes are on the same places and so the methods will find the attributes where they expect to. You cannot however call a method of class b on an object to class c. Okay because they have different attributes in the third position. We may have completely different types. It may not make sense to invoke a b method on c object but thats just fine because if we look at our inheritance hierarchy over here well see that b and c are actually unrelated. They are both subclasses of a but they have no relationship to each other. B is not a subclass of c and c is not a subclass of b and so anything beyond their shared ancestry with a can be completely different in the layout. So more generally if we have a chain of inheritance relationship so lets say we have a base class a1 and a1 inherits some a1 and a3 inherits some a2 and so on with some class a and inheriting at the bottom of this of this chain after some long sequence of of other intermediate. Some classes you know what is the layout of all these classes going to look like. Well theres going to be a header. Okay the three word header and that will be followed by a1s attributes. And then followed by a2s attributes followed by a3s attributes and so on all the way down to ans attributes down here. Okay. And if you look again so what we talked about before each prefix. Of this header is essentially a valid object a valid one of these objects. If I look at the first set of attributes everything up to the end of a1 and attributes that forms a valid layout for one object is I stop with the a2 attributes. I have a I have a I have a valid layout for a2 object going all the way from the header down to you know including the a1 and a2 objects. And then a3 includes all a1 a2 and a3s attributes and so on. Okay? So each prefix Of of this object Of this a and object has a correct layout for some for some super class of A n. This is the first of the series of videos on programming language semantics and in particular on the semantics of cool Before we dive into technical details though I want to spend a few minutes talking about what programming language semantics are and why we need them. The problem we have to address is that we need some way to say what behavior we expect when we run a Kuhl program. So for every kind of Kuhl expression for everyone we have to say what happens when its evaluated and we can regard this as the meaning of the expression. Somehow give rules to specify what a particular what kind of computation of a particular expression does. And I think its useful to look back and see how we dealt with this with similar problems in defining other parts of cool okay the earlier things that we looked at in this course. So for example For Lexical Analysis we defined a family of family of tokens using regular expressions And for the the syntax of the language we used Context Free Grammars to specify the the structure of the how words could be strong together to form valid sentences in Kuhl And then for the semantic analysis we gave formal typing rules And now were to the point that we have to talk about how the programs actually running so we have to give some evaluation rules and these are going to guide how we do code generation of optimization or you going to determine what the program should do and what kind of transformations we can do on programs to make them run faster or use a space or what other what any other kind of optimization that we would like to perform. So far weve been specifying the evaluation rules somewhat indirectly. Weve been doing it by giving a complete compilation strategy down to stack machine code and then weve been talking about the evaluation rules for the stack machine or actually translation the stack machine into assembly code And that is certainly a complete description. You can take the generated assembly code and get it right out of the machine and see what the program do es and that would be a a legitimate description of the behavior of the program And the question then is you know why isnt that good enough. Why isnt just having a code generator for the language. Why is that already a good enough transcription of what how the code is supposed to be executed And The answer to that is maybe a little hard to appreciate without having a written a few compilers But in a nutshell people have learned through hard experience that assembly language descriptions of language implementations language implementations have a lot of irrelevant detail. Theres a lot of things that you have to say when you get such a complete executable description that was not necessary to say about how the program executes. So for example the fact that we use a stack machine thats not intrinsic to the implementation of any particular programming language. There are other co-generation strategies that we could have used so you know you dont have to do the stack machine to implement the language which way the stack grows. Whether it grows towards high addresses or low addresses you could implement it either way. How it it yeah exact representation of integers in a particular instructions actually used to execute or to implement certain language constructs. All of these things are are a are one way or or one particular way to implement the language but we dont want them to to be taken as the only way that the language could be implemented. So what we really want than it has a complete description but one that is not overly restrictive One that will allow a variety of different implementations. And when people have not done this when people have not tried to find some relatively high level way of describing the behavior of languages theyve been inevitably gotten into the situation where they a where people would just have to go and run the program on a reference implementation or to decide what it does. And so this is not a very satisfying a situation because of the reference implementation is not completely correct itself. It will have bugs and there will be artifacts of the particular way it was implemented that you didnt mean to be part of a language but because there was no better definition wind up becoming fixed and have sort of accidents of the way the language was implemented the first time. So there are many ways to actually specify semantics that would be suitable for our task and it turns out that these are all equally powerful but some of them are more suited to various tasks than others so the one that were going to be using is called operational semantics. So operational semantics describes program evaluation via execution roles on an abstract machine we just gave a bunch of rules that say you know from particular expression how it should be executed. You can think of this as a very very high level kind of co-generation And this is most useful for specifying implementations and it is what were going to use to describe the semantics of Kuhl. I want to mention two other ways of. Of specifying programming language semantics because theyre theyre important and you may come across them at some point outside of this class. One is the notational semantics and here the programs meaning is actually given as a mathematical function. So the program text is mapped to a function that goes from input and outputs and this this is this function is an actual function that exist in the mathematical sense And this is a very elegant approach but it uses complexities into finding an appropriate class of functions and we dont really need to consider for the purposes of just describing an implementation. And another important approach is axiom semantics and here program behaviors described in some kind of logic And the basic kinds of statements that you write in this language or in this in this in the axiomatic semantics is that if execution begins in a state satisfying x then it ends in the state satisfying y where x and y are formulas in some logic And this is a very common foundation for syst ems that analyze programs automatically that tries to prove facts about programs either to prove theyre correct or to discover bugs in programs. In this video were going to begin our discussion of formal operational semantics. Just as we did with lexical analysis parsing and type-checking. The first step in defining what we mean by a formal operational semantics is introduced the notation and it turns out that the notation we want to use for operational semantics is the same or very similar in the notation we use in type-checking. We are going to be using logical rule of inference. So in the case of type-checking the kinds of inference rules we we presented proofings of the forms that in some context. We could show that some expression had a particular type in this case to type c. And for evaluation were going to be doing something quite similar. We will show that in some contacts now that this is going to be a different kind of context that we had in typing so this is going to be an evaluation context as oppose to a type context and so what goes in the context will actually be different. But for the moment all that really matters is there is some kind of a context and in that context were going to be able to show some expression evaluates to a particular value b. So as an example lets take a look at this simple expression e1 + e2 and lets say that using our rules which we I havent shown you yet but lets say we had a bunch of rules and we could show in the initial context. That e1 in that same context okay? So these context are going to be the same that e1 evaluated to the a value of five and e2 also in that same context evaluated to the value of of seven then we could prove that e1 + e2 evaluated to the value of twelve. If you think about it what this rule was saying is that if e1 evaluates to five and e2 evaluates to seven then if you evaluated the expression e1 + e2 youre going to get the value twelve. And whats the context doing well it doesnt do a whole lot in this particular rule. But remember what the context was for in type checking. The context was for giving values to the free variables of the expression. And so we need for an expres sion like e1 + e2 to say something about what the values are the variable that might appear in e1 and in e2 in order to say what they evaluate to and and therefore to say what the entire expression e1 + e2 will evaluated to. Now lets be a little more precise about whats going to go in the context. So lets consider the evaluation of a a expression or statement like y gets x+1. Okay so we are going to assign y the value x+1 and there are two things that we have to know in order to evaluate this expression. First of all we have to know where In memory of valuable start. So for example the variable x here were going to have to go and look up excess value and then add one to it And then that value is going to be stored in whatever memory location holds the value for y okay so there is a mapping from variables Two memory locations. Okay and that is called In operational semantics the environment and this is a little confusing maybe because it use environment for other things. Okay so now lets forget about as all we uses of the word environment. We were talking about the operational semantics what the environment means is the mapping the association in between variables and where that variable store in memory. And then in addition were going to need a store and thats going to tell us what is in the memory. So just knowing the location for a variable isnt quite enough. When we if we know the value of x if we know the location for x for example or as as important because were going to get the value of x but we also have to know exactly when value is stored there and so store. Is going to be a mapping for memory locations Values. These are the values that are actually stored in the memory so its two levels of mapping. We associate with each variable and memory location And then each memory location will have a value in it. So lets talk about the notation that were going to use for writing down the environment and the store. So as we said the variable environments have variables to locations and were going to w rite that out. In the following way were going to just have this as a list of variables and location pairs separated by columns and this environment for examples of variable a is it location l1 And variable b is in location l2. And another aspect of the environment is that its going to keep track of the variables that are in scalps and the only variables that will be mentioned in the environment are those currently in sculpted in the expression that is being evaluated. Now as we said stores map memory location to values and well also write out stores as lists of pairs. So in this case the memory location l1 in the store contains the value five and the memory location l2 contains the value seven And we will also separate these pairs by an arrow. And just to make the stores look a little bit different from the environment so that we wont confuse the two. Theres an operation on stores which is to replace of value or update of value. So in this case were taking the store s and were updating the value at location l1 to b12 And this defines a new store s prime. So keep in mind here that the stores are just functions list in our model and so we can define a new store by taking the old function or the old store has and modifying it at one point. So this defines a new store as prime such if I apply s prime to the location l1 I get off the new value twelve and if I apply s prime to any other location any location different from l1 I get out the value that the store held in s sorry the value of the location in store s. Now in Cool we have more complicated values and integers. In particular we have objects and all the objects of course are instances of some class and were going to need a notation for representing objects in our operational semantics. So well use the following way of writing down an object. An object will begin with its class name. In this case the class name x and it would be followed by a list of the attributes. In this case the class x has n attributes a1 through an And associated with each attribute will be the memory location whether an attribute stored so attribute a1 is stored location l1 up through attribute and which is stored at location ln. And this would be a complete description of the object because we know where in memory the object the object is stored. We can use the store to look up the value of each of those attributes. There are few special classes in Kuhl that dont have attribute names and well have special way overriding them. So integers only have a value and and that will be written as int with a single value in parens the value of the integer similarly for brilliance. They have a single value true of false and strings have two properties the length of the string and the sting constant. Theres also a special value void typed object and well use the term void in our operational semantics to representative and briefly here so void is a a special and that there are no operations that can be before and on void except for the test is void. So in particular you cant dispatch the void even though it has typed objects that will generate runtime error. The only thing you can do is to test whether the value is void or not. And concrete implementation we typically use a null pointer which represent void. Now were ready to talk about in more detail what the judgments will look like in our operational semantics so the context will consist of three pieces. The first piece is a current self object. The second piece is the environment which is again the mapping from variables to the locations where those variables are stored and the third piece is the memory the store. The mapping from memory locations to the values held at those locations All right? So in some context an expression e will evaluate to two things. First of all e will produce a value so for example we saw before that the expressions seven + five would produce the value twelve thats one result to the evaluation. But the second thing is that well produce a modified store. So the expression e maybe a complicated piece of code Maybe a whole program is on the right and it might have a slight statements that update the contents of the memory And so after e is evaluated there will be a new memory state that we have to represent and so s prime here represents the state of memory after evaluation And now those are couple of things here. First of all the current self object and the environment dont change. They are not changed by evaluation so which object is the self parameter to the current method and. Well the mapping between variables and memory locations that is not modified by running a running an expression and that makes sense I mean you cant update the self object in Kuhl and we dont have access in in any form to re-locations or variables stored and so those two things are in variant. They dont they are variant under evaluation. They dont change when you run a piece of code. However the story does change so the contents in the memory may be modified so thats why we need a store for both before evaluation and after evaluation. And one more detail these judgments of this form always has a qualification. That judgment only holds if the evaluation of e terminates. So if e goes in to infinite loop then youre not going to get a value and youre not going to get a new store. So this kind of the judgment must always be read as saying that if e terminates then e produces a value v and a new store s prime. Summarize the results of evaluation is a value and a new store. And where the new store models the side effects of the expression And once again there are something dont change as a result of evaluation. And this is actually important for compilation because well be able to take advantage of the fact that they dont change to generate efficient code so the variable environment doesnt change the value itself which object were talking about doesnt change and notice here as another detail. That the contents of the self objects the attributes inside the self object might change they might get updated but t he locations where the attributes are stored do not change. So the layout of the object where the object stored doesnt change and thats all were saying here the actual contents of the object which of course is a part of the mapping of the store those might get updated by evaluation. And also the operational semantics allows for non-terminating evaluations. Thats the last point here and so the meaning that the judgments only holds on the assumption that the that the expression actually completes. In the next couple of videos were going to be looking at the details of the Cool operational semantics going over the semantics of each individual kind of expression. Well start with easy ones and work our way up to the more complicated ones. So the easiest rules are the rules for the constants in Cool. So the value true the expression true I should say evaluates to a Boolean with the value true And it doesnt modify the store so the store is unchanged because it doesnt do any updates obviously And theres a corresponding rule for false And integers are very very similar so if a integer expression integer literal i will evaluate to and integer object with the value i and again the store is not modified by such evaluation. And finally strings if a if s is a string literal of length n then it will be evaluated by the string object of which the properties n and the string constant s. The evaluation of identifiers is very straightforward given that we have both in environment in the store. So to evaluate an identifier and this would be a variable lane y x or y or for What do we do? Well first we look up in the environment where that identifier is stored so now we give us back a memory location l. So by in this case And then we look up in the store what the value is at that memory locations So we use the same memory location here as an argument to the store to get back the value that that that variable currently has And notice I just have a reference this is a read of memories so this is loading I think it was loading the value of the variable. This does not affect the store so the store is the same before and after. This is just looking at the value of the variable not updating the variable. The expression self just evaluates to the self object. So this is a place where we just make use of the fact that the self object is part of the environment so lets just copy them over here as the result of expression and that was again that the store is unaffected by evaluation of self. Now lets see of a more slightly more complicated evaluation is evaluated in particular the assignment expression. So an assignment consists of two parts an identifier that is being updated and an expression that is going to give us the new value. So for example just to remind you we might have something like x gets one + one so one + one here would be the expression e and x would be the identifier All right? And so in order to evaluate the assignment the first thing we have to do is we have to know what value were going to be writing into the identifier. So what is the what is the update were going to perform? So the first thing to do is to evaluate e okay? And notice here that e is evaluated in the same environment so it has the same three components here and here all right? So it just says the first thing we do is we run e. Okay. That will give us back a new value. Were going to get back on value b excuse and possibly an updated score so you can arbitrate a piece of code. You could yourself have assignment statements in it so the story that we get out might be different. Alright so e produces the value of e and an update store s1 And now its actually due to assignment what do we do? Well we have to know what memory location was supposed to update so we look up the memory location for id and that would give us some location else of id. And then we modify the store with the new we modify the store at that point with the new value so we replace the location l i d or we update the value of l i d to be the value of e the value b and we do that in store s1 which gives us a new store s2. And Ive noticed. That s2 is the store that results from the evaluation of e okay? So after we do the assignment the assignment returns the value b which is of course the value of a of running e. And it returns the updated store s2. Next lets talk about the operational rule for addition. So to evaluate e1+e2 what we are going to do? So first we evaluate e1 And notice that is done in the same context as the context of the entire e xpression okay? So the components the context here for evaluating one are exactly the same as the components for the overall expression e1 + e2. So when we evaluate e1 its going to give us a value of e1 and its also going to give us an updated store s1 And then were going to evaluate e2 and notice here And I think context is different. The soft objects in the environment are the same; same but now were running e2 in the new store s1. And what does its saying is that if e2 has has assignments or variable references in it those assignments and variable references have to be done on the store that resulted from running e1 okay? So its very important that we get that any side effects would happen in running e1 are visible or that are seen by the expression e2. So we run e2 in this environment were going to get the value of e2 and the updated store s2 And then the result of the entire expression is going to be b1 + b2 and the results it store will be the store s2. And notice here how the stores tell you the order in which you have to evaluate the expressions. So because e1 is evaluated in the same store as the overall expression that tells you that e1 has to be evaluated first. And then because e2 is evaluated in the store thats produced by e1 that tells you that e1 e2 excuse me has to be evaluated after youve evaluated e1 and then the fact that. S2 is the result of the whole thing. It tells you that E2 is also the last thing that you evaluate during the execution of this particular expression. Okay lets take a look at the statement block and just a variety here on the change my colors. How are we going to evaluate the a statement block of statements e1 through en okay so while this is semantics this is that we should run I mean order beginning of e1 and the results of the entire execution will be the lets say the value of the entire block with the value of the last expression. And this this rule just says that. So first we evaluate e1 and also its done on the same store as the overall expressio n as it tells you it has to come first and that produces a new store s1 and the value b1. Okay And then e2 is evaluated in the store s1 and it produces the store s2 and so on And then expression en is evaluated in the store sn - one and it produces a value of bn and an updated store s (N). Okay? And then the result of the whole thing is the value of vn and also the updated store s (N) and this tells you this would really tell you the order which had to evaluate the sub expressions. The dependencies here on the store of course you do evaluate e1 and then e2 and then e3 and so on so you have to do them to net order to get the side effects to get. You know the side effects in the correct order for all of these expressions And furthermore it also tells you the only value that youre going to keep is the value vn. Notice that none of the other values that are produced here are used for anything. They dont appear anywhere else in the rules. Lets think what weve learned so far and do a small example. So we want to know what happens when we evaluate the block. X gets assigns seven + five thats the first statement and the second and the last statement in the block is just the expression four. And the first thing we have to do is to say what the context did and which we are going to evaluate this and the context consists of three parts. Therell be a self object and in this case it doesnt really matter whats in the self object because self is not referred to in the program and so it wont play any role in the evaluation. But we we still need it so there still be therell be some self object out there just wont get used and Now in the new environment which tells us the locations where all the free variables in the programs. So well just need a place for x is going to be stored and so s will be stored in some location l And then we know our memory content is where our store is and lets just say that at l we have initially the value zero okay? So now we can use our rules to run this program or to evaluate thi s program. Im going to make this line here much longer And recall that that evaluation of block consist of the evaluation of the all the statements within the block. Okay so the first one is going to be s gets seven+5 and that will be evaluated in the same environment as the overall expression. So we have up here. So the same context excuse me and I should say I often slip and I realize and say environment for the entire left hand side of one of these judgments. Ill try to be consistent. And just use environment for the for the second components of the context often in the literature people call the entire thing on the left hand side of the environment thats why they make this mistake but you know for this instead of notes Im trying to be consistent the entire all the components on left hand side together are called the context and the environment is just the second component. The mapping from variables to their locations Anyway coming back to the example The first statement in the block is s gets seven+5 alright. And then were going to have the second statement as well. And we know that the self object and the environment wont change but we dont know what the store will be. The store might be different so Ill leave the store empty for now and well figure that out later and were going to be evaluating the expression four. Okay so this is the structure of the evaluation now in progress. We should look at at this first statement trying to make some forward progress on that one. So to evaluate the assignment what do we have to do? Well the very first thing we have to do is we have to evaluate the right hand side so were going to have the context for that is going to be the same And the context weve been looking at all on because its the first thing thats actually going to happen is to evaluate Seven + five okay? And now Im leaving a little space down here for the rest of the assignment role which were not going to fill in just yet. Now to evaluate the plus expression we have to evaluate the first express ion and the second expression okay? And so how do we do that? Well we know finally I think how to do that because were finally down and were going to have a single integer there and that we already have a rule for okay and so an integer literal evaluates to institute your object okay And inside that object is just about the value okay. And the store is unmodified All right? And then similarly for the other argument here Okay. So the five will also evaluate to an integer object with the value of five and the store will be unmodified okay so thats the two sum expressions of this edition and so now we can fill in the results here so to take the contents of the two integers well add them. That will also be integer object so were going to have the integer object twelve. And the store has not been changed okay? So the the store that we get out of here happens to be the same as the store that went in just because this expression had no assignments in it okay? And now were ready to do the assignment. Okay. So how do we do that? Well we have to form a new store. Alright so were going to have a new store which will the L gets zero with the value of l. Number which way my notation went here I think its the number comes first and were going to put twelve in the location l and of course thats store was just equal to the store where l has the value twelve okay And so now what happens down here and we do the assignment and we get out The new value. Okay so the value of the right hand side is twelve and we have a new store where the location l has twelve all right? So now were going to evaluate the second statement in the block and that will be done in the store where l has the location twelve and of course this is just an integer And so that will evaluate to the integer constant of four the integer value excuse me 4or integer object containing the integer object with the value four and our store. And its just going to fit not quite all right? And thats then the result of the entire evaluation . So this block will produce the value four an integer object with the value four and an updated store where location l has the value twelve. So the next expression I would like to take a look at is the if then else expression and to evaluate if then else what do we do? Actually there should be if then else. See of course so evaluating if then else as well. First we have to evaluate the the predicate and its done in the same store the same context as the overall expression and if the result is true If the if the Boolean predicate returns the value true. Then we want to evaluate just the true branch and not the false branch so thats why you only see here evaluation of e2 and e3 isnt mentioned anywhere and just know here that the predicate may have side effects and so e2 is evaluated in whatever store that e1 produces And then the results of the entire expression is the value of e2. Okay thats v and also just the final store is produced by running the then branch. And there is a symmetric rule for what happens if the predicate evaluates the false. In that case you would evaluate e3 and not e2. Next well take a look with what happens with while loops and cool. So there are two cases. First if the predicate of the while loop evaluates the false okay. Well in this case the loop body is not going to execute alright so the first thing we do is we evaluate the predicate and thats done in the same context as the evaluation of the overall expression and if the predicate is false then we exit the loop and so the results of the loop is void. The value void and just whatever store resulting from evaluating the predicate. The other possibility is that the predicate evaluates the true. So here we evaluate the predicate again in the same context as the overall loop. And if the predicate evaluates the true then were going to run the loop audio once. Okay. So well evaluate the loop audio and also thats done in this in whatever store results from evaluating the predicate. Evaluating the loop audio is g onna give us a value of v and a new store s2 and then what we need to do is we need to go back around and execute the loop again and how can we do that? Well were really just running the whole loop in the new context. So the next thing we do is we evaluate the entire loop. Right in the new store So after I execute the loop by loop body one time then we go around and just evaluate the loop again And when this may run for zero or more iterations alright. And when I finally terminate if it terminates it will produce it will produce a new store s3 evaluated while because always produced the value of a void. And then what well produce for the entire loops for the entire expression is the value void and the update and store s3. The next interesting expression to take a look at is the let expression. So recall how what this looks like so let and cool has a variable thing declared and its type and an initializer which is optional so this is the value that the identifier will be initialized to and then the expression in which that new variable is available. And so how do we evaluate this? Well first were going to evaluate the initial value of the of the new variables. So we evaluate e1 and as usual thats done in the initial store it produces possibly modified store. And now the question is what are going to whats going to be the context here for the evaluation of e2 for the body of the latter? And so it seems clear that its going to involve s1 because it has all the updates from e2 but it also has to have this new identifier in it. And so how are we going to do that? So what we want is to have a new environment e but with new binding of ID to refresh location. So were introducing a new variable. Remember that the environment has tracked all the free variables so this is one situation where you should going to extend the environment e with the new binding alright. And that location the location for the new variable has to be a fresh location. We dont want to conflict with any other memory locations we are already using. Okay And so were going to allocate a new memory location for the variable And then the store the new store will also will be like as one as we said we have to include all the the values for s1 But also we can have these new location for the variable and thats going to have the initial value of the variable e1. To express that we need a new location were going to introduce a new operation on the store which gives us a new fresh location. So new lo applied to a store its just going to give us some location that isnt being used by the store. So the store has a domain where its a mapping from locations to values and well just pick some new location that isnt in the current list of locations within the store and that will be the one returned or that will be one that will be the one returned by new lo. Okay so new lo if you can think of As modeling the memory allocation function in the runtime system. So then here we can write out the rule. So this is the most complicated rule we seen so far. So Ill just take a moment to walk through it All right? So the first thing we do is we evaluate e1 the initializer for the new variable okay? So just like before this is done in the same context as the overall expression and this is going to give us a value for e1 and an updated store all right? Then in the updated store using the updated store here we find an unused location l new. Okay And then were going to create a store where that new location has has the value of e1. So were going to store the value of e1 at that new location. Were going to update the store s1 to reflect that and further more were going to extend our environment with the new identifier which will be stored at this new location and this is the context then. Okay with this updated environment in store in which we evaluate the body of the lab which will produce the value b2 and possible update in store s2 and those are the results of the overall expression. In this video were going to continue and complete our discussion of cool operational semantics. Well be taking a look with the two most complex operations in cool the allocation of the new object and dynamic dispatch. Well begin by giving an informal discussion of what happens when a new object is allocated in Kuhl. So the first thing that has to happen if we have to allocate space for the object and essentially that means having enough space for the object attributes. Were going to have to allocate a location for every attribute of the object of class t if what were doing is allocating a new t object. Then were going to set the attributes of of that object to their default values and well in a few minutes well see what the default values are and why we need to set the set the attributes to defaults. And then we evaluate the initializers so every attribute in the class declaration can have an initializing expression. Were going to evaluate those and set the resulting attribute values And then we return the newly allocated objects. So these are the steps that are involved in setting a new object and as you can see its actually more than just allocating a little bit of memory. Its actually quite a bit of computation going on in allocating new objects in cool. Every class has a default value associated with that class. So for integers the default value is zero. For Boolean the default value is a Boolean false and for strings the default value is the empty string And then for any other class that isnt one of these three basic classes or any other class the default value is void. In the operational rules were going to need a way to repair to the attributes of a class. So were going to define a function called class that takes a class name and returns the list of attributes of of that class. So here we have all the attributes of class a lets say that a1 through and in addition this functions also going to tell us for each attribute declared type of the attribute and the expression that initiali zes the attribute. And one other important feature of this list is that it includes all the attributes of class a including the inherited ones. And theres another detail which is in what order these attributes appear and these are actually become important when we define the semantics of how attributes are initialized and the rule is the attributes are listed in greatest ancestor first order And what do I mean by that? Lets say that we have three classes a b and c and a Im sorry b inherits from a. And c inherits. From b. Okay lets say that a defines two attributes a1 and a2 and b defines two attributes b1 b2 and c defines two attributes c1 and c2. Then class of c. Well list the attributes in the following order. First well come a1 and then a2 Because a is the greatest ancestor okay its the the closest to the root of the object hierarchy and the attribute was in class a or within any class its always listed in the order that it textually appear. So first comes a1 and a2 and of course the type in the initializer are also lets see here most of these attributes but were just concentrating here in the order in which the information appears. So the next would come class b. So the attributes of class b will be next and of course therell be the type and initialize for those attributes and then finally the attributes of class c Again in the order in which they are listed in the class definition okay? So that defines the order of the attributes for any class. Its always in the order of the greatest ancestor down the inheritance chain to the class itself which is the argument of the class functions. At this point were ready to actually define the formal semantics of new t and let switch colors here. So were going to be allocating a new object of type and is going to be in a context with self object as zero environment e and store s. The first thing we have to do were going to figure out what kind of object it is that were actually going to allocate and the only question is whether t is se lf type or not because remember self type is not the name of an actual class. If t is not self type then the class that were going to allocate is actually a t. T is actually a class name and with that thats the kind of object that were going to allocate. If t is self type then the kind of object were going to be allocating. Is whatever the class is of the self objects? So were going to look at the dynamic type here of the self object called that x and that will be the class that we create. That will be the kind of objects that we created all right? So theres two possibilities Either object object allocating an object of type t if t is actually a class name. Otherwise its an object of the same dynamic type as the self object Alright? So now were going to look up t0 is alright. And we get out the list of the attribute types and initializers for t0. So this tells that what we have to do to construct an object of this type. Alright and the next thing we do is we allocate locations for each of the attributes. So because they were in attributes were going to allocate n locations. One for each attribute all right. And then were going to create an object with the class tag t0 and the attributes are going to be bound to these new locations. So the i attribute will be abound to the i new location that we just allocated and that were going to update the store. Okay. So were going to take our initial store and know this is the same with the store we started with. We take s and we are going to update it so that at these new locations those new locations hold the default values of for the type of each of the attribute. Okay and that gives us the store s1 and now we have to evaluate the initializer. The two actually initialize the attributes. And we have to think about what the environment is in which those attributes are initialized and remember the rule is that within initializer I mean attribute all the attributes of the class are in scope. Alright so the environment in this case for the initializers will ju st consist of the initializer or the attributes excuse me themselves. Okay so these are the attribute names and the i attributes is bound to the is new memory location holding the value the default value initially of that attribute. Alright and then finally to evaluate initializers we just evaluate them as a block in the order which they appear in the class function. This is why it was important to specify the order in the class function. So remember that these attributes include all the inherited attribute so well start by evaluating initializing attributes with the greatest ancestor and working our way down to the attributes declared within the class itself. Notice that the environment here. Which has all of the attributes in scope is an interesting point this environment has nothing to do with the environment in which new t is actually evaluation. You know these environments e and e prime are completely separate okay? So new so e prime has in scope the names of the attributes the class e is a you know is is some other environment. Theres some functions somewhere thats calling new t and the variables are in scope there are just completely different okay? But anyway evaluating this block Of initializers will yield some value. And the new store the value isnt used for anything okay? But the new store is the final store. Thats the store that we get out as a result of allocating the object and then what is the result of new t well it is the new object itself v. To summarize the semantics of New that was the first three steps allocate the object actually allocate the memory for the object and then the remaining steps initialize the objects by evaluating a sequence of assignments and the most important thing probably to understand about initialization and one of the most important things is the context in which or the stage in which the initializers are evaluated. So know that only the attribute are in scope while we emphasize that and its the same rule as of typing. So when you re type checking a class declaration only the attributes are in scope of the you know for the initializers of the class and then as the same naturally the same thing that we use when we actually evaluate the initializers at runtime. And the initial values of the attributes are the default values and then then we need the defaults because precisely because the attributes are in-sculpt inside their own initializers. So it could be for example its perfectly reasonable like Kuhls to have an initializer lets say like this. And Im just going to I may leave all the types here just to save time but I can assign and attribute a the value of a and this is perfectly okay because the right hand side of the intializer has all the attributes and scope and for this to make sense a has to have some kind of default value. It has to have some initial value so because I might read it before I might read an attribute before I have actually finished computing its initializer All right? And the last point here is that notice that in the initialization or in the yeah in the initialization of an object self is the object itself is the self object. And what do I mean by that? I forgot to mention this on the previous slide just flipping back to that slide for a moment notice here. That in the evaluation of the initializers what is the context the self object is v the self object is v this is the new object that we have just constructed. And so its perfectly fine for e1 or en the initialization expressions over here and refers to stealth and what they were referred to if they use self is the object that is being initialized. Alright Returning to this to our summary you know it might be a little bit of a surprise how complicated the. Semantics of new is in cool and its not just cool that has that property. In fact every object oriented language language has a fairly complex semantics for the initialization of new objects and its a combination of features like inheritance and the ability of initializers to refer to the attributes that leads to this kind of complexity. Now lets talk about the semantics of dynamic dispatch and well follow the same plan that we did the semantics of new for us giving for us have an informal discussion and high level description of how the evaluation of dynamic dispatch works and then well look at the formal operational rule. So the first thing it happens in evaluating a dispatch is that well evaluate the arguments e1 through en and next well evaluate the target object e0 so that expression to get the actual object to which were dispatching. Next were going to look at the dynamic type of the target object. So after we evaluate the zero were going to look at its class peg is And then were going to use that type to figure out which function which function f were supposed to use. So were going to go and look in the method table for the class x and see what method it has for f. Then were going to create new locations and an environment for the call. Alright and were going to set up a new locations for the actual parameters. Were going to initialize the those locations with the actual arguments. Where s itself to be the target object and then were going to evaluate the body of f. Now in order to do the look up of a method in a class were going to need some representation of what methods exist and which class is in our operational rules. So were going to find a function eval stands for implementation and the implementation in a class a of a method f is is going to be first of all the list of formal parameters. So its going to tell us what the formal parameters are of f and then the body of f Whatever the the function body of f is. Now were ready to actually discuss the details of the formal operational semantics of method dispatch in Kuhl. Im going to switch colors here again just for contrast. So as we said the first thing we do is we evaluate the n arguments. So this first in lines take care of that ad notice that each arguments thats evaluated may have side effects. So it starts in some store but it may produce a different store. So after weve done all of this well have the n arguments evaluated and some store s (N). The next thing that happens is we evaluate zero. This is the expression to which we are dispatching and that would give us an object v0 and some updated store s (n) + one. Okay And now we have to inspect v0. We want to know whats inside of v0 what v0 is made of and in particular were interested in the classed tag of v0 and well also be interested in the contents of its attributes. The locations associated with its attributes but first lets focus on the class tag. Alright because were going to use that class remember this is the dynamic type of the zeros and what kind of objects the zeros actually is when the program is running. And were going to use that class to look up the definition of f that we should run. So we look for the method f in class x. We want to know its implementation and in particular we get the names of the former parameters. Okay x1 through xn and we get the body of the function or method. Alright So the next thing we have to do is we have to allocate space in the memory or in the store for the actual parameters of the method call. So we allocate new locations. Okay one for each actual argument and that were ready to build an environment in which we can evaluate the method alright? So what is this environment going to consist of? So we have to think about what names or in-scoped inside of a method. Well all the attributes of the class are in-scope. Okay. So this is a class x with attributes a1 through an so the environment will have those names to find a1 through an. And now what are the attributes or locations of those attributes. Well those are the locations of. The zero thats the object that were dispatching to that were going to be the self object and the attribute names will refer to the attributes of of self alright. So those locations here are the locations of of the attributes in the object v0. Now in addition the formal paramete rs are also in scope inside of the method body. So we add to this environment with just the attributes all of the formal parameters okay and they are at the new locations l(x1) up to l(xn). Okay? And notice one slight subtlety about the way this is defined were taking an initial environment which Ill show here with Ill Ill color these braces in blue. So were defining and initial environment of the attributes and then were doing updates to that okay? So were instead of just defining x1 to map two l sub x1 were saying were replacing The definition of x1 in this environment in the blue braces with one and maps x1 and l(x1). Why do we do it that way? Well the thing is that a method may have a formal parameter that is the same as an attribute name so for example I could have a class a that has an attribute little a in it And it also has a method f that takes a formal parameter named a. Okay And if I do that and of course Im leaving out types and lots of other things here. So here I have an attribute named a thats declared. And then I have a method that takes the argument called a. And then the question is when I refer to a. Inside of the body of the method what a do I get? Is this a is this a bind to the formal parameters is it bind to the attribute? And the answer we have to get one answer or the other the answer in Kuhl is that it binds to the formal parameter that hides the the outer name. Okay and thats and thats enforced here in the rule by these updates. So if a formal parameter has the same name as one of the attributes it will replace the definition of the attribute in the environment. Okay. Once we get the environment set up we need to set up our store what what are the changes to the store? What we just have to store the actual value of each argument at the location for that argument. And finally we are ready to evaluate the functioning body and the interesting part here is the context in which thats done. So notice here that the that the self object in in the context of running the method f is the object to which are dispatching. Okay? And then the environment is e prime the new environment we just set up and once again notice that this is a complete change of context that e prime the environment e prime has nothing to do with the environment e. E prime is built completely from scratch using only information about the method for calling it doesnt borrow anything from the from the environment where the method originated where the method was called from. And finally all of this is done in the store that has reflects all the side effects performed by evaluating the arguments by evaluating e0 and by extending the store with the locations for the actual parameters. So to evaluate the body of the method we get back a value and another updated store and that value in store are the results of the entire execution of the dynamic dispatch. In this video Im going to give a very brief introduction to Intermediate Code and its use in compilers. So the first question to address is what is Intermediate Code or an Intermediate Language? And as the name suggests an Intermediate Language is just that its a language thats intermediate between the source language and the target language. So keep in mind what a compiler does. So a compiler takes a program written in some source language. And it provides a translation of that program into some target language And so in this class for example where often our source language is cool and our target language is mixed assembly code. Now an Intermediate Language actually lives in between these two and a compiler that uses an Intermediate Language will first translate its source language into the Intermediate Language and then later translate the intermediate the code in the Intermediate Language into the target language. And you might wonder well why make life so difficult? Why when why do something in two steps if you can do in one step? And it turns out that for many purposes this intermediate level here is actually quite useful precisely because it provides an intermediate level of abstraction. So in particular the intermediate level may have more details in it than the source language. So for example if we want to optimize register usage you know a source language like Cool has no notion of registers at the source level and so theres no way to even express the kinds of optimizations you might want to do with registers. So an Intermediate Language that exposes that at that amount of detail at least have registers in it will allow you to talk about and and write algorithms that could try to improve the use of registers in the program. On the other hand the Intermediate Language which will also have fewer details than the target. And so it might be for example if the Intermediate Language is a little bit above the level of the parti cular instruction set of a particular machine and therefore its easier to retarget that that intermediate level of code to lots of different kinds of machines. Precisely because doesnt have all the grubby details in a of a particular machine. And experience has shown that this is actually a pretty good idea to have Intermediate Language. And almost all compilers have an Intermediate Language. I In fact in their implementation and some compilers have more than one. Some compilers actually translate through an entire the series of Intermediate Languages between the source and target language. Now were only going to consider one Intermediate Language for the rest of this course. The kind of Intermediate Language which were going to look at is going to be a high level assembly. And so as I suggested on the previous slide this language is going to use register names but it will have an unlimited number so we can use any number of registers that we like. Were not bound to 32 or 64 registers. The control structures will look a lot like assembly language. In particular there will be explicit jumps and labels on instructions. And the language will also have op codes in it so itll look like assembly language level op codes. But some of these op codes will be higher level. So for example we might have an op code called Push. And Push would end up translating into several concrete assembly language instructions for a particular target machine. In the intermediate code that well be looking at every instruction will have one of two forms. It will either be a binary operation or it will be a unary operation. And always the arguments on the right hand side in this case the y and the z will be either registers or constants. They could also be immediate values. And this is a very very common form of Intermediate Code so widely used and so widely used it actually has a name. Its called Three Address Code because every instruction has at most three addresses in it. Two arguments at most two arguments and then a destination. Now to see that this code is actually low level notice that you know higher level expressions that involve multiple operations will have to be translated into a sequence of instructions that do only one operation at a time. So for example if I have the expression x = sorry x + y z and let me put in parens here to show the association. So the times binds more tightly than the plus were going to have to this cant be written directly in an intermediate  language of this form. Instead we would have to write it something like the following. We have to first compute y z and assign that to a new register or a temporary or you know a new register t1 to hold the intermediate value. And then we would have to use t1 to compute x + t1 which of course is the value of the entire expression and that would end up getting stored in another register. I noticed that one effect of forcing you to use only one operation at a time. You see you do one primitive operation at time and then the result of that has to be restored in a register. One effect of that is to give every subexpression of the program a name. So if I look back at this expression here I see you know like y z is anonymous. That in this expression x + y &lt;i&gt;z the expression y &lt;/i&gt; z itself doesnt have a name. And by rewriting it like this I actually name that intermediate result. So again just to summarize this point one consequence of having to write out compound expressions as a sequence of instructions that do a single operation in time is that every intermediate value will be given its own name. Generating Intermediate Code is very similar to generating assembly code and were not going to go into this in any detail because it is so similar. But I will sketch it for you you know briefly. The main difference between generating assembly code and generating intermediate code is that we can use any number of registers in the Intermediate Language to hold intermediate results. To generate intermediate code we could write a function called IGEN for Intermediate Code Generation that takes two arguments. It takes the expression for which were generating code and it takes the register into which the results of that expression should be stored. And to give you just one example and this is the only example that Ill do. Lets take a look at generating intermediate code for a+ expressions. I wanna generate code for e1 + e2 and I want the results of that to be stored in the register t okay? So the first thing Im going to do is Im going to generate code for the subexpressions and I need some place to store the results of the sub expressions so Im just going to make up new register names for those results. So Ill generate code for e1 and store that in some register t1 and Ill generate code for e2 and Ill store the results of that in some register t2. And then we can just compute the sum. So t = t1 + t2 and notice that this is a Three Address Instruction. So were sticking to the rules here and only using three Address Instructions In our Intermediate Code Generator. And also notice that because we have an unlimited number of registers this actually leads to very simple code generation of intermediate code. In fact its even a little bit simpler than generating code for a stack machine. Recall that in a stack machine we had to save the intermediate results here of e1 on the stack. And that involved you know more than one instruction to actually push the result and adjust the stack pointer and things like that. And here we can just save it in a register and and then just use that register name later on. So that is actually all I have to say about Intermediate Code for this course. You should be able to use Intermediate Code at the level in which we are going to be using it in in lectures. The in the future videos well actually be looking at Intermediate Code quite a bit and using it especially to express certain kinds of optimizations. You should also be able to write simple Intermediate Code programs and you should be able to write algorithms that work on Intermediate Code. But Im not going to expect you to know how to generate Intermediate Code because were not going to discuss it any further. And quite frankly it doesnt introduce any new any idea. Thats really just a variation on the cogeneration ideas that weve already discussed in quite a bit of detail. We are now ready to begin our next major topic Program Optimization. In this video were just going to give overview discussing why we want to perform optimization and what the trade-offs are for compilers and deciding what kind of optimizations to implement. Optimization is the last compiler phase that were going to discuss. Lets just very briefly review the compiler phases. First there is lexical analysis and then thats followed by parsing. Then we have semantic analysis. And after that we talked about code generation. And now were going to talk about optimization okay? So optimization actually comes before code generation because we want to improve the program before we commit it to machine code but it is of course the last one that weve discussed. But just point out here optimization fits in between generally semantic analysis and code generation and in modern compilers this is where most of the action is. Its usually has by far the most code and its also the most complex part of the compiler. Now a very basic question is when we should perform optimizations? And we actually have some choices. We could perform them on the abstract syntax tree and a big advantage of that is that its machine independent but for many optimizations we want to do this it turns out that the abstract syntax tree will be too high level that we cant actually even express the optimizations we want to perform because those optimizations depend on lower level details of the machine or of the kind of machine that were generating code for that arent present in the abstract syntax tree. Another possibility would be to perform optimizations directly on assembly language and the advantage here that all the details of the machine are exposed. We can see everything that the machine is doing. We can talk about all the resources of the machine and so in principle any optimization we want to perform can be expressed at the assembly language level. Now a disadvantage of doing optimizations on assembly language is that they are machine-dependent. And then we would have to potentially re-implement our optimizations for each new kind of architecture. And so as we mentioned in the previous video another option is to use an intermediate language And the intermediate language has the advantage potentially if its designed well of still being machine independent. Meaning it can it can be a little bit above the level of the concrete details of very very specific architectures. I mean it can still represent a large family of machines but while at the same time exposing enough optimization opportunities that the compiler can do a good job of improving the programs performance. So we will be looking at optimizations that work on intermediate language that has operations given by this grammar. So in this case a program is a sequence of statements and a statement consists of either an assignment Which could be a simple copy or a unary or binary operation. We can push and pop things from a stack and then we have a couple of different kinds of jumps. We have a comparison in jump where we compare the value of two registers and then conditionally jump to a label. We have unconditional jumps and finally there are labels the targets of jumps. And the identifiers here are the register names and we could also use immediate values on the right hand side of operations instead of registers and the typical operators were just going to assume some typical family of operators like + - &lt;i&gt; etcetera. Now optimizations typically&lt;/i&gt; work on groups of statements and one of the most important and useful statement groupings is the basic block. So a basic block is a sequence of instructions and typically we want it to be the longest possible sequence of instructions. So we want it to be maximal and this sequence has two properties. First of all there are no labels except possibly for the very first instruction. And there are no jumps anywhere in this sequence of instructions except possibly for the last instruction. And a basic block the ide a behind a basic block and the reason we require these two properties is that its guaranteed to flow the execution is guaranteed to proceed from the first statement in the block to the last statement in the block. So the flow of control within a basic block is completely predictable. Once we enter the block once we begin at the first statement of the block which might have a label there will be a sequence of statements. That must all execute before we reach the last statement which could potentially be a jump to some other part of the code. But once we get here once we get to this very first statement then were guaranteed to execute the entire block without jumping out And furthermore theres no way to jump into the block. You couldnt just come from some other random part of the program and begin execution say at the second or third instruction. The only way into the block is through the first statement and the only way out is through the last statement. Say heres a example basic block and just to show you why basic blocks are useful. Lets observ that we can actually optimize this piece of code. Okay because three always executes after two. This instruction here always execute after this instruction. We could change that third instruction to be w = three x. Okay because we can see here that t is getting two x + x or two x and here were adding in another x and so w is actually always equal to three x. And a question then so that that is certainly a correct optimization and and its correct exactly because statement two is always guaranteed to execute before statement three. Another question we might be is whether we can eliminate this statement so once we replace this by three x you know maybe we dont need this assignment anymore if this was the only place that t was used if t was a temporary value that was computed only to compute the the value w. And then we can delete this statement and this depends on the rest of the program. We have to know whether t has any other uses someplace else in the program w hich we cant see just by looking at the single basic block. The next important grouping of statements is a control flow graph. And a control flow graph is a just a graph of basic blocks. And so theres an edge from block a to block b. If execution could pass from the last instruction in a to the first instruction of b. So essentially the control flow graph just shows how control flow can pass between the blocks and there isnt of course no interesting control flow within the block. We know that the basic block will just execute from the first instruction to the last instruction. So the control flow graph is a way of summarizing the interesting decision points in a in a procedure or a other piece of code showing where some interesting control flow decision is actually made. So heres a simple control flow graph consists of two basic blocks. The first basic block is outside of the loop and consists of some initialization code. And then we have one basic block here in the loop. The basic block consists of these three instructions. And at the bottom of the block is a branch a testing branch where either we exit and go someplace else or we loop around and execute the loop body again okay? And the body of a method can always be represented as a control flow graph. The convention that well use is always a distinguished entry node so a distinguished start node of the control flow graph and typically itll just be obvious itll be the one listed at the top. And then there will be some return nodes or one or some nodes of which you can return from and you know you have a return statements in the procedure. And return nodes or places where you exit the procedure will always be terminal. Meaning there will be no edges out of those blocks. Now the purpose of optimization is to improve a programs resource utilization. And for the purposes of this classroom when we talk about optimization in in our examples and in the videos were gonna be talking about execution time. And were gonna be talking about were g onna be talking about making the program run faster. And this is mostly what people are interested in. So most compilers do spent quite a bit of effort on making programs run faster but its important to realize that there are many other resources that we could optimize for. And actually for any resource that you can imagine there probably is a compiler out there that spend some effort optimizing for an insert domain domains of application. So for example there are compilers we might care about code size. We might care about the number of network messages sent other things that are commonly optimized for our memory usage disk accesses so so databases for example. Try to minimize the number of times you access the disk and and power for battery powered devices. And the important thing about optimization is that it should not alter what the program computes. The answer still must be the same okay? So were allowed to improve the programs resource utilization but we cant change what the program will produce. Now for languages like C and Cool and all of the languages that youre probably familiar with there are three granularities of optimization that people typically talk about. One is called local optimization and those are optimizations that apply to a basic block in isolation. So these are optimizations that occur within a single basic block. Then there are what are called global optimizations and this is really misnamed because its not global across the entire program. What people mean by global optimization is that implies to a control flow graph. Its global across an entire function alright so so global optimizations would apply to a single function and optimizer across all the basic blocks of that function. And finally there are inter-procedural optimizations these are optimizations that work across method boundaries. They take multiple functions and move things around to try to optimize the collection of functions as a whole. Many compilers do one in fact almost all compilers do one. Many many compilers today do two but not very many actually do three okay? So you see decreasing numbers of compilers doing these optimizations as you move up in the granularity and partly thats because the optimizations are more difficult to implement so its just more work to implement the inter-procedural optimizations but also because a lot of the payoff is in the more local optimizations. So expanding on that last point a little bit more. It turns out that in practice while we know how to do many many optimizations. Often a conscious decision is made not to implement the fanciest optimization that is known in the research literature. And thats kind of an unfortunate thing from my point of view being somebody whos really likes compilers and spent a lot of time thinking about optimization. And maybe its a little bit hard to accept for the professional compiler researchers that that people dont always want to implement the latest and greatest optimization. But its worth understanding why that might not be the case and it boils down essentially to software engineering. Some of these optimizations are really hard to implement I mean theyre just complicated to implement. Some of the optimizations are costly in compilation time. So even though the compiling happens offline it is not part of the running of the program you know the programmer still has to wait while the optimizing compiler compiles does its compilation and if it takes hours or in some cases days to optimize a program you know thats not necessarily great. And some of these optimizations have low pay off. They might always improve the program but they might only do it by a very small amount and unfortunately many of the fanciest optimizations in the literature have all three of these properties. Theyre complicated they take a long time to run and they dont do very much. And so its not so surprising That and not all of these to get implemented in production compilers. And this actually you kn ow points out what the real goal is in optimization. What we really want is maximum benefit for minimum cost. Were really talking about a cost benefit ratio. So like optimization costs a certain amount in code complexity complexity of the compiler In programmer time I mean waiting for the compiler to run and and the benefit the amount that it improves the program has to be sufficient to justify those costs. Now we are ready to begin talking about actual program optimizations and we begin with local optimizations. Local optimization is the simplest form of program optimization because it focuses on optimizing just a single basic block so just one basic block and in particular there is no need worry about complicated control flow we are not going to be looking at the entire method or procedure body. Lets dive right in and take a look at a couple of simple local optimizations. If x is an integer valued variable And from here on well assume that x has type-ins. So let me just write that down. Were going to assume that x has type-ins in all of our examples on this slide. Then the statement x=x+0 well that doesnt change the value of x. Zero is the additive identity for +. Were just going to assign x the value it currently has. And so this statement is actually useless. It can just be deleted from the program. Similarly for x=x&lt;i&gt;1. Multiplying by one&lt;/i&gt; will not change the value of X and so that statement can also be removed. And in this case these are great optimizations because we actually save an entire instruction. Now some statements cant be deleted but they can be simplified. A simple example of that is if we have x=x&lt;i&gt;0. So that can be replaced by the&lt;/i&gt; assignment x=0 And again we have we still have a statement here. We still have to execute a statement. But This statement may execute more quickly because it doesnt involve actually running the the the times operator. It doesnt involve referencing the value of X. Presumably X is registered that doesnt really cost anything. But you know its possible that this instruction over here will execute faster than this instruction over here. Now on many machines thats not the case. In fact this assignment of this this assignment on the right will take the same amount of time as the multiplication on the left but as we will see. Having a assignment of a constant to a variable will actually enable other optimization so this is still a very worthwhile transformation to do. An example thats almost certainly an optimization is replacing the exponentiation operator Raising a value to the power of two by an explicit multiply. So here were computing y^2 And over here we just replace that by y&lt;i&gt;y. Why is this a good&lt;/i&gt; idea? Well this explanation operator here is almost certain not a built in machine instructions. Probably this is gonna wind in our generated code being a call into to some built in math library. And there will involve a functioning call overhead. And then there will be some kind of general loop in there to do the right number of multiplies. Depending on what the exponent is. So in the special case where we know that the exponent is two. Its much much more efficient. To just replace that call to exponentiation by an explicit multiply. Another example of substituting one kind of operation for another In a in a special situation Is if we have a multiplication by a power of two. We can replace that by a left bit shift So here multiplying by eight. Thats the same as shifting the binary representation of x over by three bits And I and That will you know in fact compute the same thing. And it doesnt even have to be a power of two. If we have a [inaudible] location by some other number that is not a power of two that can be replaced by some combination of shifting and and subtractions. Okay? So we can replace the multiply by some combination of shifts and and arithmetic operations Simpler arithmetic operations. Now these last two here I should point out you know these are interesting transformations. On modern machines generally this will not result in any kind of speed-up because on modern machines the integer multiply operation is just as fast as any other single instruction. Now on historical machines these were actually significant optimizations. So all of these instructions together are examples of algebraic simplifications. So that just means exploiting properties of the mathematical operators to replace more complex instruc tions or more complex operations by simpler ones. One of the most important and useful local optimizations is to compute the results of operations at compile time rather than at run time if the arguments are known at compile time. So for example lets say we have a three-address instruction x=y op z. And it happens that y and z are both constants. These are both immediate values. These are you know literals in the instruction. Then we can actually compute the results of the right hand side at compile time and replace this by an assignment to a constant. So for example if we have the instruction x=2+2 that can be replaced by the assignment x=4 And another example which is a very common and important one is if the predicate of a conditional consists only of immediate values. Then we can pre-compute the result of that conditional And and decide what the target of the conditional will be. What the next instruction will be at compile time. So in this case we have a predicate which is going to be false because two is not less than zero And so we will not take the jump And so this instruction can just be deleted from the program. If we had the Otherwise if two is greater than zero so if this is some predicate to valuate true Then we would replace this conditional by the jump. Okay this would become an unconditional jump. Alright And this class of optimizations is called constant folding And as I said this is one of the most common and most important optimizations that compilers perform. Now there is one situation that you should be aware of and which can be very dangerous and this situation is actually very instructive as well. And so while it isnt that common I I wanted to mention it because it really illustrates some of the subtleties of program optimization and programming language semantics. So what is this dangerous situation? So lets consider the scenario where we have two machines. We have a machine X And we have a machine. Why? Okay and now the compiler is being run on machine X. And the compiler is producing code. Generated code this is the generated code produced as the output of the compiler over here. Thats gonna be run on machine Y. So this is a cross compiler. Okay So you are running the compiler On one machine but youre generating code for a different machine and why would you want to do that? Well. The the common situation in which you want to do this is that this machine Y over here is a very weak machine. So weak in the sense that its very slow and has very limited memory. Maybe very limited power then its beneficial to develop your program and even compile it on a much more powerful machine. So many embedded Systems codes are developed in exactly this way. Code is developed on some powerful workstations that are actually compiling it for some small embedded device that well executes the code. Now the problem comes If x and y are different. So consider the situation where x and y are different machines different architectures. Alright And Ive been implying that they are but they dont have to be. I mean I mean you could compile on one kind of architecture and run the same code on the same architecture. But the interesting situation is when x and y are different architectures. And so lets consider something like you know in in you know machine X lets say we have the instruction A=1.5+3.7. Mm-kay And you would like to constant fold that down to a=5.2 Alright? Now the problem is that if you simply execute this as a floating point operation on architecture x the round off and you know the floating point semantics in architecture x maybe slightly different from these semantics on architecture y. It could be that if you do that in architecture y directly that you might get something like a.5 you know a=5.19. There might be a small difference in the floating point result depending on whether you execute the instruction here or here. And this becomes significant in the case of constant folding and and cross compilation. Because some al gorithms really depend on the floating point numbers being treated very very consistently. So if youre going to round off the operation one way you need to do it that way for every time you do that particular operation And by shifting the computation from comp from run time when it would have executed an architecture y back into the compiler winds of executing architecture x. You can change the results of the program. So how do cross compilers actually deal with this? So so compilers that want to be careful about this kind of thing what they will do is they will represent the floating point numbers as strings inside the compiler and they will do the obvious long form addition and multiplication division operations are the floating operations directly on the strings. Keep the full precision Inside the compiler And then in the generated code produced the literal that is the full precision flowing point number And then let the architecture of the architecture y decide how it wants to round that off okay? So thats the really careful way to do constant folding of floating point numbers if youre worried about cross compilation. Continuing on with local optimizations another important one is to eliminate unreachable basic blocks. So whats an unreachable basic block? That is one that is not the target of any jump or fall through. So if I have a piece of code that can never execute and it might never execute because theres no jump that jumps to the beginning of that piece of code and its not it doesnt follow after another instruction that can fall through to it. Well than that piece of code that basic block is just not gonna be used its unreachable and it can be deleted from the program. This has the advantage of making the code smaller. So obviously since the basic block is unreachable its not contributing to the execution costs of the program in terms of the instruction count. So the code is never executed. So its not really slowing down the code because you know extra instructions are being executed But making the program smaller can actually make it run faster because of cache effects. So the instructions have to fit into memory just like just like the data. And if you make the program smaller it makes it easier to fit the program in memory and you may increase the spacial locality of the program. Instructions that are used together may now be closer to each other. And that can make the program run more quickly. Before continuing on I want to say a word or two about why unreachable basic blocks occur. So why would a programmer in their right mind ever write a program that had code in it that wasnt going to be executed? And theres several actually ways in which unreachable code can arise and its actually quite common. So this is an important optimization getting rid of the unreachable code is actually fairly important. Perhaps the most common situation Is that the code is actually parameterized with code that is only compiled and used in certain situations. So for example in C It would be sorta typical to see some code that looks like this. If debug then you know executes something where debug is a pound defying constant. So in C you can define names for literals. So you say something like this. You might define debug. To be zero and so you might see a program that had this piece of code in it and what this literally means is that this piece of code is equivalent to if zero then blah blah blah. Alright so so when youre compiling without debugging you have debug to find the zero when youre compiling with debugging you would change this line to define debug to be some non zero constant. So in this case we are compiling without debugging. What will happen? Well well see that this predicate is guaranteed to be zero the constant folding will take care of that. And that will result in an unreachable basic block on the ven branch and then that code can be deleted And so essentially the compiler is able to go through using the optimizer and strip out all  of the debugging code. That isnt going to be used since your compiler without debugging. Another case where unreachable code comes up is with libraries. So very frequently programs are written to use generic libraries. But the program might only use a very small part of the interface. So the library might supply 100 methods to cover all the situations that various programmers are interested in. But for your program you might only be using three of those methods. And the rest of those methods could potentially be removed from the final binary to make the code smaller. And finally another way that unreachable basic blocks occur is as the results of other optimizations. So as we will see optimizations frequently lead to other to more optimizations. And it could be that just through other rearrangements of the code that the compiler makes some basic block redundant and and able to be deleted. Now some optimizations are simpler to express if each register occurs only once on the left-hand side of an assignment. So that means if each register is assigned at most once then some of these optimizations are easier to talk about. So were gonna rewrite our intermediate code always to so that its in single assignment form. So this is called single assignment form. And all that means is that if we see a register being reused like over here we have two assignments to the register X. Okay. Were just going to introduce another register name for one of those assignments. So in this case Im just gonna rename the first use of X here definition of X here to be some new register B. Ill replace the uses of that X by the name B and now I have an equivalent piece of code that satisfies single assignment form. Every register is assigned at most once. Lets take a look at an optimization that depends on single assignment form. So were going to assume the basic blocks are in single assignment form and if they are then were going to know That a definition of a register is the first use of that register in th e block And so in particular were also ruling out things like this. So there could be something like this where X is read. And then later on X is used. Okay. Sorry X is read and then later on X is defined. So were not going to allow this. This register here would have to be renamed to something else say Y And then uses of X later on here are renamed to Y. Alright so were going to insist that whenever we have a definition Of a register in a basic block. That is the first use of that register in the block. Alright and if if thats true if we main if we put things in that form and thats thats easy to do as weve seen. Then when two assignments have the same right hand side theyre guaranteed to compute the same value. So take a look here This example. So lets say we have an assignment x=y+z. And then later on we have another assignment w=y+z. And we said that there could only be one assignment to x in any basic blocks. So all of these instructions that are alighted here they cant be assigning to X. And they also cant be assigning to y and z. Y and z already have their definitions. So y and z cant be changed. And that means that x and w here actually compute the same value. And so we can replace the second computation Y plus C by just the name that we already have for it X. Okay and this saves us having to recompute values. Alright so this is called common sub expression elimination. Common its a rather long name. Sub expression. The elimination. And this is another one of the more important compiler optimizations. This is actually something that comes up surprisingly often. And saves quite a bit of work if if you perform this optimization. So another use of single assignment form is that if we see the assignment w equals x in a block. So here the register w is being just copied from the register x. Then all subsequent uses of w can be replaced by uses of x. So for example Here we have an assignment to b And then we have a copy a is=to b. And then down here w e have a use of a in the last instruction. Well that use of a in the last instruction can be replaced by a use of B. And this is called copy propagation okay? Propagating copies through the code And by itself notice that this makes absolute no improvement in the code its only useful in conjunction with some of the other optimizations. So for example in this case after we do the copy propagation it might be the case that this instruction can be deleted. If A is not used any place else in the code then this instruction can be removed. Now lets do a little more complex example and use some of the optimizations that weve discussed so far On a slightly bigger piece of code. So we are starting with this piece of code here on the left and we are going to wind up with this piece of code here on the right. And how does that work? Well first we have a copy propagation so we have A is assigned the value five. And so we can propagate that value forward. And replace the use of a later on by five and I should say. That when the value is propagated is a constant rather than a registered name is called Constant propagation instead of Copy propagation but its exactly the same thing. We we we have a single value assigned on the right hand side either a register name or constant and we are replacing uses of that in later instructions by that register name or constant. Okay? So once we have replaced a here by five now we can do constant folding and now we have two constant arguments for this instruction. So this two times five can be replaced by the constant ten. Now notice we have another assignment of a constant to a register and so we can propagate that constant forward. We can replace the subsequent uses of X by the number ten. And now we have more opportunities for constant folding ten plus six can be replaced by the value sixteen. Alright now we have another another value here which is a a constant assignment so another instruction here which is just an assignment of a constant to a register so we can p ropagate that constant forward. Alright then we wind up down here with ten times sixteen And I see over here in my final example here I didnt bother to propagate the ten to x. But we can do that and this So we can either do this optimization. So x times sixteen if we didnt do the propagation would be equivalent to x left shift four. Or we can just replace this by ten times sixteen. Thatd be even better. We wind up achieving the value 160. Returning to an idea I mentioned a couple of slides ago. Lets say there is an assignment in a basic block. Some registered W is assigned some value thats computed on the right hand side. Lets say that W the registered name is not used anywhere else in the program. It doesnt appear anywhere not only in this basic block but in any other part of the procedure in which this statement appears. Well then the statement is dead and can be just deleted from the program And dead here means it does not contribute to the programs result. Since the value that we write into W is never referenced anywhere W is never used doing the computation of W in the first place was a waste of time so we can just delete that computation. Heres a simple example. Lets assume that the register a is not used any place else in the program. And the first thing we have to do so heres our initial piece of code. The first thing we do is we put it in single assignment form. And so Ive renamed here this register x to be register b. Okay and once we do that let me do that so well say that B=Z+Y and A=B and then we propagate this forward. Alright so weve now replaced this use of A by B so this takes us to this state where we have this piece of code. Now we can see that we have an assignment to A. A is not used in the subsequent instruction. We already said that A is not used anywhere outside of the basic block and so the assignment a=b can be deleted and we wind up with this shorter basic block. Now each local optimization actually does very little by itself. And some of these optim izations some of these transformations that are presented actually dont make the program run faster at all. They dont make it run slower either but by themselves they dont actually make any improvement to the program. But Typically the optimizations will interact. So performing one optimization will enable another. And we saw this in the little example that I did a few slides ago. So the way to think about an optimizing compiler is that it has a big bag of tricks. It has a lot of. Individual program transformations that it knows And what it is going to do when faced with a programs optimize its going to rummage around in its bag looking for an optimization that applies to some part of the code. If it finds one it will do the optimization it will do the transformation and then it will repeat. Itll go back and look at the program again and see if theres another optimization that reapplies. Then it will just keep doing this until it reaches a point where none of the optimizations it knows about can be applied to the programming. Next well take a look at a bigger example and try applying some of the optimizations that weve discussed to it and see how far we get. And of course this example has been constructed to illustrate many of the optimizations that we discussed. So the first thing we can do. There are a couple of opportunities for algebraic simplifications. So we can replace the squaring up here by a multiply. And down here we had a multiply by two which we can replace by a left shift of one. Next we can observe that we have some copies and constants. So we have a constant assignment to b and a copy assignment to c And those can be propagated forward to the uses of b and c. Once weve done that we can do constant folding. So here the assignment to e The opera- the arguments to the shift are all constants And so that can be replaced by an assignment that e gets the value six. Next we could observe that we have a common sub expression that we could eliminate that both a and d have the value x times x. So the assignment to d could be replaced by a copy that d now gets the value of a. Now we have two opportunities again for copying constant propagation the assignment to D and the assignment to E can be propagated forward. And finally we can do a bunch of dead code elimination. So assuming that none of these values B C D or E is used anyplace else in the program all four of these statements can be deleted. And this is where we actually get some real performance improvement. So here we actually are now saving entire instructions and thats the best kind of savings that we can have And so we wind up with this as our final form. So notice that a is assigned the value x&lt;i&gt;x. F is&lt;/i&gt; then assigned the value a+a And then g is assigned the value six&lt;i&gt;f. Now this is&lt;/i&gt; not quite as fast as it could be alright? Theres actually one more algebraic optimization that could be done. We can notice here that f is actually=to two&lt;i&gt;a&lt;/i&gt; And then we could do some rearrangement here to discover that g=12&lt;i&gt;f. Sorry sorry&lt;/i&gt; twelve x a. Alright And then this statement assignment to F might become dead code and we could delete it from the program. I think some compilers would actually find this but I believe that even current state of the art compilers many of them would not discover this last rearrangement to the program. In this short video Im going to say a few words about a variation on local optimization that applies directly to assembly code called Peephole Optimization. The basic idea here is that instead of optimizing on intermediate code we could do our optimizations directly on assembly code And people optimization is one such technique. The peephole is stands for a short sequence of usually continuous instructions. So the idea is that we have our program. We can see we can think of it as a long sequence of instructions and our peephole is some window onto this program. So if we have a peephole of size four we can think of ourselves as staring through a small hole at the program and all we can see is a short sequence of four instructions and then we can optimize that sequence. So then we can slide the peephole around and optimize different parts of the program And the what the what the optimizer will do is it will you know stare at this short sequence of instructions and if it knows a better sequence it will replace that sequence by the other one and then it will repeat this as I said. You know applying other transformations to to possibly the same or other parts of the assembly program. So people optimizations are generally written as replacement rules. So the well have the window of instructions on the left. So itll be some sequence of instructions and well know some other sequence of instructions that we would prefer on the right. So if we see this instruction sequence on the left then well replace by the one on the right-hand side. So for example if I have a move from register b to register a and then I move back from register a to register b well thats the second move is useless can can just be deleted as a way to replace this two instruction sequence by a one instruction instruction sequence. And this will work provided that theres no possible jump target here. So if if theres no possibility that the code would ever jump to this instruction then that instruction can be removed. Another example If I add i to the register a and then I subsequently add j to the register a I can do a constant folding optimization here and combine those two add two additions into one addition where I add the sum of i = j to the register A. So many but not quite all of the basic block optimizations that weve discussed in the last video can be cast also as peephole optimizations. So for example if we are adding zero to a register and were storing it in another register well that can be replaced by a register move. If were moving a value from the same register to itself so this is like a self-assignment well that instruction can just be deleted replaced by the empty sequence of instructions. And together for those two instructions would be those two optimizations excuse me would be able to eliminate adding zero to a register. So first this would get translated into a move from a to a. And then the move from a to a would get deleted. And as this little example illustrates just like with local optimizations people optimizations have to be applied repeatedly to get the maximum effect. I hope this simple discussion has illustrated for you that many optimizations can be applied directly to assembly code and that theres really nothing magic about optimizing intermediate code. So if you have a program written in any language source language intermediate language assembly language. It makes sense to talk about doing transformations of programs written in that language to improve the behavior of the program. And its also a good time here to mention that program optimization is really a terrible term. The compilers do not produce optimal code and its purely an accident if a compiler were to somehow generate the best possible code for a given program. Really what compilers do is they have a bunch of transformations that they know will improve the behavior of the program. And theyll just improve it as much as they ca N. So really what program optimization is all about is program improvement. Were trying to make the program better but theres no guarantee that we will reach the best possible code for a given program. In this video we are going to continue our discussion on global data flow analysis by taking a look at how global constant propagation works in detail. To begin lets review what the conditions are to do global constant propagation. So to replace a use of a variable x by a constant k we have to know the following property. That on that on every path to the use of x the last assignment to the variable x is x equals the constant k okay? And this has to be true again on every path to the use of x. Now global constant propagation can be performed at any point where this property holds. What were going to look at in this video is the case of computing the property for a single variable x at all program points. So were going to take one were going to focus on one variable x and were going to compute whether its a constant at every program point. Its easy to extend the algorithm to compute this property for all variables. One very simple but very efficient way to do that is just to repeat the computation once for each variable in the method body. The way we are going to compute the information that we want is to associate one of the following values with the variable x at every point in the program. And lets start with the last one here we will assign x this special value here which is pronounced top if x is not a constant. So if we cant figure out whether x is a constant at a particular point in the program then well just say x is top at that point. And this is going to be our safe situation. Its always okay to say we dont know what the value of x is and when we say that x has a value top and we could say we we were essentially saying we dont know whether x is a constant or not at this point in the program x could have any value. Alright? Now another possibility is that we will say that x is some constant c okay? So this is a particular constant and if we say that x is a constant c at a program point that means in fact at that program point we believe o r we have proven that x is always that constant. Now there is a third possibility which is not immediately intuitive perhaps. But as we will see plays a very important role in algorithms for for global constant propagation. And in fact in all global data flow analysis and that is bottom okay? So this value is pronounced bottom and intuitively the idea anyway that is kind of opposite of top alright? And the interpretation of bottom is going to be that this statement never executes alright? So when we dont know whether a statement is even executed at all we will say that x at that point has a value bottom. Meaning that as far as we know that point in the program is never reached. It doesnt matter what the value of x is at that point because that statement never executes. Alright so were going to assign x one of these three kinds of values. Either bottom some constant or top. Lets begin by working through an example by hand and our goal is going to be for every program point to decide whether x could be a constant definitely not a constant or whether we think that statement might not ever execute okay? So execution will begin at the top of this control flip graph. So this the entry point and before executions begins we dont anything about the value of x. So Im not making any assumptions about what code came before this basic block and so to be safe I will say that at this point x has some unknown value. We dont know what the value of x is it could be anything. So x = T is the property that we want entry to the first basic block. Now after the assignment x = three that was indicated there where what point were talking about. So after the assignment x = three well definitely will know that x is the constant three. Alright now theres something here thats worth pointing out which is that our program points the points that were attaching this knowledge to or these these facts to are in between the statem ents. So when I say x = three at this program point what I mean is that after x after this assignment has executed x = three but before this predicate of the conditional has executed I know that x = three okay? So the program points are in between statements and theres a program point before and after every statement. Alright so the next thing that happens is this conditional branch. Notice that the branch doesnt update x doesnt even refer to x so after the branch executes well definitely knows that x = three on both branches. Alright now lets do the right hand branch. The next thing that happens is the assignment to y that would not affect the value of x. So after the assignment to y well still know that x = three alright? Now lets take a look at the left hand branch so the first thing that happens over here is another assignment to y. Well that wont affect the value of x. After the assignment of Y well know that x = three. And now comes to the assignment of x alright. So after this assignment happens at this program point were going to know that the value of x is different. Were going to know that x = four alright. So now after this statement we know x = four and after this statement over here we know x = three alright? Now what do we know then about what happens before this statement okay? The a = two x and I just want to point out here. I said that theres a program point before and after every statement And so this program point here which is before this assignment to a is different from the program points that are after x = four and y = zero. So intuitively after x that x = four and over here after y = zero we still know that were on this path is x = three. But when we reach the point before a = two x we no longer know which path were coming from. This is the point of the merge of these two paths that both lead to this statement. And what can we say about the value of x here? Well there is no constant that we can assign to x because on o ne path x is three and on the other path x is four. And so what we have to say here is that before this assignment executes a = x sorry x = T. We dont know what the value of x is. Another way of saying it is we know we we dont know that x is a constant alright. So after the assignment executes it doesnt affect the value of x we will also have that x = T. Now notice that once we have the global constant information once we know for every program point what the state of x is its going to be very easy to perform the optimization. We simply look at the information associated with the statement and then it will tell us whether x is a constant when that statement executes or not. And if x is a constant at that point then we can replace that use of x by the constant. And crucial question of course is how do we compute these properties. So we did this example by hand but how in a systematic fashion an arbitrary control flow graph do we actually compute these properties for x for every program point? Now were ready to talk about data flow analysis algorithms and theres one basic principle that you see in all of these algorithms thats worth mentioning right away. And thats that the analysis of a complicated program can be expressed as a combination of very simple rules that relate the change in information between adjacent statements. So were just going to focus on local rules and the way were going to build our global data flow analysis is actually by a combination of rules that look only at a single statement and its neighbors. The idea behind the rules is going to be the push or transfer information from one statement to the next And so for each statement s were going to compute information about the value of x immediately before and after s. Remember thats where those are the program points that we want to attach information to. So in particular were going to have a function C. It stands for constant information and C will take three arguments takes the name of the variable x. It takes the stat ement that were talking about the particular statement in the program that were looking at. And then either in or out and this is what distinguishes the value of x before s executes versus the value of x after s executes. Were going to be defining a set of transfer functions that push information or transfer information from one statement to another. And in the rules for constant propagation we need to talk about a statement and its predecessors. So were going to say that every statement s has some set of immediate predecessors p1 through pn alright? So its either of these statements that lead in one step to the statement s. Lets do our first rule. So we have a statement s and it has some set of predecessor statements P1 P2 P3 P4. And the situation that were interested in here is lets assume that x is top at the program point after one of these predecessors. So after some predecessor it doesnt matter which one if it happens that x is top at the program point after that predecessor well then x has to be top before the execution of s okay? So thats what this rule says. It says if the out of any predecessor for x is top then the in of s for x is also top. Alright and this makes sense. It says that if we dont know whether x is a constant on some path that leads to s well then we dont know that x is a constant at s. Because for all we know execution came down that particular came from that particular predecessor and so we cant make any prediction about whether s is whether x is a constant before s executes. Now lets look at another situation. Lets say that x is some constant C after the execution of some predecessor. And that on a after another predecessor a distinct predecessor x is a different constant D. So D is not equal to C. Well then what do we know about x at the program point before s executes? Well we dont know anything x has to be top because we dont know which constant s will be since we dont know which path will reach s at run time. And this is the situation that we saw in the example we did by hand. Another possibility is that the predecessors all agree on what the value of x could be. So lets say that we have you know predecessor here and that after it executes x is known to be the constant C and x is known to be the constant C after this predecessor and x is known to be the constant C after this predecessor. Theres one other possibility. Lets say that after this predecessor over here all we know is that x is bottom okay? And so what the rule says is that if we have this situation where either x has the property bottom after a predecessor or all the predecessors agree on the particular constant that x could be then before at the program point before s executes we know that x is going to guarantee to be the constant C. And if you think about it for a second its easy to see why this is correct. First of all clearly if we come along one of the paths where x is known to be the constant C since they all agree and then when we get to s x will definitely have the value C. What about the bottom case? Well remember what that means. That means that this statement is never reached so theres some predecessor P here which never executes. Which means if P never executes then we could never reach S along this path from P. So the only paths that will reach s are the ones where x is known to be a constant alright? So thats why its okay in this situation say that x if control if execution reaches s at all its guaranteed to reach it in a state where x is the constant C. One last possibility is lets say that x is bottom for all the predecessors okay? And what does that mean? Well that means that every predecessor of S never executes so theyre all unreachable. And therefore if every predecessor of x never executes s itself can never execute and so we can conclude that entry to s x is bottom. The first four rules that we just looked at relate the out of one statement to the in of the next. We also have to have rules that relate the in of a statement to the out of the same statement. So we have to push information from the input of a statement to the output of the same statement. So once again there are several cases. And lets take a look at an easy one first. If x is bottom on an entry s if the program point before s well that says that at the that s is never reached that s never executes. And therefore x will be bottom after s after s as well. So if the program point before s is never reached the program point after s definitely cant be reached either. Another possibility is that were assigning x to constant C in this statement. In that case the out of the statement is going to be equal to C. Alright so it doesnt matter what the state of x was before the statement after we execute the statement x will be the constant C. And I should say there is a conflict with the previous rule. Okay it could be that x is bottom before the statement. So rule six has lower priority than rule five. So we so if we could say that x is bottom after the statement we would prefer you to say that so rule five would be applied first and then if rule five does not apply. So if x is some other constant D or x = T then we would apply this rule and we would conclude that x is the constancy afterwards and that makes sense. If x is d or x is the is top that means that control as far as we know can reach this statement. And then what were saying here is that well after the execution of this statement if control can reach this statement after the execution of it x is guaranteed to be the constant C. Another possibility is that we have an assignment to x but the right hand side is more complicated than a constant. So this case is for everything other than the constant assignment. Okay so this F here just stands for some more complicated expression than just a simple constant. And in this case we were just going to say we dont know what the value is were not going to try to guess what the result of that computation is and well just say that x = T. X w e dont know what the value of x is after the execution of this statement. And once again rule five takes precedence so if rule five applies then we would apply then then we would use that rule instead of rule seven. But if control can reach this statement so up here x = C or x = T. Then well apply rule seven and conclude that x is top after the statement. And finally Rule eight another possibility is that were assigning to some variable other than x. And in that case if x = k before the statement then we just keep that value. Okay so whatever x was before the statement bottom a constant or top if the assignment is to some other variable other than x then x will have the same property after the statement executes. Now we can put these rules together into an algorithm. For every entry point for every entry statement to the program were going to say on entry that we dont know anything about the value of x. So the program point before that entry point were gonna say that x has an unknown value top. And then everywhere else were going to say that the value of x is bottom okay. And this is actually important so were going what this intuitively is doing is its saying well as far as we know except for the entry point to the program which can definitely be executed we dont know whether any of the other statements in the control flow graph are actually ever executed and so were going to assume initially that theyre not. And were just going to say that x has the value bottom everywhere except at an entry point And now what were going to do is a kind of constraint satisfaction algorithm. Were going to pick some statement that doesnt satisfy one of the rules one through eight. And then were going to update it using the appropriate rules. So well look for places in the control flow graph where the information is inconsistent according to the rules and then well update the information to make it consistent with the rules. Lets take a look at our example again. So were going to start out by saying x = T at the entry point and then were going to have all of our other program points And let me indicate them here. Okay so these are all the other program points that we have to be concerned with. And there again theres a program point before and after every statement. And we are going to say the x = bottom for all of these. So again what this means is that as far as we know control doesnt reach any of these points. We have not yet proven to ourselves that any of these statements can execute. And now we just look around in the program and try to find places where the information is inconsistent according to the rules and then we update the information. Let me switch colors here. So when we begin the information is consistent everywhere except at this first statement because if x is T before and were assigning x to value three. Well then we should not have x = bottom as the result. In fact this should be x = three. It should be the appropriate information here and once we update that then we see that this next statement is inconsistent because now we know this statement is reachable. We have a statement here and were concluding that the point after is not reachable which is not not correct according to the rules. So that I believe that this is an application of rule eight. We have a statement here that doesnt refer to x as and so whatever the value of x was before the statement becomes the value of x after the statement so that becomes x = three. And then now we can see that this information is inconsistent. The out of the statement here is not consistent with the in of the statement here. In this case you know its just one predecessor. And so the the value should be the same so x should be three. At this point And similarly x should be three at this point. Here we have an assignment to a variable other than x. That should information should be the same before and after the statements same thing here. Now we have an assignment x. The point before that assignment is reachable and so sin ce this is a constant assignment we should know that x is that constant after the assignment. So here again we have a in and out issue so the out of this statement is not consistent with the in of this statement. So this is going to have to be updated but now what should this be? Well we have two inconsistent predecessors and so this has to be top and then finally an assignment to x sorry an assignment to a state to a variable other than x so the information should just propagate across. And that same is updated like this so now x is known to be top afterwards. And now if we look around at all the program points wed see that all the information is consistent. All the rules if you if you if you check whether the information before and after a statement or across a statement. Im sorry or between predecessors and successors is correct its correct everywhere according to the rules and so were done. In this video were gonna continue our discussion of analysis of controlled flow graphs by focusing on what is undoubtedly the most interesting aspect of the whole problem the analysis of loops. Heres an example of control flow graph with a loop in it. And it turns out that the need for the special element bottom in our analysis is intimately tied to the analysis of loops. And so lets just think about how we would do our constant propagation example analysis with this particular control flow graph all right. So what do we know about x? Okay. So initially we dont know anything so before we enter the control flow graph its value its top and and after the assignment of three well know that x has the value three. The conditional branch here the predicate wont affect the value of x. So itll be three on both branches. The assignment to y wont affect it so itll be three here as well. And now we come here okay and lets focus on this statement right here. So the rule is that the analysis of x at y equals zero. Okay So with a value of x right here before before the assignment to y. Is a function of all the predecessors. So we need to know what the value of x is on both of the incoming edges. Okay. Well we dont have a value down here yet. So the question is you know what is the value of x here on this edge? And in order to figure that out wed have to look at its predecessors. Okay what are its predecessors? Well theres this point here after the predicate theres this point here between the two statements and then theres this point here after the execution of y. Were just following the edges backwards here. Looking at you know where we need to know information for x. We need to know it here we need to know it here and we know it here alright? And then because of this edge that means we again need to know it at both of the predecessors of y = zero. So now were in the loop and this isnt too surprising. I mean if you have if information about x depends on t he predecessors of a statement and you do follow that recursively then youre gonna wind up going around loops like this. And and theres no good way at least theres no no particularly immediately obvious way to solve this problem. So how do we I get information about the predecessor the predecessors of y = zero when they depend on themselves? So to be more precise looking at that particular statement again in order to compute whether x is constant at the point right before the statement y information depends on his predecessors which include y = zero. Okay so this is the conundrum. So how are we to solve this recursive problem. And theres a standard solution that that is actually used in many areas of Mathematics and not just in the analysis of of loops. When you have these kinds of recurrence relationships or recursive equations. And the standard solution is to break the cycle by starting with some initial guess. So you have some initial approximation that is really not perhaps even expected to be the final result but allows you to get going. So and so what were going to do is that because of the cycles all of the points all the program points have to have values at all times. And so were going to assign an initial value and that is what bottom is for. And the initial value bottom means so far as we know control never reaches this point. Remember this weve said this quite a while ago on several videos ago. And this will allow us to make progress. And to see that lets go ahead and analyze this control flow graph now where we assume that all points and at all points initially x has the value bottom except at the entry point. So the entry point is special. Here we assume that we dont know anything about x because we know the control reaches the initial point. But initially were going to just say well x is bottom everywhere else. Okay so [inaudible] the bottom there [inaudible] bottom there okay Im gonna just fill in all the values. And Im just writing it everywhere here. And theres really another one right here after the merge of these two paths. So I indicate that. All right so there now we have our initial setup and now remember what the procedure is we go and look where the information is inconsistent and we update it. So where is the place where the information is inconsistent? Well clearly its not correct here all right because we know that after if if control reaches the point before x = three then after the assignment x will be equal to three. Again the predicate will not change the value of x so we have to update the results of the two branches after the predicate and after its assignment that doesnt affect x to make that information consistent that we have that. Now lets go back to our interesting case. Here we know that x = three on this branch coming in to y = zero. And so far as we know control never reaches the other predecessor. So were gonna start out by assuming that that that part that path is never taken. And if that path is never taken then it wont contribute anything. And s at this point in the program we will know that x = three. So assuming that all this information is correct we will be able to conclude that x = three at this point. And notice how weve been able to break the cycle here and get started. So we just assume that the you know this last edge in the cycle never executes and if thats not correct well find out later and this value down here will become something other than bottom and then well update the assignment again. Alright so lets continue on. So we have x = three before y is assigned zero. So the assignment of y will not affect the value of x. So make the information afterward consistent well have to make x=3 there. Now we have a merge of two paths. Okay. So the at this point here before the execution of this assignment we will also know that x = three. The assignment a will not affect x. Well update that point there and the predicate will not affect the value of x. So well know that x = three on the back edge. And now this information has changed. We now know the control can reach this edge cuz we followed the control path all the way here. We have some new information about x and so now we have to double check that everything is still okay. So here we have x = three on this edge x = three on this edge and our previous conclusion that x = three on the entry to the statement y = zero. Well that is also consistent. There are no places left in the control flow graph that are inconsistent. So all the information is consistent with all the rules And so were done and this is the final analysis. Were able to conclude that all at all of these points here like I say every point except the entry point that x is in fact the constant three. In the last several videos weve been talking about doing a kind of abstract computation. Computing with elements like Bottom the Constants and Top. And in this video were going to start to generalize those ideas a little bit. And the first thing were going to talk about the first step towards that generalization is to talk about orderings of those values. First Id like to introduce a technical term. These values that we compute within program analysis things like Bottom the Constants and Top these are called Abstract Values. And thats just to distinguish them from the Concrete Values so the Concrete Values are the actual run-time values that a program computes with. Things like actual objects and numbers and things like that. And the Abstract Values here the program analysis uses are in general more abstract. Some particular Abstract Values can stand for a set of possible Concrete Values. And in a particular set of Abstract Values were using for concept propagation theres only one very Abstract Value and thats the Top and it stands for any possible run time value. So it stands for the entire set of run time values. Anyway it turns out that there is a way to simplify the presentation of of the analysis that we have been discussing by ordering the Abstract Values. So were going to say is that Bottom is less than all the constants and that and all the Constants are less than Top. And so if we draw a picture with the lower values drawn towards at the bottom picture and the higher values drawn at the top. And and edges between values where theres a relationship we get this diagram here. So you have bottom down here underneath all the other values Bottom is less than every Constant. Okay. So notice that all the constants are here on the middle level alright? And also notice that the constants are not comparable to each other alright? So this ordering is different than the numeric ordering. So zero is not less than one for example. Zero and one are inco mparable as are every other pair of Constants. So you have you know Bottom at the Bottom. You have all the Constants in the middle and theyre incomparable And then bigger than everything else is Top. Now with the ordering defined theres a useful operation we can define on collections of elements and that is the Least Upper Bound or LUB alright? And and this means is taking the smallest element that is bigger than everything in the Least Upper Bound. So for example if I have the Least Upper Bound of Bottom and one that is equal to one okay? If I had the Least Upper Bound of Top and Bottom that is equal to Top. And perhaps more interesting one the Least Upper Bound of one and two so two incomparable Constants here. And remember the meaning of the Least Upper Bound its the smallest element in the ordering thats bigger than everything over which were taking the Least Upper Bound. So we just have two things here in our Least Upper Bound. But the Least Upper Bound of one and two the smallest thing thats bigger than both of them or greater than or equal I should say both of them is Top okay? And so the Least Upper Bound then if you think about it if you draw draw our picture again. So we had Bottom and we had Top and if you pick out some points here lets say we want to take the Least Upper Bound of Bottom and two youre just picking the smallest thing thats bigger than both. Well thats going to be two itself similarly two on Top you will get Top. And then if have anything thats incomparable then you have to pick something thats bigger than both of them and in this case that will always end up being Top for this very simple ordering alright? Then given this idea of the Least Upper Bound it turns out that rules one through four all theyre doing is computing the Least Upper Bound. So the in of a statement is just equal to the Least Upper Bound of the out of all the predecessors. Alright and thats all that rules one through four were saying. And if you remember what we had there we had you know we had a bunch of predecessors and then theres some kind of statement s and all were doing is whatever the information is on these predecessors were just taking the Least Upper Bound over it all right? And that is the information on entry to to s. The ordering on the Abstract Values also helps to clarify another important aspect of our analysis algorithm which is why it terminates. So remember the algorithms termination condition is to repeat to repeatedly apply the rules until nothing changes until there are no more inconsistencies in the control flow graph and theres no information left to update. Well just because we say were going to repeat until nothing changes that doesnt guarantee that eventually nothing changes. It could be that that goes on forever that we always introduce new inconsistencies with every update and we never actually get to the point where all the information is consistent. So the ordering actually shows why that cant happen and the algorithm is guaranteed to terminate. So remember that in every program point except the entry point the values start as Bottom. So they start at the lowest place in the ordering. And then if you look carefully at the rules its easy to see that the rules can only make the values increase at a program point. So Bottom can be promoted can be changed at a given program point up to some Constant and and and another update could raise that Constant to Top but of course once we get the Top theres no greater element. And if the rules can only make the elements increase then eventually we have to run out of elements that could be increased okay? So what that says is that each piece of information were computing for every statement for every variable and for either in or out it can change at most twice okay? So it can go from a Bottom to a Constant and from Constant to a Top but after that it will never be updated again. And what this means is that the constant propagation algorithm that weve described is actually linear in program size. So the number of steps is gonna be bounded by the number of c values that were trying to compute times two cuz each one of those could change two times. And since theres one value for the entry and exit over the in and out of every statement the total number of steps that the algorithm can possibly take is the number of program statements times four. In this video were going to look at another global analysis called liveness analysis. So in the past several videos weve looked at a procedure for globally propagating constants through a control flow graph And lets heres heres one of the control flow graphs weve been looking at and recall that this algorithm that we discussed would be sufficient to show that we could replace this use of x here by the constant three. And once we do that. This assignment x might no longer be useful. It might not be used anywhere And so we could potentially delete this statement from the program And that would be a real optimization an important optimization to do. However we can only do that if x is not used elsewhere in the program. So lets be a little more careful about what we mean by saying that x is not used. So down here is a use of x a reference to x in a statement. And clearly this particular reference to x is use picking up the value thats defined by this right x here. So we say that the right of x here is live. This one is live. Okay And what that means is that the value may be used in the future. So live equals may be used. In the future Okay? So the value written to x at this line of code maybe used by some subsequent instruction And here its not just that it may be used. Its actually guaranteed to be used because theres only one path. And that one path has a reference to x on it before theres another assignment to x. Okay? So this particular value of x as written here is guaranteed to be used. But in general we dont require that. We just mean there has to be a possibility that it will be used. Now in contrast lets take a look at this. Other statement in this example Here we assign x a value three but this assignment x this value of x is never used. This one is dead. Alright? Because the value three here is overwritten by the value four before theres any use of the variable x Okay? So this particular right to x will never see the light of day. Itll never get used by any part of the program. And we say that it is dead. So to summarize a variable x is live as a statement S if there exist some statement that uses x. Okay. So some other statement S prime that uses x and there is some path from S to S prime and there is no intervening assignments on that path to x. Alright? So there needs to be an assignment to x at some statement S there is some path through the program that reaches a read of x. Add sum statement to S prime and along that path there is no right to x Okay? And if this situation arises then we say that this value written in this first statement s is live. Now if a value is not live then it is dead. And a statement that assigns to x is going to be dead code if x is dead after the assignment. So if we know that immediately after the assignment immediately after this assignment to x there is no possibility that a value of x will be used in the future. Well then the assignment was useless and the entire statement can be removed. Alright So dead assignments can be deleted from the program But notice that in order to do that we have to have the liveness information. We need to know whether x is dead at this point. So once again what we want to do is to have global information about the control flow graph. In this case the property is whether x will be used in the future. We want to make that information local to a specific point in the program so we can make a local optimization decision. Alright And just like for constant propagation were going to define in a an algorithm for performing liveness analysis And its going to follow the same framework. If were going to express liveness in terms of information transferred between adjacent statements just as we did for copy of constant propagation And its gonna turn out that liveness is actually quite If its simpler or somewhat simpler than constant propagation since its just a Boolean property. Eh you know its either true of false. Alright So lets take a look at some of the rules for liveness. So here were defining what it means for x to be live at this point here. So were immediately after p is x live And its going to be live. Remember what the intuition is. The intuition is that a the variable x is live right after p if the value of x is used on some path. On one of the paths that begin at p. Alright And so in order to know whether its live were going to take the liveness information at each of the input points. So that would be here here here and here. So each of the successor statements after p And were gonna ask is x live at any of those points? So its just a big or over the liveness of x and all of the successors of p And thats the liveness of x at the out of p. Next lets consider the effect of individual statements on the liveness of x. So the first rule is that if we have a statement and it reads the value of x Okay? So here we have an assignment statement and on the right hand side it refers to x so its reading x Then x is live Before that statement. Clearly x is just about to be used on the end of this statement and so x is live at that point. Alright? So if a statement or if if a statement reads the value of x then the in of that statement x is true. Sorry the liveness of x is true. A second case is when a statement writes the value of x So here we have an assignment to x And the rest of the statement does not refer x Does not read the value of x. So theres no x in E. Okay So in this situation x is not live before the statement. X is not live or we can say that x is dead Before the statement And why is that? Well were overriding the value of x so whatever value x had before this statement is never gonna be read. Okay Because the ee here the right-hand side of the assignment doesnt refer to x And so immediately before the statement the current value of x is never gonna be used in the future. And so x is dead at that point. And finally the last case is what if we have a statement that does not refer to x? Okay So it neither reads no r writes x. Well then whatever the line this is of x after the statement it has the same liveness before this statement. So if x is live here. Then x will be live here. Okay and similarly if x is dead After the statement. Then x must be dead before the statement. And thats because x if x is not use in the future after the statement S then it still want be use in the future before the statement S. Since the statement S neither reads nor write x. So those are the only four rules and now I can give the algorithm. So initially we left the liveness information for x be false at all program points And then we repeat the following until all the statements satisfy the rules one through four and just has its the same algorithm that we used for constant propagation. We pick some statement where the information is inconsistent and then up update the information at that statement with the appropriate rule. So lets do a simple example something with a loop. So lets begin say by initializing x to zero and then what should our loop body do? Well we can check whether x is equal to ten and if it is well well exit the loop And lets assume that x is dead on exit. So x is not refer to outside of the loop. In other wise if x is not ten Then we will increment x and well branch back to the top of the loop. So this is a very very silly little program. It just counts to ten and then exits. Well lets do the lightness now to see where x is life. So since x is dead here on exit its clearly gonna be dead on the out Of of this conditional on this branch Okay? So I should say that x is not live. So were using booleans here so thats xs liveness would be false And were assuming And x is also not live everyplace else initially. Okay And so theres a program point in there also Where the liveness of x is false. Okay So now lets propagate the information. Well so here we have read of x. And let me switch colors here. So here we have a read of x. So in fact the informations inconsistent here because ri ght before this statement since we have a read of x x must be live. So in fact x is live at this point. Now notice that this statement both reads and writes x. Okay? But the rule that says x is live before when we do a read takes priority here Because the read happens before the write. So well read the old value of x before we write the new value of x Okay. So the old value of x does get used and thats why x is live immediately before this statement. Okay so then heres another read of x. Okay so on the so the point immediately before this when I left out one program point here x is also Y. Okay And then following edges backwards well that means x is gonna be live on the back edge of the loop And its also gonna be live by going into the initialization block. Alright? Now we come back around here and we see that were done cause x is already known to be live within the loop body. And now live x is also live here And then the question is you know what about this point on the entrance at the entrance to the control flow graph? Well theres a right of x And with no read of x on the right-hand side. So in fact x is not live on entry to this control flow graph. So in fact x is dead at this point. So whatever value x has when we enter the control flow graph it will never be used in the future. Alright and so that is the correct liveness information for every program point in this example. Now another thing you can see from our little example is that values change from false to true but not the other way around. So every value starts at false and it can change at most once. To say that the value is actually live the property becomes true and then it wont ever change back to false again. So going back to orderings We only have two values in this analysis false and true And the ordering is that false is less than true. Okay And we know so everything starts at the lowest possible element of the ordering and they only move up and so they can be promoted to true but no t vice versa And so since each value can only change once termination is guaranteed. That eventually were guaranteed to have consistent information throughout the control flow graph and the analysis will terminate. In this video were going to begin a discussion of Register Allocation which is one of the most sophisticated things that compilers do to optimize performance and also involves many of the concepts that weve been discussing in global flow analysis. Recall that intermediate code can use unlimited numbers of temporaries and this simplifies a number of things. Particularly it simplifies optimization so we dont have to worry about preserving the right number of registers in the code. But it does complicate the final translation into assembly code cuz we might be using too many temporaries and this is actually a problem in practice. So its not uncommon at all for intermediate code to use more temporaries than there are registers on the target machine. The problem then is to rewrite the intermediate code to use no more temporaries than there are machine registers and the way were going to do that is were going to assign multiple temporaries to each register. So were going to have a many-one mapping. A many to one mapping from temporaries to registers okay? And clearly theres a little bit of an issue here if we really are using many temporaries we will not be able to fit them all into a single register. So there needs to be some kind of a trick and well say what that trick is in a few minutes and there will be situations actually when this will fail well have to have some kind of back up plan. But our default plan is to try to put as many temporaries as possible into the same machine register. And doing all of this without changing the behavior of the program. So how can we do this? Magic thing. How can we actually make a single register hold multiple values? Well the trick is that its fine for registers to have local values as long as it only has one value at a time. So lets consider this program Im going to switch colors here. Okay. Simple three statement program and notice here that a is used in the first two statements. So its written in the first statement read in the second stateme nt e is written in the second statement and read in the third statement and that is only written in the third statement. And actually these three values a e and f they dont ever really co-exist at the same time but at the time weve read a we are really done with it. Weve all the uses that they are going to have in this little code fragment. Here Im assuming that a and effort are not used anywhere else and so it turns out that a e and f could all actually live in the same register. Alright thats assuming that a and e are dead after their uses. And what will that look like well lets allocate them all to a particular register r1 and lets assign c d and b into their own individual registers and the code would like this r1 would be r2 + r3 and then r1 would be r1 + r4 and r1 would be r1 - one. And so now notice how this is just a transliteration of the code over here into registers but there is a many one mapping of names on the left to register names on the right. A register allocation is an old problem. In fact it was first recognized way back in the 1950s in the original Fortran project but originally register allocation was done with a fairly crude algorithms and who is rapidly or very quickly noticed that was actually a bottle neck in the quality of code generation that actually limitations on the ability of register allocation and do a good job have a really significant effect on the overall equality overall quality of the code that compilers could produce. And then about 30 years later in 1980 a breakthrough occurred where people discovered or a group of researchers at IBM discovered a register allocation scheme based on graph coloring. And the great thing about this scheme is that its pretty simple. Its easy to explain. Its global meaning it takes advantage of information from the entire control flow graph at the same time and also happens to work well in practice. And heres the basic principle that underlies the modern register allocation algorithms. So if I have two temporaries t1 and t2 I want to know when they can share register. So theyre allowed to share a register and theyre allowed to be in the same register if they are not live at the same time okay? So like I said any point in the program in most one of t1 or t2 as live. And we are more concise which I already said was partially is is that if t2 t1 and t2 are live at the same time okay? Meaning that theres theres some program point were both are live then they cannot share a register alright? So this is the negative form of the statement and it just tells you that if if you need two values at the same moment in time then they have to be in separate registers. Lets take a look at a control flow graph and now we know that in order to do the register allocation to solve the register allocation at least in this in this way were going to need liveness information. So lets compute the live variables for each point of this program. So here it is and Ill just walk through it very quickly. Lets assume that on exit from this loop that only b is live. So b is the output of this piece of the code and its used elsewhere but none of the other variables are live. So now if we work backwards remember that line is a backward analysis. Well see here that b is written so its not live before the statement but f and c are read. So both c and f are live before this basic block. Okay and similarly if we if we go up another level here here we see that e is now alive and f is dead because f was written here and e was read. And over on this path here we have another exit where b is live and now at this point here right after this basic block the set of lot variables that are live is b c and f because b is live on one path and c and f are live on the other path. Remember for something to be live it only has to be live on some in some future possible evolution of the execution. So on some path out of this node is a variables live then its live at the exit from this. Working backwards here. B c and f are live here because e is read. And b c and f are not referred to in this statement and so they just propagate upwards. Here b is removed from the live f because its written but d is added and set here and similarly for the other edges in this graph. If you go and check all the other edges you will see that the live set is correct and it just follows from the simple rules we gave in the previous video. But how are going to use the liveness information to do register allocation? Well were going to construct and undirected graph and in this graph there will be a node for each temporaries so each variable will have a node in the graph and therell be an edge between two temporaries if they are live simultaneously at some point in the program alright? So backing up and looking at our little example here we can see for example at this point in the program c and e are both live. Theyre both in the live set after this basic block executes. So c and e cannot be in the same register. Alright continuing on this is called this data structure this graph is called the Register Interference Graph or RIG for short. And again the basic idea is that two temporaries can be allocated in the same register if there is no edge connecting them in the register interference graph. So heres a register interference graph for our example. This is the graph constructed from the code and the line analysis that were given a few slides ago and you know its easy to read off from the graph what the constraints are. So for example b and c cannot be in the same register because b and c are connected by an edge. Okay seeing that theyre live simultaneously at some part some point in the program and so they have to live in different registers. On the other hand there is at there is no edge between b and d okay. So this edge is missing and therefore its possible that b and d could be allocated in the same register. They are live ranges all the times in which they are alive do not overlapped. So a great thing about the register interference graph is that it extracts exactly the information needed to characterize a legal register assignment. So it gives us a representation of all the possible legal register assignments. Now I havent said I havent actually get a register assignment out of the register interference graph but the first step is to characterize the problem in some kind of precise way. And the graph of cannot live in the same register constraints does that for us. The other thing that is good about is a is a global view of the register requirements meaning its over the entire control flow graphs. So takes into account information from every part of control flow graph which will help us to make good global decisions about what value is very important to live in registers. And finally the other thing to notice is that that after reconstruction the register allocation for algorithm is going is architecture independent. I havent shown you the algorithm so you just have to believe the statement for the moment but its going to turn out that were not going to depend on any property of the machine except for the number of registers. So thats the only thing we need to know about the machine in order to take a RIG and and do register allocation using it. In this video we are going to continue our discussion of register interference graphs and talk about how to use RIGS to come up with register assignments for procedures. And were going to look at one particular technique thats popular called graph coloring. So first a couple of definitions. A graph coloring is an assignment of colors to nodes such that the nodes connected by an edge have different colors. So if I have a graph lets say with with three nodes and its fully connected so every node connect to every other node. And then and then a coloring of this graph would be an assignment of colors such that every pair of nodes are connected by an edge have a different color. So for example I could color this node blue and I could color this node green and I could color this node black okay. And then that would be a valid coloring of the graph because each pair of neighbours has a different color. And then the graph is k-colorable if it has a coloring that uses k or fewer colors. In our problem the colors corresponds to registers so we want to do is to assign colors or registers to the graph nodes. And were going to let k the number the maximum number of colors were allowed to use be the number of machine register. So the actual number of registers present on the architecture for which were generating code. And then if if a RIG if a registered interference graph is k-colorable then theres going to be a register assignment that uses no more than k registers. So lets take a look at an example rig and for this particular graph there is no coloring. It turns out that it uses fewer than four colors. But there is at least one for coloring of this graph. And then here it is so Ive used colored labels but also register names so that you can see what registers we might assign to each of the nodes. And just notice that although there are many more than four temporaries or four nodes in this graph we do manage to color it with only four colors and some of the nodes have the same color. So for example d and b are allocated the same color as are e and a. Just to remind ourselves where this register interference graph came from here is the original control flow graph again. And once we have the coloring of the graph now we can do the register assignment. We can replace the temporaries by their corresponding register names and then we get this control flow graph. So here weve just renamed each of the variables of the program with its register that it was assigned to. And now were very close as you can see to having code that we can emit and execute on the target architecture. The graph  coloring here is like that we discussed in the previous video doesnt always succeed in coloring an arbitrary graph. And it may well get stuck and not be able to find a coloring. And so in that case the only conclusion we can reach is that we cant hold all the values that wed like to register. We have more temporary values and we have registers to hold them. And those temporary values have to live somewhere so where should they live? Well theyre going to have to live in memory. Thats the only other kind of stories that we have. And so were going to pick some values and spill them into memory. The ideas that we have the picture in your mind should be. A bucket and it can hold a fixed amount of stuff. Those are the registers and when it gets too full some of the stuff spills over and and ends up some place else. Now when does the graph coloring here do get stuck? Well this only situation which we wont be able to make progress as if all the notes have k or more neighbors. So lets take a look at our favorite register interference graph when we will be using at our examples and now lets say that our the machine we want to use only has three registers and so we instead of finding a 4-coloring of this graph we need to find a 3-coloring. So lets think about how to find the three coloring of this graph. If we apply the heuristic well remove A from the graph but then were going to get stuck. Because once you take A out of the graph and its edge is out and every node thats left has more than has three or more neighbors as at least three neighbors. So theres no node that we can delete from the graph and be guaranteed to be able to find the coloring for it with the heuristic that we discussed in the previous video. So in this situation what were going to do is were going to pick and know that there is a candidate for spilling. This is a know that we or a temporary that we are probably or we think we may have to assign into a memory location rather than to our register and let is assume for the sake of this example that we pick f and we talk later about how to choose a the know to spill theres a number of different ways to to chose the particular know to spill but for the illustration of this example it doesnt matter how pick we just have to pick one to remove from the. Graph. As were going to say were going to remove that we going to spill F. So what well do then is well remove f from the graph just like before and then well continue with our simplification and this will now succeed because once we move F from the graph we can see that all the nodes well actually several of the nodes have fewer than three neighbors and so B C and D. Sorry B and D both only have two neighbors when [inaudible] E and C will only have one neighbor each and so clearly coloring will now succeed and heres one order that well succeed with this reduced graph. After we decide to spill f and we successfully color the sub-graph now we have to try to assign a color to f and it could be we could get lucky and discover that even though f had more than there neighbors or three or more neighbors when we remove it from the graph it could be that when we go to construct the coloring for the sub-graph that. Those neighbors actually dont use all of the register. It could wind up being at all those neighbors for example or assign to the same register and so there are plenty of registers left over to assign to f. And so this is called optimistic coloring. So we pick a candidate for spilling. We tried to color the sub-graph. Once we have a coloring for the sub-graph now we see if we just get lucky. And are able to assign a register to F. In which case we can just go ahead and continue the color of the rest of the graph as if nothing had happened. So in this case lets take a look what happens. Were going add F back into the graph. And. And look at all and look at its neighbors and we see that we have a neighbor thats using r1. We have a neighbor thats using r2 and we have a neighbor thats using r2 and we have a neighbor thats using r3. And so on in this case optimistic coloring will not work so in fact F had more than K neighbors and after we color the sub-graph it turns out that those neighbors are using all K. In this case three all three of the register names. And so F where there is no register left over for F and were going to have to actually spill it and store in memory. So if optimistic coloring fails as it does in this example then we spill f. So what were going to do is allocate the memory location for f and typically what that means is that well allocate a position in the current stack frame. Lets call this address fa for the address of f. And then were going to modify the control flow graph. Were going to change the code for that compiling. So before each operation that reads f were going to insert a load that loads from that address to current value of f into a temporary name. Okay that makes sense because if the value is out of memory then if we have an operation that needs to actually use the value. Were going to have to load it from a memory first then to the register. And similarly after each operation that writes F were going to insert the store so were going to save the current value of F into its location in memory. So here is the original code from which we constructed the registry interference graph and notice that there are few references to f in here and we just highlight them alright. So we have a couple of reads we have a right and so now what are we going to do? So here we had the use of F the read of F in this statement and now we preceded that by a load. And notice that Ive given a new name here. Ive called this F1. And thats because the different uses of F in the control flow graph dont all have to have the same temporary name. And actually it would be a good idea to separate them so each distinct to use of F will get its own name. So here we load the value of F and then it get to use in the statement. Here we have a right to f and so we store the current value of f and those argument to a different name f2. So thats temporary is computed here as going to be stored and its called f2. And finally the third use of f theres another load of f right here. Which is then used in this computation here of b. Okay. So that is the systematic way to modify the code to use f in storage. And now we have to recompute the aliveness of f. And so what happens there. Well here is the original aliveness information from which we computed the register interference graph okay. And now notice that f is gone. We no longer use f in the programs so we can delete all the places where we mentioned that f was live and now we have the three new names f1 f2 and f3. And we have to add in their aliveness information so it creates a new program points here where we inserted statements. And of course where we have a load of the current value of f that value if live right before the use in the next statement. Here we have the right of the current value of f and thats live right before the store and then heres another load of the current value of f which is live until the store Im sorry until the use in the next statement. Okay. And so now notice here that f used to be live in many many many places in the in the code. And now not only is f or the the different versions of f live in fewer places also weve distinguish them. So it actually separate the different uses of f and so this will have their own nodes in their own set of interferences in the graph and they wont share them with the other users of f and that will actually also reduce the number of edges in the graph. To summarize the example on the previous slide once we have decided that we are actually going to spill a temporary f that means were going to change the program where have loads and stores to the program and now were going to have a different program and thats going to change our register allocation problems. Were going to have to recompute the aliveness of information were have to rebuild the restrain interference graph and then were going to have to try again to color that block graph. Now it turns out that this new aliveness information is almost the same as it was before. So all the temporary names other than f are not much affected by the by the new statements that are added. There are a few program points where they might be live but I replaced they were alive before and theyre still alive. And F itself has changed fairly dramatically. Its like this information has changed really dramatically. Certainly the old name F is no longer used and so its like this information goes away and then weve also split F into three in this case three different temporaries. One for each of the different uses of F in the control flow graph. And I noticed that each of these new uses of F or these new versions of F is live in a very very small area so a load. In this video we are going to continue our discussion of as filling. For a load instruction The thing that were loading the temporary that were loading fi is live only between the load and the next instruction where its used and similarly for a store. Its score of a temporary fi is live only between the store itself and the proceeding instruction. The one they created fi. And the effective is is to greatly reduce the live range of the spilled variable. So whatever name we decide to spill by adding the load and stores right next to the places where those values are used We dramatically reduced the live range and in addition as I mentioned in the previous live by splitting the name f into multiple different name we also you know avoid sharing. Those different liv e ranges between the different versions of F. So because the live range of F is reduced by spilling. It has fewer interferences in the new program than it did in the old program. And so what that means the particulars in the rebuild [inaudible] interference graph F will have fewer neighbors. Some of the neighbors that it had before have gone away because it just live in fewer places. So if we look at the new register interference graph we can see that among all the different versions of F. Remember that F has been split into three temporaries in this graph. We see that they only interfere with D and C. Whereas before f have several other neighbors in the graph. And now in fact this new graph is three tolerable. Of course it might be the case that we cant just spill one name. We might have to have just spill several different temporaries before the coloring is found. And the tricky part is to siding what to spill. So this is the hard decision that has to be made during restore allocation. Now any choice is correct. Its only a question of performance so you know some choices of spilling will lead to better code than others but any choice of spilling is going to resolve in a correct program. And theres heuristics that people use to pick which temporaries to spill and here are a few or I think three of the most popular ones. One is to spill the temporaries and have the most conflicts. And the reason for that is that this is the temporary. The one thing that you can move into memory that will most affect the number of interferences in the graph. So the idea is by possible spilling justice on variable. Well remove enough edges from the graph that they becomes tolerable with the number of registers we have. Another possibility is a spilled temporaries that have few definitions and uses. And here the idea is that by spilling those since theyre not used very much the number of lows in storage will have to add will be relatively small and so if a variable just isnt used in many places then the actual cost in terms of additional instructions that are going to be executed to spill it is relatively small. And another one and this is actually the one that I think that all the compilers implement is to avoid spilling an inner loops. So if you have a choice between spilling a variable thats used within the. Innermost loop for the program and one that is used some place else. You probably preferred this that you spill the one that is used not in the innermost loop absolutely because again that will result in fewer loads in stores. You really want to avoid adding additional instructions to your inner loop. In the last few videos weve talked about managing registers. In this video were going take a few moments to talk about another very important resource the cache and what compilers can and cant do to manage them. Modern computer systems have quite elaborate memory hierarchies. And so if we were to start at the closest level to the processor itself we would find that on the chip there are some number of registers. And these are very fast access. So typically that can be accessed in a single cycle so at the same rate as the clock frequency. And the problem is that its very expensive to build such high performance memory. And so we dont get to have very much of it typically. You know you might have 256 say to 8K bytes of registers total available to you on a given processor. Now a very significant portion of the die area and the modern processor would be devoted to the cache. And the cache is also quite high performance but not quite as high performance as registers. Maybe on average it would take three cycles just service something from the cache but you get a lot more of it. And modern processors would have up to a megabyte of cache. Then much further away from the processor is the main memory the DRAM and this is much more expensive to allocate to access in time you know typical values would be twenty to 100 cycles and I think you know its more on 100 toward the 120 these days in most processors but you get quite a lot of it. You get between 32 megabytes. That would be fairly small machine up to four gigabytes for maximally provisions processor. And finally farthest away is typically disk. And this takes a very very long time to get to hundreds of thousands or millions of cycles but you can have enormous amounts of storage out there gigabytes to terabytes of storage. As I said there are limitations on the size and speed of registers and caches. And these are limited as much by power actually as as anything e lse these days. And I and so its you know very important people would like to have as much register and cache as possible but there are real constraints on how big and how fast we can make these relative to the speeds of the processors. Now unfortunately the cost of a cache miss is very high as we saw in the previous slide. If you you could get something in a couple of cycles from the cache. But if its not in the cache then it could take you a couple of orders of magnitude longer to get it out of the main memory. And so for this reason people you know try to build caches in between the processor and the main memory to hide that latency of the main memory so that most of the data is in the cache. And typically it requires more than one level of cache these days to match a fast processor well with the speed of a very large main memory. So you know very common now to have two levels of cache and processors and some processors even have three levels of cache. So the bottom line is that its very important to for high performance to manage these resources properly. Particular to manage the registers and the cache as well if you want your program to perform well. Compilers have become very good in managing registers and in fact I think today most people would agree that for almost all programs compilers do a better job at managing registers than programmers can. And so its very worthwhile to leave the job of allocating registers or assigning registers to the compiler. However compilers are not good at managing caches. And while theres a little bit that compilers can do and thats what were going to talk about in this rest of this video for the most part if programmers want to get good cache performance they have to understand the behavior of the cache is on the machine and have to understand what their program is doing you have to understand a little bit about what the compiler is capable of doing and then they still have to  write the program in such a way that is going to to be cache friendly. So its still very much an open question. How much a compiler can do to improve cache performance? Although there are a few things that weve found compilers can do reliably. So to see one of those things that compilers can actually do lets take a look at this example loop. So what we have here we have an outer loop on j and inner loop on i and then in each iteration of the inner loop were reading from  some vector you know performing some computational net value and storing the results into the ith element of the A vector. Now as it turns out this particular program has really really terrible cache performance. This is going to behave very badly. And so lets think about whats going to happen. So lets imagine our cache you know as some block of memory okay. And so whats going to happen here. I mean whats whats the first iteration going to be? Well were going to you know load and store some function of that into . And so whats going to get loaded into the cache is and the sake of argument lets say they land in the first two elements in the cache. And then were going to do the second iteration of this and well well load and write it into and so and will be loaded into the cache all right and so on. And this will repeat over and over and over again loading one element of a and one element of b the important thing to notice is that all of these references to a and to b are misses okay. Every single one of these is a cache miss because on each iteration of the loop we refer to new elements okay. So were not referring to the same elements as we were on the previous ones. So now lets ignore for the moment the fact that there may be multiple elements in the same cache line okay. So some of you probably are aware already. That when we fetch data from memory we dont just fetch the one word okay. So typically when we refer to for example you know is stored here will fetch an entire cache line which will be some block of memory and that may well have you know other elements of b in it. So we might get a couple other elements of b into the cache at the same time but the important thing here is that on every iteration of the loop were referring to fresh data okay. And and if these data values are large enough if they take up an entire cache line then each iteration of the loop is going to be a cache miss for both elements and we wont get any benefit of the cache. And this loop will run at the rate of at the rate of the main memory and not at the rate of the cache. Now the other thing thats important here is that this loop bound here is very large and I picked it to be very large to suggest that its much larger than the size of the cache. So as we get towards the end of the loop whats going to happen is we will have filled up the whole cache so this whole cache will be filled with values from a and b and then its going to start clobbering values that are already in the cache. And if this loop you know if the size of these vectors lets say twice the size of the cache by the time we come around and complete the entire execution of the. Inner loop. Whats in the cache is the second half of the a and b arrays its not the first half of the a and b arrays. And so then when we go back around and execute another iteration of the outer loop now whats in the cache is also going to be not the data that were referencing. And so when we come back around and begin the execution of the inner loop the second time. And we refer to and and and . Whats in the cache is the values from the high numbered elements of the a and b vector and not the low numbered elements. And so these references are all misses again. And so the the basic problem with this loop is a loop thats structured like this is that almost every memory reference and if and if the data values are big enough again that they fill an entire cache line then it will be every single memory reference is a cache miss. Now instead lets consider an alternative structure for the same program. Here Ive put the i loop at as the outer loop and the j loop as the inner loop. And here what we do is we load . And we write and then we repeat that computation ten times on the same data values. And so here well get excellent cash performance. Well well have a miss on the first reference but then on the subsequent nine references the data will be in the cache or will completely exhaust our computation on those particular a and b values. And then well go on to the next a and b values. Well finish the inner loop and go on to the other and do one more iteration of the outer loop. And so the advantage of this structure is that it brings the data into the cache and then it uses that data as much as possible before going on to the next data. Rather than doing a little bit on every data item and then going back you know doing one pass and then going back and sweeping over all items items again and doing another little bit. Alright so this particular structure where weve exchanged the order of the outer loops sorry exchanged the order of the inner and outer loops it computes exactly the same thing but it has much better cache behavior. And it probably run more than ten times faster. Now compilers can preform this simple loop interchange optimization. This particular kind of optimization is called loop interchange where you just switch in the order of loops. In this particular case its very easy to see that thats legal and the compiler could actually figure it out. Not many compilers actually implement this optimization because in general its not easy to decide whe ther you can reverse the orders of of the loops. And so usually a programmer would have to figure out that they wanted to do this in order to improve the performance in the In this video were going to start our discussion of garbage collection or automatic memory management. This will take us a few videos to get through and this first video is just an overview of the problem. And then well talk about specific techniques in subsequent videos. To set the stage lets first talk about the problem that were trying to solve. So if one has to manage memory manually meaning you have to do all the allocation and deallocation explicitly yourself that is a hard way to programming leads to certain kinds of bugs that are very difficult to eliminate from programs. So in particular these days you see this primarily in C and C++ programs. Those are the main languages that are used that have manual memory management. And the kinds of storage bugs that you can get because it has manual memory management are things like forgetting to free unused memory so thats a it means a memory leak. Dereferencing dangling pointers overriding parts of a data structure unintentionally. And actually theres a few more things although these are probably the three most common problems that people have and these bugs are really hard to find. And I want to emphasize that these kinds of bugs are often some of the very very last bugs to be found in in complex systems. They often persist into production and sometimes for a very long time after the code is in production use. And why is that? The reason is that these these kinds of bugs storage bugs typically have effects that are far away in time and space from the source and so how can that happen? Well lets think about some object in memory and now lets say only on interesting you might have some fields lets say you have a few fields and I am keeping some pointers to it. So somewhere on program is a reference to this particular object and now I come along and free it. So I am doing my own memory management like free this object but I forget that I had this pointer. And so now whats happen all the storage has been freed its no longer really valid memory but the pointer still exist to it. And then when I come along and allocate something else it might allocate the same piece of memory. So this might now be a different kind of object okay. So I might have a different type here even. In this memory might be used for something completely different and now I have a pointer that says it thinks its a red object its pointing to a blue object. And when I come in and write stuff into this object of course Im just writing nonsense. So I this whatever piece of code holds this pointer thinks its still the old kind of object. It will write some bits in here and when I go in some other part of the program possibly quite far away go out and read out this is a blue object Ill just get some random garbage and that will probably cause my program to cash. So this is a very very old problem. Its been studied since at least the 1950s. It was first thought about carefully in list. And there are some well-known techniques for completely automatic memory management so you dont have to manage memory yourself. And this only became mainstream actually in the 1990s so with the popularity of Java. Prior to that time there was really no mainstream language that used automatic memory managements so thats really just in the last now almost twenty years that garbage collection and automatic memory management in general became a popular mainstream programming technique. So the basic strategy in automatic memory management is is pretty simple. So when an object is created when we allocate a new object the system the run time system will find some unused space for that object and it will just allocate it. So whenever you say new of some class name in Cool. Some memory is automatically allocated by the system some previously unused memory is automatically allocated by the system for that object. And if you keep doing this over and over and over again and after awhile youre going to run out of space. So eventually there is no more unused space left for additional obj ects. And at that point you have to do something. You have to reclaim some of the space in order to allocate more objects and the observation that garbage collection systems rely upon is that some of the spaces being used is probably occupied by objects that will never be used again. So they some of these objects are not going to be referred to again by the program and if we can figure out which objects those are which objects are not longer going to be used. Then we could deallocate them and reuse the space for new objects. So the big question is how can we know that an object will never be used again? And most of the garbage collection techniques that are out there today rely on the following observation then thats that a program can only use the objects that it can find and what do we mean by that? So Im going to switch colors so lets take a look at this piece of code so whats going to happen? Well when we execute this the first thing that happens is we allocate an A object alright. And its assigned x so x will have a pointer to that object. And then in the body of this let whats going to happen well were going to assign x the value that y points to so y is another variable. It points to some other objects in memory okay. And whats going to happen when we execute this assignment is that were going to remove the old value of x and x now is going to point to this object. Now observe that this object a is unreachable. Meaning it has no references to it. There are no longer any pointers to it. And how do I know that? Well a brand new here when it was created. I only created one pointer to it x and then I immediately assigned x to something else. So I dropped the only pointer to A. There is no reference to A anywhere in the program. And so the program will never be able to find it again. The program if no variable or data structure in the program has a pointer to A then A can never be referred to by the program in the future. So any kind of subsequent execution of the program has no p ointers to A and therefore it will never use A again and so the space ray can be reclaimed and used for another object. Now it turns out that we need a more general definition of object reachability than this example illustrates so lets take a look at that. Were going to say that an object x is reachable if and only if one of the following two things is true. So either A register contains a pointer to x. So either the x is reachable immediately from some register. Remember that the registers contain things like the local variables in there and the intermediate expressions and theyre just you know the values that the program has immediate access to or another reachable object y contains a pointer to x. And so what does this say? Well this says youre going start at the register so you know the program might be implemented using a few registers. And then youre going to look at all the things that those registers point to all the objects that they point to. And you will look at the pointers in those objects and everything they can point to okay. And some of these things might overlap. I mean some of these there might be multiple things which are reachable by more than one path starting at the registers. But the complete side of things that you can reach beginning at the registers and following all the possible pointers those are all the reachable objects. And then the complement of that set an unreachable object is one that isnt reachable. So all the other objects the ones that you were not able to reach by recursively starting at registers and following pointers as far as you could those objects can never be used. Because clearly the implementation can only access things through registers and and then only find additional things by you know loading pointers out of objects that it could reach from the registers. So anything that it can reach by some sequence of sub-steps will never be used again and is garbage. So lets take a look at another example that illustrates some interesting aspects of re achability and its use in automatic memory management. So what does this example do? The first thing it does it allocates an A object on the heap and assigns that to the variable x. So x is a pointer to that object. And then it allocates a B object and y will point to that object. And then it assigns the value of y to x alright. So well have this configuration and and now lets draw a line here okay and well come back and lets remember this point in time what things look like at this point in time. And then were going to go off and were going to execute this conditional. And notice that this conditional is going to do. Its going to always be true alright? So the predicate will always be true so itll never take the false branch. All its going to ever do is take the true branch and whats it going to do there is immediately going to overwrite x. And so x is going to wind up pointing at some other new object. It doesnt matter what it is. And now lets say that at this point right here is where we try to do a garbage collection. So you know for some reason this is the point where the program stops and tries to collect unused memory. And what can it collect? Well just like before cuz the example up to this point is essentially the same. We can see that this object is unreachable okay. So the first A object becomes unreachable at that point and it can be collected. Now what about the second object? Well it is reachable its clearly reachable. Its reachable through x okay at that point and its also reachable as it happens through y. And so its not garbage and its not going to be collected but notice that the x value is always going to be overwritten okay? So the program the compiler doesnt know that this branch is always going to be true. So it doesnt realize that the value that x has at this point wont ever be used again but that value is immediately going to be overwritten every time we take this conditional. And furthermore if y is not used any place else in the program if y i s dead at this point. Lets say that y is dead here. Then neither one of these references to B is ever gonna be touched again. So in fact the B value will never be used again even though it is reachable. And so what this tells you is that reachability is an approximation. And by that I mean its an approximation for the objects that will never be used again. What were really interested in when we do garbage collection is collecting objects that will never be used in the future execution of the program. Because obviously that space is wasted and could be put to some other use that might be better and reachability approximates that. So if an object is unreachable it definitely wont be used again however just because an object is reachable its not a guarantee that it will be used again. So now lets talk about how we do garbage collection in Coolc. So Coolc has a fairly simple structure. It uses an accumulator in which of course points to an object and that object may point to other objects and so on. So we have to trace all the objects reachable from the accumulator but we also have to worry about the stack pointer so theres also stuff reachable from the stack. And each stack frame of course may contain pointers like and you know for example the method parameters that are stored on the stack. Each stack frame may also contain some non-pointers alright? So if I think about the layout of each activation record there would be some mix of pointers and non-pointers. Things like the return address so we have to know the layout of the frame. But if we do know the layout and of course the compiler is deciding on the layout so it naturally does know the layout it can find all the pointers in the frame. Essentially the compiler has to keep a record for each kind of activation record it builds for each methods. If activation record for a method foo and lets say that activation record has four slots then the compiler would need to keep track of which one of these were pointers to objects. And perhaps the second  and the fourth element of the frame are always pointers to objects and the other two are always non-pointers. So the somewhere the compiler has to keep track of this information so that the garbage collector will know at Run time when its looking at an activation record for foo where the pointers that it needs to follow are. So in Coolc we start tracing from the accumulator and the stack and these are called the roots okay. So in garbage collection terminology the roots are the registers from which you begin tracing out all the reachable objects. And if we do that here what we can do so you see we have our object here we have our accumulator excuse me and our stack pointer and so we can just walk through. This little diagram of memory and find all the reachable objects so the acummulator points to object A so well mark that as reachable. And A points to C so well mark it as reachable. C points to E so well mark E as reachable. The stack pointer has a couple of frames on it. The first frame has no pointers. The second frame points to E. Weve already touched that one. Its already marked so we can mark it again but it doesnt matter as long as it gets marked by somebody and now everything that is not marked is unreachable. So what objects didnt we touch and are traversal of the reachable objects? Well those are objects B and D. And so those are unreachable objects and they can be reclaimed and we can reuse their storage. Now one interesting thing to note here is that just because an object has pointers to it it does not mean it is reachable so notice here object D. Object D actually has a pointer to it okay and yet object D is unreachable and why is that? Well because the only pointers to it are from other unreachable objects. So its important here to you know just understand that its not the case that every unreachable object has no pointers to it. There will be some unreachable objects or there may be some unreachable objects that actually do have pointers to it to them but they will on ly come from other unreachable objects. So every garbage collection scheme has the following steps. Were going to allocate space as needed for new objects so we just go ahead and allocate new space as long as we have space so whenever we need it. And when space runs out we need to compute what objects might be used again. And generally thats done by tracing objects reachable from a set of root registers and then were going to free the complement of that set. Were going to free the space used by the objects not found in part A. And I want to say that some strategies do perform garbage collection before the space actually runs out and well actually look at one of those in a future video. In this video were going to talk about the first of three garbage collection techniques that were going to look at in detail. First one is mark-and-sweep. Mark-and-sweep works in two phases. And its called not surprisingly mark and sweep. So the mark phase is going to trace all the reachable objects. So when memory runs out and we stop to do the garbage collection the first thing were going to do is go and trace out all the reachable objects. And then the Sweep phase is going to collect all the garbage objects. And to support this every object is going to have an extra bit somewhere in it called the mark bit. And this is reserved from memory management and its not going to be used by anything except the garbage collector. And initially before we start a garbage collection the mark bit of every object will always be zero. And thats going to be set to one for the reachable objects in the mark phase. So when we mark an object we mark it with a And that indicates that the object is reachable. So here is the mark phase. Its going to be a work list based algorithm and so initially our work list consists of all the roots so all the initial pointers held in registers and then while the work list the to-do list is not empty were going to do the following. We pick some element v out of the to-do list well remove it from the to-do list okay. And then this is the crux of the algorithm. If the object v is not already marked then we mark it okay. So we say mark bit to one and then we find all the pointers inside of it alright. And we add those to our work list. So everything every point gets added to work list. Now if v is already marked well then we have already processed it and weve already add all the things it points to to the work list. And so we just need to do nothing there is no else branch and we just drop it from the to-do list. So once weve completed the mark phase and every reachable object has been marked then the sweep phase is going to scan th rough the heap looking for objects that have mark bit zero. And the sweep phase is just going to march through all of memory. Its going to start at the bottom of the heap and walk over every object in the heap and check its mark bit. And so any of the objects that it finds that have mark bit zero they were not visited in mark phase and theyre clearly not reachable. S all those objects will be added to a free list. And as we go through the memory is one other detail thats important. Any object that has its mark bit set is gonna have its mark bit reset to zero. So that way its ready for the next garbage collection. So here is the pseudo-code for the sweep phase and this will function size of p is going to size of block the size of the object that starts at pointer p alright. And as youll see this is actually the reason that we have the size of objects encoded in the object in COOL. So remember in the header for COOL objects there is a size field that is so that the garbage collector as its walking through memory can figure out how big the objects are. Anyway we start at the bottom of the heap. And while we havent reached the top of the heap we do the following. We look at where were pointing and then well always be pointing to the beginning of an object. So we check to see if the mark bit of that object is one. And if it is well then it was a reachable object. So we just reset its mark bit to zero. Otherwise if its mark bit was zero then were going to add that block of memory okay which is the size of the object to the free list. And finally in either case okay were going to increment p by the size of the object that it points to so we point to the next object. Then well just repeat that loop over and over again resetting the mark bits of things that were reached and adding things that were not reached for the free list until weve touched every object in the heap. Heres a little example. So were starting out here with a a heap and were gonna assume theres just one root for simplicity. And here are all the objects and initially their marked bits are zero and we do have a free list an initial free list over here. Notice that you know theres a little bit of memory that is on the free list. Okay. So after the mark phase what has happened? Well weve gone through and touched all the reachable objects. So we started with A and of course we set its mark bit to one. And then we followed pointers reachable from A set the mark bit there. Follow the pointer reachable from C set the mark bit there. And so we wind up A C and E being marked nothing else is marked okay. And now the sweep phase will go through memory its going to reset all the marked bits to zero. And as it finds unreachable objects in this case B and D its going to add them to the free list and so what well wind up the free list will wind up being a linked list of of of blocks of memory that are available for future allocations. In this video we are going to look at the second garbage collection technique stop and copy. In stop-and-copy garbage collection memory is organized into two areas. We have an old space thats used for allocation and so all of the data that the program is currently using lives in this area called the old space. And then theres a new space which is reserved for the garbage collector. And so this is not used by the program its just for the GC. And so the first decision in stop-and-copy garbage collection is that the program can only use half the space. And there are some techniques more advance techniques for stop-and-copy garbage collection that allow the program to use more than half the space. So this isnt as bad as it sounds but fundamentally a fairly significant fraction of the space has to be reserved for the garbage collector. Now the way allocation works is that theres a heat pointer here in the old space and everything to the left of the heat pointer is currently in use. This is where all the objects have already been allocated in this area that I just shaded here in red. And then when it comes time to allocate a new object we simply allocate it at the heap pointers. So the heap pointer will simply bump up and some block of space will be allocated to a the next object that we want to do. And it will just keep marching through the old space allocating as you allocate more objects. Okay so allocation just advances the heap pointer so one of the advantages actually of stop-and-copy is a very simple and fast allocation strategy. Now eventually of course if we allocate over and over again were going to fill up the old space and so garbage collection will start GC will start when the old space is full. And what its going to do is going to copy all the reachable objects all the reachable objects from the old space into the new space. And the beauty of this idea is that when you copy the reachable objects the garbage is left behind. So you simply pickup all the data that youre using and move it over to the new space and all the junk that you didnt need anymore is left behind in the old space. And then after you copy stuff to the new space first of all since you left the garbage behind youre using less space than you did before the collection. So theres some space available now in the new space for allocating new objects. And then you simply swap the roles of the old and new space. So the old and new spaces are reversed what was old becomes the new and what was new becomes the old and then the program resumes. So lets take a look at a quick example here just to get a idea of how this works. Lets say we have our old space over here this is the old space and we have one root which is this object A. And so what were going to do well were going to make a copy of all the objects reachable from A. Were gonna move them over to the new space. And what thats going to look like well here it is afterward. But lets trace it out. So we started A we follow pointers from A we can see theres a pointer to C okay so C is going to be reachable and theres a pointer to F  okay. And then F points back to A and thats all the reachable objects so we copy them. And notice when we copy them we also copy their pointers and now the pointers have all been changed. So in the copy of A it now points to the copy of C okay. And of course C will point to the copy of F and theres a little issue here this line is not in the right place so it should look like that. And then F points back to the copy of A. So what we know when the object and move their pointers and we adjust them so that weve really copied the whole graph of objects over to the news space. Now were using less space so theres some free space here okay. And now this will become the old space. This now our old space and this is now the new space which we will use for the next garbage collection. To summarize the discussi on so far one of the essential problems in stop-and-copy is to make sure that we find all the reachable objects and we saw this same problem with mark-and-sweep garbage collection. Now the thing that really distinguishes stop-and-copy is that were going to copy these objects. So when we find a reachable object we copy it into the new space. And that means that we have to find and fix all the pointers that point to that object and this is actually not obvious how to do alright. Cuz when you find an object of course you cant see all the pointers that point into that object. So how are we going to do that? Well here is an idea. Well we copy the object were going to store in the old version of it it was called a forwarding pointer to the new copy. So lets take a look at what that would how that would how that looks like. So we have our old space we have our new space. And lets say we discover some reachable object A in the old space. So what were going to do is were going to make a copy of it over here in the new space and thats easy enough to do. But now what were going to do is were gonna take A and were going to reuse its space and were gonna store whats called a forwarding pointer in it. So were going to yeah first of all were going to mark somehow that this has been copied. So this will have some special mark on it which Ill just you know indicate with here with a purple bar something. This is were marking someway so that we can tell this object has already been copied. And then at a. At a distinguished location in the object were going to store the forwarding pointer. And you can think of this as like a forwarding address. So if you know where somebody lives you can go to their house and if they have moved you can ask for the forwarding address. And thats exactly and then you can go off to their new house wherever theyve wherever theyve gone to and presumably find them. And so thats whats going to happen here. If we have a pointer that points into this object later on and maybe much later on in the garbage collection we may discover this pointer we may follow this pointer find out it points in this object realize that this object has moved because weve marked it and the object was moved. And then we can use the forwarding pointer to find out where the new object is and then update this pointer wherever it is to point to the new object. Now just like with mark-and-sweep we still have the issue of how to implement the traversal of the object graph without using any extra space. Again when these garbage collection algorithms they only get used they only get run in low memory situations. And you cant assume that you can build unbounded data structures to use with the garbage collectors. The garbage collector really needs to work in constants base. And now here is the idea that will that is used in stop-and-copy algorithms to solve the problem. So were going partition in new space and this is just the new space here into three contiguous regions. Were going to have well start with the one on the far right. Were going to have the empty region where were allocating new objects. And theres an allocation pointer that points to the beginning of that region. So this is the region that were filling up with objects that were copying over and this is just empty unused space. Now immediately to the left of that region are the objects that have already been copied but not scanned okay? This is copied and not scanned. And what does that mean? Well that means that the object has been copied over. And so weve actually you know made a copy of the object into the new space. But we havent yet looked at its pointers. We havent yet looked at the pointers inside the object to see where they go. And then to the left of that are the objects that have been copied and scanned. These are objects that have been copied over. And weve also processed all the pointers inside of those obje cts. And so you can think of this area here between the scanned pointer and the allegation pointer this is the work quest. So these are the objects that still need to be processed. These are the objects that have been copied over but might yet still point to objects that havent been copied. And so these are the objects where we have to look at their pointers to see whether they point to something that still needs to be copied over to finish the garbage collection. Returning to our little example Im now going to walk through how a stop-and-copy garbage collector would collect this particular heap step by step. So notice that we only have one root object and its A okay I just want to point out that A has one pointer which points to object C alright. So at the very first step what were going to do is were going to copy the A object over to the new space okay. And this is literally a byte for byte copy. So we just take the bytes of A and we do a copy without you know doing any inspection of the interior of the object over to the new space. And hows that work? Of course our allocation pointer isnt in initially right here at the beginning of the new space. And then we add and we copy this one object over. And then that means allocating an object and so now the allocation pointer points to the first word of memory beyond the object that we just allocated okay. Now what happens when we copy it over? Well because it is just a byte for byte copy all the pointers in A still point to the objects as they pointed to before which are the objects in old space. And notice now that this copy of A points to the object C in the old space. The other thing we do is we leave a forwarding pointer in the old copy of A. So we mark A as having been copied thats why its grayed out. Indicates that this object has already been moved. And that this dotted line here indicates that somewhere we stored a pointer to the new copy of A. And now were ready to begin the algorithms. And not ice that we have some objects here that have been copied but not scanned so this is our work list. So now its going to repeatedly work off of those objects and how do we know theyre objects in there? Well we just compare the scan and the allocation pointers. So if theyre if they are different if theres an object in between the scan and the allocation pointer at least one object between the two then theres work to do. Theres an object that needs to be scanned that and and possibly resulting in more objects being moved and allocated. So what happens next? So object we we process A so we walk over A and find all the pointers in A. And we copy any objects that it points to that havent already been moved. And so before we said you know the A this this copy of A pointed to the old copy of C. So now what we discover the C object it hasnt been moved its still in the old space. So we copy it over and we update the pointer in A to point to the new copy of C. Now of course and then the scan pointer moves over A. Weve scanned all the pointers in A alright. And the allocation pointer also moves because we had to allocate space for C. And of course C is just a byte for byte copy of what was in the old space. And so it any pointers that it has that point to objects that havent been moved yet moved yet just point back into the old space. So in this case the object C points to the object F in the old space. And I probably should indicate here heres the original dividing line you know this is the old space over here and this is the new space over there alright. And finally we mark C as having been copied having been moved to the new space and we left a forwarding pointer to it in case so we can fix any pointers that point to C that we come across in the future. And now we have to continue scanning objects that have been copied but not scanned. And we can see that there is an object between the scan and the allocatio n pointer namely C and so well now process all the pointers in C. Next we scan C. And we discover that it points to F. Which hasnt been moved yet and so we copy F over into the new space and we update the pointer in C. And now C has been copied and scanned okay. So the scan pointer moves past C and of course F again is a byte for byte copy and so all its pointers into old space are still pointing to old space in particular F points to A and the allocation pointer is moved again because we moved F alright. And now we have to process F. And this will be the last object that we move. And what happens well we discover that F points to A okay. And A is already marked as having been moved and it has a forwarding pointer. So instead of copying A again we simply update the pointer in F that pointed to the old version of A to point to the copy of A okay. And so now F is completely scanned. All the pointers in F have been processed. We didnt allocate any new objects so the allocation pointer didnt move and now the scan pointer and the allocation pointer are equal. There are no objects in between them and so our work list is empty and this is the garbage collected heap. This is a complete graph a complete copy of its A of the graph of reachable objects from the old space. So now were done and we simply swap the role of the new and old space and we resume the program so that when the program starts running again it will allocate out of this area and itll be on the allocation pointer until it fills up what is now the old space you know and then this will be the new space that will be used for the next garbage collection. Heres a pseudo code algorithm outlining how stop-and-copy garbage collection should work. So while the scan and allocation pointers are different remember we keep running until the scan pointer catches up with the allocation pointer and theyre equal. What were going to do is were going to look at the object that is at the scan pointer. That well call that objec t  and then for every pointer in O were going to do the following. Were going to find the object O that that pointer points to. And then there are two cases. One is that there is no forwarding pointer alright. And if theres no forwarding pointer then we have to copy that object to new space and that will involve allocating a new object and updating the allocation pointer. Then were going to set and here it says the first word they really shouldnt emphasize the first word. Set a word. So its a distinguished word thats whats important. We have to know which word were going to use and will always be the same word. But anyways we set a word of the old object to point to the new copy. We mark the old object as copied. Mark old object as copied okay. So thats so that we can tell if we ever come to a pointer to it again we know its already been moved and then we change p the pointer to point to the new copy of O alright. So if there was thats what we do if there is no forwarding pointer. And if there is a forwarding pointer then we simply update the pointer to point to the same place as the forwarding pointer. And we just repeat this loop over and over again until weve scanned all the copied options. In this very short video Im going to say a few words about a technique called Conservative Garbage Collection that can be used for languages like C and C++. To review Automatic Memory Management relies on being able to find all the reachable objects. And it also needs to be able to find all the pointers in an object. Now the difficultly with doing garbage collection for a language like C or C++ is that its very difficult or even impossible to identify the contents of objects in memory with 100 percent reliability. So if we see two words in memory you know it might be a list cell that has a data and next field. So we see just two words here. And there are some bit patterns in here 0s and 1s. Okay how do we know whether these are both pointers? It could be that one is a pointer and the the other is not in the case of a list cell. So one of these fields is just data like an injure and another one is a pointer. Or it could be something like a binary tree node where both of these words are pointers. And because of this weakness really in the C and C++ type systems we just cant guarantee that we know where all the pointers are. Now it turns out that it is possible to extend garbage collection techniques to work with languages like C and C++. And the basic idea or insight is that its always okay to be conservative. And if were not sure whether something might be used in the future then we will just keep it around. And remember that graph reachability is already a conservative technique. What we really want is to keep around the objects that will just be used in the future but the reachability in the object graph is an approximation to that so because reachable objects might be used. And now the problem with C and C++ is that we dont know where the pointers are. We dont have a guarantee from the type system about where the pointers are. And so the basic trick is that if something looks like a pointer then we will treat it as a pointer. All we have to do is be conservative and if we are not sure wh ether a given word of memory is a pointer. Then we can just treat it as a pointer and keep whatever it points to around. If we and as long as we are not going to move it or change it that would be okay. And so how how do we decide whether a particular word of memory is a pointer? Well it should be a line meaning you know it should end in some zeros to indicate that it was pointing if it was a pointer it was pointing to a word boundary. And then whatever pattern it is if we interpret it as an address it has to be a valid address. So it should point to the data segment. And Noah said you know these two conditions will rule out all kinds of data and memory. So for example any small integer is probably not going to be interpretable as a valid address in the data segment. So you know most likely only things that are pointers or very few things that are not pointers will be treated as pointers. And what were going to do then is if it looks like a pointer were going to consider it to be a pointer. Well follow it and then well end up overestimating the set of reachable objects. We may keep around some stuff that isnt reachable at all. But thats alright its always okay to keep around more stuff than necessary. Now we still cant move the object alright? Because we cant update the pointers to them. If we dont know that something is a pointer we certainly dont want to change it okay? And you know for example if we thought something was a pointer and it was actually an account number and then we updated the pointer when we move the object we would just completely change what the program does. So this only really works when you mark this way. In this video were going to conclude our discussion of automatic memory management with the third and last technique were going to talk about for garbage collection called Reference Counting. So the basic idea behind reference counting is that rather than waiting for memory to be completely exhausted were going to try to collect an object as soon as soon as there are no more pointers to it. So as soon as we discard the last pointer to an object and it becomes unreachable we will try to collect it at that point in time. And how can we do this? Well as the name suggests were going to count the number of references to each object. So in each object we are going to store the number of pointers to that object. So if I have an object in memory and it has say three pointers to it from other objects then somewhere in this object is going to be a dedicated field that contains the number three. And if this number ever drops to zero if we discard these pointers and this number becomes a zero then we know that nobody is pointing to this object and it can be free. And what this means is that every assignment has to manipulate the reference count in order to maintain an accurate count of the number of pointers pointing to an object. So allocating a new object returns an object with a reference count of one. So when a object is created by new it will already have a reference count of one. The pointer that is returned is the only reference to the object. And so were gonna write the reference count of an object x is rc of x. And now when we have an assignment x gets assigned y were going to have to update the reference counts of both the object that x points to and the object that y points to before the assignment. So what happens here? So if y points to p so lets draw our objects here so y is a local variable and it points to some object p in memory and x is also a local variable and it points to some object o. Okay? So now x is getting the value of y and so thats going to move this po inter from where pointer before pointing to the same thing as y. So whats going to happen while ps reference count is going to go up by one and os reference count is going to go down by one. And since we decremented os reference counts as we dropped this pointer to the object o we need to do a check to see if the reference count has become zero. And if the reference count has dropped to zero then we can free the memory for o. And then in addition to updating the reference counts and checking whether the reference count of o became zero we actually need to do the assignment itself alright? So every assignment I want to stress that every single assignment in the program its now translated into these four operations that need to be done to maintain the reference counts. There are tradeoffs in reference counting. It has advantages and disadvantages. One of the big advantages is that it collects garbage incrementally without large pauses in the execution. So for for kind for applications where large pauses would be problematic say real time applications or interactive applications reference counting can really help because it minimizes the length of the longest possible pause. Okay so your program will never go to sleep. And just appear to stop running for some period of time because its off collecting garbage. It always collects the garbage in small incremental amounts and so that you never see a long pause. Reference counting or at least a basic implementation of reference counting is also quite easy to implement. Its very straight forward to go through and modify the code to add reference counts. So you can easily imagine a code generator that would simply generate different code for for the assignment operation than it normally did if you were adding reference counting to an implementation. So really the the changes that are needed for a simple implementation of reference counting to a compiler are not that pervasive. Now there are some disadvantages  to reference counting. One is that manipulating the reference counts at each assignment is really quite slow. So if you remember what happens we have a couple of updates to reference counts so we have to update you know the reference count of two objects. To do an assignment. This is this is the code to do an assignment and then we have an if statement. And then we actually do the assignment itself. So theres two reference count updates thats has to see if our reference count became zero and then we actually do the assignment. So the overhead here is substantial. Youre taking every single assignment in the program and blowing up its cost by at least four or five times. And that will have a very noticeable impact on the performance of many programs. Now it is possible to optimize reference counting. So for example if we had two updates to the same object say within a basic block or even within a control flow graph a compiler a smart optimizing compiler could frequently combine those reference count operations together. So instead of updating the reference count to the object two times it can just update it one time. And similarly if there were even more reference count updates potentially all of those could be coalesced within some region of the program. The problem with that is that is becomes very tricky to get that right. So a a simple implementation of reference counting is quite slow. But easy to get right. A very sophisticated implementation of reference counting or highly optimized implementation of reference counting is somewhat faster. Still has a noticeable performance impact if youre reference counting all objects but it is substantially faster. However its quite tricky to get it correct. The other problem with reference counting is that it cannot directly collect circular structures. So to see this lets draw a little heap with a circular structure. And so lets say we have a local variable x and it points to some object in t he heap. And that object has a pointer to another object alright? And that object that second object then has a pointer back to the first object. Okay so here x is pointing to say a circularly length list of length two alright? And if we add in the reference counts here what would those look like? Well this object down here the second object here actually one reference to it so its reference count will be one. And this first object has two pointers to it one from x and one from the other object and so its reference count is two. Okay so here is our little heap and we can see that there is no garbage here because all the objects are reachable from a a local variable or variable of the program. Now if we were to assign a new value to x lets say that we have the assignment x gets null. Alright so this pointer goes away. But whats going to happen? Well when we do that assignment were going to change the reference count here of this object its now gonna be one. And if we look at this heap we now see that these objects these two objects are unreachable. Okay so these are unreachable. But notice that the reference counts are not zero so we cant collect them. The garbage collector or the reference counting implementation will check the reference counts and see oh these are one and so we cant delete them. And then what it cant see is that the only references to these objects are from other unreachable objects. So the bottom line is that reference counting cant collect circular structures and there is only really two ways to deal with that. One is if the programmer remembers whenever a circular structure is going to become unreadable to somehow break the circularity. So for example before we clobbered the pointer to x here we remembered to go in and say set you know this pointer here to null. If we nulled out one of the pointers in this cycle so that there was no longer a cycle then the reference counting would work correctly because then the reference count of this object would go to zero when when this pointer was dropped from x and that would cause the reference count of this object also to go to zero after this object was deleted okay? The other possibility is to back reference counting by some other garbage collection technique that can collect cycles. And so in some reference counting systems for example most of the garbage collection is done by reference counting but every now and again once in a very very while you might want to mark and sweep collector to go through and clean out any circular but unreachable data structures. In the next several videos were going to apply what weve learned in the class to analysis of various features of Java. This will give us a chance to both look at a real programming language and how its designed and been done. And also to talk about some features that art in Cool that we havent been able to cover thus far in the course. For the perspective of this class Java is a kind of Cool on steroids. Its Cool plus more features many more features. At at their core Java and Cool are very similar. Java and Cool are both typed object oriented garbage collected languages. They were both designed in the early 1990s and so they share a common culture there with the ideas that were current at that time. So in this video Im just going to give a little bit of history of Java so that will be the focus really of this rather short video. And then in the subsequent videos well talk about all the features or some of the features of Java that are not in Cool. And well use the ideas of course that weve been discussing all along through the class to explain a little bit about those ideas. But I think these are all important language constructs that are unfortunately were too time consuming or too complicated to add to the course project. So I think its useful to use a language like Java began as a project called Oak at SUN Microsystems and it was originally targeted at set top devices. And so what was a set top device? This was a little box that was gonna sit on top of your TV So you had your TV screen and then there was going to be this little thing up here that was going to sit on top of the TV that was going to control all your cable programming. So this would essentially would connect to some kind of network and it would help you to do you know to make your TV more interactive. Okay so this was back in the days before every TV was really a computer by itself. The initial develo pment of of Oak took several years. I believe the project ran from about 91 to 94 and what happened at least as I understand it is that this set top device market just never took off. So this never became a popular never never became a popular idea with consumers and and so there really was a kind of limited up side or a limited potential for for Oak at the time. And then something happened the Internet happened. So also in the early the Internet and it became obvious sometime 93 94 that there was going to be a need for programming languages that really addressed these specific issues on the Internet. And in particular people were very concerned about security. And they didnt want to be downloading lots of binaries that were programs written in C and passing those around on the Internet because it was just really no guarantee that those programs would work as intended and not crash your machine. So you know they needed to be able to share code over the Internet from other people that you didnt necessarily trust completely meant that we needed safer languages than languages like C and C++ and so there was an opportunity there for a a new language and there were several candidates actually. Besides Java Tickle and Python were very serious candidates to become the Internet programming language eventually the backing of SUN Microsystems the backing of SUN gave to Java helped it to really gain a very very strong presence on the Internet. But you know the point of this story is that every new language needs a killer app. Every programming language rides into the world on the back of some application. So there has to be some kind of new application that people want to write that the existing languages dont serve very well and that provides the opportunity and therefore makes it worthwhile for people to learn a new programming language. And so the fact that Ja va was a very safe language it had the garbage collection. It had the type system that made it well suited at the time to the the needs of the emerging needs of internet programming. And it became very popular I think primarily because of that reason. And if you if you recall there was a lecture or a video early on on the economy of programming languages. And I would recommend actually that if you havent watched that one then you go back and take a look because there I discuss some of these ideas about how languages are adopted in more detail. So Java also came into existence in a particular technical environment and its very common. In fact its normal for new programming languages to borrow heavily from their predecessors and new languages are often mostly recombinations of ideas from previous languages in a new design perhaps with some innovations thrown in. And the particular influences on Java. Again this is at least so far as I understand it so the type system in Java or its its commitment to types probably is traceable to Modula-3 and the ideas there where people try to build a sort of programming language that would scale in a realistic way to large systems but also be strongly typed. The object orientation of Java is traceable to languages like Objective C and C++ and and also Eiffel which also had the idea of interfaces which is a prominent feature in Java. And finally Java is quite a dynamic language meaning that there many things that are not done staticly. Theyre done dynamically so features like reflection would be one example of that. And there are actually quite a few other features and there is some history there. Theres some shared culture there with Lisp. This was a or is a functional family language but is also a very very dynamic language. As I said at the beginning this video is just an introduction and overview and in the next few videos well be looking at specific features of Java and how they work. And that will includes things like exceptions interfaces and threads. As well as a bunch of other features that well discuss at least briefly. One thing to realize is that Java is a big language. It is not simple. The language manual for Java runs to hundreds and hundreds of pages. It has lots of features. And perhaps more importantly lots of feature interactions. So. The hard part of designing a language of course is getting all the feature interactions right. All the combinations of the features and you know making sure there are no surprising interactions between them. In this video were going to take a look at Java Arrays. Lets say we have two classes a and b and that b is a sub class of a. And lets think about what happens when we execute the following piece of code. So the first thing were going to do is were going to allocate an array of bs. So this is an array thats supposed to hold Bs okay? And were going to have an array variable b here that points to it. And then were going to have another variable array variable A that also points to the same array as B. But notice the type of a. So a is an array of As little a here is an array of As and b is typed as an array of Bs. And now what were going to do is were going to assign into a sub zero okay into the first position of a a new a object. And that should be fine right? Because a is an array of As and that seems like that should work out alright. Alright so here there will be an a sitting in the first position. And then were going to access b sub zero which because A and B point to the same array is the same element as a sub zero. And were going to call some method that is not declared in A. Now remember B is a sub-type of A alright? So B has all the methods of A. A but B might have more methods. And since this is an array of Bs we should be able to call all the B methods on it and yet here when we call some methods thats declared in B but not in A we are going to get a run-time error because the object stored in the array is actually an A object at the first position. To understand whats going on in this example we have to take a look at the sub typing rules in Java. So if we use a subtype of A if B inherits from A thats one case. So if B and B inherits from A then B is a subtype of A. And thats just like in Cool and and most other object oriented languages. And were were very familiar with that from our lectures in type checking. Further more type sub typing is transitive. So if C is a subtype of B and B is a subtype of A then C is also a subtype of A. Okay a nd thats also completely standard. But then theres this other rule thats not quite standard or is definitely nonstandard. And thats that an array of Bs is a sub-type of an array of As if the element types are in a sub-type relationship. So if B is a sub-type of A then array of B is a sub-type of array of A. And Cool doesnt have anything like that Cool doesnt have arrays so it wouldnt even have the opportunity to have something like that. But this is also not the way its done in other languages that have objects and sub-typing. So lets take a look at our little example again and let me explain it in a slightly different way. So the issue here is that we have a area of memory and it actually doesnt matter here. Its not essential that this be an array. Whats important is thats an updatable part of memory so that we have pointers to it. We have two pointers to it a and b and we can they can both read and write this part of memory. So this could be just a single cell it doesnt have to be an array of multiple cells. But whats important is that there is some memory location that both of these point to that they can both read and write okay? And the trouble comes and by the way that theres a name that thats called Aliasing okay? So when you have two names two program names for the same part of memory that is called aliasing and here you know we have the the two arrays A and B that point to the same area of memory okay? Now aliasing is very common in real programs since not bad by itself but the problem in this example is that A and B have different types okay? And in general if you have aliasing updatable references okay? Meaning if two names for the same location that location is both readable and writable so it can be updated through the two names. And those two names have different types then that is going to be unsound okay? Were not going to have a sound type system and to see the problem lets say here in this case what was it? We had that B type B was sub type of A okay? And what did that mean? Well that meant is we could do a wright through this pointer okay? And write an A object into this location and then we could read that out through this point over here as a B object. But now it doesnt have all the methods and and fields of A and treating it as the object we could potentially use an operation on it thats undefined. And you can see that it doesnt help if we swap the roles of of A and B alright? So in particular if we reverse the if we reverse the sub-typing relationship so that A was a sub type of B we can do exactly the same problem because aliasing is symmetric. We just do the write through the B pointer and the read out of the A pointer swapping the roles of the recent right here and we have exactly the same problem. So in general multiple aliases do updatable locations with different types is unsound okay? And this problem actually has come up in many different programming languages. Java is not the only programming language to have had this issue. Its a fairly subtle aspect of type systems and in many languages have done things similar to Java where theyve created a problem really for the static type system by wanting to have a sub-typing work through arrays. Now the standard solution or the solution thats used in I should say in many languages and is probably most widely accepted in the programming languages research community is that you need a different sub-typing rule for arrays. So we would say you know the rule that is commonly used the standard solution to this problem at the type level is that to do the following things. So you only allows sub-typing on arrays. So you know an array of Bs is a sub-type in array of As only if B and A are the same type. If B = A. And if you think about that for a second if we have an array and now we have our two pointers to it A and B and we know the type of A the subtype or the type of B. Well that only h appens if the element types are are equal. And so we cant create two references to an updateable location with different types. Okay and that will guarantee soundness of of the type of the type system. So Java fixes the problem differently. So instead of statically checking that array accesses will all be type correct Java does this at run time. And so whenever an assignment is done into an array at runtime Java checks whether the type of the object being assigned in compatible with the type of the array. So when you say new B sub ten in Java Java will remember inside the array that this was supposed to be an array of Bs. And then whatever you assign into the array it will check that the thing youre assigning is either a B or a sub type of B. Now this obviously adds an overhead on array computations so every assignment to an array is going to have have a type check on it at run time. And fortunately though the most kinds of arrays are arrays of primitive types in particular arrays of ints and arrays of floating point numbers and these are not affected because the primitive types are not classes. Theres no subtyping on them and so you can never create an array say of floating point numbers with any kind of subtyping relationship that would result in this problem. So so that were saved or in better shape for the primitive types and they dont need these extra checks. But if you have arrays of objects then we do assignments into those arrays in Java theres additional run time overhead. In this video were going to talk about programmer defined exceptions. So think about the following typical programming scenario. Youre deep into some fairly complex section of code and you come to a place where you could experience an unexpected error. That there could be something that could happen that would violate some important property of your program. So for example maybe youre going to discover theres a place where you could be out of memory or theres some data structure that doesnt satisfy some variant. So a list thats supposed to be sorted that is not or something like that. And the question is how do you handle these errors? How do you write code that will handle the error gracefully and not make your program really really ugly? So a popular solution to this problem in many languages including Java is add a new kind of value to the language called an exception. And well have a couple of control constructs for dealing with exceptions. Here is the two that are probably the most popular. And these are as they appear in Java. So we can throw exceptions and what this does is it causes an exception to be created at this point wherever the throw occurs. And that exception will simply propagate out of the program. It will basically halt the execution of the program at that point and any containing constructs will also throw the exception. So the exception will you know simply propagate up out of the code that thats currently executing until it hits a try catch. So how does this work? Well we can try something. We can execute some expression here and this will be some expression. And if this expression throws an exception if the throw occurs somewhere inside this expression then we will catch that exception and there can be a binding here to say what we are going to call the exception value. So this is like a let we will grab the exception that comes out of here. Well call it X and then we can execute this piece of cleanup code to how to handle the exception in some way. And so the basic idea behind this thi design for handling exceptions is that the place where you have the exception the place where you actually detect the exception might be somewhere deep inside the code and not a very good place for actually dealing with the exception. So what you want to do is get out of that part of the code get back to some higher level point where you can clean things up deal with the exception and then retry perhaps some larger block of code. Heres a little example of using exceptions. So here we have our main method. And what are we going to do? Were going to have a try-block that just calls a function X. And if that raises a an exception then we will catch the exception. In this case we dont do anything with the exception we just print out a message saying there was an error and we terminate the program. So we dont do anything very smart but we do catch the exception and at least print out an error rather than just terminating. So what does x do? Well X simply throws an exception. So this function X allocates an exception object. This is just a value its just a class like everything else. But it has a special property that it can be thrown. So when we throw it thats when X terminates abnormally and we end up in the catch block of the try-catch expression back in the main method. Now in the last couple of slides I gave you a very informal description of how exceptions work and it it might not have been very clear and in fact its hard I think to give a very clear description without some kind of formal description of how exceptions are supposed to behave. But fortunately weve looked at operational semantics in this class and so now youre familiar with those kinds of descriptions of language behavior and I can actually describe you pretty succinctly how try-catch actually really works alright. So were going to give operational rules here for Try-Catch expressions. And I just noticed I just poin t out here that I had some kind of font problem and so I had to write in the turnstiles by hand in this slide. So those handwritten characters are all supposed to be the turnstile character. Now to more important things theres a distinction with exceptions. Okay there are two kinds of states that an exception object could be in. It could just be an ordinary value. So when I say new exception object in Java so when I say something new something exception class is just an ordinary value. At that point it just behaves like any other object. But then there is a distinction when the object is thrown. So when the exception is actually thrown it becomes a special kind of value and it gets treated differently alright. So were going to distinguish between an ordinary object V okay. And objects that have been thrown okay which are then active exceptions. Alright? So lets take a look at some of the operational rules for the exception constructs. So heres one rule for its try-catch-block. And what this rule says is that if an expression evaluates to an ordinary value. If it doesnt throw an exception then the results of the try-catch-block is just that value. So the way the try-catch-block works is you evaluate the expression in the try-block. If that terminates normally with a value then the results of the whole expression is just that value alright? Now the other possibility is that youll evaluate a try catch block and when you go to evaluate the expression of the try-block E1 it will throw an exception. So it could result in a thrown exception. And so in this case. Okay that is that excuse me that is this case where E1 evaluates to one of these special values has been marked as a thrown exception. What do we do in that case? Well like unwrap the exception we pull out value that is in the thrown exception alright. We bind it to some local name alright thats named in the catch expression and then we evaluate the cleanup code. So with the ex ception value available we evaluate E2 and whatever the result is of E2 that is the result of the try-catch-block. How does throw work? Its very simple. So throw just takes an expression it evaluates that expression against some value V. And then it marks that value as a thrown exception as a thrown exception so it wraps the value in this T thing and that indicate that this exception now has been thrown. Now the only other thing we need to talk about is how the rest of the language all the other constructs in the language deal with these thrown exceptions. And thats very simple. We want those thrown exceptions to simply propagate out of any other kind of expression. So for example well just do one example here because the idea is the same for every other language construct. Lets say that were evaluating E1 + E2 alright. So the first thing we have to do of course is to evaluate E1 and if that happens the thrown exception. So if something goes wrong in the evaluation of E1 and E1 evaluates to the thrown in exception well then we stop the evaluation of the plus right there. We dont even evaluate E2 notice that E2 is not mentioned here above the line of one of the things to be evaluated so that E1 terminates normally with an exception then the results of the entire addition is that exception. And similarly for all the other constructs if if theres one of their sub-expressions results in an exception. In fact if the if as soon as one of their sub-expressions results in an exception they stop evaluating and propagate that exception up. The only thing that will stop the exception from propagating all the way out to become the result of the whole program is if it is caught in a try-catch-block. There are many ways to implement exceptions and here is one simple way to do it. So we encounter a try expression we will mark the current location in the stack. So we will mark the position in the stack where we encountered the try. So for example here say is our s tack. Lets say that you know the stack is going this way. We encounter a try expression here so we put some kind of marker in the stack to indicate that theres a try that was encountered there. And then you go on you know evaluating things inside of the try which might add more stuff to the stack. Now when we throw an exception if down here all of a sudden a throw occurs and were at this point in the execution whats going to happen is were going to unwind the stacks. Were going to knock everything off the stack. Were going to pop all of this stuff of the stack so its all gone back to the first try. And then well execute the corresponding catch. So here we marked you know the place and the code where there was a try and we can use that to find the expression the piece of the code that has the corresponding catch-block and well unwind the stack to that point and then begin evaluation of the catch. So at this particular design it has the disadvantage that try actually cost something. So even if you dont throw an even if you dont throw an exception you still pay something to execute a try-catch-block. You have to at least mark the stack and and remember to unmark it of course when you pop things off the stack when you leave the try-block. So more complex techniques tries to reduce the cost of try and throw and also trade off between them. And generally the thing you want to do is because exceptions are probably relatively rare in in most programs is to make the cost of try as low as possible possibly at the expense of making throws slightly more expensive. Now heres a little trivia question about Java. So what happens to an uncaught exception as thrown during object finalization? So if you dont know what object finalization is well when an object is collected when an object is garbage collected it is possible to run a method on that object to clean it up before the garbage collector actually deallocates it and this is called the finali zation method. So objects have finalization methods in in Java and those methods are essentially invoked by the garbage collector. Some garbage collector discovers that some objects this garbage is going to be clean it up it will first invoke the finalization method. And why would you wanted to do this unless say we have an object and it might have say a file handle. It might have a pointer to an open file or something like that. And now when this object becomes unreachable it will be collected by the garbage collector. But if you dont close the file well thats gonna cause problems. Having lots of open files that are hanging around without the program using them it can cause problems later on especially if you run out of file handles. Usually theres a fixed number of file handles available from the operating system. So the right thing to do is when this is garbage collected is to first close the file and essentially get rid of this pointer okay and then deallocate the object and that it was object finalization is for. So again you can define the method in Java that will be run by the garbage collector to kinda clean up any resources the object has before its actually deallocated. And now the question is if that finalization method raises an exception who catches it? Because it was invoked by the garbage collector its unpredictable when its going to be invoked and its not clear where that exception should be handled. And the answer to the question is no one handles that method or nothing handles that method. The exception is simply dropped. And so any exceptions that happen during object finalization that are not handled within the finalization method itself are simply thrown away. One of the nice innovations in Java is that exceptions are actually part of the method interface and they are checked by the compiler. So in in the example that I gave at the beginning of this lecture we had a method X that could raise an exception my exception and notice that the declaration of X actually declares that X can throw that exception. So its part of the interfaced X part of the checked interfaced X that it can raise a particular exception. And why would you want to check this at compile time? Well the observation that was made actually in the original Java project was that there were many exceptions that could be raised by Java programs and people easily lost rack of what possible exceptions could be raised they didnt know what exceptions they had to handle. And in fact when they added this to the language the compiler would actually enforce now that a method declared every exception it could raise. They discovered lots of places in the in the compiler where there were exceptions being raised but not properly handled. So this led to better air handling in in the Java compiler itself and and I think people generally agree that this is een a good idea because it helps programmers to write more robust code because they can see exactly which exceptions they have to handle. Now there are some exceptions to this rule. In particular theres theres some kinds of runtime errors that dont have to be part of the method signature simply because its very hard to check statistically that the method would never raise them. So things like dereferencing null or interprocedural overflow dont have to be handled and declared in the interface. But for the most part any exception that a a method can raise has to be declared as part of its interface in Java. And then there are other mundane-type rules about the particular design for exceptions in Java. So for example throw has to be applied to an object of type exception it cant be applied to an object of an arbitrary type. But overall exceptions are rather nicely done in Java and that this particular idea of of declaring the types of exceptions that a method can raise was a new idea in Java. In this short video were gonna take a look at interfaces in Java. Interfaces specify relationships between classes without using inheritance. So here is an example: uh we have an interface here called the point interface. And a point interface can have a a bunch of methods in it and and we just declare the the signature of those methods. You can also have other things besides methods but uh the main thing that uh theyre used for usually is for a a method interface. So uh heres an example of a particular method the move method and it takes some arguments and has a particular return type. Now any other class or any class excuse me thats going to implement the point interface has to provide a method uh with the same signature. So if this see because the point interface has a move method the. A class will have to have a move method with the same signature as the move method in the declared interface. And if the point interface had other methods ah then the point class would also have to implement those methods by you know having a method of the same name ah with the appropriate types of arguments and result. Now it says in the Java language manual that Java programs can use interfaces to make it unnecessary for related classes to share a common abstract super class or to add methods to objects. And the translation of that is that interfaces play the same role as multiple inheritance in c plus plus. So interfaces uh are really analogous uh to to multiple inheritance. And the reason for that is that a a class can implement multiple interfaces. So if I have a class x and it implements a three interfaces a b and c. This means an extra object can be treated as an A object or B object or a C object in the appropriate context. So its like or almost as if X has three superclasses A B and C. Now uh there are some important differences uh but there is the effect and so if I wanted to have a class that gets functionality or implements a functionality uh several uh uh interfaces thats I mean we do very directly in java just by saying if the classes going to implement all those interfaces. &gt;&gt; Now here is an example of an application of that so think about a graduate student ah working at Stanford or some other university so ah typically graduate students are students okay they take classes and have propertys that students ah have they get degrees and grades and things like that. &gt;&gt; Graduate students also work for the University? &gt;&gt; They are often teaching assistants in classes or research assistant [inaudible] so there [inaudible] another role which is university employee and if I have gone to ah trouble ah in my. I university personnel management software to implement functionality to deal with students and to implement functionality to deal with employees. Well then I would like to make use of that when I get around to thinking about how Im going to implement uh the functionality for graduate students and one way to do that. Would be if I had a class with implement if I had excuse me an interface for employees and interface for student and I would say that graduate student could be both okay. So a graduate student can implement both the employee interface and student interface. And and the reason thats this is a good idea is it is actually hard to do this if ah you only have single inheritance. If you think about it if I had set things up so that I had some employee classes and some student classes and now I want to make graduate students. Well now what am I going to do? Well if I have my employee class. I can make grad students a subclass of that but now how do I get the student functionality and similarly. If I have a student class I can make graduate student a subclass of that. But now how do I get the employee functionality? So in single inheritance youre forced to choose a single class to inherit from. And the advantage of interfaces is that it will let you get functionality or implement functionality or express the relationship at least of functionality to multiple kinds of things. And so I can have one uh graduate student class that implements both the employee and the student functionality. So how are interfaces different from inheritance? Well probably the biggest difference is that its not possible to implement interfaces as efficiently as inheritance. And thats why you have both. So youd prefer to use inheritance uh if you can because its going to be more efficient than interfaces. And what makes interfaces less efficient? Well the primary thing. Is that if the class is implementing interfaces need not be at fixed offsets. In fact we will not be able in general uh to assign methods in interfaces to fixed offsets inside of a class implementation or an object implementation. So lets take a look at an example. Heres our point interface again. Now say we have one class point when we saw this one before [inaudible]. And it implements the move method has to implement the move method. And then we have another class that also points to point interface but it also implements some other stuff. Okay so it might implement some other methods that arent part of that interface. So now how would we decide you know where to put the move method. Well the natural thing that that weve discussed. [inaudible] Say of course declared so if we did that clearly the move method will not be ah in the first position in both of these classes. Now. We could imagine uh a separate compiler pass that were trying to sort the methods. So that say all the methods of the point interface always appeared in the same position and in the same order in any class implements the point interface. But that doesnt work as soon as we have um multiple interfaces being implemented. So lets say that the point two class here implemented another interface A. So how can we then implement interfaces? Well so its going to be a little more complex than usual to implement in this batch say to a method f where e has some interface type. So if e is typed as having some interface and now were calling it the f method of that interface then were going to have to do a little bit more work. And so heres one approach this approach is actually quite inefficient but you will see that it will work. And there are other approaches that are more efficient but thats not particularly important so heres one way that can work. So each class of implements interface is going to have a look-up table associated with it that maps method names the string names of methods to those methods themselves. And then. Uh we can hash the method names for faster look-up and we can actually compute. Uh those hashes at compile time. And so the idea would be that if when we have an object. Ah somewhere in the object probably at the dispatch pointer. The dispatch pointer you know will point off to a list of methods sort of the normal methods of the class. But somewhere say maybe at the end of the dispatch table there will be another pointer to some kind of look up table that maps names. Two two methods to quote. Okay. So somehow associated with every object of every class ah we will have this look up table that will map ah the the names of interface methods to the actual codes for those methods that influence them. All right and wed already decided uh that the for the point interface that the move method should go first. It should be the first method in the class. Well if we had made a similar decision for the a interface some method in that interface that should always be listed first in the give to all the methods and all the interfaces so that they can be maintained across all of the uh classes that implement those interfaces. At least classes that are declared and all the interfaces that are declared. And thats kind of un-Java. And that we dont want to uh force people to declare all the classes and the interfaces once and not be able to extend them in the future. Alright so the bottom line is that methods in interfaces do not live at fixed process in In this video were going to talk about Coercions which is a feature of type systems that appears in many languages and we will be looking specifically how coercions are done in Java. Java allows primitive types to be coerced in certain context. And coerced means converted from one type to another. So heres an example lets take the expression one + 2.0 and the difficulty with this expression is that the the one here is is an integer and the 2.0 is a floating point number. And there is no way to add an int to a float directly. We either have to convert the integer to a float and then do the add as floating point numbers or convert the floating point number to an integer and then do the addition as integer addition. So they have to be converted to a common representation before we can actually do the operation. And the normal thing to do and the thing that Java does is to convert the integer to the floating point number 1.0. Now a coercion the right way I think to think of coercions is theyre really just primitive functions that the compiler inserts for you. So its like you left out a function call and the compiler notices that and puts it in. So in this particular example what would be the function call? Well there we can think of there being a primitive function that converts integers to floating point numbers in the obvious way. And so really this expression here gets converted to the expression into float applied to the number one plus 2.0. All right. So coercions are probably best thought of as a convenience for you the programmer to let you avoid having to write some function calls. And so where it is obvious that a type conversion is going on the compiler can insert the function that performs that type conversion for you. And most languages really have extensive coercions so the conversions are very very common particularly between numeric types and so this is not just Java. This is really many different programming languages of all styles that have lots of differe nt kinds of coercions. Now Java in particular distinguishes two kinds of coercions and casts. You have widening coercions and these will always succeed. Alright so that means that Java will always put them in and there will never be any complaining from the compiler or the runtime system about them and we already saw one of those so the conversion from int to float is an example of a widening cast. Now narrowing casts may fail if the data cant be converted to the desired type. So in particular float to int well this will work fine. Something like 2.0 can be converted in obvious way to two but if youre converting something that doesnt have an integer representation something say like 2.5 you know theres a question of what we should do here. Okay and for such narrowing casts where there isnt a a clear mapping whether we should go or you know or whether we should try here or round up or whatever then Java will actually complain and not let you do it. Okay perhaps for better example of the kind of narrowing cast thats that Java will complain about is something like a down cast. So if I have two classes A and B. And B is a subtype of A and then I have something of type A. Well I can cast it to B. I can say lets say I have X which is a type A and then I can have an expression where I try to convert x to a B object. So here I have a cast Ive indicated that I want to treat this expression x as a B object and this will type check okay. So the the compiler will let this through since B is subtype of A. But at run time its actually going to check whether x is actually a B object and if its not youre going to get an exception. So this can fail at run time if the object thatt x actually holds at the point of the cast is not a B object. So the rule in Java is that narrowing cast must be explicit. You have to put the function in yourself. You have to put in the typecase in the code so that its obvious that you really want to do it but widening casts in coercions can be implicit so its alright. If youre widening if youre either promoting to a super type or you are converting between initial type where its clear that the one type embeds in the other then those can be filled in for you by the compiler. And heres a little Java trivia question. So it turns out that there is one type in Java for which there are no coercions or casts defined. Okay so there are no implicit conversions or even explicit conversions from that type to any other type. And the answer to the question which is the only one is bool. Okay so only the type boolean has no conercions or casts to another type. Now personally Im not a big fan of coercions. I think that its clearly a convenience for programmers its clearly something that is widely accepted as being necessary in programming languages because casts implicit casts and conversions are so ubiquitous but I do think that it tends to lead to programs that have behavior thats different from what the programmer probably expected. And heres a good example from the language PL/1 which recalls stood for Programming Language one designed by IBM in the 1960s. And had many many features in it so weve talked about PL/1 a few times in this class. And one thing that PL/1 had was very extensive cast and coercions and this could lead us to some surprising behaviors. So heres an example we have a A B and C are strings of three characters so its important to know here that the length three is part of the type. So B is string 123 C is 456 and then A is going to be B + c and the question is what is A? And and you probably wont guess so let me show you what I think is the right answer. So first of all the question is what happens with this + operation here? Well so that is going to be interpreted as an integer + so B and C are both going to be cast to integers and the and this will be done as a integer arithmetic. So B will get converted to the number 123. C will get conver ted to the number 456 okay? And then well add them and well get out the number 579. Okay so the result of this expression is 579 but A is also a string of three characters so this has to be cast back to a string. Now it turns out that this cast happens in two steps. First this this number here is cast to a string of the default length okay. And the default length happens to be six so this is cast to a string looks like this. Theres three blanks followed by 579. And then that string of six characters is converted to a string of three characters. And we just take the first three characters and so we get out that and so the answer is that this program stores a string of three blanks in A. Which is probably not what was expected. In this video were gonna talk a little bit about concurrency in programming languages and in particular Javas use of threads. Java has concurrency built in through threads and Im not going to explain what a thread is from first principles in this video. So Im going to assume a little bit of background but well say a few words here about what threads are. So a thread is like its own program. It has its own program counter meaning it has an instruction that its executing and it has its own set of local variables and activation records. And a Java program or any program written in any language with threads may have multiple threads at the same time. So abstractly we can think of threads as being a series of exec of of statements that are executed. Each one of these threads again has its own set of local variables. But the threads may refer to common data in the heap. So they could refer to the same heap data structures. And each thread is executing a particular instruction so lets say that the threads are all here we have three threads one two and three. And theyre each at some instruction or some statement in the program. And then there is a scheduler and at each step of execution the scheduler will pick one thread to execute. And itll execute one statement. And this is conceptual. Meaning this isnt exactly the way its usually implemented. And then it will repeat this loop. So itll pick a thread itll execute one statement of that thread and itll just keep doing that over and over again. So we might for example the schedule might pick thread one and execute this first statement. And then it might pick thread two and execute this statement and then it might pick thread three and execute that statement. And then it might decide well to execute another statement of thread two and then it might execute several statements of thread one. And then it might come back and execute a couple statements of thread three and then thread two might get to go again for a while and so on. All right so the threads execute in some order. And its non-deterministic at each step of execution which thread will execute how many of its instructions will be executed. And and thus the threads may inter-lead the instructions in the threads may inter-lead in a relatively or in fact completely arbitrary order. Alright? Now coming back to how this is done in Java thread objects in Java all have class threads. So theres a special class that you have to inherit from in order to be a thread. And what you get when you inherit from the thread class is you will have start and stop methods for beginning and ending the thread. Alright? And theres some other special properties of threads. And in particular one thing that threads can do is to synchronize on objects. So a a a thread can acquire a lock on an object through the synchronized construct. And so if I say synchronized xe in Java what that means is that the program will acquire a lock on x before it executes e. So the procedure here will be to lock x then evaluate e and then unlock x alright? So its a structured synchronization construct. And within while it is executing the expression e it will hold a lock on x. And this is the primary way really almost the only way in Java to get synchronization between multiple threads. So this is how we one can control the set of interleavings because while one thread is executing this particular block of code no other thread can execute this block of code and also hold a lock on the same object x. Now could two threads could execute this same syntactic construct if they were locking if if their local variables actually referred to different objects. But theyre guaranteed not to interfere with each other not to interweave if they tried to lock the same object x alright? Now theres one shorthand in Java which is used more commonly than this form the synchronized construct. And as the synchronization can be put on methods. We can say synchronized f where this is a method definition. Alright? An d what this means is that when this method is called that this object will be locked. So here the object thats going to be locked is implicit. And when synchronized is attached to a method name or a method declaration that always means that this parameter will be the synchronized or locked object. Lets take a look at the simple example and think about what happens if we have two methods one of which calls the method two of the class simple and one of which calls the method fro. So lets take a look at that lets say we have thread one and thread two. And now thread one is going to invoke the method two and thread two is going to invoke the method fro. All right? So one possibility here lets say that that two gets to run all the way to completion before fro executes anything. So then well have a = three and b = four okay? And then fro will run and it will print out the string a = three b = four. Okay? So thats a relatively simple straight forward case. You know another possibility is that thread two gets to run before thread one ever does anything. So thread two executes all of its instructions before thread one executes anything at all. In which case what will be printed. Well the fro will print out a = one b = two alright? And two will then run and it will set after this executes. So after this after fro finishes executing it will then set a = three and b = four. Thats another possibility and both of those are fine alright? But then there are some other odd possibilities and lets take a look at one of those. What happens if the thread is actually enter leave in a non-trivial way. So lets consider the following possibilities. Lets say that two executes these assignment a = three. And now fro executes the first part of the print. So it does the read of a and starts building up this output string okay? So its going to print out here a = three alright? And then now lets say that fro actually goes ahead and gets to run some more and also goes ahead and prints out the rest of this. Okay? So actually does the second read of b so the n it will print b = two. All right? And then one will run the rest of the way through excuse me b = four. And so here we got an output that doesnt seem quite right. Here we got we were able to see an intermediate state where thread one had only executed partially. And so what came out over here in fro show you know just a partial update of the variables a and b. So one had been written but not the other. And if we didnt want to do that if we thought this was wrong we would have to use synchronization in order to control that. So lets take a look then at using synchronization to try to prevent this from happening. And Ill tell you right upfront that this piece of code or this attempt is incorrect and actually it doesnt solve the problem at all. But it also illustrates probably the most common thread programming error that Java programmers make. And lots of people including professional programmers make this mistake and lots of production Java programs have this particular mistake in them. So its a very instructive example I think. So lets take a look here. Lets assume we have the the two threads again. The thread is going to call two and the thread is going to call fro. And lets say that in our heap there is only one object simple and lets just call it s. So this is globally in the entire heap just one object s of the simple class. Alright? So what is lets say that thread one is going to go first alright and the first that its going to do because its synchronized method is its going to lock the this parameter of the call since theres only one simple only one object of the simple class that has to be the object s so its going to lock s. Alright now well prevent any other thread from acquiring the lock on s while while thread one holds that lock. So then thread one can go ahead and execute the statement a = three. And now though we could have interruption and thread two can get to run. And notice here that thread two doesnt check the lock. It goes ahead execute this code over here. In the f ro method but thats not synchronized there is no synchronize keyword there. And so just the fact that somebody else holds the lock on a simple object doesnt prevent another method from accessing the fields or the data of that object if that other method doesnt itself check the lock. So if the other method is not synchronized it will just go ahead and and and and execute ignoring the fact that another thread holds the lock on the object. So in this case this will just this can just run to completion. And well print out a = three b = two. Okay? So we only see one of the two updates. And and then the scheduler can come back in. Lets the other thread run and it would run to completion and unlock the object at the end. And you could see here that this particular attempted fix has achieved nothing. Actually all the possible inter leavings of the two methods that were that existed without any synchronization still exist if only one of the two methods is synchronized. And the reason this error is common is because frequently people think well I you know if reads are okay I can always read from things in parallel and that wont cause any problems because Im not altering any data. Its my writes that have to be synchronized so if Im going to write to fields of objects well that needs to be coordinated with other methods because writes are dangerous but reads somehow dont interfere. And the point here is that if having only one method or only having one half of the accesses to the of two accesses to shared data be synchronized doesnt help because synchronization only works if everybody is checking the lock. So both the reader and the writer need to check the lock in order to restrict the set of possible interleavings of these two methods. So what would be a correct way to do it? Well is just to put the synchronized keyword on both methods. And now. Its not possible to have the interleaving we saw before its not just only two possible outputs. One there are only two possible strings that could be p rinted. One is that a = one and b = two. So in this case the fro method executes before the two method so thats fro before two okay? I mean all of fro before all of the two method. And the other possibility is a entirety before the fro method. And those become the only two possible inter-leavings when both methods here are synchronized. Im going to conclude this video by making a couple of other comments on Javas threads. So one property we would like is that even if there is no synchronization a variable should only hold values that were actually written by some threads. So what do I mean by that? Lets say that we have two assignments. This is in thread one where were assigning a the value of 3.14 then in thread two were assigning a the value order what do we expect? Well we expect that a ends up being equal either to 3.14 or 2.78 alright? Now what we dont want is for a to wind up being some other value okay? I mean what if a turned out to be 3.78 for example okay? This would be bad we dont want this right? Because this value 3.78 was never written by either thread. Okay this value was somehow manufactured. And Ive chose 3.78 to kind of indicate what could potentially go wrong. If we somehow wound up with a mix of the bits or the the pieces of the number from thread one and thread two. If they were re-combined in some strange way then we could create a number that was assigned to a that didnt exist in either thread. Okay it was never actually written in either thread. Now Java does guarantee that the rights of values are atomic. Meaning that if I write to a value if I sign a primitive type to something that is going to happen atomically and wont be interfered with by another assignment to the same memory location except for floating point doubles. So this does not hold writes to doubles or not necessarily atomic. Now why would that be? Well a double is a floating point number but it consumes twice the memory. Thats why its called a double it consumes two words. Okay and what that means is that if a here is a double so lets assume that a is a double. That means that this write of 3.14 actually translates into two machine instructions. We have to write the high part of a equals something and then the lower part of a. So the two machine instructions to write both of the words that represent a after writing the high half and the low half. Okay because there isnt a a primitive double word write on most machines. And the same thing would happen in thread two. This would get broken up into two assignments to the two halves of a. And then just from what we discussed before you can see that these could interleave in some way and you might wind up with the unfortunate situation that the high half of the representation of a is written by thread one and the low half is written by thread two and then you can get a number like this like you know something not exactly 3.78 but some mix of the bits from the write in thread one and the write at thread two and you would create whats called an out of thin air value. Okay and clearly out of thin air values are bad. Okay you do not want those. And and Java guarantees again that the rights of almost all the primitive data types are going to be atomic so you cant get out of thin air values. But for performance reasons this is not the case for doubles. All right. So so so for fro as a what it says in the manual is that as a concession the current hardware they do not require that rights to doubles be atomic unless you the programmer go and mark the type as volatile. So you have to declare doubles to be volatile and if you do that then theyre guaranteed to be atomic writes. Okay so if you were writing Java programs using Java threads and you were programming threads that read and write doubles concurrently then you need to be careful to declare those double types volatil e at least currently and this may change in the future and Im sure theyd like to change it. But currently you have to declare the doubles volatile to guarantee that the writes will be atomic. More generally or actually somewhat separately this is actually a separate point here Java concurrency semantics are actually kind of hard to understand in detail. And this this issue around out of thin air values is is one aspect of this. There are several other aspects of it. And and and this is really not Javas fault. It turns out the concurrency semantics are hard and actually this is kind of at the frontier of research. We dont really understand exactly what we want or what the right thing is to do to specify the behavior of languages in concurrent settings. And thats not to say that we dont understand anything. We do have some languages in perfectly concurrency semantics but in a language flow in language feature is Java there are a number of things that are not completely clear how they should be implemented on certain machines. So this has been a huge amount of work done on this problems specifically for Java and actually java was the first mainstream language to have first class threads in it and then try to integrate that with all other language features that all other modern languages features that we like. It was so surprising actually that we have run into some trouble understanding how they are supposed to work in all situations. So this is one area of Java that I would say still under debate. And while for them while I figure out straight forward things with threads everything will work fine. If you are doing there are some areas of the language where if you try to use them with threads you can get into a little bit of trouble. Alright so it surely pays to try to understand. Java concurrency and the threads if youre writing significant concurrency Java programs In this video were going to wrap up our discussion of Java by taking a look at a couple of additional topics and how they are integrated into the language design. Consistent with Javas dynamic nature Java allows classes to be loaded at runtime. But this means that you can actually add functionality to an executing Java program so while its running by loading a new class. And this creates potential issues with type safety and security because now there is a distinction between compiled time and load time. So type checking of the source takes place at compile time and this is the kind of type checking we discussed in earlier in earlier videos. But the the loader when you actually go to load a class youre loading bytecode youre not loading source and its not being type checked again. And it could be that this bytecode didnt come you know from a trusted source. This bytecode might not be the output of a compiler that did type checking before it produced the bytecode. So the bytecode might not actually satisfy the type assumptions of the Java implementation. So essentially we have to check the bytecode again. And a and a procedure called bytecode verification takes place when the class is loaded alright. And and byte code verification is really a type checking of bytecode. Thats thats essentially what it does. The procedure is a little bit different because we dont have you know the code here is much lower level and so the algorithms look a little bit different. But what theyre really doing is type checking the thebytecode. So now the loading policies are handled by something called the class loader. And the class loader is a special class in Java and it decides what classes can be loaded and actually early on in Java a bunch of security problems were discovered. Aware an attacker could get control of the class loader install its own class loader that would be much more permissive than the Java standard class loader and subve rt the system. But those issues were fixed quite awhile ago alright. And another interesting thing about Java is that the classes may also be unloaded. So you dont you can not not only load classes you can also unload classes. And the last time I checked this was not particularly well specified in the definition and so its a little bit unclear exactly what it meant when you unloaded the class and what happened to all the existing objects for example of that class. Now Id like to spend a few minutes talking about initialization in Java which is quite complex and this shouldnt be too much of a surprise because if you remember initialization in COOL was also pretty complex and Java is just a superset of COOL so it has all the initialization issues that COOL has plus much more. And now the main source of complication is concurrency but other language features also add to the complexity of initialization in Java. And in fact you could do worse. If you want to understand a new object oriented language then study how it does object initialization and class initialization. Because essentially what happens in initialization is that all the features of the language are going to be interacting and you have to explain what all those interactions are and how they are sorted out in order to have a well-defined initialization procedure alright. So now lets talk about class initialization. We wont talk about object initialization well just talk about initializing classes. So this is how the the object that represents a class actually gets initialized when that class is first brought into the program. And so the first thing to know is that a class is initialized when a symbol in a class is first used okay not when the class is loaded alright? So if you reference any symbol in the class at the first time that happens that will cause the class to be initialized. And the reason for doing this is if you are going to have an error in class initialization th is will cause that error to happen in a predictable place. So if you have an error and you run the you you have an error in class initialization if you run the program five times you know that error will probably happen in the same place every time. So itll be repeatable and predictable where the error occurs. If instead we had the error happened where you loaded the class at the time that you loaded the class well the class might be loaded at lots of different times. And and and so this this this error here the error in the initialization of the class would become non-deterministic if we didnt if we didnt delay the initialization until some deterministic point in the execution. So now Ill discuss the procedure for initializing class objects in Java. And the first thing I should stress is that this idea of a class object is something that Java has that COOL does not have I mentioned this on the previous slide. But just to be completely clear what is a class object? A class object is just what it sounds like it is the object for a class. It represents a class. Okay this is not an instance of the class. This is an object which is the class okay. So this is an object which is the class it has all the information about the class so you know it tells you what the type of the class is what the fields of the class are and everything else. So this is used for introspection or reflection. And its necessary in Java because of features like dynamic loading. So if you know if you want if you dynamically load a class though you want to be able to use that class you have to have some way of querying what the what kinds of methods and things the class has and that is what the class object is for. So there is one object there is one class object for each class in Java alright. So when you load a class the first thing you have do is to initialize the class object. And how is that done? Well we lock the class object for the class alright. And if th at if that object is already locked by another thread then well simply wait on the lock okay. So we will wait until somebody tells us that its okay to proceed. Now once we obtain the lock on the class we have to do a check to see if the class is already being initialized alright. So and it could turn out that it is our thread it is the same thread is already initializing the class. And how could that happen? Well remember that a class can have fields of the same type. So I could have a class of class called X and then it could have a field of type X in it. And the way classes are going to be initialize if were going to have to initialize the class itself and then and were going to do that by recursively initializing the classes for all the fields or at least making sure of the classes for all the fields are initialize. And if we have a recursive structure here with the same class mention in a field as in a name as the name of the enclosing class then we will get the situation where the thread initializing the class may attempt to initialize the same class again. So if we discover that were already initializing this class we simply release the lock and we turn. Now another possibility is that the class is already initialized. So if when we finally get the lock we discover that some other thread got in there and initialized the class before we have a chance to. well then theres nothing to do and we just return normally alright? Now if neither one of these things is true okay if we get the lock and we discover that the class is not already initialized and that were not already in the process of initializing the class then we will mark the class to to note the initialization is in progress by this thread okay. So well indicate you know this class is being initialized and that we are initializing it and then well unlock the class. Alright the next thing that happens is well have to initialize the superclass and that will m ean initial and then well initialize all the fields in textual order. But because Java has what are called static and final fields we will initialize those first okay. So static final fields will get initialized before any other fields in textual order. And of course we have to give every field of default value before initialization just as in COOL. So this step step five is very similar to what goes on in COOL. Now if theres an error during the initialization so some part of the initialization throws an exception then were going to mark the class as erroneous okay were going to mark this class as no good and cant be used and and thats the best we can do. So if theres an exception during initialization we just have to give up on that class. And so it gets a special mark on it saying that its erroneous. And if there are no errors if we succeed in initializing the class and with and without any errors then were going to lock the class again. We will label the class as initialized alright? And then well notify the threads that are waiting on the class object. So. Anybody who was locked waiting on the class object will now be alerted that the object is is ready and then well unlock the class. Okay and so thats a rough outline of how class initialization in Java works. I skipped over a few things and oversimplified it a bit. So this isnt the complete description but these are the main points and they and they illustrate how the various features of the language have to interact. So you have to worry about concurrency you have to worry about exceptions you have to worry about static and final fields you have to worry about inheritance. I mean all these things have to be dealt with together in the design of a single algorithm to do class initialization. Stepping back for a moment this discussion of class initialization in Java illustrates a general point about designing complex systems. So in any system with a certain number of featu res and every system is going to have some number of features lets call it N because you want to provide some functionality obviously the thing the systems suppose to d so its going to have features to do those things. But as you add features you get lots of interactions potential interactions between the features and if we think about even just the pairwise interactions. If I have N features then Ill have I dont know about N^2 pairwise feature interactions. And the point there of course is that as I add features the number of possible interactions grows super linearly in the number of features I mean it grows much more quickly than the number of features. And so adding the next feature youre going to have to consider all of the previous features that you already have in the system and how this new feature affects them and this is why it becomes very difficult to extend or build systems that have a lot of features alright. And this is just the pairwise features. These are just this is just considering pairwise interactions between one feature and another. If I have to start worrying about subsets of features Im thinking about how all possible subsets of features might interact with each other well then this step this number of of potential interactions will grow not just it will grow in fact exponentially. So itd be you know way more than quadratic. And the bottom line here is that big feature-full systems are hard to understand. You know this is you know a general lesson in Computer Science and any kind of discipline that wants to design complex systems and and this lesson applies to programming languages. It applies to every other kind of software system that you might want to build. But and somehow it has a particular force in programming languages because these interactions between the features you know these are the features of the programming language they happen at a very fine grain. And these things can be really can b e composed arbitrarily and so you really do have to work out in language design you know what all the interactions are in order to have a language that people that programmers can actually understand and use productively. Alright? And that really I think is the big big idea that one of the big ideas that weve talked about throughout the course. And and I hope one of the things that you would take away from this lecture at least in particular. So to summarize and to conclude our discussion of Java I think Java is a is a well-done language. By production standards it is extremely well-done. So its one of the best designed and best specified languages thats in in use today. It brought several important ideas into the main stream. So when it was new it brought ideas that had been around for a long time but had not found their way into a production language that was very very widely used and in particular Java was the first language to be very widely used in in commercial settings. They had strong sets of typing there that had real guarantees they were you know provided by the type-system and also there was a manage language and had a garbage collected memory. But that doesnt mean its perfect. And it and Java also includes some features that at the time that it was designed that we didnt fully understand and I would say you know that this are probably the rough areas where theres still some roughness in the in in the Java design. So things like the way the memory semantics work in the presence of concurrency you know probably still has most people would agree I think you now has some problems and some some little gray areas that as a program you probably want to stay out of. And the other thing is that Java has a lot of features. And as I said before when you have a lot of features youre going to have even more feature interactions and that leads to complexity that becomes difficult to manage.