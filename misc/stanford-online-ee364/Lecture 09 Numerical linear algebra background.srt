1
00:00:00,650 --> 00:00:05,820
The second section of course of course was
on a applications in

2
00:00:05,820 --> 00:00:11,530
geometry in approximations statistics
areas like that.

3
00:00:11,530 --> 00:00:15,270
And this last section in the course is on.

4
00:00:15,270 --> 00:00:18,230
Numerics and algorithms how do you
actually solve these problems?

5
00:00:18,230 --> 00:00:22,210
Now, you have been solving in these
problems more well for

6
00:00:22,210 --> 00:00:25,410
the last two parts of the course you have
been solving these problems.

7
00:00:25,410 --> 00:00:28,200
But presumably at this point you don't
know how it's done and

8
00:00:28,200 --> 00:00:34,190
the main goal of this section, the whole
section of the course is to demystify.

9
00:00:34,190 --> 00:00:37,150
How it is that we solve these convex
problems, right.

10
00:00:37,150 --> 00:00:38,680
So that's that's the idea.

11
00:00:38,680 --> 00:00:43,190
Now, most of you are not going to write
your own solvers.

12
00:00:43,190 --> 00:00:45,650
Right.
Actually, some of you might have to,

13
00:00:45,650 --> 00:00:50,120
if you solve, if you go into areas where
the problems are absolutely gigantic,

14
00:00:50,120 --> 00:00:52,770
you know, something like CVX is not
going to work,

15
00:00:52,770 --> 00:00:54,800
you're going to have to develop your own
solver.

16
00:00:56,490 --> 00:00:59,450
Or if you go into like real time systems
where you need to be able to

17
00:00:59,450 --> 00:01:02,080
solve optimization problems in micro
seconds.

18
00:01:02,080 --> 00:01:04,950
Again something like cvx is not going to
work for you and

19
00:01:04,950 --> 00:01:07,860
you're going to really have to known how
its done.

20
00:01:10,050 --> 00:01:12,710
I should say a couple more things about
that.

21
00:01:12,710 --> 00:01:16,060
I still believe strongly, its not just a
question of demystifying it but

22
00:01:16,060 --> 00:01:20,400
I believe its very important for everyone
to know a little bit

23
00:01:20,400 --> 00:01:24,460
About how these problems are solved and
specifically what you want to

24
00:01:24,460 --> 00:01:28,460
know is the link between problem structure
and how fast we can solve it.

25
00:01:28,460 --> 00:01:30,500
So that's really, if there was one take
away for

26
00:01:30,500 --> 00:01:35,450
the entire last part of this course, it's
to understand that.

27
00:01:35,450 --> 00:01:36,150
That one thing.

28
00:01:36,150 --> 00:01:38,570
You should be able to look at that problem
and say "Ooh!

29
00:01:38,570 --> 00:01:42,310
That's got a hundred thousand variables,
but I can solve that super fast, because I

30
00:01:42,310 --> 00:01:48,120
recognize the structure." So that's what
this whole section of the course is about.

31
00:01:48,120 --> 00:01:52,500
Now we are going to go back today and
start, well, at the very beginning.

32
00:01:52,500 --> 00:01:56,600
We are going to talk a little about
numerical algebra.

33
00:01:56,600 --> 00:02:01,120
Some of you probably know this, others of
you probably don't.

34
00:02:01,120 --> 00:02:03,040
People who have used things like...

35
00:02:03,040 --> 00:02:06,030
Matlab have certainly been using this as a
user,

36
00:02:06,030 --> 00:02:10,630
as a consumer but you may not know
actually how it's done.

37
00:02:10,630 --> 00:02:14,440
And what we'll do today is just look at
the basics and

38
00:02:14,440 --> 00:02:17,900
I should also say how it fits into the big
picture.

39
00:02:17,900 --> 00:02:20,210
So the big picture is, this lecture,

40
00:02:20,210 --> 00:02:25,130
we're going to talk about How do you solve
a linear equation.

41
00:02:25,130 --> 00:02:26,220
So that's fine.

42
00:02:26,220 --> 00:02:30,060
And there's a finite algorithm for that
and we'll look at how that's done, and

43
00:02:30,060 --> 00:02:31,870
we'll we'll see we'll see how it's done.

44
00:02:31,870 --> 00:02:33,110
It's gaussian elimination.

45
00:02:33,110 --> 00:02:35,188
You've been doing this since high school I
assume.

46
00:02:35,188 --> 00:02:40,290
Okay then I'll tell you how this fits into
the rest of the course.

47
00:02:40,290 --> 00:02:42,480
Right?
So in the rest of the course we'll see how

48
00:02:42,480 --> 00:02:45,940
do you minimize, we'll start by talking
how do you minimize a smooth function.

49
00:02:47,580 --> 00:02:48,900
well, with start with how do you,

50
00:02:48,900 --> 00:02:53,180
we'll start with how do you minimize a
quadratic function.

51
00:02:53,180 --> 00:02:55,770
Well, you set the gradient equal to 0.

52
00:02:55,770 --> 00:02:58,050
But the grading of a quadratic fuction is
a linear function, and

53
00:02:58,050 --> 00:03:02,170
so, minimizing a quadratic function,
solving linear equations.

54
00:03:02,170 --> 00:03:07,060
So if you like, you can think of this
whole stuff we're going to look at now,

55
00:03:07,060 --> 00:03:12,920
how do you solve linear equations, as how
do you minimize a quadratic funtion.

56
00:03:12,920 --> 00:03:14,820
So, that's the optimization connection.

57
00:03:16,270 --> 00:03:24,330
Okay so, we will first start by talking
matrix structure and algorithm complexity.

58
00:03:24,330 --> 00:03:28,300
Matrix structure is, you know, somethings
are obvious like what is, you know,

59
00:03:28,300 --> 00:03:33,180
diagonal matrices, identity matrices
sparse matrices.

60
00:03:33,180 --> 00:03:34,060
We'll talk about that.

61
00:03:35,690 --> 00:03:40,440
Then we'll talk about the dominant method
used to solve linear equations.

62
00:03:40,440 --> 00:03:42,000
And that's using factored matrices.

63
00:03:42,000 --> 00:03:43,510
So you'll factor a matrix and

64
00:03:43,510 --> 00:03:47,580
then solve, do a sequence of solves, with
the factored, with the factors.

65
00:03:48,610 --> 00:03:51,210
We'll talk about some very famous
factorizations, the so called LU,

66
00:03:51,210 --> 00:03:54,210
that's going to be for lower triangular,
upper triangular,

67
00:03:54,210 --> 00:03:57,970
the Cholesky factorization and something
called the LDL transpose

68
00:03:57,970 --> 00:04:02,990
factorization where L is lower triangular,
D is diagonal and L is upper triangular.

69
00:04:02,990 --> 00:04:05,820
Actually we will see in some cases D is
block diagram.

70
00:04:07,630 --> 00:04:10,960
Then, we'll look at a general construction
called block elimination in the matice

71
00:04:10,960 --> 00:04:15,450
inversion lemma and this is something that
shows you how it

72
00:04:15,450 --> 00:04:20,820
is that you can solve sometimes absolutely
enormous sets of linear equations.

73
00:04:20,820 --> 00:04:24,630
Very efficeintly by explointing some
structure that's present.

74
00:04:24,630 --> 00:04:25,640
And the last thing we'll talk about,

75
00:04:25,640 --> 00:04:28,312
though very briefly is solving
undetermined equations.

76
00:04:31,770 --> 00:04:34,720
So first let's just start with solving Ax
equals b.

77
00:04:34,720 --> 00:04:37,180
Where A is n by n.

78
00:04:37,180 --> 00:04:38,450
Okay?
And in fact let's say on

79
00:04:38,450 --> 00:04:42,330
that laptop right there, that's not mine,
but I know the, I know the number.

80
00:04:43,630 --> 00:04:45,180
I wa, I want to ask you all,

81
00:04:45,180 --> 00:04:49,150
how long do you think it takes to solve
the equation Ax equals b?

82
00:04:49,150 --> 00:04:52,610
Where x is size 1,000.

83
00:04:52,610 --> 00:04:55,414
Before you say anything, I want to point a
few things out.

84
00:04:55,414 --> 00:05:01,020
That's solving 1,000 equations, for 1,000
variables.

85
00:05:01,020 --> 00:05:04,740
The size of A, is a million coefficients.

86
00:05:04,740 --> 00:05:05,310
Everybody got it?

87
00:05:06,410 --> 00:05:07,290
Okay?
And now what I

88
00:05:07,290 --> 00:05:10,070
want to know is how fast can I solve it?

89
00:05:10,070 --> 00:05:11,800
The answer is 30 milliseconds.

90
00:05:11,800 --> 00:05:14,170
Okay, and I happen to know the number on
that thing is about ten.

91
00:05:15,610 --> 00:05:22,850
So actually, this is an amazing thing, and
it's actually worthwhile for

92
00:05:22,850 --> 00:05:26,950
all of us to sit and think how
unbelievably amazing that is, right.

93
00:05:28,640 --> 00:05:30,370
I mean that is ridiculous.

94
00:05:30,370 --> 00:05:31,090
Right?
That's something that

95
00:05:31,090 --> 00:05:35,320
takes like a billion, we'll see it takes,
generic methods take a billion operations.

96
00:05:35,320 --> 00:05:38,200
It's ten, fifteen miliseconds on that.

97
00:05:38,200 --> 00:05:39,610
Okay?
It's got multiple cores,

98
00:05:39,610 --> 00:05:42,160
does all sort of things are happening to
make that number.

99
00:05:42,160 --> 00:05:47,200
But just the idea actually that it's less
than a second is unbelievably impressive.

100
00:05:47,200 --> 00:05:48,180
Okay?

101
00:05:48,180 --> 00:05:51,370
So well we'll get to that.

102
00:05:51,370 --> 00:05:53,810
So it, so it basically for general methods
right?

103
00:05:53,810 --> 00:05:57,430
The general methods means you, you're
exploiting nothing in the matrix a,

104
00:05:57,430 --> 00:05:59,090
absolutely none, right?

105
00:05:59,090 --> 00:06:03,910
All the entries are assumed to be non-zero
there's no particular structure.

106
00:06:03,910 --> 00:06:07,940
The structure would be something like a
might be A discreet foray transform or

107
00:06:07,940 --> 00:06:09,830
a discreet cosine transform.

108
00:06:09,830 --> 00:06:12,250
In which case, of course, you can solve it
faster than n cubed.

109
00:06:12,250 --> 00:06:14,060
You can solve it in like n log n.

110
00:06:14,060 --> 00:06:15,740
Okay so that would be structure.

111
00:06:15,740 --> 00:06:17,740
Other structure would be sparsity.

112
00:06:17,740 --> 00:06:21,660
Right, that would be that A has a lot of
zeroes.

113
00:06:21,660 --> 00:06:25,100
Right, one definition of sparsity goes
something like this: it's.

114
00:06:25,100 --> 00:06:29,330
A matrix that has enough zeros that it's
worth your while to take advantage of it.

115
00:06:29,330 --> 00:06:32,780
That's 1967; I mean it's a perfectly good
definition, right?

116
00:06:32,780 --> 00:06:35,720
So, okay.

117
00:06:35,720 --> 00:06:39,870
So general methods, it says that these
things grow, like N cubed.

118
00:06:39,870 --> 00:06:43,994
oh, so for fun, how long would it take
that laptop there to solve?

119
00:06:43,994 --> 00:06:47,882
Say a hundred, hundred equations,ah and

120
00:06:47,882 --> 00:06:53,110
a hundred variables that's 10 thousand
coefficients in the matrix.

121
00:06:54,170 --> 00:06:55,186
So the answer was what 30?

122
00:06:55,186 --> 00:06:59,170
>> Micro seconds
>> Micros seconds, thank you, okay.

123
00:06:59,170 --> 00:07:00,720
So I mean that's ridiculous,

124
00:07:00,720 --> 00:07:06,150
by the way this also hints Why you should
know about all those things, right.

125
00:07:06,150 --> 00:07:10,310
These numbers are so ridiculous that it
means these things can now be embedded on

126
00:07:10,310 --> 00:07:14,860
all, it means the amount of computation
you can do can now, is unbelievable.

127
00:07:14,860 --> 00:07:18,700
You can put stuff, you can actually embed,
all of these things can be embedded.

128
00:07:18,700 --> 00:07:21,495
They can, and things that would normally
take like seconds to run.

129
00:07:21,495 --> 00:07:23,810
>> Can run in milliseconds.

130
00:07:23,810 --> 00:07:25,000
They can be embedded in things.

131
00:07:25,000 --> 00:07:26,650
I mean, crazy stuff can happen.

132
00:07:26,650 --> 00:07:27,690
So, OK.

133
00:07:30,030 --> 00:07:31,640
All right.
So, basically for

134
00:07:31,640 --> 00:07:33,520
general, these are the generic methods.

135
00:07:33,520 --> 00:07:34,640
This grows as N cubed.

136
00:07:34,640 --> 00:07:37,880
And I'll, I mean I'll tell you what the
generic methods are, shortly.

137
00:07:37,880 --> 00:07:39,560
what, what they are.

138
00:07:39,560 --> 00:07:42,190
And this is, this is the part that I
think.

139
00:07:42,190 --> 00:07:44,660
I want everybody to know.

140
00:07:44,660 --> 00:07:49,100
When A is structured, and we'll talk about
all sorts of structure, it's way faster.

141
00:07:49,100 --> 00:07:53,270
Or generally, substantially faster than n
cubed.

142
00:07:53,270 --> 00:07:55,530
Okay, and structured is stuff like banded.

143
00:07:55,530 --> 00:08:00,900
A banded matrix is 1 Where, basically, a i
j is zero,

144
00:08:00,900 --> 00:08:02,830
if the difference between i and j is big
enough.

145
00:08:02,830 --> 00:08:04,080
Bigger than some thresh hold.

146
00:08:04,080 --> 00:08:09,910
Right, systems like that can actually be
solved in linear time, okay.

147
00:08:09,910 --> 00:08:15,500
So, I mean these are just things you did,
you wouldn't I mean unless you knew this,

148
00:08:15,500 --> 00:08:20,600
it's not obvious right, so what that say
is I can solve a million variables with

149
00:08:20,600 --> 00:08:24,620
a million equations on that laptop,
actually I can do that on my phone but

150
00:08:24,620 --> 00:08:29,510
on that,that laptop I solve a system of a
million equations,a million variables.

151
00:08:29,510 --> 00:08:32,260
If, if it's banded with a bandwidth of
like five.

152
00:08:32,260 --> 00:08:32,990
Right?

153
00:08:32,990 --> 00:08:35,720
And not only that, it can solve, I mean,
unbelievably fast.

154
00:08:35,720 --> 00:08:38,260
I won't do the calculation now but it'll
be shocking.

155
00:08:38,260 --> 00:08:38,910
OK?

156
00:08:38,910 --> 00:08:42,200
And if you didn't know that, you'd think
well that's,

157
00:08:42,200 --> 00:08:43,610
that's a big hard problem or something.

158
00:08:43,610 --> 00:08:44,550
You multiply it by n cubed.

159
00:08:44,550 --> 00:08:45,195
Okay.

160
00:08:46,660 --> 00:08:48,020
so, what are the basic ideas?

161
00:08:48,020 --> 00:08:50,210
Well this goes back to the 60s.

162
00:08:50,210 --> 00:08:51,850
It's still sort of relevant.

163
00:08:51,850 --> 00:08:53,580
It's less relevant that it was.

164
00:08:53,580 --> 00:08:54,820
I'll say a little bit about that.

165
00:08:54,820 --> 00:08:57,030
But, it's still not a bad indicator.

166
00:08:58,060 --> 00:09:03,170
so, basic concept is a FLOP, that's
supposed to be a Floating Point Operation.

167
00:09:03,170 --> 00:09:05,800
And roughly, it's something like an
addition, a subtraction,

168
00:09:05,800 --> 00:09:09,970
a multiplication, division of 2 floating
point numbers.

169
00:09:09,970 --> 00:09:12,010
Now, there's some older definitions where.

170
00:09:12,010 --> 00:09:14,260
It's an addition plus a multiply and

171
00:09:14,260 --> 00:09:16,320
you might include things like square roots
in there.

172
00:09:16,320 --> 00:09:20,110
All of that is nonsense because square
roots cost like 50 times more.

173
00:09:20,110 --> 00:09:23,770
A division is something like, cost you 20
times more than an add.

174
00:09:23,770 --> 00:09:25,890
So for all of these things, the,

175
00:09:25,890 --> 00:09:30,000
the key idea now is just to not to take
the numbers too seriously, right?

176
00:09:30,000 --> 00:09:33,360
They're to be considered to be within a
factor of ten.

177
00:09:33,360 --> 00:09:35,220
Right?
And that, that, that's the right number.

178
00:09:36,550 --> 00:09:40,390
these, the flop was invented sort of when,
you know,

179
00:09:40,390 --> 00:09:44,060
main CPUs didn't do floating point
arithmetic, you know.

180
00:09:44,060 --> 00:09:47,520
Then basically had to like send a message
over to some giant board that

181
00:09:47,520 --> 00:09:49,010
was like your floating point board.

182
00:09:49,010 --> 00:09:50,260
And all sorts of stuff happen.

183
00:09:50,260 --> 00:09:52,340
And then when that multiplying was
finished.

184
00:09:52,340 --> 00:09:53,910
Right?
Something would come back.

185
00:09:53,910 --> 00:09:54,710
Everybody, you know, and

186
00:09:54,710 --> 00:09:59,210
so in that setting, sure, counting the
number of flops made total sense.

187
00:09:59,210 --> 00:10:02,290
It's ridiculous now right because, you
know it, with the number of,

188
00:10:02,290 --> 00:10:04,710
I mean there's all sorts of things you
could do there not even getting into like

189
00:10:04,710 --> 00:10:09,060
minor details but you can do crazy stuff
all in parallel and stuff like that.

190
00:10:09,060 --> 00:10:09,560
So, okay.

191
00:10:10,675 --> 00:10:11,740
>> All right.

192
00:10:11,740 --> 00:10:13,070
So, the, the, the idea, and

193
00:10:13,070 --> 00:10:16,870
this traces to the 60s and is still
actually somewhat relevant.

194
00:10:16,870 --> 00:10:18,160
It goes something like this.

195
00:10:18,160 --> 00:10:19,660
To estimate the complexity of an [UNKNOWN]
what you do,

196
00:10:19,660 --> 00:10:20,870
is you'd express the number FLOPs.

197
00:10:23,000 --> 00:10:24,880
It usually, it's a polynomial.

198
00:10:24,880 --> 00:10:27,730
It's a polynomial function of the problem
dimensions.

199
00:10:27,730 --> 00:10:30,070
And then what you do is you drop.

200
00:10:30,070 --> 00:10:31,730
All but the leading terms, right?

201
00:10:31,730 --> 00:10:36,790
So, you know, if it's something like, you
know, 1/2 n cubed plus 2 n

202
00:10:36,790 --> 00:10:40,020
squared plus blah, blah, blah - something
like that - you just say, well, it's one.

203
00:10:40,020 --> 00:10:42,280
The complexity's 1/3 n cubed.

204
00:10:42,280 --> 00:10:45,620
And honestly, these days, the 1/3 doesn't
make any difference at all.

205
00:10:45,620 --> 00:10:49,070
Because you're not shooting for - you
know, factor of 2 is ridiculous.

206
00:10:49,070 --> 00:10:51,740
You could not predict using these methods,
right?

207
00:10:51,740 --> 00:10:54,180
Factor of 10 is the right, the right
amount.

208
00:10:54,180 --> 00:10:57,250
Now you know this is not an accurate
predictor of

209
00:10:57,250 --> 00:10:59,360
computation on modern computers.

210
00:10:59,360 --> 00:11:02,360
Right.
It, you can get it within a factor of ten

211
00:11:02,360 --> 00:11:06,280
if, if thing you know in the absence of
certain bad things and

212
00:11:06,280 --> 00:11:10,520
actually if things are done really well
right like it's optimized code it once

213
00:11:10,520 --> 00:11:11,970
again becomes a reasonable predictor.

214
00:11:11,970 --> 00:11:13,290
So it's kind of an interesting thing.

215
00:11:14,520 --> 00:11:18,540
but, you know it's still, it's, it's very
useful as a rough estimate of complexity.

216
00:11:18,540 --> 00:11:21,230
It's still, it's still gives you that,
right?

217
00:11:21,230 --> 00:11:24,570
but, you know, do not come along and say
oh, you know, this one is

218
00:11:24,570 --> 00:11:27,580
half the number of flops therefore, it
must run faster or something.

219
00:11:27,580 --> 00:11:30,910
I mean things like your, your access
pattern and, and,

220
00:11:30,910 --> 00:11:35,270
and Locality of reference and memory are
way more important than factors of to and

221
00:11:35,270 --> 00:11:37,360
flop counts, and things like that, so,
okay.

222
00:11:37,360 --> 00:11:40,570
I mean another nice thing is that a lot of
these things are done for

223
00:11:40,570 --> 00:11:44,080
us by people who know what they're doing,
namely those who write like compilage and

224
00:11:44,080 --> 00:11:46,130
things like that, so that, that's the good
news.

225
00:11:46,130 --> 00:11:47,180
Okay, so.

226
00:11:49,000 --> 00:11:53,130
Let's look at some, operations and this is
a conceptual division.

227
00:11:53,130 --> 00:11:55,730
It's a vector-vector, matrix-vector,
matrix-matrix product.

228
00:11:55,730 --> 00:11:59,200
And I should say this is something you
should probably all know about.

229
00:11:59,200 --> 00:12:00,850
There's no reason not to.

230
00:12:00,850 --> 00:12:05,270
There's something called BLAS and it
traces to the 60s, 70s.

231
00:12:05,270 --> 00:12:09,350
It's called it, it stands for Basic Linear
Algebra Subroutines.

232
00:12:09,350 --> 00:12:10,660
Right?
And believe it or

233
00:12:10,660 --> 00:12:14,380
not, the, the standards of these are
written in I, well four tran.

234
00:12:14,380 --> 00:12:15,460
That gives you a rough idea.

235
00:12:15,460 --> 00:12:16,300
OK?

236
00:12:16,300 --> 00:12:17,590
And by the way still.

237
00:12:17,590 --> 00:12:20,040
So, and then they have of course,

238
00:12:20,040 --> 00:12:22,650
these are re implimented in all sorts of
real languages actually.

239
00:12:22,650 --> 00:12:26,700
So anyway this is basic linear and they
call these things,

240
00:12:26,700 --> 00:12:32,470
this is called blass- Level one, this is
blast two, and that's blast three.

241
00:12:32,470 --> 00:12:33,320
Right.

242
00:12:33,320 --> 00:12:35,810
You don't need to know that, but actually
that's not true.

243
00:12:35,810 --> 00:12:37,360
An educated person just knows these
things.

244
00:12:37,360 --> 00:12:39,250
You don't have to know the details of
these things.

245
00:12:39,250 --> 00:12:41,520
But you should know but there's blast
level one, blast level two, and

246
00:12:41,520 --> 00:12:42,140
blast level three.

247
00:12:42,140 --> 00:12:43,520
And I'll say a little bit about what,

248
00:12:43,520 --> 00:12:46,196
what they mean and what why it's important
to know about.

249
00:12:46,196 --> 00:12:50,510
Okay.
So vector-vector operations this would be

250
00:12:50,510 --> 00:12:52,740
something like, you know, calculating an
inner product, right?

251
00:12:52,740 --> 00:12:55,390
You calculate an inner product, yeah, you
can count the number of flops.

252
00:12:55,390 --> 00:12:59,010
It's like 2n minus 1 flops, 2n if n is
large, right?

253
00:12:59,010 --> 00:13:01,350
So that's something like that.

254
00:13:01,350 --> 00:13:04,704
So you'd say it's order 2n or something
like that.

255
00:13:06,490 --> 00:13:07,950
Another one would be like a sum.

256
00:13:07,950 --> 00:13:10,340
Something, something like that.

257
00:13:10,340 --> 00:13:12,940
By the way if you're taking an inner
product it, it what,

258
00:13:12,940 --> 00:13:15,540
would also depend if, if the vectors are
sparse.

259
00:13:15,540 --> 00:13:18,660
Then, needless to say this could be way
faster.

260
00:13:18,660 --> 00:13:21,920
And it actually depends, I mean, quite a
bit on You know,

261
00:13:21,920 --> 00:13:25,350
the data structure used to describe sparse
vectors and things like that.

262
00:13:25,350 --> 00:13:26,080
So.
Okay.

263
00:13:27,330 --> 00:13:29,150
The other thing let's just say about an
inner product,

264
00:13:29,150 --> 00:13:31,750
I mean just to give a hint, sort of, about
modern stuff,

265
00:13:31,750 --> 00:13:37,260
is something like an inner product, is, is
easily paralyzable.

266
00:13:37,260 --> 00:13:43,490
Right so so on a modern on on some modern
platform it'd be divided up

267
00:13:43,490 --> 00:13:48,050
if you do an inter plot of 2 million long
vectors they have multiple threads and

268
00:13:48,050 --> 00:13:50,180
each one would be taking inner product in
chunks and

269
00:13:50,180 --> 00:13:53,250
then those would be added together in
things like that so.

270
00:13:53,250 --> 00:13:54,010
That's.

271
00:13:54,010 --> 00:13:56,430
I'm just saying.
That's by the way how you get the 20

272
00:13:56,430 --> 00:14:02,010
millisecond, 30 millisecond numbers to
solve a thousand equations on that laptop.

273
00:14:02,010 --> 00:14:05,190
Okay?
So, but it's not bad.

274
00:14:05,190 --> 00:14:10,240
If you want to think of something serially
stepping through like,

275
00:14:10,240 --> 00:14:11,870
please get me the next x i.

276
00:14:11,870 --> 00:14:13,370
Please get me the next y i.

277
00:14:13,370 --> 00:14:15,000
Please multiply them together.

278
00:14:15,000 --> 00:14:17,710
And then, plus equals that on some, some,
you know,

279
00:14:17,710 --> 00:14:20,210
you're inner product plus equals x i y i.

280
00:14:20,210 --> 00:14:21,590
If you want to think of that.

281
00:14:21,590 --> 00:14:22,860
Go ahead, it doesn't hurt.

282
00:14:22,860 --> 00:14:24,400
It's charming, it's simple.

283
00:14:24,400 --> 00:14:27,560
But, you should understand that's not how
it works.

284
00:14:27,560 --> 00:14:29,950
I mean, unless you write the code right
like that, right?

285
00:14:29,950 --> 00:14:33,200
In which, in which case it'll run,
whatever, 10 times slower, like so.

286
00:14:33,200 --> 00:14:33,880
Okay.
All right.

287
00:14:35,390 --> 00:14:37,340
Next one is matrix vector product.

288
00:14:37,340 --> 00:14:39,380
That's y equals a x.

289
00:14:39,380 --> 00:14:43,850
So here you multiply a matrix by a vector
and what is it?

290
00:14:43,850 --> 00:14:45,670
Well, you know there's many ways to think
of it.

291
00:14:45,670 --> 00:14:47,870
But, one way if you're just counting
FLOPs, it doesn't matter,

292
00:14:47,870 --> 00:14:52,050
is that it's basically, you take the inner
product of each row of A with x.

293
00:14:52,050 --> 00:14:52,640
Okay?

294
00:14:52,640 --> 00:14:57,490
So, it's basically calculating M inner
products in our N.

295
00:14:57,490 --> 00:15:00,180
And that's something like 2 MM.

296
00:15:00,180 --> 00:15:01,680
Right?
And you forget the 2.

297
00:15:01,680 --> 00:15:03,220
This is like, that's a joke.

298
00:15:03,220 --> 00:15:05,600
Right?
It's order M, N.

299
00:15:05,600 --> 00:15:06,950
Right.
So, it's the number,

300
00:15:06,950 --> 00:15:08,480
it's vaguely number entries in A.

301
00:15:09,670 --> 00:15:14,200
Now if A is sparse, this is obviously a
whole lot less.

302
00:15:14,200 --> 00:15:14,720
Right?

303
00:15:14,720 --> 00:15:19,510
And but actually, [INAUDIBLE] how fast it
is, is quite a complicated topic and

304
00:15:19,510 --> 00:15:23,190
depends on exactly what data structure's
used to represent the sparse matrix and

305
00:15:23,190 --> 00:15:24,280
all sorts of stuff like that.

306
00:15:24,280 --> 00:15:27,500
Right?
But Again, it's not easy at least at a,

307
00:15:27,500 --> 00:15:30,240
without looking, getting into the details
to realize that,

308
00:15:30,240 --> 00:15:34,160
if a matrix is a million by a million,
with maybe 10 million non-zero entries,

309
00:15:34,160 --> 00:15:36,860
you only have to multiply it by the 10
million non-zero entries, right?

310
00:15:36,860 --> 00:15:38,680
So, okay.

311
00:15:40,200 --> 00:15:41,830
And it can be less, it's a structure.

312
00:15:43,660 --> 00:15:46,200
so, for example, here's one.

313
00:15:46,200 --> 00:15:48,970
If I gave you the matrix A.

314
00:15:48,970 --> 00:15:53,080
As a, a low rank product, right?

315
00:15:53,080 --> 00:15:55,110
So if I gave you the matrix A.

316
00:15:55,110 --> 00:15:57,140
Let's suppose a is million by million.

317
00:15:57,140 --> 00:16:00,220
But I give it to you as a million by ten.

318
00:16:00,220 --> 00:16:02,900
Multiply it by ten by million, right?

319
00:16:02,900 --> 00:16:06,090
So it's what you would get if you ran pca,
or sbd, or

320
00:16:06,090 --> 00:16:07,735
something like the low rank approximation.

321
00:16:07,735 --> 00:16:09,530
>> Number 1, you'd actually be able to
store it.

322
00:16:09,530 --> 00:16:11,420
You can't store a million by million
matrix, right?

323
00:16:11,420 --> 00:16:13,460
So, but you could store it this way.

324
00:16:13,460 --> 00:16:17,090
And the multiply would certainly not be a
million squared.

325
00:16:17,090 --> 00:16:18,640
It would be way less.

326
00:16:18,640 --> 00:16:20,370
It's order of a million.

327
00:16:20,370 --> 00:16:23,470
In fact, it's like 20 million times that
because, 10 by 10.

328
00:16:23,470 --> 00:16:24,620
Everybody got this?

329
00:16:24,620 --> 00:16:30,180
Right.
So, okay any of BLAS level three.

330
00:16:30,180 --> 00:16:31,610
These are matrix matrix products.

331
00:16:31,610 --> 00:16:33,420
So, that means multiply two matrices.

332
00:16:33,420 --> 00:16:36,660
I mean that's the canonical one, is
multiply two matrices.

333
00:16:36,660 --> 00:16:40,710
And here, it's actually the product of the
dimensions, basically, right?

334
00:16:40,710 --> 00:16:43,780
If these are like m by n, m by p.

335
00:16:43,780 --> 00:16:45,390
It'd be, it's mnp.

336
00:16:45,390 --> 00:16:48,020
So it's the product of the, of the
dimensions.

337
00:16:48,020 --> 00:16:48,900
Okay.

338
00:16:48,900 --> 00:16:51,750
That are sparse, it's much less and

339
00:16:51,750 --> 00:16:56,530
you get different things if if they're
symmetric, you can divide things by two,

340
00:16:56,530 --> 00:16:58,900
I mean, that typically doesn't matter but
that's, that's the idea.

341
00:17:00,110 --> 00:17:02,030
And now, this is probably where I, I mean,

342
00:17:02,030 --> 00:17:05,330
again, we, these are now things We're not
going into these details, but

343
00:17:05,330 --> 00:17:08,670
I'll just say a little bit about it so
that you appreciate it, right.

344
00:17:08,670 --> 00:17:11,310
So, everybody here knows how to multiply
two matrices, right.

345
00:17:11,310 --> 00:17:14,270
I mean it is, you could write a five-line
C program that would do this,

346
00:17:14,270 --> 00:17:15,650
every here could, right.

347
00:17:15,650 --> 00:17:16,440
It can be truly trivial.

348
00:17:16,440 --> 00:17:19,000
It could be like how do you get C i k?

349
00:17:19,000 --> 00:17:21,070
Why, you, you walk along A and

350
00:17:21,070 --> 00:17:25,700
you walk along B and multiply the, the
associated entries.

351
00:17:25,700 --> 00:17:26,660
And then add it all up.

352
00:17:26,660 --> 00:17:28,070
right?
So, you could write a five line C program.

353
00:17:28,070 --> 00:17:29,190
Right?
It'd be a bunch of fours and

354
00:17:29,190 --> 00:17:31,030
wouldn't even be five lines, probably.

355
00:17:31,030 --> 00:17:36,000
It'd be 5 lines with a lot of commenting
but, okay and you can compile it.

356
00:17:36,000 --> 00:17:38,435
And, of course, it would do, like you know
whatever.

357
00:17:38,435 --> 00:17:40,390
Two MNP FLOPs by the time it runs.

358
00:17:41,530 --> 00:17:42,110
In fact.

359
00:17:44,370 --> 00:17:48,440
If you were to do, if you were to use
LAPACK, that,

360
00:17:48,440 --> 00:17:52,150
that's the, again, this is just for
cultural background.

361
00:17:52,150 --> 00:17:54,590
You do not need to know this although it's
not bad to know it, okay?

362
00:17:54,590 --> 00:17:59,800
So, LAPACK is just, this is an open source
package of linear algebra routines.

363
00:18:00,930 --> 00:18:03,010
and, you know, multiplying two matrices.

364
00:18:03,010 --> 00:18:06,390
This I mean doesn't seem like there be any
if you don't think about this doesn't seem

365
00:18:06,390 --> 00:18:14,750
to be any subtlety there right we all know
what CIK is, its sum over J, AIJ be JK

366
00:18:14,750 --> 00:18:20,140
right I mean it don't look like a whole
lot of room for optimizing there right.

367
00:18:20,140 --> 00:18:24,010
I can give you something a little more
complicated like sorting or

368
00:18:24,010 --> 00:18:27,100
something like that or finding the
shortest path But

369
00:18:27,100 --> 00:18:30,290
then you can imagine somebody smart could
do something better.

370
00:18:30,290 --> 00:18:31,990
Everybody, everybody see what I'm saying
here?

371
00:18:31,990 --> 00:18:34,210
I mean, come on, calculating the product
of two matrices?

372
00:18:35,860 --> 00:18:40,370
You know, one person's implementation, you
can beat a naive one by a factor of 100?

373
00:18:40,370 --> 00:18:42,460
That doesn't seem right.

374
00:18:42,460 --> 00:18:45,410
Does anyone know, by the way, how that's
done, and why?

375
00:18:46,530 --> 00:18:47,790
Its actually kind of interesting.

376
00:18:47,790 --> 00:18:51,040
I mean, this is just for cultural
background, but its good to know.

377
00:18:51,040 --> 00:18:53,640
Here's how you multiply two thousand by
thousand matrices.

378
00:18:53,640 --> 00:18:56,280
Here's how you do it if you use this
package.

379
00:18:56,280 --> 00:19:01,640
What happens is, thousand by thousand
matrices are broken down into things like

380
00:19:01,640 --> 00:19:04,700
ten by ten block matrices with blocks that
are hundred by hundred.

381
00:19:04,700 --> 00:19:06,090
Everybody following this?

382
00:19:06,090 --> 00:19:08,640
Now, matrix multiplication you can do
block wise.

383
00:19:08,640 --> 00:19:09,410
Right.

384
00:19:09,410 --> 00:19:14,330
So, what that says is you end up
multiplying 200 by 100 matrices

385
00:19:14,330 --> 00:19:19,430
by blocking them and then each block you
do 100 by 100 multiply.

386
00:19:19,430 --> 00:19:20,210
Okay.
Now the 100 by,

387
00:19:20,210 --> 00:19:21,760
how do you do 100 by 100 multiply?

388
00:19:21,760 --> 00:19:23,990
It's blocked down to 10.

389
00:19:23,990 --> 00:19:27,700
So each of those is a ten by ten matrix of
ten by ten matrices.

390
00:19:27,700 --> 00:19:28,370
Everybody got this?

391
00:19:28,370 --> 00:19:31,690
And then so what's really happening is
you're doing giant piles of

392
00:19:31,690 --> 00:19:33,330
ten by ten multiplicates, and

393
00:19:33,330 --> 00:19:37,300
by the way the parameters of ten I made up
are totally made up and it would differ.

394
00:19:37,300 --> 00:19:38,790
Depends on your architecture.

395
00:19:38,790 --> 00:19:39,390
Alright?

396
00:19:39,390 --> 00:19:41,950
Okay, so that's called blocking.

397
00:19:41,950 --> 00:19:46,930
And the idea behind blocking is you can
arrange it if you do it just right.

398
00:19:46,930 --> 00:19:49,940
If the block's in the right sizes like
that ten by ten multiplied might hap,

399
00:19:49,940 --> 00:19:53,950
might happen in, you know, the ten by ten
multiplied might happen with registers or

400
00:19:53,950 --> 00:19:54,600
something, right.

401
00:19:54,600 --> 00:19:58,800
And then the hundred by hundred might fit
in L, might just fit in L1, right?

402
00:19:58,800 --> 00:19:59,610
And stuff like that.

403
00:19:59,610 --> 00:20:02,825
So if you do this right, if you do your
blocking right.

404
00:20:02,825 --> 00:20:05,460
>> Then, you, and not only that.

405
00:20:05,460 --> 00:20:07,780
You could do part of it in parallel and
stuff like that.

406
00:20:07,780 --> 00:20:10,320
If you have multiple cores, you know,
somebody's multiplying these.

407
00:20:10,320 --> 00:20:11,470
Somebody's multiplying these.

408
00:20:11,470 --> 00:20:13,150
You exploit all this stuff.

409
00:20:13,150 --> 00:20:16,720
And that's actually how matrices get
multiplied.

410
00:20:16,720 --> 00:20:18,930
OK?
On, at, at least on a,

411
00:20:18,930 --> 00:20:20,840
you know, on sort of a modern processor.

412
00:20:20,840 --> 00:20:21,550
Right?

413
00:20:21,550 --> 00:20:23,230
So, that, that, that's the idea.

414
00:20:23,230 --> 00:20:24,400
And, and if you wa, and

415
00:20:24,400 --> 00:20:26,540
the memory access pattern is such that
there are never [UNKNOWN].

416
00:20:26,540 --> 00:20:29,820
And by the way, how do they, how do they
choose the block sizes?

417
00:20:29,820 --> 00:20:32,100
That's they have something called atlas,
right?

418
00:20:32,100 --> 00:20:33,360
That's [LAUGH] really kind of cool.

419
00:20:33,360 --> 00:20:35,070
I mean, you don't need to know any of
these things, right?

420
00:20:35,070 --> 00:20:36,050
But it's sort of fun.

421
00:20:36,050 --> 00:20:38,800
That's automatically tuned linear algebra
software.

422
00:20:40,190 --> 00:20:41,530
This all open source stuff.

423
00:20:41,530 --> 00:20:45,300
And what it does, it takes your computer
and it just said, I try 10, right?

424
00:20:45,300 --> 00:20:46,530
And it does it, and

425
00:20:46,530 --> 00:20:51,120
it factors matrices for a while and it's
say I'll try blocking it as 9 by 9.

426
00:20:51,120 --> 00:20:55,440
It tries 9, it tries 11, it tries 12, and
it just does bisection or

427
00:20:55,440 --> 00:20:58,070
just tries a bunch of them and then half
an hour later.

428
00:20:58,070 --> 00:21:02,500
It's decided on your architecture what the
right cash,

429
00:21:02,500 --> 00:21:04,580
what the right blocking sizes are.

430
00:21:04,580 --> 00:21:07,540
Okay, completely imperdible, everybody got
this?

431
00:21:07,540 --> 00:21:09,770
So, and when you see these curves their
absolutely amazing.

432
00:21:09,770 --> 00:21:12,340
It would be things like, you would see
something going like,

433
00:21:12,340 --> 00:21:14,100
here's, here's a function of n.

434
00:21:14,100 --> 00:21:15,510
And say it would be, it would be looking
like that.

435
00:21:15,510 --> 00:21:16,370
That's a log, log plot.

436
00:21:16,370 --> 00:21:18,240
Then all of a sudden you would see this.

437
00:21:18,240 --> 00:21:20,960
[LAUGH] Right, and, and you'd say, and by
the way, what would this be?

438
00:21:24,060 --> 00:21:29,310
Yeah, so this is something that's just
barely fitting in some cache, right?

439
00:21:29,310 --> 00:21:30,950
And this is something that, that.

440
00:21:30,950 --> 00:21:33,420
This is called bad blocking.

441
00:21:33,420 --> 00:21:38,610
This is, this is where your block sizes
were exactly You know a handful of

442
00:21:38,610 --> 00:21:40,970
things to big to fit in some cache.

443
00:21:40,970 --> 00:21:44,260
So basically it's a disaster, something
like that.

444
00:21:44,260 --> 00:21:46,750
In fact this can even go down that's the
cool part.

445
00:21:46,750 --> 00:21:49,330
This can be nonmonotonic but any way okay.

446
00:21:49,330 --> 00:21:53,880
So that's more than you need to know about
all of this but it is good to know.

447
00:21:53,880 --> 00:21:56,970
Actually it is good to know even if you're
interests are quite theoritical or

448
00:21:56,970 --> 00:21:57,770
mathematical.

449
00:21:57,770 --> 00:22:00,670
This is good to know beacuse if you do
theorical or

450
00:22:00,670 --> 00:22:05,320
very mathematical things You know, here's,
this is how it hits the road.

451
00:22:05,320 --> 00:22:08,700
It hits the road right here, in, in LA
Pack in most cases.

452
00:22:08,700 --> 00:22:11,620
Well, except in giant sparse things, but
we'll talk about that.

453
00:22:11,620 --> 00:22:14,070
But a lot of times, you do some cool
statistics,

454
00:22:14,070 --> 00:22:15,870
you do any you do [UNKNOWN], it's this.

455
00:22:18,490 --> 00:22:21,580
Now we're going to talk about solving
linear equations,

456
00:22:22,590 --> 00:22:25,220
and The one way to do it.

457
00:22:25,220 --> 00:22:27,130
We'll first talk about classes.

458
00:22:28,130 --> 00:22:30,880
classes, of, of, matrices.

459
00:22:30,880 --> 00:22:32,880
Where solving Ax equals b is easy.

460
00:22:32,880 --> 00:22:36,390
And then there's a simple trick that puts
it all together.

461
00:22:36,390 --> 00:22:37,900
So, one would be this.

462
00:22:39,010 --> 00:22:39,890
Diagonal matrices.

463
00:22:39,890 --> 00:22:44,510
Right, so if A is diagonal then solving a
equals b.

464
00:22:44,510 --> 00:22:45,460
Is simple.

465
00:22:46,590 --> 00:22:49,270
oh, by the way, one interesting thing is
people will,

466
00:22:49,270 --> 00:22:55,320
when people write A inverse b,
mathematically it's what you mean.

467
00:22:55,320 --> 00:22:57,190
That's what x is.

468
00:22:57,190 --> 00:23:00,830
But it is universally understood, even if
people write it

469
00:23:00,830 --> 00:23:04,970
in an algorithm statement that it doesn't,
that you do not parse it this way.

470
00:23:04,970 --> 00:23:10,235
A inverse times b because that would
suggest code like, you know,

471
00:23:10,235 --> 00:23:14,350
a.in, you know, times b and eh, eh,

472
00:23:14,350 --> 00:23:17,925
except in extremely rare cases you would
never compute it that way.

473
00:23:17,925 --> 00:23:20,320
But,.so, a inverse be is sort of like a,

474
00:23:20,320 --> 00:23:24,350
a phrase that when someone describes an
algorithm you should see.

475
00:23:24,350 --> 00:23:26,560
And it doesn't mean form inverse and

476
00:23:26,560 --> 00:23:28,870
multiply by b Mathematically it's the
same.

477
00:23:28,870 --> 00:23:30,480
The semantics is identical.

478
00:23:30,480 --> 00:23:35,070
But you should understand that this is so
common so, and people will even say things

479
00:23:35,070 --> 00:23:37,780
like inverting that matrix is easy and
they don't mean inverting the matrix.

480
00:23:37,780 --> 00:23:39,520
They mean solving the equation.

481
00:23:39,520 --> 00:23:41,190
Okay, so this is just, so, okay.

482
00:23:42,660 --> 00:23:45,720
So here, you know, computing a inverse b.

483
00:23:45,720 --> 00:23:47,160
If, if A is diagonal.

484
00:23:47,160 --> 00:23:48,410
It's incredibly easy.

485
00:23:48,410 --> 00:23:49,310
It's just you divide.

486
00:23:49,310 --> 00:23:50,520
It's N FLOPs.

487
00:23:50,520 --> 00:23:51,050
You know, roughly.

488
00:23:51,050 --> 00:23:52,360
Whatever it is.

489
00:23:52,360 --> 00:23:53,290
OK?

490
00:23:53,290 --> 00:23:55,680
The next one is interesting.

491
00:23:55,680 --> 00:23:58,370
Right?
It says that a matrix is lower triangular.

492
00:23:58,370 --> 00:23:58,920
Right?

493
00:23:58,920 --> 00:24:01,330
And so, that says, you know it looks like
this.

494
00:24:01,330 --> 00:24:05,730
A11, A21, A22, you know, something like
that.

495
00:24:05,730 --> 00:24:06,380
Okay?

496
00:24:06,380 --> 00:24:11,460
And you're multiplying this by, you know
x1 up to xn and that's supposed to be,

497
00:24:11,460 --> 00:24:13,820
you know b1 up to bn.

498
00:24:13,820 --> 00:24:16,720
Okay?
And yeah, how do you, how do you do this?

499
00:24:16,720 --> 00:24:17,650
You look at the first row.

500
00:24:17,650 --> 00:24:24,100
The first row says a1, 1, x1 equals b1, so
x1 is, is obviously a b1 array 1, 1.

501
00:24:24,100 --> 00:24:24,650
Right?

502
00:24:24,650 --> 00:24:27,630
Oh, and by the way, what exception do you
throw if a1, 1 is zero?

503
00:24:27,630 --> 00:24:30,740
What if we have a divide by 0 exception?

504
00:24:32,770 --> 00:24:34,550
Yeah.
You throw a message back which

505
00:24:34,550 --> 00:24:36,780
is like A singular.

506
00:24:36,780 --> 00:24:39,450
So, so I'm not going to calculate a A
inverse b for you.

507
00:24:39,450 --> 00:24:40,252
Right, so okay.

508
00:24:40,252 --> 00:24:45,795
So once you know x1, right?

509
00:24:45,795 --> 00:24:47,730
And there's lots of ways to think of this.

510
00:24:47,730 --> 00:24:53,860
Once you know x1, the second row says A21
x1 plus A22 x2 equals b two right?

511
00:24:53,860 --> 00:24:55,320
But you know x one so

512
00:24:55,320 --> 00:25:00,130
you just plug in that value subtract it on
the right and then divide it by A22.

513
00:25:00,130 --> 00:25:03,460
So the algorithim looks osmething like
this an dyou

514
00:25:03,460 --> 00:25:05,300
can sort of calculate the number of flops.

515
00:25:05,300 --> 00:25:07,705
I mean it doesn't matter but you can
kind of get a rough idea what it is.

516
00:25:07,705 --> 00:25:09,570
>> This thing is kind of growing.

517
00:25:10,790 --> 00:25:13,140
the, oh this, and it's got a, a great
name.

518
00:25:13,140 --> 00:25:14,810
This is forward substitution.

519
00:25:14,810 --> 00:25:19,160
And the reason it's called forward
substitution is you calculate XI.

520
00:25:19,160 --> 00:25:25,070
And then XI, from then on, you substitute
that value into the equation.

521
00:25:25,070 --> 00:25:27,750
Right?
And so, you can see, you do kind of like.

522
00:25:27,750 --> 00:25:31,150
Length 1, length 2, length 3, length 4,
and it's growing.

523
00:25:31,150 --> 00:25:31,920
And you know that the sum of

524
00:25:31,920 --> 00:25:34,550
those numbers is something like the square
over 2, right?

525
00:25:34,550 --> 00:25:37,890
And if you do it exactly right, it's just
like n squared, right?

526
00:25:37,890 --> 00:25:39,480
You count blocks, that doesn't matter.

527
00:25:39,480 --> 00:25:40,820
It's n squared blocks.

528
00:25:40,820 --> 00:25:42,390
Okay, and that's called forward
substitution.

529
00:25:42,390 --> 00:25:46,150
And there's a, a similar one, if a matrix
is upper-triangular.

530
00:25:46,150 --> 00:25:48,490
And what do you imagine it would be, you
know, the same thing.

531
00:25:48,490 --> 00:25:50,440
It's like called, it's backwards
substitution.

532
00:25:50,440 --> 00:25:52,850
Right?
You start by getting XN first and

533
00:25:52,850 --> 00:25:53,770
going backwards.

534
00:25:53,770 --> 00:25:55,020
Okay?
So these are, and these are,

535
00:25:55,020 --> 00:25:55,890
just completely simple.

536
00:25:55,890 --> 00:25:57,030
There's nothing complicated here.

537
00:25:59,180 --> 00:26:03,110
This is really interesting when A is
sparse.

538
00:26:03,110 --> 00:26:06,509
Because when you have a lower triangular
sparse matrix.

539
00:26:07,730 --> 00:26:08,250
I don't know.

540
00:26:08,250 --> 00:26:10,610
Go ahead and give me a rough number.

541
00:26:10,610 --> 00:26:11,790
So for a sparse matrix.

542
00:26:11,790 --> 00:26:15,420
1, and a very important attribute is
number of non zeros,

543
00:26:15,420 --> 00:26:18,210
sometimes written NNZ, okay?

544
00:26:18,210 --> 00:26:23,590
So I just roughly, if I have, in terms of
the number of non zeroes.

545
00:26:23,590 --> 00:26:26,430
How many FLOPS does it take to do forward
or

546
00:26:26,430 --> 00:26:30,800
backward substitution with a lower
triangular, upper triangular matrix?

547
00:26:30,800 --> 00:26:32,990
That's sparse, in terms of the number of
[INAUDIBLE] zeros.

548
00:26:34,130 --> 00:26:35,320
What is it?

549
00:26:35,320 --> 00:26:36,470
Yeah, it's winning.

550
00:26:36,470 --> 00:26:39,550
In fact it equals, kind of, to the n and
z.

551
00:26:39,550 --> 00:26:40,590
Everybody got that?

552
00:26:40,590 --> 00:26:43,710
Because, basically each of these
coefficiants appears once.

553
00:26:43,710 --> 00:26:44,260
Right?

554
00:26:44,260 --> 00:26:46,600
And so, it's about linear.

555
00:26:46,600 --> 00:26:47,470
Right?
So, that means if I

556
00:26:47,470 --> 00:26:50,280
have a million by million matrice Right?

557
00:26:50,280 --> 00:26:52,670
With let's say 50 million non zeros,

558
00:26:52,670 --> 00:26:55,810
that's you know on average of 50 per non
zeros per row or column.

559
00:26:55,810 --> 00:26:58,140
That's 50 million, 50 million flops.

560
00:26:58,140 --> 00:26:59,870
How fast can I do it on there?

561
00:26:59,870 --> 00:27:01,440
This rough order magnitude.

562
00:27:05,650 --> 00:27:09,460
You can assume that's between 1 and 10
gig, gig gigaflops something like that.

563
00:27:09,460 --> 00:27:11,170
So the answer is milliseconds.

564
00:27:31,730 --> 00:27:33,530
Another one would be orthogonal.

565
00:27:33,530 --> 00:27:37,550
So if a matrix is orthogonal then in fact
solving,

566
00:27:37,550 --> 00:27:41,800
well computing a inverse b is the same as
well a transpose b.

567
00:27:41,800 --> 00:27:44,000
Right?
So its just a matrix vector multiplied,

568
00:27:44,000 --> 00:27:46,310
its ordered n squared in general but

569
00:27:46,310 --> 00:27:49,280
it turns out it actually depends how the
orthogonal matrix is stored.

570
00:27:49,280 --> 00:27:51,130
There are other data structures.

571
00:27:51,130 --> 00:27:52,740
Here's a very famous one.

572
00:27:52,740 --> 00:27:53,510
And it looks like this.

573
00:27:53,510 --> 00:27:55,560
That's, that's called, that's a
reflection.

574
00:27:55,560 --> 00:27:56,130
Right?

575
00:27:56,130 --> 00:27:58,070
That's, that's an orthoganol matrix.

576
00:27:58,070 --> 00:28:03,580
And you can see it is an identity minus a
rank one matrix.

577
00:28:03,580 --> 00:28:04,650
Right?
We're going to

578
00:28:04,650 --> 00:28:07,220
talk a lot about that later today.

579
00:28:07,220 --> 00:28:11,750
But so there that can be done in order n
flops.

580
00:28:11,750 --> 00:28:13,100
Right, so very fast.

581
00:28:13,100 --> 00:28:17,120
Right.
So, so for example, one, I mean, it's not

582
00:28:17,120 --> 00:28:22,090
completely standard, but it, you would
occur often is one data structure for

583
00:28:22,090 --> 00:28:27,000
representing an orthogonal matrix is as a
product these things,

584
00:28:27,000 --> 00:28:28,720
these reflections here.

585
00:28:28,720 --> 00:28:32,400
And you would actually give them, you
would give the u , u1, u2, u3,

586
00:28:32,400 --> 00:28:36,640
u4 and you can see the multiple, multiple
actually would be the same to multiply,

587
00:28:36,640 --> 00:28:40,000
multiply by the transpose which is the
inverse,

588
00:28:40,000 --> 00:28:45,010
each of those would simply be linear in
that in that data structure,

589
00:28:45,010 --> 00:28:50,270
right and if I now want this permutation
matrix so permutation matrix.

590
00:28:50,270 --> 00:28:51,580
Well, it's kind of silly.

591
00:28:51,580 --> 00:28:52,150
Right?

592
00:28:52,150 --> 00:28:59,220
If I solve, you know, P x equals b, all I
do is permute the interest, right?

593
00:28:59,220 --> 00:29:01,490
And actually by this 1960's version of
things.

594
00:29:01,490 --> 00:29:06,260
When you say, how many flops is that, the
answer is zero.

595
00:29:06,260 --> 00:29:09,200
There's no floating point in just doing,
you're copying.

596
00:29:09,200 --> 00:29:14,380
You're copying I, I guess you, all you're
doing is you're copying, you know,

597
00:29:14,380 --> 00:29:17,820
whatever it is eight bytes or, how many
bytes is a, a double?

598
00:29:17,820 --> 00:29:18,580
Eight?
So

599
00:29:18,580 --> 00:29:22,720
you're copying [UNKNOWN] eight bytes from
here to there and around like that.

600
00:29:22,720 --> 00:29:26,540
So, and oh, by the way, if that's a
hundred million long vector, and

601
00:29:26,540 --> 00:29:30,240
their access pattern is completely random
or whatever This could, you could

602
00:29:30,240 --> 00:29:33,995
end up with, this could be not, it could
be 0 flops and actually take some time.

603
00:29:33,995 --> 00:29:35,506
Okay?

604
00:29:35,506 --> 00:29:40,520
[COUGH] Alright okay, so alright and

605
00:29:40,520 --> 00:29:44,740
now we get to, how do you solve Ax equals
b?

606
00:29:44,740 --> 00:29:46,140
and, it's a general method.

607
00:29:46,140 --> 00:29:50,140
It's, it's, it's, it's a high level, It's,
it's, it's factor solved.

608
00:29:50,140 --> 00:29:52,440
And there are some very important things
that actually to know about it.

609
00:29:52,440 --> 00:29:54,620
And some, if you hadn't seen these stuff
before,

610
00:29:54,620 --> 00:29:58,290
you'll learn something shortly and it'll
be quite real I mean,

611
00:29:58,290 --> 00:30:01,420
it'll be something you should know, and
it's not expected.

612
00:30:01,420 --> 00:30:03,290
So the idea is this, there's two steps,
there's factor and

613
00:30:03,290 --> 00:30:08,540
there's solve And factor says you take
your coefficent matrix and

614
00:30:08,540 --> 00:30:11,140
you factor it as a product of simple
matrices.

615
00:30:11,140 --> 00:30:13,040
By the way, you've seen tons of things
like that.

616
00:30:13,040 --> 00:30:16,180
Like SVD, eigon decomposition, eigen
vector,

617
00:30:16,180 --> 00:30:18,630
you know, you've seen lots of these
things.

618
00:30:18,630 --> 00:30:20,640
Right?
QR factorization.

619
00:30:20,640 --> 00:30:24,480
Alright, so what you do is you factor it
as a product of simple matrices.

620
00:30:24,480 --> 00:30:25,650
Sometimes it's two,

621
00:30:25,650 --> 00:30:29,970
it just it could be as few as two and it
could be as many as four or five, right.

622
00:30:29,970 --> 00:30:33,760
Actually sometimes it can be a variable
number where some of those things have

623
00:30:33,760 --> 00:30:37,530
exotic data structures, like it could be
these products of reflections, right.

624
00:30:37,530 --> 00:30:38,240
OK.

625
00:30:38,240 --> 00:30:44,070
So well, one example is you can actually
understand the fft this way.

626
00:30:44,070 --> 00:30:44,980
I don't knwo if people know.

627
00:30:44,980 --> 00:30:48,130
If you know what the fft is, great and
I'll say what it is.

628
00:30:49,260 --> 00:30:50,720
It's a specific matrix.

629
00:30:50,720 --> 00:30:54,690
It's the dfd matrix, the discreet
[UNKNOWN] transform.

630
00:30:54,690 --> 00:30:57,190
It's a Vandermon matrix with a bunch of
complex numbeers in it.

631
00:30:57,190 --> 00:30:58,390
Do you know what I'm talking about?

632
00:30:58,390 --> 00:30:59,400
Fine.

633
00:30:59,400 --> 00:31:01,810
It basically calculates a discreet
[UNKNOWN] transform.

634
00:31:01,810 --> 00:31:07,320
And there's an interesting thing where you
take an order n, matrix like that, and

635
00:31:07,320 --> 00:31:10,830
you can factor it into a product of log
and

636
00:31:10,830 --> 00:31:13,810
matrices, which have very specific
sparsity patterns.

637
00:31:13,810 --> 00:31:18,560
Okay, and it turns out, if you do this and
then exploit some of the structure,

638
00:31:18,560 --> 00:31:20,480
you get an N log N factorization.

639
00:31:20,480 --> 00:31:22,970
Okay, so, alright, let, let's move on.

640
00:31:22,970 --> 00:31:26,610
So, these are things like, you know,
diagonal, upper, lower, triangular, and

641
00:31:26,610 --> 00:31:27,450
so on.

642
00:31:27,450 --> 00:31:29,405
And then I mean this is totally obvious.

643
00:31:29,405 --> 00:31:31,490
>> What is A inverse B?

644
00:31:31,490 --> 00:31:34,140
Well, I mean, the inverse of a product is
just the product of the inverse.

645
00:31:34,140 --> 00:31:35,480
It's in the other order, like this.

646
00:31:35,480 --> 00:31:36,860
And then you parse it.

647
00:31:36,860 --> 00:31:38,490
This is extremely important this way,

648
00:31:38,490 --> 00:31:41,210
remembering that you're never computing
any of these inverse.

649
00:31:41,210 --> 00:31:42,080
Right?

650
00:31:42,080 --> 00:31:44,530
You're, you parse it this way.

651
00:31:44,530 --> 00:31:45,930
Right?
So, you should actually think of

652
00:31:45,930 --> 00:31:46,870
that as a method.

653
00:31:46,870 --> 00:31:49,290
A1 inverse that you call on B.

654
00:31:49,290 --> 00:31:50,350
so.

655
00:31:50,350 --> 00:31:51,440
That, that's what happened.

656
00:31:51,440 --> 00:31:54,540
So, it, you just un, you just, you unwind,
right?

657
00:31:54,540 --> 00:31:58,160
So you write A as a product, calc,
calculate inverse B,

658
00:31:58,160 --> 00:32:01,530
you call the inverse methods from
backwards, right?

659
00:32:01,530 --> 00:32:02,900
And you just unwind everything.

660
00:32:02,900 --> 00:32:05,010
So it looks like, that's the idea, right?

661
00:32:05,010 --> 00:32:07,680
So, so that's the idea.

662
00:32:08,850 --> 00:32:13,300
Now this immediately has some.

663
00:32:13,300 --> 00:32:15,140
Pretty cool implications.

664
00:32:15,140 --> 00:32:18,270
It says the following, and this, this is
like very important.

665
00:32:18,270 --> 00:32:21,500
It says that suppose you need to sol, had
solve a bunch of

666
00:32:21,500 --> 00:32:25,750
equations with the same coefficient matrix
but with different right hand sides.

667
00:32:25,750 --> 00:32:29,320
Okay, that comes up in tons of places,
right?

668
00:32:29,320 --> 00:32:31,950
I mean Void, tons.

669
00:32:31,950 --> 00:32:33,130
Right?

670
00:32:33,130 --> 00:32:35,350
So suppose that's your problem.

671
00:32:35,350 --> 00:32:36,190
Right?
And what you, by the way,

672
00:32:36,190 --> 00:32:38,050
you'll see tons of cases where this comes
up.

673
00:32:38,050 --> 00:32:38,570
Okay.

674
00:32:38,570 --> 00:32:42,600
Then, what it says is, you factor the
matrix.

675
00:32:42,600 --> 00:32:45,780
That typically costs more we'll talk about
that in a minute, then the,

676
00:32:45,780 --> 00:32:47,370
then the solve.

677
00:32:47,370 --> 00:32:48,830
Right?

678
00:32:48,830 --> 00:32:53,270
So you factor it and you cash the
factorization.

679
00:32:53,270 --> 00:32:58,280
Now what that means is it says that if you
want, if you have, if you

680
00:32:58,280 --> 00:33:04,170
want to do multiple right hand sides you
get one factorization, here, and m solves.

681
00:33:04,170 --> 00:33:05,010
Okay?
So, for

682
00:33:05,010 --> 00:33:08,930
the most common methods by the way,
solving linear equations,

683
00:33:08,930 --> 00:33:12,110
the factor plus n cubed, the factor steps.

684
00:33:12,110 --> 00:33:17,290
And the back solves cost n squared the
solve steps.

685
00:33:17,290 --> 00:33:20,040
Okay, and now I'm going to ask you a
question.

686
00:33:20,040 --> 00:33:25,010
To solve 1000 linear equations with 1000
variables

687
00:33:25,010 --> 00:33:28,660
on that machine is, I'll just make up a
number.

688
00:33:28,660 --> 00:33:30,390
We'll fix it as, I like your number.

689
00:33:30,390 --> 00:33:31,120
30 miliseconds.

690
00:33:31,120 --> 00:33:32,220
It's 30 milliseconds,

691
00:33:32,220 --> 00:33:37,052
and I want to know how much time does it
take to solve two equations.

692
00:33:37,052 --> 00:33:40,180
AX equals B1, AX equals B2, with the same
A.

693
00:33:40,180 --> 00:33:42,756
It's 30 milliseconds to do one.

694
00:33:42,756 --> 00:33:44,320
How many is it to do two?

695
00:33:44,320 --> 00:33:48,276
By the way, the naive method just calls,
solve AB twice.

696
00:33:48,276 --> 00:33:51,210
And the answer is, what did we say, 30
millisecond?

697
00:33:51,210 --> 00:33:51,780
60 milliseconds.

698
00:33:51,780 --> 00:33:52,690
And what is the correct answer?

699
00:33:52,690 --> 00:33:53,400
30.

700
00:33:53,400 --> 00:33:55,860
It's studen, is 30.

701
00:33:55,860 --> 00:33:56,620
OK?

702
00:33:56,620 --> 00:33:58,110
All right?
How many does it.

703
00:33:58,110 --> 00:34:00,440
So, by the way, that's pretty weird.

704
00:34:00,440 --> 00:34:05,960
Everyone got that, that the cost of
solving two linear equations, if you,

705
00:34:05,960 --> 00:34:09,930
if you, if you cash the factorization, is
actually the same as one.

706
00:34:09,930 --> 00:34:13,320
And I don't think this is obvious,
frankly.

707
00:34:13,320 --> 00:34:15,510
It is not obvious unless you knew this,
right?

708
00:34:15,510 --> 00:34:19,341
It, it's an absolutely real phenomenon and
it's, it's the key to tons of things.

709
00:34:19,341 --> 00:34:21,770
Okay?
Because of things being sort of

710
00:34:21,770 --> 00:34:23,690
either viable or not.

711
00:34:23,690 --> 00:34:24,650
Right?

712
00:34:24,650 --> 00:34:31,250
How much would it cost me to solve, let's
say, 25 linear equations.

713
00:34:31,250 --> 00:34:34,950
25 different righthand side same A on that
laptop.

714
00:34:34,950 --> 00:34:37,350
I'm doing 25 times more work.

715
00:34:37,350 --> 00:34:39,720
And time is the same.

716
00:34:39,720 --> 00:34:44,940
The answer is yes because the
factorization dominates everything.

717
00:34:44,940 --> 00:34:47,610
Once you factor you can it simply.

718
00:34:48,640 --> 00:34:54,060
If whatever your background is, you didn't
know it, this can actually have

719
00:34:54,060 --> 00:34:59,780
a huge impact on all sorts of stuff So
let's look at some factorizations.

720
00:34:59,780 --> 00:35:03,180
By the way, I'm not going to cover
factorizations here so

721
00:35:03,180 --> 00:35:08,080
that's an entirely different topic and
they're not that hard actually to,

722
00:35:08,080 --> 00:35:09,050
to work out how these factorizations go.

723
00:35:09,050 --> 00:35:10,810
It's a whole world in itself.

724
00:35:10,810 --> 00:35:12,550
But this just so you know at the highest
level.

725
00:35:12,550 --> 00:35:14,380
We can't go into the details of
factorizations.

726
00:35:14,380 --> 00:35:14,930
So.

727
00:35:14,930 --> 00:35:16,230
Here's the most famous one.

728
00:35:16,230 --> 00:35:17,770
Is LU factorization and

729
00:35:17,770 --> 00:35:20,750
by the way it's completely equivalent to
gaussian elimination.

730
00:35:20,750 --> 00:35:24,790
It says any nonsingular matrix you can
factor as PLU.

731
00:35:24,790 --> 00:35:28,540
Where P is a permutation matrix, L is
lower triangular,

732
00:35:28,540 --> 00:35:31,660
U is upper triangular and obviously L and
U have to have non-zeros on

733
00:35:31,660 --> 00:35:36,298
their diagonals cause if there was zero on
the diagonal of L or U, what would happen?

734
00:35:36,298 --> 00:35:36,886
Yeah.

735
00:35:36,886 --> 00:35:39,820
It's non-invertible.

736
00:35:39,820 --> 00:35:44,590
The determinant is zero and so you have no
right solving Ax equals b, okay?

737
00:35:44,590 --> 00:35:46,520
So, okay.

738
00:35:46,520 --> 00:35:49,230
So, and this factorization, we're not
going to go into it

739
00:35:49,230 --> 00:35:54,600
although it is just Gaussian elimination,
the cost is order n cubed, FLOPs.

740
00:35:54,600 --> 00:35:55,928
Right?
So, that, that's how that works.

741
00:35:55,928 --> 00:35:56,752
Okay?

742
00:35:56,752 --> 00:35:58,310
And so, here it is.

743
00:35:58,310 --> 00:36:01,680
Here's how you solve a set of linear
equations by LU factorizations.

744
00:36:01,680 --> 00:36:03,140
So, you're given A and B.

745
00:36:04,150 --> 00:36:07,170
The first thing you do is you factor A as
PLU.

746
00:36:07,170 --> 00:36:08,620
That's N cubed FLOPs.

747
00:36:10,130 --> 00:36:11,680
You then, then how do you solve?

748
00:36:11,680 --> 00:36:13,160
Well, you go backwards, right?

749
00:36:13,160 --> 00:36:16,550
So, you solve PZ one equals B, that's like
zero flops or

750
00:36:16,550 --> 00:36:18,060
whatever, I mean, whatever you like.

751
00:36:18,060 --> 00:36:20,210
It's, you premuting the entries of B.

752
00:36:20,210 --> 00:36:23,640
Right, then you do forward substitution
back substitution, and

753
00:36:23,640 --> 00:36:26,040
you can see this is like two N squared.

754
00:36:26,040 --> 00:36:26,730
Right.

755
00:36:26,730 --> 00:36:31,000
And so, I mean the numbers don't really
matter here but

756
00:36:31,000 --> 00:36:33,000
the point is n is typically big enough.

757
00:36:33,000 --> 00:36:37,120
N would have to be tiny, before like 10 or
something, then all of this is off anyway.

758
00:36:37,120 --> 00:36:40,960
But n would have to be, so if n is any
reasonable number 10, 100,

759
00:36:40,960 --> 00:36:45,890
1000 or something like that, then the n
cubed thing is bigger, right?

760
00:36:45,890 --> 00:36:46,830
And so that's the idea.

761
00:36:46,830 --> 00:36:49,980
And so it's basically about n cubed,
right?

762
00:36:49,980 --> 00:36:53,350
It's, it's, it's the, it's, it's the
complexity, okay?

763
00:36:53,350 --> 00:36:55,230
So that, that, and that's LU
factorization.

764
00:36:57,210 --> 00:36:59,730
The back solve, notice it's order n
squared.

765
00:36:59,730 --> 00:37:02,410
So, in fact, you would say the following.

766
00:37:02,410 --> 00:37:04,770
the, let's see, how shall I put it.

767
00:37:04,770 --> 00:37:06,400
Okay, here's a good way to conceptualize
it.

768
00:37:07,720 --> 00:37:10,090
One way to think of solving a x equals b
for

769
00:37:10,090 --> 00:37:12,790
different right hand sides, is something
like this.

770
00:37:12,790 --> 00:37:16,500
The first time you solve it, it costs you
n cubed.

771
00:37:16,500 --> 00:37:20,530
Thereafter At, after you ca, you factor A
and

772
00:37:20,530 --> 00:37:23,970
cash it, you get a discount by an or, by a
factor n.

773
00:37:23,970 --> 00:37:24,990
Everybody got that?

774
00:37:24,990 --> 00:37:25,710
Right?
So, so

775
00:37:25,710 --> 00:37:28,630
the first one might take you half a second
and

776
00:37:28,630 --> 00:37:33,120
then the next one is going to take you
submillisecond, right?

777
00:37:33,120 --> 00:37:35,000
I mean, so these are actually very
important things to know about.

778
00:37:35,000 --> 00:37:35,500
Okay.

779
00:37:37,240 --> 00:37:39,880
now, sparse LU factorization.

780
00:37:39,880 --> 00:37:42,050
This is quite interesting.

781
00:37:42,050 --> 00:37:44,260
And I'm not going to get into the details,
but

782
00:37:44,260 --> 00:37:46,770
this is actually very good stuff to know
about.

783
00:37:49,110 --> 00:37:51,640
If any, if you have a sparse matrix A,

784
00:37:51,640 --> 00:37:57,210
the most common factorization method is to
factor it into a product of four matrices.

785
00:37:58,540 --> 00:37:59,850
And you know by the way they have other
names for

786
00:37:59,850 --> 00:38:02,410
these you know p1 and p 2 were sometimes
called you know row and

787
00:38:02,410 --> 00:38:06,110
column permutations or something but that
is what they do.

788
00:38:06,110 --> 00:38:10,690
That is you factor it into a product of
four things a permination LUP okay.

789
00:38:11,940 --> 00:38:14,940
Now the fact is mathematically you only
need

790
00:38:14,940 --> 00:38:19,710
one of those permutations to ensure the
existence of a factorization.

791
00:38:19,710 --> 00:38:21,850
Right.
That's the mathematically because it

792
00:38:21,850 --> 00:38:23,340
follows from the previous one.

793
00:38:23,340 --> 00:38:27,750
Right, the reason you'd use two and

794
00:38:27,750 --> 00:38:30,800
reason you would use a permutation both on
the left and

795
00:38:30,800 --> 00:38:36,360
on the right is that the L that's actually
happening here is this.

796
00:38:36,360 --> 00:38:41,200
As P1 transpose A P2 transpose Right is L
U.

797
00:38:42,600 --> 00:38:46,160
So what you're really doing is permuting
the rows and

798
00:38:46,160 --> 00:38:51,100
columns of A, and then calling an
unpermuted L U factorization on it.

799
00:38:51,100 --> 00:38:51,900
Okay?

800
00:38:51,900 --> 00:38:52,620
Now, by the way,

801
00:38:52,620 --> 00:38:57,220
if you choose P1 and P2 wrong, L U
factorization doesn't exist, right?

802
00:38:57,220 --> 00:38:59,980
So let's assume you've chosen them
correctly and they work.

803
00:38:59,980 --> 00:39:02,630
It turns out that A is sparse.

804
00:39:02,630 --> 00:39:09,100
The sparsity pattern of l and u is,
depends on the permutations.

805
00:39:09,100 --> 00:39:13,460
Okay so if you pick a good permutation l
and u are quite sparse.

806
00:39:13,460 --> 00:39:15,780
And that, of course, is going to affect
your solve time.

807
00:39:15,780 --> 00:39:17,630
Because, your what, your solve time.

808
00:39:17,630 --> 00:39:18,680
We've already looked at that.

809
00:39:18,680 --> 00:39:20,030
Back and forward substitutions.

810
00:39:20,030 --> 00:39:21,350
Basically n and z.

811
00:39:21,350 --> 00:39:23,540
On L, and then Z on U.

812
00:39:23,540 --> 00:39:24,800
Alright.
That's the number of flops.

813
00:39:24,800 --> 00:39:28,240
So the point is, if I pick 1 permutation
P1 and P2,

814
00:39:28,240 --> 00:39:32,570
so that, L has a milion non zeroes and

815
00:39:32,570 --> 00:39:36,510
I pick another one so that L has a hundred
million non zeroes.

816
00:39:36,510 --> 00:39:38,260
Right.
It's not just a memory issue but

817
00:39:38,260 --> 00:39:39,080
also a run time.

818
00:39:39,080 --> 00:39:39,952
Everybody see this?

819
00:39:39,952 --> 00:39:44,370
So okay, this is actually quite
fascinating It turns out that when

820
00:39:44,370 --> 00:39:48,960
you do an LU factorization on a matrix,
right?

821
00:39:48,960 --> 00:39:54,380
Then the permutation can have a huge
effect on the sparsity of the result.

822
00:39:54,380 --> 00:39:55,700
How do you choose P1 and P2?

823
00:39:56,770 --> 00:39:58,110
Well, you can ask lots of questions.

824
00:39:58,110 --> 00:40:01,990
For example, obvious question would be,
how do you choose row and

825
00:40:01,990 --> 00:40:03,970
column permutations of a sparse maker so

826
00:40:03,970 --> 00:40:08,070
that it's, it's LU factorization has the
fewest numbers of non-zeros.

827
00:40:08,070 --> 00:40:11,270
It's a totally natural question and it's
mp heart.

828
00:40:11,270 --> 00:40:13,400
Okay?
So, fine.

829
00:40:13,400 --> 00:40:14,960
So what do people do?

830
00:40:14,960 --> 00:40:17,560
Well, it's, it means everything, these are
all heuristics.

831
00:40:17,560 --> 00:40:18,420
Okay?
And, actually,

832
00:40:18,420 --> 00:40:20,080
they're more than heuristics, right?

833
00:40:20,080 --> 00:40:24,020
it's, they're even, they're very stringent
requirements, right?

834
00:40:24,020 --> 00:40:28,350
Because what people do who worked on
sparse matrices is the following.

835
00:40:29,600 --> 00:40:32,860
If you want to, so what happens is,
something like this.

836
00:40:32,860 --> 00:40:38,090
Well, you choose these entries using
various methods, right?

837
00:40:38,090 --> 00:40:41,720
One of the methods depends on the sparsity
pattern,

838
00:40:41,720 --> 00:40:43,950
it also depends on the values of A, right?

839
00:40:43,950 --> 00:40:46,110
As you're going along you might choose,

840
00:40:46,110 --> 00:40:49,630
the P is actually built up while you're
doing the factorization.

841
00:40:49,630 --> 00:40:51,820
Okay?
So that, that that's how this works.

842
00:40:51,820 --> 00:40:54,040
And, and I'm not going to go into any
details, but that's sort of the idea.

843
00:40:54,040 --> 00:40:55,770
And it's, it's worth knowing about this.

844
00:40:55,770 --> 00:40:56,330
Okay.

845
00:40:56,330 --> 00:40:57,670
And, what's the cost?

846
00:40:57,670 --> 00:41:01,070
Well it's usually, way less than n cubed.

847
00:41:01,070 --> 00:41:01,620
Right.
By the way,

848
00:41:01,620 --> 00:41:05,910
if it gets even close to n cubed It means
you have implemented an extremely

849
00:41:05,910 --> 00:41:11,580
inefficient method when you should have
just called a,a dense LU pack routine,

850
00:41:11,580 --> 00:41:13,460
right, which would've blocked everything
nicely for you.

851
00:41:13,460 --> 00:41:15,060
That's, that's all it means, right?

852
00:41:15,060 --> 00:41:18,330
But, but this is a big deal.

853
00:41:18,330 --> 00:41:23,330
So you know, if you're lucky this is
>> This is actually I'd, a,

854
00:41:23,330 --> 00:41:26,310
a, a very big give, by the way, everyone
here is a beneficiary of this.

855
00:41:26,310 --> 00:41:27,500
I can tell you that right now.

856
00:41:29,280 --> 00:41:30,790
so, and the reason is very simple.

857
00:41:32,420 --> 00:41:38,080
CVX, for example, calls SDPT3 or SEDUMI.

858
00:41:38,080 --> 00:41:42,830
All of these things use sparse matrix
methods to actually do the heavy lifting.

859
00:41:42,830 --> 00:41:45,620
That's if you profile any of these
solvers, that's all they're doing.

860
00:41:45,620 --> 00:41:47,330
The only thing they're doing is solving
linear equations.

861
00:41:47,330 --> 00:41:48,490
So, there's nothing else.

862
00:41:48,490 --> 00:41:50,660
Linear, And they're literally solving a x
equals b.

863
00:41:50,660 --> 00:41:53,120
Now a is symmetric, but that's another
story, it doesn't matter.

864
00:41:53,120 --> 00:41:54,770
Same idea goes, right.

865
00:41:54,770 --> 00:41:57,820
And they're all spa, and, and they're
exploiting sparsity very heavily.

866
00:41:57,820 --> 00:42:00,120
Okay, now they, what's at,

867
00:42:00,120 --> 00:42:04,200
there's a little, I mean it's a bit funny,
right, because.

868
00:42:04,200 --> 00:42:06,420
You know, if someone comes along over your
shoulder,

869
00:42:06,420 --> 00:42:08,960
when you're plot feeling with CVX, and
somebody says, what are you doing.

870
00:42:08,960 --> 00:42:12,230
Well, I'm doing convex optimization, and
they go, well, you know,

871
00:42:12,230 --> 00:42:14,160
how do you know it works, and you go,
that's the whole point.

872
00:42:14,160 --> 00:42:16,920
It's just like, it's, it's tractable.

873
00:42:16,920 --> 00:42:19,380
I mean, we can do that, I mean, you know,
had a whole course, and

874
00:42:19,380 --> 00:42:20,610
that, that was the whole point.

875
00:42:20,610 --> 00:42:23,390
There's other problems you can't solve,
but these you can really solve.

876
00:42:23,390 --> 00:42:28,640
The, as a practicle matter the only reason
you can solve them, as a practicle matter,

877
00:42:28,640 --> 00:42:33,110
is because the euristics of choosing the
permentations for

878
00:42:33,110 --> 00:42:35,420
solving linear systems of equations are so
good.

879
00:42:36,490 --> 00:42:37,720
Everybody got that?

880
00:42:37,720 --> 00:42:40,890
Don't, but do not tell anyone outside this
class that.

881
00:42:40,890 --> 00:42:44,050
Right?
This is a secret we keep together here.

882
00:42:44,050 --> 00:42:44,580
Right?

883
00:42:44,580 --> 00:42:47,750
So, if anyone asks you, what, why does
that work?

884
00:42:47,750 --> 00:42:48,380
Is that heuristic?

885
00:42:48,380 --> 00:42:50,270
You say absolutely not.

886
00:42:50,270 --> 00:42:52,210
This is not a heuristic.

887
00:42:52,210 --> 00:42:54,110
And in theory it's not, right.

888
00:42:54,110 --> 00:42:58,260
If, the worse thing that can happen is
your permutation fails spectacularly and

889
00:42:58,260 --> 00:42:59,890
you avert to n cubed.

890
00:42:59,890 --> 00:43:00,728
That is not good.

891
00:43:00,728 --> 00:43:03,350
[INAUDIBLE] 10,000 or 50,000.
So, but now you know.

892
00:43:03,350 --> 00:43:07,938
This is why, roughly speaking, you can
solve equations.

893
00:43:07,938 --> 00:43:11,730
10,000 variables, 100,000 variables, a
million variables.

894
00:43:11,730 --> 00:43:12,670
Million by million.

895
00:43:12,670 --> 00:43:15,890
You can, kind of, solve, sparse equations.

896
00:43:15,890 --> 00:43:20,560
And, so, the correct way to say it is if
the gods, god or

897
00:43:20,560 --> 00:43:24,140
gods, who control The [UNKNOWN]

898
00:43:25,200 --> 00:43:32,140
of permutations used in sparse
factorization are smiling on you, and

899
00:43:32,140 --> 00:43:33,950
that depends on the offerings you have
made to them likely.

900
00:43:35,220 --> 00:43:38,960
If they are smiling on you then you can
solve equations that would just

901
00:43:38,960 --> 00:43:42,980
otherwise be completely and totally out of
question, everybody got this right but

902
00:43:42,980 --> 00:43:47,650
there are some sparsity [UNKNOWN] Where
you don't have to rely on

903
00:43:47,650 --> 00:43:53,900
the gods of heuristic, sparse permutation
selection algorithms.

904
00:43:53,900 --> 00:43:57,140
And we'll talk about some of, well I'll
tell you one right now.

905
00:43:57,140 --> 00:44:01,310
If the matrix is banded, you don't need to
make any offerings at all.

906
00:44:01,310 --> 00:44:03,270
That's, it's really going to work.

907
00:44:03,270 --> 00:44:04,770
Always, m'kay.

908
00:44:04,770 --> 00:44:08,960
But you know for general sparse matrices
this is not true, okay, so.

909
00:44:08,960 --> 00:44:13,710
Now this is very good stuff to know about
even if you, if you're more interested in

910
00:44:13,710 --> 00:44:16,870
sort of more theoretical stuff, this is
very good stuff to know about.

911
00:44:16,870 --> 00:44:19,270
So now, well we're going to encounter
more,

912
00:44:19,270 --> 00:44:23,500
I mean what we're going to encounter are
some things that are generally not

913
00:44:23,500 --> 00:44:27,220
just square, but you actually have sine
structure.

914
00:44:27,220 --> 00:44:30,450
There's the Cholesky factorization.

915
00:44:30,450 --> 00:44:32,290
If you have a positive definite matrix,

916
00:44:32,290 --> 00:44:36,740
A, then you can factor it as it LL
transfer \g, but it's not.

917
00:44:36,740 --> 00:44:41,750
What it says is, it's an LU factorization,
with L = U.

918
00:44:41,750 --> 00:44:44,450
Well kind of duh, or aesthetics tells you
that right.

919
00:44:44,450 --> 00:44:47,090
because what's a matrix that's symmetric.

920
00:44:47,090 --> 00:44:50,370
Well it's transposes the same, or you swap
rows and columns, the same thing.

921
00:44:50,370 --> 00:44:53,900
So, of course, the U and the L, sh, that's
all this is, okay.

922
00:44:53,900 --> 00:44:55,800
And L is lower triangular And

923
00:44:55,800 --> 00:45:01,300
in fact, generally people require just to
normalize that.

924
00:45:01,300 --> 00:45:04,020
That the entries are positive.

925
00:45:04,020 --> 00:45:08,400
One very interesting fact about Cholesky
factorization, is a numerical one.

926
00:45:08,400 --> 00:45:10,270
It actually is stable.

927
00:45:10,270 --> 00:45:11,490
Again we're not really talking about that.

928
00:45:11,490 --> 00:45:13,670
It means you do not have to permute, you,

929
00:45:13,670 --> 00:45:17,780
you can say immediately, that if you
calculate that in double precision.

930
00:45:17,780 --> 00:45:19,990
You aren't, you're not going to have any,
any trouble, right.

931
00:45:19,990 --> 00:45:22,610
That your er, you can bind the error in L.

932
00:45:22,610 --> 00:45:23,460
And stuff like that.

933
00:45:23,460 --> 00:45:26,000
As a, as a practical matter, it just
works.

934
00:45:26,000 --> 00:45:27,620
In this case, and the cost is 1/3 n cubed.

935
00:45:27,620 --> 00:45:28,530
And, you know?

936
00:45:28,530 --> 00:45:30,088
Again, the difference between 2/3 and

937
00:45:30,088 --> 00:45:32,790
1/3 is, like, zippo in, in current things,
right?

938
00:45:32,790 --> 00:45:38,940
Where cache, cache misses is way more
important than some factor of two.

939
00:45:38,940 --> 00:45:39,490
But, you know?

940
00:45:39,490 --> 00:45:40,640
You get the, rough.

941
00:45:40,640 --> 00:45:42,420
[INAUDIBLE] , I mean, this, you save
because.

942
00:45:43,420 --> 00:45:46,660
Well, you don't have to calculate
separately in an L and a U, right?

943
00:45:46,660 --> 00:45:47,700
You're just calculating one L.

944
00:45:48,710 --> 00:45:49,370
Okay.

945
00:45:49,370 --> 00:45:52,500
So, how do you solve things by Cholesky
factorization.

946
00:45:52,500 --> 00:45:54,520
By the way, this would come up in least
squares, right?

947
00:45:54,520 --> 00:45:57,540
That's your most, that's your most basic
convex optimization problem,

948
00:45:57,540 --> 00:45:58,450
least squares.

949
00:45:58,450 --> 00:46:00,230
You can do it by Cholesky factorization.

950
00:46:00,230 --> 00:46:01,190
So.

951
00:46:01,190 --> 00:46:05,120
Here, you'd factor A, as l l transpose,
and you do forward and

952
00:46:05,120 --> 00:46:08,070
backward substitution, and the cost is
about 1 3rd n, cubed.

953
00:46:08,070 --> 00:46:11,390
Okay, so that's, that's the, that's the
idea, that's Cholesky factorization.

954
00:46:12,740 --> 00:46:15,580
Now sparse Cholesky factorization.

955
00:46:15,580 --> 00:46:19,190
It looks just like sparce LU, but there's
actually a very critical difference, and

956
00:46:19,190 --> 00:46:20,080
I'm going to say what it is.

957
00:46:21,130 --> 00:46:24,560
And you probably don't need to know it at
this level, but

958
00:46:24,560 --> 00:46:25,720
these are good things to know about.

959
00:46:25,720 --> 00:46:26,720
So here, here it is.

960
00:46:28,040 --> 00:46:31,990
As before, there's P one and P 2, you do
pre- and post-multiply.

961
00:46:31,990 --> 00:46:36,980
Or, if you like, you can say that what
you, another way to write this Is,

962
00:46:38,370 --> 00:46:44,410
is to say just to think of it this way
first you permute A then call

963
00:46:44,410 --> 00:46:47,650
a Cholesky factorization on their
perumuted version right so

964
00:46:47,650 --> 00:46:50,040
that, that, that's a a good way to think
of it.

965
00:46:50,040 --> 00:46:50,920
Oh and by the way.

966
00:46:50,920 --> 00:46:52,270
Everybody has a different thing, you know.

967
00:46:52,270 --> 00:46:55,710
Sometimes it's lower triangular, sometimes
it's upper triangular.

968
00:46:55,710 --> 00:46:57,870
The permutation either goes like here, or
there.

969
00:46:57,870 --> 00:47:03,080
So you always, anytime you actually look,
use something that does a factorization.

970
00:47:03,080 --> 00:47:05,560
You have to look at what, exactly what
they mean by it.

971
00:47:05,560 --> 00:47:07,420
And it'll probably be different from these
notes.

972
00:47:07,420 --> 00:47:07,920
Right, so, but.

973
00:47:08,960 --> 00:47:09,930
That's, this is too bad.

974
00:47:09,930 --> 00:47:11,305
There's no standards.

975
00:47:11,305 --> 00:47:15,800
Okay, now here you always have a Cholesky
factorization.

976
00:47:15,800 --> 00:47:21,860
But the, the permutation can have a huge
effect on the sparsity of L,

977
00:47:21,860 --> 00:47:27,790
and therefore on the memory required to
carry this out, and second, and of course,

978
00:47:27,790 --> 00:47:32,810
it also has, since the number of nonzeros
in L is basically the flop count.

979
00:47:32,810 --> 00:47:34,990
That's going to affect the run time.

980
00:47:34,990 --> 00:47:36,070
Okay.

981
00:47:36,070 --> 00:47:40,780
Now, unlike the sparse LU factorization
where you're actually choosing P1 and

982
00:47:40,780 --> 00:47:43,350
P2 as you go, as you factor the matrix.

983
00:47:43,350 --> 00:47:44,540
Right?
Because you actually looking at

984
00:47:44,540 --> 00:47:47,750
entries and you want to, you don't,
want to avoid numbers.

985
00:47:47,750 --> 00:47:48,750
They're too small.

986
00:47:48,750 --> 00:47:51,110
If you wanted to, if you're going to
divide by something.

987
00:47:51,110 --> 00:47:54,900
Here it turns out that P can actually be
chosen

988
00:47:54,900 --> 00:47:59,470
entirely on the basis of the sparsity
pattern of A.

989
00:47:59,470 --> 00:48:00,560
Okay?

990
00:48:00,560 --> 00:48:06,320
So, in other words, all that mat-, you
take the sparsity pattern of A,

991
00:48:06,320 --> 00:48:09,580
you don't even have to know what the
entries, the numbers are going to be,

992
00:48:09,580 --> 00:48:13,570
Right, if you can make a graph out of it
that's a very common thing, and

993
00:48:13,570 --> 00:48:18,930
then you chose an elimination ordering
right but you do this once right.

994
00:48:18,930 --> 00:48:22,970
So, that says if you are going to do
something like solve ax equals b with

995
00:48:22,970 --> 00:48:25,840
different values of A multiple times.

996
00:48:25,840 --> 00:48:29,590
When A is sparse definite, you can
actually cash the permutation.

997
00:48:29,590 --> 00:48:32,030
I don't know if you got that but this will
be,

998
00:48:32,030 --> 00:48:36,900
this will, the usefulness will be clear in
about two weeks.

999
00:48:36,900 --> 00:48:38,260
You can cash the factorization.

1000
00:48:40,360 --> 00:48:41,460
And that's the idea.

1001
00:48:42,590 --> 00:48:45,340
anyway, so the cost is usually you know,
way less than and and

1002
00:48:45,340 --> 00:48:46,750
all that kind of stuff.

1003
00:48:46,750 --> 00:48:47,979
So that's, that's how that works.

1004
00:48:49,230 --> 00:48:54,120
okay, another factorization we going to
see is the so

1005
00:48:54,120 --> 00:48:58,280
called LDL transpose factorization and
that's this if you

1006
00:48:58,280 --> 00:49:04,240
have a symmetric matrix not necessarily
not necessarily a positive definite

1007
00:49:04,240 --> 00:49:09,540
you can factor it as LDL transpose right
and D.

1008
00:49:09,540 --> 00:49:11,810
P is permutation matrix.

1009
00:49:11,810 --> 00:49:16,610
L is lower triangular unit, so you have l
i equal 0, d is block diagonal, and

1010
00:49:16,610 --> 00:49:20,390
it's either got 1 by 1 blocks or 2 by 2 to
blocks, right?

1011
00:49:20,390 --> 00:49:23,470
So and you get sort of the same thing,
right?

1012
00:49:23,470 --> 00:49:24,850
So this would be kind of one,

1013
00:49:24,850 --> 00:49:28,400
this would be one of the methods of, of
sort of choice.

1014
00:49:28,400 --> 00:49:31,315
Oh, and I think I, I can give you a rough
idea of what

1015
00:49:31,315 --> 00:49:33,960
>> What backslash doesn't MatLab.

1016
00:49:33,960 --> 00:49:35,120
I mean, just roughly.

1017
00:49:35,120 --> 00:49:35,760
It's something like this.

1018
00:49:35,760 --> 00:49:40,220
When you type A backslash B, I think it
looks at the diagonal of, of A.

1019
00:49:40,220 --> 00:49:44,040
And if they're positive, or it looks, it
quickly looks.

1020
00:49:44,040 --> 00:49:46,380
It compares some entries quickly to see
if,

1021
00:49:46,380 --> 00:49:50,010
if it's possible that A is positive
symmetric positive definite.

1022
00:49:50,010 --> 00:49:50,660
Right?

1023
00:49:50,660 --> 00:49:52,070
So, it guesses that.

1024
00:49:52,070 --> 00:49:53,510
Then, it attempts the Cholesky factor.

1025
00:49:53,510 --> 00:49:55,160
If it finds that it is, if it,

1026
00:49:55,160 --> 00:49:59,510
if there's some evidence that A is
symmetric, and positive definite, like,

1027
00:49:59,510 --> 00:50:02,510
for example, it has a negative on the
diagonal, it doesn't do this.

1028
00:50:02,510 --> 00:50:03,050
It's all over.

1029
00:50:03,050 --> 00:50:03,960
It's not positive definite.

1030
00:50:03,960 --> 00:50:06,920
So, it attempts to do a Choloesky
factorization because it's numerically

1031
00:50:06,920 --> 00:50:07,835
stable and fast.

1032
00:50:07,835 --> 00:50:09,190
Okay?

1033
00:50:09,190 --> 00:50:13,280
But, if it turns out A wasn't, then the
Cholesky fails.

1034
00:50:13,280 --> 00:50:16,780
Then, it falls back and does something
like an LU or something like that.

1035
00:50:16,780 --> 00:50:18,290
I mean this is roughly what it does.

1036
00:50:18,290 --> 00:50:18,930
Right?

1037
00:50:18,930 --> 00:50:19,880
So, okay.

1038
00:50:19,880 --> 00:50:22,670
And it won't a sparse factorization unless
A is like

1039
00:50:22,670 --> 00:50:26,450
explicitly stored flagged as sparse.

1040
00:50:28,790 --> 00:50:29,330
Okay.

1041
00:50:29,330 --> 00:50:31,020
Now we're going to talk about something.

1042
00:50:32,450 --> 00:50:37,220
It's about an elimination method, but it
actually ties to a whole bunch of stuff.

1043
00:50:37,220 --> 00:50:40,480
And it turns out it's actually kind of the
same as selecting the permutation, but

1044
00:50:40,480 --> 00:50:41,840
we'll get to that.

1045
00:50:41,840 --> 00:50:45,100
So what we'll do is let's just take x
equals b and block it.

1046
00:50:45,100 --> 00:50:46,930
So, all we're going to do is we're
going to,

1047
00:50:46,930 --> 00:50:48,750
we're going to write it as blocks.

1048
00:50:48,750 --> 00:50:52,830
We're going to partition x that'll
partition b,

1049
00:50:52,830 --> 00:50:54,430
and we're going to write it that way.

1050
00:50:54,430 --> 00:50:56,460
So, it's just two sets of equations.

1051
00:50:56,460 --> 00:50:59,410
It's a1 1x 1 plus a1 2x 2 equals b1,

1052
00:50:59,410 --> 00:51:02,720
and then a2 1x 1 plus a2 2x 2 equals b2,
right?

1053
00:51:02,720 --> 00:51:04,040
So that's, that's it.

1054
00:51:04,040 --> 00:51:04,700
On this block, you know.

1055
00:51:04,700 --> 00:51:06,340
And let's see what happens.

1056
00:51:06,340 --> 00:51:10,160
What you do is from the first equation,
basically, is a one one,

1057
00:51:10,160 --> 00:51:14,100
x one plus a one two, x two equals b one.

1058
00:51:14,100 --> 00:51:18,160
And what you do is assumingn a one one is
non singular, right?

1059
00:51:18,160 --> 00:51:23,540
You put this over there then premultiply
the a one one and that,

1060
00:51:23,540 --> 00:51:26,130
that says that you can write x one.

1061
00:51:26,130 --> 00:51:29,160
As a function of X2 and you get this,
okay.

1062
00:51:29,160 --> 00:51:32,380
And so the idea is now we're going to

1063
00:51:32,380 --> 00:51:35,940
eliminate X1 from the system of equations,
right.

1064
00:51:35,940 --> 00:51:40,030
So, all we do, is we plug this value of X1
into this, right.

1065
00:51:41,210 --> 00:51:43,370
By the way, this is, this is actually
just, sort of,

1066
00:51:43,370 --> 00:51:46,060
like you know, this, this substitution
stuff.

1067
00:51:46,060 --> 00:51:48,780
And with, but with matrices, blocks,
right?

1068
00:51:48,780 --> 00:51:52,670
So, what happens is, if you plug this
equation into that, right.

1069
00:51:52,670 --> 00:51:57,980
You get A 2 1 times this thing, plus A 2 2
times x 2 equals b 2.

1070
00:51:57,980 --> 00:52:01,380
And when the smoke clears, that's a set of
equations for x 2.

1071
00:52:01,380 --> 00:52:02,740
And you get.

1072
00:52:02,740 --> 00:52:05,350
This equation right here okay.

1073
00:52:05,350 --> 00:52:06,960
So this just you've just elim and

1074
00:52:06,960 --> 00:52:10,000
you would say things like you have
eliminated X1 okay.

1075
00:52:10,000 --> 00:52:15,330
This matrix you have seen before that's
the shirt compliment okay and

1076
00:52:15,330 --> 00:52:17,319
you've seen it in any legitimate class
you've taken.

1077
00:52:18,440 --> 00:52:21,370
Because a legitimate class uses linear
algebra, and then,

1078
00:52:21,370 --> 00:52:23,540
you can't use linear algebra and not see
that, right?

1079
00:52:23,540 --> 00:52:26,510
So you take probability class, that's
conditional, that's conditional, you know,

1080
00:52:26,510 --> 00:52:30,440
covariance or some, that's the conditional
variance, covariance matrix, right?

1081
00:52:30,440 --> 00:52:31,370
You take other things.

1082
00:52:31,370 --> 00:52:34,050
Take circuits, mechanics, anything,
physics.

1083
00:52:34,050 --> 00:52:35,540
This comes up.

1084
00:52:35,540 --> 00:52:38,560
Okay, so that's the sure compliment.

1085
00:52:38,560 --> 00:52:43,850
So what it says is that when you eleminate
variable, a block or variables, or

1086
00:52:43,850 --> 00:52:46,560
a block of variables from a set of linear
equations.

1087
00:52:46,560 --> 00:52:48,010
You're left with a set,

1088
00:52:48,010 --> 00:52:51,580
with a linear equation that involves the
sure compliment, okay?

1089
00:52:51,580 --> 00:52:55,000
So actually, that's, I mean that's good
thing to know about, okay.

1090
00:52:56,000 --> 00:53:00,440
So, by the way you would form this matrix,
you'd solve X2 and

1091
00:53:00,440 --> 00:53:04,230
then you would recover X1 using that
formula, okay.

1092
00:53:04,230 --> 00:53:07,940
And so, here's a completely generic,
completely generic algorithm for

1093
00:53:07,940 --> 00:53:09,500
solving this block equation.

1094
00:53:09,500 --> 00:53:12,640
And it says this It says, yeah I mean
you're going to form,

1095
00:53:12,640 --> 00:53:14,810
you're going to form this matrix.

1096
00:53:14,810 --> 00:53:16,970
That's the sure his thing compliment, it's
called S here.

1097
00:53:16,970 --> 00:53:21,970
Right, so you, you form this thing, a 1 1
inverse a 1 2.

1098
00:53:21,970 --> 00:53:26,490
Now, by the way, that can be formed lots
of different Wait oh, let me ask that.

1099
00:53:27,870 --> 00:53:32,290
How would you if there were no structure
in a11 and

1100
00:53:32,290 --> 00:53:37,630
a11 was let's say a 1000 by 1000, and
suppose a12 was 1000 by 10.

1101
00:53:37,630 --> 00:53:44,061
Please tell me how you would form a 11
inverse, a 12.

1102
00:53:45,870 --> 00:53:47,110
What would you do?

1103
00:53:47,110 --> 00:53:47,980
So A 1 1.

1104
00:53:47,980 --> 00:53:49,500
Thousand by thousand.

1105
00:53:49,500 --> 00:53:53,200
Inverse times, A 1 2 is a thousand by 10.

1106
00:53:53,200 --> 00:53:56,180
How do you, how do you calculate that
product?

1107
00:53:56,180 --> 00:53:57,390
It li-, it's a product.

1108
00:53:57,390 --> 00:53:58,980
How do you do it?

1109
00:53:58,980 --> 00:54:00,020
A 1 1 cash.

1110
00:54:00,020 --> 00:54:01,520
And do 10 back solves.

1111
00:54:01,520 --> 00:54:02,530
Done.
Perfect.

1112
00:54:02,530 --> 00:54:03,730
Okay?
So.

1113
00:54:03,730 --> 00:54:05,010
All right.
So, but, you just write,

1114
00:54:05,010 --> 00:54:06,930
people just write stuff like form A11
inverse A12.

1115
00:54:06,930 --> 00:54:07,770
Okay.

1116
00:54:07,770 --> 00:54:10,060
And assuming you know what you're doing.

1117
00:54:10,060 --> 00:54:11,320
Okay.

1118
00:54:11,320 --> 00:54:12,660
so, you form this, your compliment.

1119
00:54:14,030 --> 00:54:16,240
Then you solve this, your compliment
equation, and

1120
00:54:16,240 --> 00:54:18,950
then, you use this equation to reconstruct
X1.

1121
00:54:18,950 --> 00:54:20,540
Okay?
So, that's the method.

1122
00:54:20,540 --> 00:54:21,630
All right?

1123
00:54:21,630 --> 00:54:24,510
And, you know let's, let's, let's figure
out how much it costs.

1124
00:54:24,510 --> 00:54:27,790
And the answer is, it's F plus N2S.

1125
00:54:27,790 --> 00:54:29,190
That's the cost of factoring A11.

1126
00:54:29,190 --> 00:54:31,920
And S is the solve step.

1127
00:54:31,920 --> 00:54:32,420
Right?

1128
00:54:33,740 --> 00:54:34,850
These are the back solves.

1129
00:54:34,850 --> 00:54:36,070
Right?
As you, as you said,

1130
00:54:36,070 --> 00:54:38,050
that there's one factorization and some
solves.

1131
00:54:39,230 --> 00:54:40,110
Step 2 cost this.

1132
00:54:40,110 --> 00:54:40,980
And what we're going to do here is

1133
00:54:40,980 --> 00:54:43,210
we're just going to assume on the second
step as dense.

1134
00:54:43,210 --> 00:54:44,180
I mean, because you could work it out,

1135
00:54:44,180 --> 00:54:48,240
when it's not Right and then the the the
last step

1136
00:54:48,240 --> 00:54:52,600
is a something that's again we're going to
just assume its dense generic nothing and

1137
00:54:52,600 --> 00:54:56,440
we'll just do we'll just solve that
equation and that's going to be a N,

1138
00:54:56,440 --> 00:55:00,060
N, N two cubed right so the total thing
looks something like this.

1139
00:55:00,060 --> 00:55:00,880
Now.

1140
00:55:00,880 --> 00:55:02,210
Here's what happens.

1141
00:55:02,210 --> 00:55:07,560
If, if you apply this method now, to a
matrix

1142
00:55:07,560 --> 00:55:12,930
that has no structure whatsoever, then f
equals this, and s equals that.

1143
00:55:12,930 --> 00:55:17,550
And you plug it into this formula, and you
get back, actually, exactly.

1144
00:55:17,550 --> 00:55:21,140
The fact you get, there's no savings what
so ever okay?

1145
00:55:21,140 --> 00:55:23,040
So that was a completely silly thing to
do.

1146
00:55:23,040 --> 00:55:26,110
I mean, it doesn't matter that's how you
do it right.

1147
00:55:26,110 --> 00:55:27,290
Actually, it's interesting because it
shows you,

1148
00:55:27,290 --> 00:55:28,940
it shows you what blocking is right?

1149
00:55:28,940 --> 00:55:32,450
because there is, this is blocking 2 by 2
now you can imagine what blocking 10 by

1150
00:55:32,450 --> 00:55:33,830
10 looks like, it's just this.

1151
00:55:33,830 --> 00:55:35,000
Okay, so that's it.

1152
00:55:36,290 --> 00:55:37,315
Here is where it's useful.

1153
00:55:37,315 --> 00:55:41,070
>> When A11, has structure, right?

1154
00:55:41,070 --> 00:55:45,818
So, the factor, with the factor cost and
the solve cost on A11 is low,

1155
00:55:45,818 --> 00:55:51,680
then, this delivers, this delivers big
savings.

1156
00:55:51,680 --> 00:55:52,960
Here's an example.

1157
00:55:52,960 --> 00:55:55,710
Suppose that A11 was diagonal.

1158
00:55:55,710 --> 00:55:56,400
Right?

1159
00:55:56,400 --> 00:56:00,770
That means the factor cost is zero and the
solve cost is n1.

1160
00:56:00,770 --> 00:56:02,640
That's just, just divide.

1161
00:56:02,640 --> 00:56:03,260
Right?

1162
00:56:03,260 --> 00:56:06,280
Then the number of flops is this, and this
is absolutely amazing, so

1163
00:56:06,280 --> 00:56:12,360
this is that if you see a set of linear
equations that look like this.

1164
00:56:13,840 --> 00:56:16,500
Right?
So, It looks like that.

1165
00:56:16,500 --> 00:56:20,402
I'm going to, I'm going to draw the,
non-zeroes.

1166
00:56:20,402 --> 00:56:21,410
Right?

1167
00:56:21,410 --> 00:56:25,370
Oh sometimes people call this a spy
diagram, I have absolutely no idea why,

1168
00:56:25,370 --> 00:56:26,550
but anyway, whatever.

1169
00:56:26,550 --> 00:56:27,730
I, it doesn't matter.

1170
00:56:27,730 --> 00:56:29,680
I'm just showing you the non-zeroes.

1171
00:56:29,680 --> 00:56:30,490
Okay?

1172
00:56:30,490 --> 00:56:34,270
Now there's many other ways to understand
this, and and I'll,

1173
00:56:34,270 --> 00:56:37,350
I'll show A, a couple and the once the way

1174
00:56:37,350 --> 00:56:40,720
they are purely mathematical you probably
have seen before, OK.

1175
00:56:40,720 --> 00:56:44,520
So and also introduces a very interesting
idea so you what that is.

1176
00:56:44,520 --> 00:56:50,000
So lets , lets takes this one suppose you
wanted to solve A plus BC and

1177
00:56:50,000 --> 00:56:55,850
the idea here is that B is like you know
it, its so this is low rank.

1178
00:56:55,850 --> 00:56:56,840
Looks like that.

1179
00:56:56,840 --> 00:56:58,890
And here's, and here's a.

1180
00:56:58,890 --> 00:57:00,320
And let's assume a has structure.

1181
00:57:00,320 --> 00:57:01,620
So let's make a banded.

1182
00:57:01,620 --> 00:57:03,220
You know?
That's, 'cuz it's pretty,

1183
00:57:03,220 --> 00:57:04,520
and it has structure.

1184
00:57:04,520 --> 00:57:07,490
So, there's your, your sparsity pattern.

1185
00:57:07,490 --> 00:57:09,830
That's a plus bc, okay?

1186
00:57:09,830 --> 00:57:10,680
It could be anything else.

1187
00:57:10,680 --> 00:57:12,030
It could be sparse.

1188
00:57:12,030 --> 00:57:13,510
It could be.

1189
00:57:13,510 --> 00:57:16,680
Forty eight, you know, DFT, it could be
anything, you know,

1190
00:57:16,680 --> 00:57:18,490
it could be something that you have a fast
transformer for.

1191
00:57:18,490 --> 00:57:21,360
It could be some wave lit thing or
something, I don't care, right?

1192
00:57:21,360 --> 00:57:24,770
And then so the people would call this is
like structured plus low rank.

1193
00:57:25,940 --> 00:57:27,100
That's what this is.

1194
00:57:27,100 --> 00:57:29,570
So it could be sparse plus low rank matrix
or something.

1195
00:57:29,570 --> 00:57:30,070
Okay.

1196
00:57:31,220 --> 00:57:36,220
Now the dif, the sparsity pattern of A
plus BC is generally full.

1197
00:57:36,220 --> 00:57:39,570
Severy, you know, cause, you take an
outer, you just take B and

1198
00:57:39,570 --> 00:57:40,610
C to be columns.

1199
00:57:40,610 --> 00:57:44,530
Or, you know, B is a column and C is a row
vector.

1200
00:57:44,530 --> 00:57:47,780
Then it's just an outer product, it sprays
entrys all over the place.

1201
00:57:47,780 --> 00:57:52,890
Right, so, so then, a generic, a generic
method there will be no sparsity, nothing.

1202
00:57:52,890 --> 00:57:55,640
So, how you can solve these is very
clever, and

1203
00:57:55,640 --> 00:57:57,960
in fact the first step, there's a name for
it.

1204
00:57:57,960 --> 00:57:59,920
It's actually called unelimination.

1205
00:57:59,920 --> 00:58:04,640
So elimination means you take your
variables and

1206
00:58:04,640 --> 00:58:06,570
you look at one block and you say,

1207
00:58:06,570 --> 00:58:09,660
I'm going to express that in terms of the
others and then get rid of it.

1208
00:58:09,660 --> 00:58:14,040
Unelimination means that you're going to
introduce a new, a new one.

1209
00:58:14,040 --> 00:58:22,205
So what you do is you look at (A + BC)x =
b, you write that as A + By.

1210
00:58:22,205 --> 00:58:23,220
>> Right?

1211
00:58:23,220 --> 00:58:24,020
Equals B.

1212
00:58:25,060 --> 00:58:26,760
Y is actually CX.

1213
00:58:29,120 --> 00:58:30,390
Everybody got that?

1214
00:58:30,390 --> 00:58:32,310
That's just, of course, they're the same
thing.

1215
00:58:32,310 --> 00:58:33,440
They're identical.

1216
00:58:33,440 --> 00:58:36,190
But, you rewrite these equations like
that.

1217
00:58:36,190 --> 00:58:37,100
Right?

1218
00:58:37,100 --> 00:58:41,520
So, there's, there's, and , and, here, so,
you've, i mean you've done unelimiation.

1219
00:58:41,520 --> 00:58:43,550
You've introduced new variables and new
equations.

1220
00:58:43,550 --> 00:58:44,990
Everybody got this?

1221
00:58:44,990 --> 00:58:45,680
And you see this.

1222
00:58:47,590 --> 00:58:51,160
Now, you're not trained yet but you will
be.

1223
00:58:51,160 --> 00:58:55,240
And it, once you're trained, what your
eye, what you're searching for

1224
00:58:55,240 --> 00:59:00,500
is the following: easily inverted matrices
on the diagonal.

1225
00:59:00,500 --> 00:59:06,510
And your eye should, once you're trained,
go immediately to that minus I.

1226
00:59:08,260 --> 00:59:10,540
You will go, it will go immediately there.

1227
00:59:10,540 --> 00:59:16,262
Because in the world of easily inverted
matrices, I is up there and

1228
00:59:16,262 --> 00:59:16,810
-I is up there.

1229
00:59:16,810 --> 00:59:18,500
Okay?
Everybody cool?

1230
00:59:18,500 --> 00:59:20,210
Alright, now.

1231
00:59:20,210 --> 00:59:24,250
I need, see, I just have this fantastic, I
just learned about block elimination.

1232
00:59:24,250 --> 00:59:26,120
There's a, I'm going to elimi- I eliminate
that block.

1233
00:59:26,120 --> 00:59:29,740
Now if you eliminate that block you're
eliminating y and where do you end up?

1234
00:59:31,150 --> 00:59:31,650
Right back here.

1235
00:59:32,990 --> 00:59:34,220
Everybody got it?

1236
00:59:34,220 --> 00:59:37,190
So, well duh, I mean you un eliminated and

1237
00:59:37,190 --> 00:59:40,160
then eliminated the thing you had just
introduced right?

1238
00:59:40,160 --> 00:59:40,660
OK.
Fine.

1239
00:59:42,090 --> 00:59:45,085
So the key, is to, is to control.

1240
00:59:45,085 --> 00:59:46,510
>> Your urge.

1241
00:59:46,510 --> 00:59:51,800
Primal matrix structure factorization
urge, to eliminate that minus I.

1242
00:59:51,800 --> 00:59:54,600
And instead, you take the system and you
eliminate A.

1243
00:59:55,750 --> 00:59:59,290
And, in fact, that's not a bad idea
because the assumption here is that A is

1244
00:59:59,290 --> 01:00:00,850
easy to invert.

1245
01:00:00,850 --> 01:00:03,270
Right?
So, A is 48 transform or

1246
01:00:03,270 --> 01:00:04,460
some wave lifting.

1247
01:00:04,460 --> 01:00:05,050
I don't know what it is.

1248
01:00:05,050 --> 01:00:06,060
It doesn't matter what it is.

1249
01:00:06,060 --> 01:00:07,480
Right?
So, that's what it is.

1250
01:00:07,480 --> 01:00:08,360
OK.

1251
01:00:08,360 --> 01:00:11,050
If you eliminate A, so, if you
uneliminate.

1252
01:00:11,050 --> 01:00:11,986
If you go from this.

1253
01:00:11,986 --> 01:00:15,660
To this, and then you eliminate a.

1254
01:00:15,660 --> 01:00:17,080
Not the minus i.

1255
01:00:17,080 --> 01:00:19,000
You get this equation.

1256
01:00:19,000 --> 01:00:23,370
And you will recognize that as the Schur
complement here, right?

1257
01:00:23,370 --> 01:00:27,370
It's like, you now, this thing minus that
times the inverse of that times that.

1258
01:00:27,370 --> 01:00:28,730
OK.
Or other way around.

1259
01:00:28,730 --> 01:00:30,250
Some, that's that's Schur complement.

1260
01:00:30,250 --> 01:00:32,400
OK?
And you get this.

1261
01:00:32,400 --> 01:00:38,650
Now, then you solve for ax, and you can,
once you get y, you get x, okay?

1262
01:00:38,650 --> 01:00:41,480
And you can actually write this out, just
purely mathematically, and

1263
01:00:41,480 --> 01:00:42,390
it comes out to be this.

1264
01:00:42,390 --> 01:00:47,550
A plus b, c inverse, is a inverse minus,
and then this big old thing over here.

1265
01:00:47,550 --> 01:00:51,450
Okay, it's a very thing, it's got all
sorts of names.

1266
01:00:51,450 --> 01:00:53,420
Matrix inversion lemma I think is a nice
one,

1267
01:00:53,420 --> 01:00:57,560
and you'll just do a quick example and
then, and then we'll quit for today.

1268
01:00:57,560 --> 01:00:59,620
So if A were diagonal and B and

1269
01:00:59,620 --> 01:01:03,410
C were debts then that's a beautiful
example, right?

1270
01:01:03,410 --> 01:01:07,720
So that's called a diagonal plus low rank
Matrix, right?

1271
01:01:07,720 --> 01:01:11,370
Perfect, perfect example of this
structure, right?

1272
01:01:11,370 --> 01:01:16,040
Then it says you can actually solve it in
here if you,

1273
01:01:16,040 --> 01:01:18,630
you do the dumb method it's like n cubed.

1274
01:01:18,630 --> 01:01:21,100
If you'd used this method which is an
un-elimination and

1275
01:01:21,100 --> 01:01:26,610
an elimination it actually ends up it goes
from cubic in n to linear in n.

1276
01:01:26,610 --> 01:01:27,350
OK?

1277
01:01:27,350 --> 01:01:30,410
So my claim is, you just,

1278
01:01:30,410 --> 01:01:34,270
these are not small differences, right,
these are not theoretical things, right?

1279
01:01:34,270 --> 01:01:38,640
The difference between knowing this and
not knowing it means you can look at

1280
01:01:38,640 --> 01:01:41,460
something, and it could be a million by
million set of equations.

1281
01:01:41,460 --> 01:01:44,450
But if it's diagonal plus low right, now
you know.

1282
01:01:45,790 --> 01:01:49,570
I can solve that in probably in like
sub-millisecond.

1283
01:01:49,570 --> 01:01:50,600
But you have to know what you're doing.
