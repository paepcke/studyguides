1
00:00:00,012 --> 00:00:03,010
So now we'll move on to another section.

2
00:00:03,010 --> 00:00:06,078
It's going to be a whirlwind tour of some
interesting problems in

3
00:00:06,078 --> 00:00:08,335
statistical estimation.

4
00:00:08,335 --> 00:00:12,465
As with these other lectures, there's
actually much more in the book.

5
00:00:12,465 --> 00:00:14,470
We expect you to read all of it.

6
00:00:14,470 --> 00:00:15,452
Skim it all.

7
00:00:15,452 --> 00:00:17,877
Homework will cover other things.

8
00:00:17,877 --> 00:00:22,677
But we'll look at [COUGH] some of the big,
big picture questions here, and

9
00:00:22,677 --> 00:00:27,095
we'll look at some interesting examples.

10
00:00:27,095 --> 00:00:33,832
So we'll start with the idea of of
parametric distribution estimation.

11
00:00:33,832 --> 00:00:38,607
So, and I mean this is at the absolute
core of statistics.

12
00:00:38,607 --> 00:00:42,949
so, with, so, so if you have, I mean, well
there's several.

13
00:00:42,949 --> 00:00:47,293
We won't look at the non-parametric case,
we'll look at the parametric case.

14
00:00:47,293 --> 00:00:49,658
The, the parametric case is this, is that
you have a,

15
00:00:49,658 --> 00:00:52,431
you have a observed a variable.

16
00:00:52,431 --> 00:00:54,401
An outcome, of a random variable.

17
00:00:54,401 --> 00:00:57,369
But it came, you don't know which
distribution it came from, and

18
00:00:57,369 --> 00:01:00,677
you have a parameterized set of
distributions.

19
00:01:00,677 --> 00:01:03,207
So this is kind of the basic starting
point in standard,

20
00:01:03,207 --> 00:01:06,092
classical frequentist statistics.

21
00:01:06,092 --> 00:01:07,423
So, this is the idea.

22
00:01:07,423 --> 00:01:10,303
So, you have a parameterized family of
distributions and

23
00:01:10,303 --> 00:01:14,544
it's paramaterized well the parameter
we'll call x, okay?

24
00:01:14,544 --> 00:01:19,493
So, yeah ,this notation does not exactly
conform to standard statistics notation.

25
00:01:19,493 --> 00:01:22,785
I guess it'll be theta or something like
this, right?

26
00:01:22,785 --> 00:01:24,481
But that's fine, okay.

27
00:01:24,481 --> 00:01:27,286
Now, an absolutely standard method for

28
00:01:27,286 --> 00:01:33,804
estimating which distribution generated
the actual observed outcome y.

29
00:01:33,804 --> 00:01:37,884
Is maximum likelihood estimation, and what
that says is,

30
00:01:37,884 --> 00:01:42,632
you take, you take your observation,
that's y.

31
00:01:42,632 --> 00:01:45,590
You plug this into the density, it
doesn't, I mean it can also be,a,

32
00:01:45,590 --> 00:01:49,870
a discrete probability but density,
whatever you like, right, here.

33
00:01:49,870 --> 00:01:53,209
And you consider this function as a
function of the parameter, the just,

34
00:01:53,209 --> 00:01:56,900
that parameterizes the family of
distribution, right?

35
00:01:56,900 --> 00:01:59,561
And that, that's called the likelihood
function.

36
00:01:59,561 --> 00:02:00,621
And in fact, we'll take,

37
00:02:00,621 --> 00:02:04,386
usually people take a log it doesn't
matter if you're maximizing.

38
00:02:04,386 --> 00:02:06,570
And so, if you choose x by solving this
problem,

39
00:02:06,570 --> 00:02:09,613
that's maximum likelihood estimation,
right?

40
00:02:09,613 --> 00:02:12,917
So, and if you take a statistics course
you'll hear all about why this is

41
00:02:12,917 --> 00:02:16,165
a good idea, and, you know, in some cases
asymptotically optimal, and

42
00:02:16,165 --> 00:02:19,177
all that kind of stuff.

43
00:02:19,177 --> 00:02:22,422
so, this is, l of x, that's going to be
the log likelihood function, and

44
00:02:22,422 --> 00:02:25,307
you can add constraints on, on that.

45
00:02:25,307 --> 00:02:28,277
And a lot of times, they're implicit, you
know, for example x,

46
00:02:28,277 --> 00:02:32,903
the parameter might include something like
a covariance matrix, right?

47
00:02:32,903 --> 00:02:35,871
Because your, your, for example, your
family might be Gaussians or

48
00:02:35,871 --> 00:02:37,571
something like that.

49
00:02:37,571 --> 00:02:41,904
And so the mean and covariance might be
the parameter that, that characterizes it.

50
00:02:41,904 --> 00:02:45,508
And there of course, the parameter that
corresponds to the covariance obviously

51
00:02:45,508 --> 00:02:48,015
has to be positive definite, right?

52
00:02:48,015 --> 00:02:50,741
So these could, and these could be then
what you do is,

53
00:02:50,741 --> 00:02:54,651
that's, that's the domain of the
likelihood function.

54
00:02:54,651 --> 00:02:56,391
And you would simply make some, one,

55
00:02:56,391 --> 00:02:59,772
one simple way is to say that this is
zero, there.

56
00:02:59,772 --> 00:03:03,064
Because then log of it is minus infinity,
right?

57
00:03:03,064 --> 00:03:07,485
So, and that's infinitely bad if you're
maximizing, right, so that's one.

58
00:03:07,485 --> 00:03:12,139
Okay, now, the maximum likelihood
estimation problem, that's convex.

59
00:03:12,139 --> 00:03:14,462
If the log likelihood is concave.

60
00:03:14,462 --> 00:03:16,812
That, but this is, by the way this is
different from a log con,

61
00:03:16,812 --> 00:03:20,262
log concave distribution which we've
looked at before.

62
00:03:20,262 --> 00:03:20,873
This is, this is

63
00:03:20,873 --> 00:03:24,512
log concave in the parameters that
describe the family of distributions.

64
00:03:24,512 --> 00:03:27,795
Now if you know about exponential
families, then these two coincide.

65
00:03:27,795 --> 00:03:29,816
Because it's actually only an inner,
they're both,

66
00:03:29,816 --> 00:03:34,080
involve in an exponent of an inner
product, and then a normalizing constant.

67
00:03:34,080 --> 00:03:35,942
So if, if you know what I'm talking about
fine,

68
00:03:35,942 --> 00:03:38,488
if you don't, don't worry about it, okay?

69
00:03:38,488 --> 00:03:42,092
But it's a different concept, the concept
here is, is the log likelihood function

70
00:03:42,092 --> 00:03:45,921
concave, in the parameters that describe
the distribution.

71
00:03:45,921 --> 00:03:48,313
Actually something very interesting's
going to happen,

72
00:03:48,313 --> 00:03:50,081
in a lot of cases you'll have the, you'll,

73
00:03:50,081 --> 00:03:55,212
you'll sort of have what you think are the
natural parameters for distribution.

74
00:03:55,212 --> 00:03:58,687
And sure enough, it will be log concave in
those perameters.

75
00:03:58,687 --> 00:04:01,520
We're going to see examples very shortly,
right?

76
00:04:01,520 --> 00:04:05,837
But actually, in other cases, something
really cool is going to happen.

77
00:04:05,837 --> 00:04:08,051
You'll look at the, the log likelihood
function,

78
00:04:08,051 --> 00:04:11,920
as a function of what you though were the
natural perameters.

79
00:04:11,920 --> 00:04:15,082
And guess what, it won't be concave, it
just won't be.

80
00:04:15,082 --> 00:04:16,142
Here's an example.

81
00:04:16,142 --> 00:04:20,622
Let's estimate the mean and covariance of
a Gaussian, that generated, a sample.

82
00:04:20,622 --> 00:04:21,887
What could be simpler, 'kay?

83
00:04:21,887 --> 00:04:26,162
Well does that log likelihood function in
sigma, and mu, it's not concave.

84
00:04:26,162 --> 00:04:30,237
But it's going to turn out, you just do a
change of variables, and it will be.

85
00:04:30,237 --> 00:04:33,735
Now, if you, have had, you know, enough
machine learning and statistics You'll

86
00:04:33,735 --> 00:04:37,895
understand that the correct, it, it will
not be a shock to you.

87
00:04:37,895 --> 00:04:42,551
That the correct parameters there are like
sigma inverse, and sigma inverse mu, okay?

88
00:04:42,551 --> 00:04:44,655
These are actually more natural, right?

89
00:04:44,655 --> 00:04:45,624
Than sigma, and mu, and

90
00:04:45,624 --> 00:04:49,836
again, I'm, I'm, this is a bit more
advanced than I want to be at this point.

91
00:04:49,836 --> 00:04:53,900
But I just, just to mention to those
people who know about those things, right?

92
00:04:53,900 --> 00:04:58,189
And it turns out those are precisely the
parameters in an, in an exponential model.

93
00:04:58,189 --> 00:04:59,032
So, okay.

94
00:04:59,032 --> 00:05:01,776
Okay, so, oh and I should say that all of
this material, I, I,

95
00:05:01,776 --> 00:05:06,665
you know, your, our goal here is just to
go over some interesting things.

96
00:05:06,665 --> 00:05:10,025
If you've never seen this stuff before,
some of this stuff we're going to be

97
00:05:10,025 --> 00:05:14,681
talking about is going to be super simple
and completely accessable.

98
00:05:14,681 --> 00:05:17,871
But if you've had you know, some PhD level
courses in statistics or

99
00:05:17,871 --> 00:05:22,484
machine learning, then there'll be a
little bit added stuff.

100
00:05:22,484 --> 00:05:24,920
And I'm going to mention of that stuff
like I just did,

101
00:05:24,920 --> 00:05:27,951
some of the more advanced stuff for fun.

102
00:05:27,951 --> 00:05:31,023
Okay, so let's start with a very simple
setup.

103
00:05:31,023 --> 00:05:33,250
Let's do linear measurement models.

104
00:05:33,250 --> 00:05:35,846
So here x is a vector of unknown
parameters and

105
00:05:35,846 --> 00:05:38,264
we have linear measurements.

106
00:05:38,264 --> 00:05:43,631
So, we have yi, is a, its a scalar, its a
linear function of x plus a noise, right?

107
00:05:43,631 --> 00:05:46,271
And so and by the way this is a good point
to stop and

108
00:05:46,271 --> 00:05:51,803
talk about sort of just, I mean its, it
doesn't make any difference at all.

109
00:05:51,803 --> 00:05:56,270
But sort of the philosophical way you
would regard the set up, right?

110
00:05:56,270 --> 00:06:00,113
So if you are in something like signal
processing or something like that you

111
00:06:00,113 --> 00:06:04,899
would say, this says, I mean your view of
the world is you know.

112
00:06:04,899 --> 00:06:09,256
This gives you the ideal measurement if
your measurements were perfect.

113
00:06:09,256 --> 00:06:13,219
They're not perfect and vi is sort of like
an added noise, right?

114
00:06:13,219 --> 00:06:14,930
So this is your corrupted or real or

115
00:06:14,930 --> 00:06:18,182
whatever you like to call it measurement,
okay?

116
00:06:18,182 --> 00:06:21,545
And that would be the view from someone
who does signal processing or

117
00:06:21,545 --> 00:06:23,970
something like that, right?

118
00:06:23,970 --> 00:06:27,097
If you were to, if you were to do
something like get a perfect sensor, and

119
00:06:27,097 --> 00:06:31,732
run this experiment at like you know, 0 or
0.01 degree Kelvin, right?

120
00:06:31,732 --> 00:06:35,296
So there's no thermal noise or whatever
else that comes from.

121
00:06:35,296 --> 00:06:38,693
Then in fact, you get yi equals ai
transposed x, right?

122
00:06:38,693 --> 00:06:42,510
You don't, since you don't do those
things, vi is a noise, right?

123
00:06:42,510 --> 00:06:45,014
So a statistician would say the following.

124
00:06:45,014 --> 00:06:46,312
They would look at that and

125
00:06:46,312 --> 00:06:50,134
they would say, x is a y comes from a
distribution, right?

126
00:06:50,134 --> 00:06:53,662
It's a distribution, because that's a
random variable right there, the vi,

127
00:06:53,662 --> 00:06:55,941
those are random variables.

128
00:06:55,941 --> 00:06:59,667
They'd say so y comes from a distribution
and you'd say well what distribution does

129
00:06:59,667 --> 00:07:03,883
it come from, and the answer is well it
depends on what x is.

130
00:07:03,883 --> 00:07:05,632
So, x is here is a parameter that descri,

131
00:07:05,632 --> 00:07:08,995
that tells you which distribution
generated y, right?

132
00:07:08,995 --> 00:07:11,235
So I dunno if this is making, at the end
of the day,

133
00:07:11,235 --> 00:07:14,600
you end up doing the same things with
this.

134
00:07:14,600 --> 00:07:17,288
But it is actually just interesting to sit
for a minute and

135
00:07:17,288 --> 00:07:21,921
think about the different views of what y
equals ax plus v can mean.

136
00:07:21,921 --> 00:07:22,985
Okay, so, alright, so,

137
00:07:22,985 --> 00:07:27,193
there's nothing deep in this, I'm just
saying these are the two views.

138
00:07:27,193 --> 00:07:30,508
Okay, now, what you, what you do here is,
is this is y has and

139
00:07:30,508 --> 00:07:36,423
I am going to assume that the vi or rid
here with some density p of z.

140
00:07:36,423 --> 00:07:40,251
If you, if you if x has the value of x or
if the parameter has the value of x

141
00:07:40,251 --> 00:07:45,161
then the distribution the density of y is
this, right?

142
00:07:45,161 --> 00:07:48,086
Its the product because they are
independent and, and

143
00:07:48,086 --> 00:07:52,851
yi minus ai transpose x, is, well that's
exactly vi, right?

144
00:07:52,851 --> 00:07:56,145
So, and then, that, these, that, that's
the value that vi had, and

145
00:07:56,145 --> 00:08:00,356
you simply take the product of the, of the
densities.

146
00:08:00,356 --> 00:08:05,616
Because it's independent, to get the, the
density of of, of y, the vector, right?

147
00:08:05,616 --> 00:08:07,568
So that's, that's the idea, by the way,

148
00:08:07,568 --> 00:08:11,792
other people would call, something like
this, the residual.

149
00:08:11,792 --> 00:08:15,971
And the residual here, is in fact nothing
but the noise, right?

150
00:08:15,971 --> 00:08:19,912
So that's another way to, to, to think of
to think of this.

151
00:08:19,912 --> 00:08:22,272
Okay, so the maximum likelihood solution
says,

152
00:08:22,272 --> 00:08:25,910
here if you want to estimate x by maximum
likelihood.

153
00:08:25,910 --> 00:08:27,975
What you do, is we'll form, the, we'll
max,

154
00:08:27,975 --> 00:08:32,460
we'll maximize the log of this product,
but that's the sum right?

155
00:08:32,460 --> 00:08:37,366
And this tells, gives you a hint as to why
the log is going to come up often, right?

156
00:08:37,366 --> 00:08:41,710
So, you maximize the sum of the log of the
density of the residual, okay?

157
00:08:41,710 --> 00:08:43,570
Now, by the way, if the residuals are,

158
00:08:43,570 --> 00:08:48,064
have a density that, that is sort of
centered around zero, right?

159
00:08:48,064 --> 00:08:51,175
And, and it'd be kind of, if they don't by
the way that means that it's sort of

160
00:08:51,175 --> 00:08:54,286
a known offset and it's a bit silly,
right?

161
00:08:54,286 --> 00:08:55,794
But if, if they do what that says is,

162
00:08:55,794 --> 00:08:59,111
you know, these are functions that are
kind of centered.

163
00:08:59,111 --> 00:09:03,216
This function is sort of centered around
zero and then it kind of goes off, right?

164
00:09:03,216 --> 00:09:06,081
It, it's a concave function that tails off
after that.

165
00:09:06,081 --> 00:09:10,022
That's actually quite interesting because
this is, this is actually a penalty.

166
00:09:10,022 --> 00:09:12,450
This is penalty approximation, right?

167
00:09:12,450 --> 00:09:14,712
Is exactly minimizing a sum, of a
function, of,

168
00:09:14,712 --> 00:09:17,745
which which we previously called
penalties.

169
00:09:17,745 --> 00:09:21,393
It's a penalty approximation method, but
here the penalty is specifically,

170
00:09:21,393 --> 00:09:26,552
the negative log likelihood, if you
convert this to a minimization problem.

171
00:09:26,552 --> 00:09:27,492
Already got this?

172
00:09:27,492 --> 00:09:30,891
So, so in other words where you might just
say that's the penalty.

173
00:09:30,891 --> 00:09:34,359
And then give a story about your penalty
function in terms of how much it irritates

174
00:09:34,359 --> 00:09:38,971
you to have a big residual, small
residual, you know medium size residual.

175
00:09:38,971 --> 00:09:41,878
All of this kind of stuff, negative,
positive residual that would be

176
00:09:41,878 --> 00:09:45,364
your discussion of why you are using a
certain penalty.

177
00:09:45,364 --> 00:09:49,036
It says that all of that can be transposed
perfectly to a statistics framework and

178
00:09:49,036 --> 00:09:51,682
there instead of saying something like, it
irritates me to

179
00:09:51,682 --> 00:09:57,125
have a positive residual is more
irritating than a negative residual.

180
00:09:57,125 --> 00:10:00,041
What you're really saying is something
about the distribution,

181
00:10:00,041 --> 00:10:02,196
the density of a noise, okay?

182
00:10:02,196 --> 00:10:04,043
So, let's look at some examples.

183
00:10:04,043 --> 00:10:06,708
Here's one, suppose these noises are
Gaussian.

184
00:10:06,708 --> 00:10:10,549
Well then, if I take the negative log
likelihood, I get this, right?

185
00:10:10,549 --> 00:10:13,922
So that, that, that, sorry that's the log
likelihood.

186
00:10:13,922 --> 00:10:17,891
And it's this, that's a constant, and when
I'm maximizing over x, it doesn't matter.

187
00:10:17,891 --> 00:10:20,156
Here, this constant's totally irrelevant.

188
00:10:20,156 --> 00:10:22,882
And I'm maximizing this thing, well
there's a negative sign there, so

189
00:10:22,882 --> 00:10:25,907
I can just minimize this thing, that
doesn't matter.

190
00:10:25,907 --> 00:10:27,918
That, because it's a positive constant.

191
00:10:27,918 --> 00:10:30,882
And so I minimize, hey, that's just least
squares, okay?

192
00:10:30,882 --> 00:10:33,789
So, now you know, least squares is
nothing, but it is exactly maximum

193
00:10:33,789 --> 00:10:38,427
likelihood estimation under the assumption
that the noises are Gaussian.

194
00:10:38,427 --> 00:10:40,927
If you say, you have a square, and you say
what does that mean,

195
00:10:40,927 --> 00:10:45,383
you say well it means if a residual is
small, I mean that's fine with me.

196
00:10:45,383 --> 00:10:49,187
Because it's sort of small squared, but if
it's big that's not so good.

197
00:10:49,187 --> 00:10:50,012
What this says is,

198
00:10:50,012 --> 00:10:54,096
that corresponds exactly to a statistical
model, with Gaussian noise.

199
00:10:54,096 --> 00:10:55,911
Because if you have Gaussian noise, and

200
00:10:55,911 --> 00:10:58,441
you say well, if something's less than
like one sigma or

201
00:10:58,441 --> 00:11:03,924
something, you'd say, fine, okay, nope,
that's, that's entirely plausible, right?

202
00:11:03,924 --> 00:11:07,629
Whereas if something were like six sigma
or eight sigma, you'd look at that and

203
00:11:07,629 --> 00:11:12,427
you'd say, hmm, that's, that's I mean, I
can't say it's impossible.

204
00:11:12,427 --> 00:11:16,347
But it seems very strange, that, that my
fit here, or my estimation, is telling me

205
00:11:16,347 --> 00:11:21,231
that the parameter, that what I've
observed was an eight sima event.

206
00:11:21,231 --> 00:11:23,386
Everybody see what I'm saying here?

207
00:11:23,386 --> 00:11:25,732
So, okay, let's look at Laplacian noise.

208
00:11:25,732 --> 00:11:30,025
Now that's noise with a density that is a
double-sided exponential like that.

209
00:11:30,025 --> 00:11:34,217
And it's actually a little bit interesting
to look at that distribution.

210
00:11:34,217 --> 00:11:36,857
Compared to a Gaussian, what can you say
about the tails of

211
00:11:36,857 --> 00:11:40,122
a Laplacian distribution compared to
Gaussian?

212
00:11:40,122 --> 00:11:41,327
Much heavier, right?

213
00:11:41,327 --> 00:11:46,272
Because this is like, e to the minus you
know, z squared, this is e to the minus z.

214
00:11:46,272 --> 00:11:48,347
Okay, so it's way bigger tails.

215
00:11:48,347 --> 00:11:51,496
What that says is that the likelihood of a
big outlier is,

216
00:11:51,496 --> 00:11:54,673
well I mean it's going down, right?

217
00:11:54,673 --> 00:11:56,140
It's e to the minus z.

218
00:11:56,140 --> 00:12:00,443
But it's way bigger than for a Gaussian,
everybody following this?

219
00:12:00,443 --> 00:12:04,010
And then, this is a bit of a weird thing
here, right?

220
00:12:04,010 --> 00:12:05,832
This, you have that point there.

221
00:12:05,832 --> 00:12:09,800
And it even says that something like this,
I mean that, that's going to tell you,

222
00:12:09,800 --> 00:12:14,697
that, the probability, of having, of being
a little bit off.

223
00:12:14,697 --> 00:12:18,457
Actually is substantially reduced from
being right at zero.

224
00:12:18,457 --> 00:12:19,276
Everybody, and

225
00:12:19,276 --> 00:12:24,100
that, that's comes now just from
probability from that, kink, right there.

226
00:12:24,100 --> 00:12:26,962
Okay, so, this turns into, guess what, L1.

227
00:12:26,962 --> 00:12:28,745
And it makes perfect sense.

228
00:12:28,745 --> 00:12:32,903
Because now if you look at L1 you get both
of those things about what happends for

229
00:12:32,903 --> 00:12:36,278
large residuals and small residuals.

230
00:12:36,278 --> 00:12:38,938
For large residuals L1 charges you
linearly,

231
00:12:38,938 --> 00:12:42,905
where as least squares charges you
quadratically.

232
00:12:42,905 --> 00:12:46,285
So, it's not like you like, it's not like
you expect to have large residuals but

233
00:12:46,285 --> 00:12:50,422
you don't go nuts as if it were quadratic
or worse, right?

234
00:12:50,422 --> 00:12:53,662
And, at the, then you focus on small
things, small residuals, and

235
00:12:53,662 --> 00:12:55,642
this says, it actually keeps up the, it,

236
00:12:55,642 --> 00:13:00,626
it keeps up the incentive to drive a
residual to zero, right?

237
00:13:00,626 --> 00:13:03,154
And you can see that exactly here, right?

238
00:13:03,154 --> 00:13:06,336
So, so it's actually kind of interesting
that both,

239
00:13:06,336 --> 00:13:11,580
both of these could be it can be, you can
interpret both.

240
00:13:11,580 --> 00:13:15,480
But as from an intuitive description of a
penalty function, abstract penalty

241
00:13:15,480 --> 00:13:19,620
function or from the probability, the
implied probably distribution of the resi,

242
00:13:19,620 --> 00:13:24,332
of, of, of the noise in the additive noise
model.

243
00:13:24,332 --> 00:13:26,464
Let's do Uniform noise, right?

244
00:13:26,464 --> 00:13:30,928
Well, if you have Uniform noise then the
log likelihood function is well, it's,

245
00:13:30,928 --> 00:13:34,009
it's actually just a constant.

246
00:13:34,009 --> 00:13:37,915
And if basically it's constant, doesn't
matter what constant it is,

247
00:13:37,915 --> 00:13:43,439
actually provided you are consistent with
with the measurement, right?

248
00:13:43,439 --> 00:13:48,639
And so, there, you simply solve a
feasibility problem and, and

249
00:13:48,639 --> 00:13:52,839
their multiple maximum likelihood
solutions and

250
00:13:52,839 --> 00:14:00,385
you simply choose one that satisfies this,
this condition.

251
00:14:00,385 --> 00:14:05,602
And all of those are maximum likelihood
estimates, right?

252
00:14:05,602 --> 00:14:08,814
By the way, there's statistical properties
where maximum likelihood has been used for

253
00:14:08,814 --> 00:14:11,718
maybe like a 100 years or something like
that now.

254
00:14:11,718 --> 00:14:14,622
Is, most of that theory assumes things
like, that this log likelihood function

255
00:14:14,622 --> 00:14:17,845
has a, a unique maximum, or something like
that, roughly.

256
00:14:17,845 --> 00:14:20,465
Right, so, and that's not the case here.

257
00:14:20,465 --> 00:14:24,125
Okay, oh, let's, let me mention just one
other thing.

258
00:14:24,125 --> 00:14:27,236
So here what this says is that, you can
actually go backwards,

259
00:14:27,236 --> 00:14:32,040
you can start from probability
distribution and find the penalty.

260
00:14:32,040 --> 00:14:35,526
Because you just take the negative log of
the distribution.

261
00:14:35,526 --> 00:14:37,394
But you can also go backwards, right?

262
00:14:37,394 --> 00:14:38,738
So for example if, if you decided,

263
00:14:38,738 --> 00:14:41,666
if you discussed with someone in some
fitting problem what they want, and

264
00:14:41,666 --> 00:14:46,060
it's determined that what they want is
something that looks like this.

265
00:14:46,060 --> 00:14:48,826
You know, that's quadratic and then this
rises steeply.

266
00:14:48,826 --> 00:14:51,692
You know, I don't know how this would come
up but it would.

267
00:14:51,692 --> 00:14:55,587
You'd talk, you'd talk to them, you'd say
what do you feel about under fitting?

268
00:14:55,587 --> 00:14:57,770
And they'd say oh that, that's, that's
bad.

269
00:14:57,770 --> 00:14:59,342
That is not good Right?

270
00:14:59,342 --> 00:15:02,572
And you'd say, how about if you underfit
just a little bit?

271
00:15:02,572 --> 00:15:04,707
No, that's, that's, that is not good.

272
00:15:04,707 --> 00:15:07,612
And they'd say, but what about if you
underfit a lot?

273
00:15:07,612 --> 00:15:11,163
And you'd say look, because the data's got
weird outliers and stuff like that, if

274
00:15:11,163 --> 00:15:16,462
there has to be, if I have to underfit on
a couple of my, my points, it, so be it.

275
00:15:16,462 --> 00:15:18,499
I don't want to go nuts over it, 'kay?

276
00:15:18,499 --> 00:15:20,652
And that discussion, generates that.

277
00:15:20,652 --> 00:15:22,416
And then you'd say, what if you over fit
and you say,

278
00:15:22,416 --> 00:15:26,550
well the fact is if you over fit a little
bi, it doesn't make any difference at all.

279
00:15:26,550 --> 00:15:28,398
And you'd say, well what about over
fitting more, and

280
00:15:28,398 --> 00:15:30,537
then they would say well, I don't know.

281
00:15:30,537 --> 00:15:32,117
So anyway, anyway, doesn't matter.

282
00:15:32,117 --> 00:15:33,910
You end up with a story like this, right?

283
00:15:33,910 --> 00:15:34,842
We all know this now.

284
00:15:34,842 --> 00:15:36,522
You can translate a little of the story
and

285
00:15:36,522 --> 00:15:39,929
description into this, okay, what this
says is the following.

286
00:15:39,929 --> 00:15:43,908
It says, then if a statistician walks into
the room, and you say what are you doing?

287
00:15:43,908 --> 00:15:46,916
And they don't want to hear a story about
your feelings about, overfitting and

288
00:15:46,916 --> 00:15:50,361
underfitting and things like, they don't
want to hear that.

289
00:15:50,361 --> 00:15:51,417
So, what you say to them is,

290
00:15:51,417 --> 00:15:55,146
you say, oh excuse me, I'm doing maximum
likelihood estimation, right?

291
00:15:55,146 --> 00:15:58,299
This implies a distribution, and the
distribution is very simple.

292
00:15:58,299 --> 00:16:01,770
It's e to the minus this, of course you
have to normalize it but that's it.

293
00:16:01,770 --> 00:16:05,078
So you'd say to them, you say I'm doing
maximum likeliest estimation.

294
00:16:05,078 --> 00:16:07,392
And for this problem, we can even say what
it is.

295
00:16:07,392 --> 00:16:09,403
How would you describe this distribution?

296
00:16:09,403 --> 00:16:11,514
Somebody describe it to me, I mean in
words.

297
00:16:11,514 --> 00:16:14,263
So it's some weird Frankenstein
distribution on the left.

298
00:16:14,263 --> 00:16:16,197
It's got some weird exponential, right?

299
00:16:16,197 --> 00:16:17,596
On the right, it's Gaussian.

300
00:16:17,596 --> 00:16:19,822
And in the middle it's got some uniform
phase.

301
00:16:19,822 --> 00:16:21,729
Now they would think you were weird.

302
00:16:21,729 --> 00:16:24,718
But you'd, then you'd turn to the
statistician, and say, no, no, no, we've

303
00:16:24,718 --> 00:16:29,546
been, we've been doing this for years, and
we know, this, this, it looks like this.

304
00:16:29,546 --> 00:16:31,410
Okay, everybody, everybody got this?

305
00:16:31,410 --> 00:16:34,242
So the, it's actually very interesting
that you can justify these from

306
00:16:34,242 --> 00:16:36,482
different points of view.

307
00:16:36,482 --> 00:16:38,987
And you get, you end up with the same
conclusion.

308
00:16:38,987 --> 00:16:41,143
Okay, so, alright, now these things here,
by the way,

309
00:16:41,143 --> 00:16:43,458
are things that are kind of obvious.

310
00:16:43,458 --> 00:16:45,962
These are things we talked about in
approximation.

311
00:16:45,962 --> 00:16:49,392
Linear approximation, they're kind of, in
some ways they are penalty problems with

312
00:16:49,392 --> 00:16:53,231
an affine function, you know, ax minus b
or something like that.

313
00:16:53,231 --> 00:16:55,616
I mean that's what this is, but actually
what's interesting is,

314
00:16:55,616 --> 00:16:59,682
in maximum likelihood, you end up with all
sorts of interesting things.

315
00:16:59,682 --> 00:17:02,622
Where you do not end up with a, an affine
function, a norm, or

316
00:17:02,622 --> 00:17:06,186
a penalty function of an affine function.

317
00:17:06,186 --> 00:17:08,586
And so these are actually quite
interesting, so

318
00:17:08,586 --> 00:17:11,033
we'll look at a couple of these.

319
00:17:11,033 --> 00:17:13,151
So, first is logistic regression.

320
00:17:13,151 --> 00:17:15,734
So the I, so this is a distribution
actually on, so

321
00:17:15,734 --> 00:17:19,279
the outcomes are zero, one, Boolean
outcomes.

322
00:17:19,279 --> 00:17:23,749
And the probability, that you get a one is
the logistic function.

323
00:17:23,749 --> 00:17:27,592
So it's, it's e to the a transpose u plus
b divided by one plus a transpose u

324
00:17:27,592 --> 00:17:29,242
plus b.

325
00:17:29,242 --> 00:17:32,997
And the probability that y equals 0 is 1
minus this, right?

326
00:17:32,997 --> 00:17:38,065
Which in, of course, is one over one plus
e to the, a transpose u plus b, okay?

327
00:17:38,065 --> 00:17:39,342
So that's the idea.

328
00:17:39,342 --> 00:17:42,439
Now here, the idea is that u, are here.

329
00:17:42,439 --> 00:17:46,105
These are observable explanatory
variables, right?

330
00:17:46,105 --> 00:17:50,263
So I mean, I can tell you all sorts of
things about them, this would be,

331
00:17:50,263 --> 00:17:56,017
let's say a patient made it to a hospital
eventually develops sepsis.

332
00:17:56,017 --> 00:17:58,921
That, that's what we're looking at, okay?

333
00:17:58,921 --> 00:18:03,958
So, and then you, here would be all sorts
of medically relevant variables, right?

334
00:18:03,958 --> 00:18:07,026
Could, could be basic that you know also
to basic things were is

335
00:18:07,026 --> 00:18:09,228
scores given to them.

336
00:18:09,228 --> 00:18:10,794
Various anyway, it doesn't,

337
00:18:10,794 --> 00:18:14,580
doesn't matter that's the whole bunch of
things like that.

338
00:18:14,580 --> 00:18:19,269
By the way somebody's could be Boolean to,
that would be very often a case, right?

339
00:18:19,269 --> 00:18:22,071
So, it be so called categorical variables,
right?

340
00:18:22,071 --> 00:18:25,833
And so model like this and this model is
instantiated by fixing A and B

341
00:18:25,833 --> 00:18:31,405
then gives you a model of the probability
someone developing Sepsis, right?

342
00:18:31,405 --> 00:18:34,315
When they're admitted to the hospital,
right?

343
00:18:34,315 --> 00:18:37,835
So that, because you just simply take
their, their u, you evaluate this

344
00:18:37,835 --> 00:18:44,107
function, and, if it is 0.05, that's good,
and if it's 0.87, that's not good, okay?

345
00:18:44,107 --> 00:18:45,304
So that's the idea.

346
00:18:45,304 --> 00:18:49,458
All right, so the estimation problem is to
estimate a, these parameters a and

347
00:18:49,458 --> 00:18:52,131
b from observations, right?

348
00:18:52,131 --> 00:18:56,277
So that's, that's, that would be your,
that's what we're going to do.

349
00:18:56,277 --> 00:18:59,700
So we're going to work out the log
likelihood function.

350
00:18:59,700 --> 00:19:01,940
and, and then we going to maximize it,
right and it,

351
00:19:01,940 --> 00:19:06,667
I guess it wouldn't be in this class if it
didn't turn out to be concave.

352
00:19:06,667 --> 00:19:09,637
So, lets, okay, so lets take a look at
that so what we'll do is so

353
00:19:09,637 --> 00:19:13,410
you, you've been given a bunch of
observations.

354
00:19:13,410 --> 00:19:16,960
And basically what these are, these are
previous patients, right?

355
00:19:16,960 --> 00:19:19,557
Could be from the last year, from the
region, doesn't matter, so

356
00:19:19,557 --> 00:19:22,168
you just get a whole bunch of data, okay?

357
00:19:22,168 --> 00:19:24,270
And the y's are, are just zero one, right?

358
00:19:24,270 --> 00:19:27,744
So I guess you, you'd call these maybe
labels in, in machine learning.

359
00:19:27,744 --> 00:19:30,480
Alright, so what does, we'll just make it
simple and reorder them, so

360
00:19:30,480 --> 00:19:33,909
that all the ones that actually develop
sepsis are first.

361
00:19:33,909 --> 00:19:36,376
That's k of them, and all the rest did
not, right?

362
00:19:36,376 --> 00:19:37,642
Just for simplicity.

363
00:19:37,642 --> 00:19:41,674
Then, the log likelihood function well its
very simple its simply the, you,

364
00:19:41,674 --> 00:19:45,867
you take this for the, over the ones were
y was one.

365
00:19:45,867 --> 00:19:48,758
So that's, that's, that's these terms,
right?

366
00:19:48,758 --> 00:19:49,598
And then for, for

367
00:19:49,598 --> 00:19:54,291
the case were y was zero that's one minus
this which is exactly that.

368
00:19:54,291 --> 00:19:58,003
And so that, that's the log likelihood
function and this is a function of a and

369
00:19:58,003 --> 00:19:59,453
b, okay?

370
00:19:59,453 --> 00:20:04,278
Well, if you worked what this is well you
can see all sorts of bits.

371
00:20:04,278 --> 00:20:06,330
This is log of the probability of x.

372
00:20:06,330 --> 00:20:08,357
But that's just this thing, right?

373
00:20:08,357 --> 00:20:10,093
And then you get minus, and then here,

374
00:20:10,093 --> 00:20:14,966
there's this thing on the bottom, you
just, you sum over, over the all of it.

375
00:20:14,966 --> 00:20:19,032
That's log sum x, but we all know, we all
recognize log sum x is convex.

376
00:20:19,032 --> 00:20:23,476
And so, now we do this in, in discipline
convex programming or CVX style.

377
00:20:23,476 --> 00:20:27,340
We just walk from inside of the Expression
Tree out, and

378
00:20:27,340 --> 00:20:34,262
you say that's affine of, by the way you
don't then say x is convex.

379
00:20:34,262 --> 00:20:36,725
That would be a false move.

380
00:20:36,725 --> 00:20:41,702
You recognize log of one plus x as log of
e to the zero plus e to the a transpose u

381
00:20:41,702 --> 00:20:44,202
plus bi.

382
00:20:44,202 --> 00:20:48,367
And you say log sum x is convex, log sum x
of affine is convex,

383
00:20:48,367 --> 00:20:52,321
sum convex minus concave, done.

384
00:20:52,321 --> 00:20:53,945
Everybody got this?

385
00:20:53,945 --> 00:20:57,389
Okay, so what this says is if we have a
bunch of data,

386
00:20:57,389 --> 00:21:04,895
you simply maximize this function here,
and that will give you a and b.

387
00:21:04,895 --> 00:21:09,018
That'll give you parameters a and b and
now you have a model.

388
00:21:09,018 --> 00:21:13,089
You have a model that will predict predict
probabilities for new and

389
00:21:13,089 --> 00:21:16,908
unseen values of new instances, right?

390
00:21:16,908 --> 00:21:21,413
That's the idea, so, this is of course
extremely widely used.

391
00:21:21,413 --> 00:21:25,321
And depending on your background and I can
speak for my own.

392
00:21:25,321 --> 00:21:26,917
So if your background is you know,

393
00:21:26,917 --> 00:21:31,161
from a sort of traditional EE type things
with signal processing.

394
00:21:31,161 --> 00:21:35,161
You probably won't have seen this which is
sort of a pity, right?

395
00:21:35,161 --> 00:21:38,695
You've seen a lot of stuff involving
linear things and stuff like that, but

396
00:21:38,695 --> 00:21:41,535
you probably won't have seen this.

397
00:21:41,535 --> 00:21:43,185
So, okay, so, here is an example, but

398
00:21:43,185 --> 00:21:47,110
this is an example where you hardly need
anything fancy, right?

399
00:21:47,110 --> 00:21:48,668
But, just so we can draw it.

400
00:21:48,668 --> 00:21:50,228
So, the example here as n equals one,

401
00:21:50,228 --> 00:21:54,306
is one explanatory feature, it's kind of
dumb, that's u, that's here.

402
00:21:54,306 --> 00:21:55,866
And variance between zero and 10 and

403
00:21:55,866 --> 00:21:59,143
what, what are plotted here are bunch of
samples, right?

404
00:21:59,143 --> 00:22:00,391
And so, ignore this curve for

405
00:22:00,391 --> 00:22:04,512
the moment, we just take a look at it, we
would say things like this.

406
00:22:04,512 --> 00:22:09,602
Look, it's kind of, I mean, you don't need
anything fancy for u equals one.

407
00:22:09,602 --> 00:22:13,562
You look at this and you say, yeah, look,
I mean if, if u is like, two or,

408
00:22:13,562 --> 00:22:17,502
I don't know, two or three, below three.

409
00:22:17,502 --> 00:22:18,718
It's quite likely, but

410
00:22:18,718 --> 00:22:23,157
not, it's quite likely that the, that the
outcome y will be zero.

411
00:22:23,157 --> 00:22:27,002
It's, but it is not one, because look at
this guy up here.

412
00:22:27,002 --> 00:22:29,585
Okay, sorry, it's not probability, it's
not,

413
00:22:29,585 --> 00:22:33,473
the probability that your zero is not one,
right?

414
00:22:33,473 --> 00:22:35,325
Because there are exceptions.

415
00:22:35,325 --> 00:22:38,205
On the other hand, if you say, if u is
bigger than about six,

416
00:22:38,205 --> 00:22:42,628
it's, it's quite likely, that the outcome
is going to be one.

417
00:22:42,628 --> 00:22:46,514
But, it is absolutely not certain, because
look at that, okay?

418
00:22:46,514 --> 00:22:49,711
So what this shows here is the logistic
fit, right?

419
00:22:49,711 --> 00:22:53,481
And so, and it, I mean it certainly seems
about right and it has all the weight of

420
00:22:53,481 --> 00:22:58,307
statistics behind it, so it can be
justified at great length.

421
00:22:58,307 --> 00:23:00,975
And, and so the use of this would go
something like this,

422
00:23:00,975 --> 00:23:04,688
the new sample will come along and you
would be too.

423
00:23:04,688 --> 00:23:08,487
And you would say well I think the
probability is about 8%.

424
00:23:08,487 --> 00:23:10,373
That it's one, right?

425
00:23:10,373 --> 00:23:14,239
U comes along, it's eight and you'd say
its about 94%.

426
00:23:14,239 --> 00:23:18,876
Now, and u would come along and it's five
and you'd say I don't know.

427
00:23:18,876 --> 00:23:20,319
Okay?
[BLANK_AUDIO].

428
00:23:20,319 --> 00:23:24,121
So, we'll move to a completely different
thing.

429
00:23:24,121 --> 00:23:26,878
Which is this hypothesis testing.

430
00:23:26,878 --> 00:23:29,380
Actually these are all related.

431
00:23:29,380 --> 00:23:32,240
Because in fact maximum likelihood
estimation, with,

432
00:23:32,240 --> 00:23:36,973
parameter that is continuous, is something
like hypothesis testing, right?

433
00:23:36,973 --> 00:23:40,836
Because you're sort of, you're testing one
value of x versus another.

434
00:23:40,836 --> 00:23:44,432
And in fact, we're going to see that
maximum likelihood is basically, it, it,

435
00:23:44,432 --> 00:23:46,984
it's the thing that would win in a binary
hypothesis,

436
00:23:46,984 --> 00:23:50,797
hypothesis test against any other value.

437
00:23:50,797 --> 00:23:51,427
But okay, so

438
00:23:51,427 --> 00:23:56,131
here it is, this is its absolutely, most
basic form, goes like this.

439
00:23:56,131 --> 00:24:00,654
And it's the most basic form, actually,
of, well, I guess, statistics.

440
00:24:00,654 --> 00:24:03,154
It's this, what you have, is you have two
hypotheses,

441
00:24:03,154 --> 00:24:07,500
that you have a variable that was
generated either from one distribution.

442
00:24:07,500 --> 00:24:09,997
This is finite value, it's got n possible
values.

443
00:24:09,997 --> 00:24:11,877
It's either generated from one
distribution or

444
00:24:11,877 --> 00:24:13,547
another, period, that's it.

445
00:24:13,547 --> 00:24:15,497
So, that it can't get anymore basic than
this.

446
00:24:15,497 --> 00:24:18,922
I guess you could have like, two, [LAUGH]
something here, but okay.

447
00:24:18,922 --> 00:24:21,038
So you have two distributions, and you see
a sample, and

448
00:24:21,038 --> 00:24:24,028
what you want to say is you want to say
something intelligent, about whether or

449
00:24:24,028 --> 00:24:27,122
not, which distribution it came from.

450
00:24:27,122 --> 00:24:29,492
That, that's it, that's the most basic
one.

451
00:24:29,492 --> 00:24:33,717
Okay, you know some things would be
obvious here, right?

452
00:24:33,717 --> 00:24:39,390
That if one distribution you know, looked
like this, you know 0.9, you know 0.100.

453
00:24:39,390 --> 00:24:40,908
Or something like that and

454
00:24:40,908 --> 00:24:44,952
the other looked like, you know, 0.05, you
know 0.05.

455
00:24:44,952 --> 00:24:48,858
You know, 0.8 and 0.1, then, you know,
it's, I mean, you could guess, right,

456
00:24:48,858 --> 00:24:53,969
that if the, if, if in fact what you
observed was, say, this one.

457
00:24:53,969 --> 00:24:55,733
Then you can say with absolute and

458
00:24:55,733 --> 00:25:00,712
total certainty that it was this
distribution that generated it, right?

459
00:25:00,712 --> 00:25:03,832
If it turned out to be, If your
observation was one, capital X equals one,

460
00:25:03,832 --> 00:25:06,761
you could say with very good confidence.

461
00:25:06,761 --> 00:25:09,718
That in fact it was generated by the first
distribution.

462
00:25:09,718 --> 00:25:10,770
Everybody see this?

463
00:25:10,770 --> 00:25:11,504
Right?
So, and

464
00:25:11,504 --> 00:25:15,158
in fact the whole idea here is as the two
distributions get closer together,

465
00:25:15,158 --> 00:25:18,580
this gets harder to do Okay, and so now
we're just going to look at what's a,

466
00:25:18,580 --> 00:25:24,652
what's a, what is a good and, principled
way, to make this choice, okay?

467
00:25:24,652 --> 00:25:27,592
So we're going to look at randomized
detectors.

468
00:25:27,592 --> 00:25:29,812
so, a randomized detector, that's a
matrix,

469
00:25:29,812 --> 00:25:33,053
it's going to look like this, it's two by
n.

470
00:25:33,053 --> 00:25:35,105
These are the outcomes, here, over here,
so

471
00:25:35,105 --> 00:25:39,982
we write the outcomes here, and then this
is something like one and two.

472
00:25:39,982 --> 00:25:42,202
And what it is is that each column is
going to be,

473
00:25:42,202 --> 00:25:46,222
is going to tell you what you should do,
when the outcome, is if it's column four,

474
00:25:46,222 --> 00:25:50,882
if the outcome is capital X equals four,
right?

475
00:25:50,882 --> 00:25:54,762
So if you look over here, you might have
0.9 and point 0.1 that's pretty weird.

476
00:25:54,762 --> 00:25:57,402
Because what it says is something like
this its says when capital x

477
00:25:57,402 --> 00:26:01,723
equals four you should say actually its
going to be a randomized detector.

478
00:26:01,723 --> 00:26:03,585
So you are going to say with probability
0.9,

479
00:26:03,585 --> 00:26:07,114
you are going to say that it was generated
from distribution one.

480
00:26:07,114 --> 00:26:10,935
And in probability 0.1, you are going to
say is generated from distribution two.

481
00:26:10,935 --> 00:26:14,013
So, we'll talk about why you might do this
in a minute.

482
00:26:14,013 --> 00:26:17,190
Now, much more natural would be things
like this, okay?

483
00:26:17,190 --> 00:26:20,310
And that says that if capital X equals
seven, you simply say, well it was,

484
00:26:20,310 --> 00:26:22,105
it was outcome two.

485
00:26:22,105 --> 00:26:24,860
It was distribution two that it came from,
okay?

486
00:26:24,860 --> 00:26:26,002
So this is the idea.

487
00:26:26,002 --> 00:26:30,852
Why you would want these aa randomize
detector, we'll talk about in a minute.

488
00:26:30,852 --> 00:26:35,309
Actually, it's interesting, you can also
think of it as a relaxation, right?

489
00:26:35,309 --> 00:26:36,282
Of an estimator.

490
00:26:36,282 --> 00:26:38,388
Okay, oh, and if all the elements are zero
or

491
00:26:38,388 --> 00:26:42,397
one, it's called a deterministic detector,
so that's the idea.

492
00:26:42,397 --> 00:26:45,076
Alright, so, how do you choose that,
detector?

493
00:26:45,076 --> 00:26:47,276
Well, if you simply multiply you, you take
a,

494
00:26:47,276 --> 00:26:49,916
there's a detection matrix, and the
detection matrix is,

495
00:26:49,916 --> 00:26:54,813
you simply multiply t by you multiply t on
the left by p and q.

496
00:26:54,813 --> 00:26:56,297
And you get a two by two matrix, and

497
00:26:56,297 --> 00:27:00,701
the entries, can be interpreted in, in a
very, very interesting way.

498
00:27:00,701 --> 00:27:04,761
They're, basically, the probability of,
error/g, so this gives you, this is if,

499
00:27:04,761 --> 00:27:09,050
if p is the true distribution, q is the
true distribution.

500
00:27:09,050 --> 00:27:12,236
And then this tells you sort of the
probability of selecting,

501
00:27:12,236 --> 00:27:14,326
the correct one, right?

502
00:27:14,326 --> 00:27:15,302
So, these entries,

503
00:27:15,302 --> 00:27:19,832
tell you something about the probabilities
here of being right, right?

504
00:27:19,832 --> 00:27:22,576
Being right means capital X came from
distribution one and

505
00:27:22,576 --> 00:27:26,225
you would announce that it came from
distribution one.

506
00:27:26,225 --> 00:27:29,122
And you could swap one with two and that's
being right.

507
00:27:29,122 --> 00:27:32,308
Being wrong, is the distribution came
from, the sample came from

508
00:27:32,308 --> 00:27:37,160
distribution one, but you announce that it
was distribution two, right?

509
00:27:37,160 --> 00:27:42,664
And these have names like you know, false
positive and false negative.

510
00:27:42,664 --> 00:27:46,564
And you know, that means one of these
things here, anyway, so

511
00:27:46,564 --> 00:27:49,387
that's the idea, right?

512
00:27:49,387 --> 00:27:51,533
So, this is the, this is the idea, okay,
so

513
00:27:51,533 --> 00:27:54,433
Pfp that's the probability of selecting
hypothesis two if x

514
00:27:54,433 --> 00:27:59,327
is generated by distribution one and
that's a false positive.

515
00:27:59,327 --> 00:28:01,247
So you're saying it's from distribution
two but

516
00:28:01,247 --> 00:28:03,497
in fact it was from distribution one.

517
00:28:03,497 --> 00:28:06,461
And of course what you like is you want
both of these things to be zero,

518
00:28:06,461 --> 00:28:10,271
I mean you want this matrix to be I, the
identity, right?

519
00:28:10,271 --> 00:28:11,423
Oh, there is another name for

520
00:28:11,423 --> 00:28:15,150
this, I think some people call it a
confusion matrix, I think.

521
00:28:15,150 --> 00:28:20,463
alright, so, we basically have a
multi-criterion formulation, right?

522
00:28:20,463 --> 00:28:22,839
Because what you would really like, oh,

523
00:28:22,839 --> 00:28:27,598
by the way, how would you make a false
positive rate zero?

524
00:28:27,598 --> 00:28:28,789
That you can do.

525
00:28:28,789 --> 00:28:34,012
You never want to select hypothesis two if
it was generated by, what?

526
00:28:34,012 --> 00:28:38,071
Yeah, just cli, make everything negative.

527
00:28:38,071 --> 00:28:41,965
Just, and then, the great news there is
that your false positive rate is as low as

528
00:28:41,965 --> 00:28:44,376
it can get, zero, okay?

529
00:28:44,376 --> 00:28:48,447
So that's one, and so, I mean it's a real
trade off here, right?

530
00:28:48,447 --> 00:28:53,249
Alright, so if we make this a bi-criterion
problem it juts looks likes this.

531
00:28:53,249 --> 00:28:54,934
It's a very simple one.

532
00:28:54,934 --> 00:28:57,645
and, the natural thing to do is scalarize.

533
00:28:57,645 --> 00:28:58,910
So if you scalarize, right,

534
00:28:58,910 --> 00:29:03,109
this will give you the pareto curve
actually, it won't in this case.

535
00:29:03,109 --> 00:29:05,413
You'll see exactly what it does get you.

536
00:29:05,413 --> 00:29:09,061
Because the piece, the, the pareto curve
in this case is piece wise linear and

537
00:29:09,061 --> 00:29:11,672
it gives you verteses on it.

538
00:29:11,672 --> 00:29:14,292
So we'll, we'll see that in a minute.

539
00:29:14,292 --> 00:29:17,345
So you minimize this, but in fact, that,
this is,

540
00:29:17,345 --> 00:29:20,926
this is completely trivial to solve.

541
00:29:20,926 --> 00:29:26,635
For one thing, it, the whole problem
splits across the columns of t, right?

542
00:29:26,635 --> 00:29:30,661
Because the constraint's only involved to
tell you that the columns are,

543
00:29:30,661 --> 00:29:33,248
have sum up to one, right?

544
00:29:33,248 --> 00:29:36,242
And the objective is linear, so it's
completely separable.

545
00:29:36,242 --> 00:29:38,333
So, this thing splits you can decide each
one, and

546
00:29:38,333 --> 00:29:42,081
then this is completely trivial to work
out what the answer is.

547
00:29:42,081 --> 00:29:45,013
And the answer is simply this, this is
very interesting.

548
00:29:45,013 --> 00:29:47,249
It says here at lambda remember is
actually, and

549
00:29:47,249 --> 00:29:50,024
lambda has a very beautiful
interpretation.

550
00:29:50,024 --> 00:29:50,792
Lambda is our,

551
00:29:50,792 --> 00:29:55,728
is remember something that translates, I
think it's the false positive.

552
00:29:55,728 --> 00:30:00,272
I think it's the false positive, rate,
irritation at false positive, versus,

553
00:30:00,272 --> 00:30:03,624
irritation for false negative, right.

554
00:30:03,624 --> 00:30:07,943
So it's the exchange rate, between the two
things that you want small.

555
00:30:07,943 --> 00:30:11,679
That's the interpretation from duality or
multi-criterion.

556
00:30:11,679 --> 00:30:14,484
So, and the answer to this is really
simple.

557
00:30:14,484 --> 00:30:17,445
It's this It, you, you get a deterministic
detector, and

558
00:30:17,445 --> 00:30:21,370
you should choose one if P is bigger than
lambda Q.

559
00:30:21,370 --> 00:30:25,023
And this is sometimes written this way,
like that, right?

560
00:30:25,023 --> 00:30:27,669
And the other one is written of course
this way, and

561
00:30:27,669 --> 00:30:33,752
then this is called a likelihood threshold
test, a likelihood ratio test, right?

562
00:30:33,752 --> 00:30:36,504
So you simply have a threshold and so all
you do now is

563
00:30:36,504 --> 00:30:41,903
you simply take one distribution and the
other one, you take the ratio.

564
00:30:41,903 --> 00:30:45,424
And then, all you're doing is your setting
a threshold.

565
00:30:45,424 --> 00:30:49,712
And then, for any given threshold you
choose which ever which ever one has

566
00:30:49,712 --> 00:30:54,004
the higher value or something like that,
right?

567
00:30:54,004 --> 00:30:56,864
Or you sort these and then, choose those,
anyway, and

568
00:30:56,864 --> 00:30:59,746
as you vary lambda here the ratio.

569
00:30:59,746 --> 00:31:01,718
Then what you are getting are a trade off,

570
00:31:01,718 --> 00:31:06,724
you are trading off false positives, false
positive and false negative, okay?

571
00:31:06,724 --> 00:31:11,260
So, now another one you could do is
minimax detector, that's very interesting.

572
00:31:11,260 --> 00:31:15,022
So minimax detector says no, no, I care
about false positive and false negative

573
00:31:15,022 --> 00:31:20,550
equally, please minimize, in fact this
says please minimize probability of error.

574
00:31:20,550 --> 00:31:23,658
But, I want to minimize probability of
error.

575
00:31:23,658 --> 00:31:26,210
Then you get this thing, and actually
that's a real l,

576
00:31:26,210 --> 00:31:31,288
that's an honest lp, and the solution is
generally not deterministic, right?

577
00:31:31,288 --> 00:31:34,501
It's generally something where the
detector has entries that are not zero,

578
00:31:34,501 --> 00:31:36,207
one, one, zero, okay?

579
00:31:36,207 --> 00:31:38,449
So, let's look at an example, you know,

580
00:31:38,449 --> 00:31:43,652
obviously you don't need all this heavy
stuff for this kind of thing, right?

581
00:31:43,652 --> 00:31:47,585
So here's an example, here's, so, here's
two distributions I guess that's p and

582
00:31:47,585 --> 00:31:49,002
that's q.

583
00:31:49,002 --> 00:31:51,620
And you know some things are obvious,
right?

584
00:31:51,620 --> 00:31:52,718
If the outcome is one,

585
00:31:52,718 --> 00:31:57,922
it looks rather, looks like a pretty good
bet that distribution p generated it.

586
00:31:57,922 --> 00:32:02,212
If the outcome is three, it looks like a
[COUGH] quite good bet, rather good bet

587
00:32:02,212 --> 00:32:08,022
that distribution, the second distribution
q, generated the data, right?

588
00:32:08,022 --> 00:32:10,432
Just, just, just lookin' at it.

589
00:32:10,432 --> 00:32:11,497
And let's see.

590
00:32:11,497 --> 00:32:15,711
Now, if the outcome is two or four, it's
maybe less obvious, right?

591
00:32:15,711 --> 00:32:17,886
As to which one generated it, right?

592
00:32:17,886 --> 00:32:20,106
So here is the trade off curve um,
[COUGH], so

593
00:32:20,106 --> 00:32:22,686
this trade off curve it's piece wise
linear, and

594
00:32:22,686 --> 00:32:29,135
this is showing you these various this, so
this,this is the actual trade off curve.

595
00:32:29,135 --> 00:32:32,557
These actually are the points you get from
the likelihood ratio test,

596
00:32:32,557 --> 00:32:35,625
right these well I guess you get these two
because those the two

597
00:32:35,625 --> 00:32:39,578
we've talked about before Which is how,
how can you get zero false positive or

598
00:32:39,578 --> 00:32:46,997
something like that, so you get something
like this, so you see the idea.

599
00:32:46,997 --> 00:32:50,813
Oh, by the way, this picture, when invert,
I think if you turn it,

600
00:32:50,813 --> 00:32:54,091
you just flip it top to bottom.

601
00:32:54,091 --> 00:32:59,387
It, it's got a name called ROC, which is
receiver operating characteristic.

602
00:32:59,387 --> 00:33:01,925
But the minimax one, that's four, right?

603
00:33:01,925 --> 00:33:04,502
This line here is line of equal error.

604
00:33:04,502 --> 00:33:08,535
A false, false positive and false negative
and its right here, right?

605
00:33:08,535 --> 00:33:10,275
So that's the, that's the idea so, so

606
00:33:10,275 --> 00:33:14,539
its a bit strange, by the way this is one
of those cases were.

607
00:33:14,539 --> 00:33:17,903
Well as a, as a big picture we, we will
and we have talked about these and

608
00:33:17,903 --> 00:33:22,756
we will talk about it more were you
want to solve a problem.

609
00:33:22,756 --> 00:33:26,058
And you end up doing something like a
relaxation, right?

610
00:33:26,058 --> 00:33:30,642
So, if someone comes up to you and says,
fine, these are the two distributions.

611
00:33:30,642 --> 00:33:33,234
I want you to estimate, repeated, what's
going to happen,

612
00:33:33,234 --> 00:33:36,611
repeatedly is you're going to see samples,
outcomes.

613
00:33:36,611 --> 00:33:39,107
And I want you to guess which distribution
it came from and

614
00:33:39,107 --> 00:33:42,580
I want you to absolutely minimize being
wrong, right?

615
00:33:42,580 --> 00:33:45,034
Simple, you have to use the minimax
detector.

616
00:33:45,034 --> 00:33:48,040
What's weird about it is you're not
consistent, right?

617
00:33:48,040 --> 00:33:50,659
Because you'd say things like x equals
three.

618
00:33:50,659 --> 00:33:53,167
And you go, yeah, that came for outcome
one, yeah, and

619
00:33:53,167 --> 00:33:55,921
then, later they say x equals three.

620
00:33:55,921 --> 00:33:58,341
You go, mm-hm, sorry, that came from
distribution Q,

621
00:33:58,341 --> 00:34:01,448
sorry first it came from distribution P
then Q.

622
00:34:01,448 --> 00:34:06,884
So, you're not consistent, it's a, it is a
It's a non-deterministic detector, right?

623
00:34:06,884 --> 00:34:10,404
But, that is the, that is actually the
only way that you can actually minimize

624
00:34:10,404 --> 00:34:13,010
the probability of being wrong.

625
00:34:13,010 --> 00:34:13,637
So, okay, so,

626
00:34:13,637 --> 00:34:18,698
that's that, by the way, it's not very
interesting with two distributions, right?

627
00:34:18,698 --> 00:34:22,674
This actually is already very interesting,
when I have let's say ten distributions,

628
00:34:22,674 --> 00:34:27,774
they're bigger, and things like that, it's
already way interesting.

629
00:34:27,774 --> 00:34:30,142
And I should add something there you can,

630
00:34:30,142 --> 00:34:34,002
you can actually get really interesting
problems.

631
00:34:34,002 --> 00:34:38,610
If I show you 10 distributions on lets say
a 1000 outcomes or 10,000 outcomes it, its

632
00:34:38,610 --> 00:34:45,092
actually there your confusion matrix or I
think we called it the detector matrix.

633
00:34:45,092 --> 00:34:48,356
What do we call it, fine, the detection
probability matrix of

634
00:34:48,356 --> 00:34:53,002
the confusion matrix, it's got a lot of
entries in it, right?

635
00:34:53,002 --> 00:34:55,177
Like for example it's got 100, okay?

636
00:34:55,177 --> 00:34:59,362
And you think of all sorts of interesting
convex problems you could do.

637
00:34:59,362 --> 00:35:02,352
You could say things like, you say, you
know what?

638
00:35:02,352 --> 00:35:06,256
I, I absolutely do not, the probability
that you think, that you say, that you

639
00:35:06,256 --> 00:35:12,285
estimate it came from distribution seven
when it really came from distribution six.

640
00:35:12,285 --> 00:35:14,807
That absolutely can't exceed 10%, okay?

641
00:35:14,807 --> 00:35:17,192
Everyone understood what we just said?

642
00:35:17,192 --> 00:35:21,157
That's just simply a linear inequality on
one entry in that matrix.

643
00:35:21,157 --> 00:35:23,047
You see what I'm saying, right?

644
00:35:23,047 --> 00:35:25,801
And you can go on and add all sorts of
penal convex constraints and

645
00:35:25,801 --> 00:35:27,567
things like that.

646
00:35:27,567 --> 00:35:31,461
You'd say, the probability of being wrong
when the distribution come,

647
00:35:31,461 --> 00:35:35,321
is actually generated from distribution
three.

648
00:35:35,321 --> 00:35:39,477
I want the probability of being wrong, to
be no more than 10%.

649
00:35:39,477 --> 00:35:42,217
What would that be, what I just said?

650
00:35:42,217 --> 00:35:45,372
What kind of constraint on d would that
be?

651
00:35:45,372 --> 00:35:48,192
That's it, that's, nothing more, right?

652
00:35:48,192 --> 00:35:51,877
Yeah, either the, either the diagonal
element Is bigger than 0.9,

653
00:35:51,877 --> 00:35:53,768
that's just fine.

654
00:35:53,768 --> 00:35:56,541
Or, actually, the sum of the off-diagonals
is less than the number, but

655
00:35:56,541 --> 00:35:59,313
that's the same because they all add up
to, yeah.

656
00:35:59,313 --> 00:36:01,057
Okay, everybody, everybody got this?

657
00:36:01,057 --> 00:36:03,457
So, by the way, a lot of these problems
are not, I don't think,

658
00:36:03,457 --> 00:36:07,175
taught in standard statistics-type things
or machine learning.

659
00:36:07,175 --> 00:36:10,640
I mean, I think there is, they have a loss
function for all of these things and

660
00:36:10,640 --> 00:36:11,939
so on.

661
00:36:11,939 --> 00:36:13,699
But you can now do interesting things,

662
00:36:13,699 --> 00:36:17,435
just because you know about linear
programming, for example.

663
00:36:17,435 --> 00:36:22,191
Okay, we're going to go to our last sample
here of topics.

664
00:36:22,191 --> 00:36:25,133
It's actually very interesting ones.

665
00:36:25,133 --> 00:36:27,011
It's experiment design, right?

666
00:36:27,011 --> 00:36:29,006
So, and I, it's in a very specific
setting,

667
00:36:29,006 --> 00:36:32,381
it's in a setting where you get a simple
problem.

668
00:36:32,381 --> 00:36:35,915
But, it's, actually, quite interesting,
it's very good to know about, and

669
00:36:35,915 --> 00:36:38,851
it's extremely useful, so it's this.

670
00:36:38,851 --> 00:36:40,831
We're going to have m linear measurements,
so

671
00:36:40,831 --> 00:36:44,847
I have a bunch of measurements yi is ai
transpose x plus wi.

672
00:36:44,847 --> 00:36:46,057
And what we're going to do is,

673
00:36:46,057 --> 00:36:50,575
we're going to normalize things to have
all these noises N, 01, right?

674
00:36:50,575 --> 00:36:53,761
So, I, so, for example, if the noise
actually had a smaller variance, or

675
00:36:53,761 --> 00:36:57,217
something like that, I would scale it up,
and that would actually just, make,

676
00:36:57,217 --> 00:37:02,243
I'd scale a and y would scale and all that
kind of stuff, right?

677
00:37:02,243 --> 00:37:03,179
So that's the idea, so

678
00:37:03,179 --> 00:37:07,433
all the noises are normalized to have
variance one, standard deviation one.

679
00:37:07,433 --> 00:37:10,850
Okay, and so, if you want to, I mean, we
can talk about various things here, x is,

680
00:37:10,850 --> 00:37:14,755
x is a parameter we want to estimate, y is
a measurement.

681
00:37:14,755 --> 00:37:18,235
And you should interpret the size of a now
as something like a signal to

682
00:37:18,235 --> 00:37:20,352
noise ratio, right?

683
00:37:20,352 --> 00:37:22,992
Because if a is big, if a3 is big, that
means, that's a,

684
00:37:22,992 --> 00:37:25,901
that's a clean sensor measurement.

685
00:37:25,901 --> 00:37:29,538
If a3 is small, that's a crappy sensor
measurement.

686
00:37:29,538 --> 00:37:30,083
[SOUND].

687
00:37:30,083 --> 00:37:31,391
Everybody got this?

688
00:37:31,391 --> 00:37:33,601
Right?
So, because the bigger a3 is,

689
00:37:33,601 --> 00:37:39,385
the bigger this chunk is compared to the
noise which is on the order of one, okay?

690
00:37:39,385 --> 00:37:40,665
So norm of a being big is,

691
00:37:40,665 --> 00:37:44,748
is good, that's, something like a signal
to noise ratio.

692
00:37:44,748 --> 00:37:45,333
Okay, now,

693
00:37:45,333 --> 00:37:50,775
if I estimate this, well the least squares
estimate, is just that, right?

694
00:37:50,775 --> 00:37:52,825
It's just, whatever it is.

695
00:37:52,825 --> 00:37:58,225
A transpose A inverse A transponse B, and
that's this, and we write it this way.

696
00:37:58,225 --> 00:38:01,765
And what you see here Is it's a sum, of
the outer products of the ai's, and

697
00:38:01,765 --> 00:38:05,289
then inverse times, the, this sum, okay?

698
00:38:05,289 --> 00:38:07,138
So that, that's simple enough.

699
00:38:07,138 --> 00:38:10,548
Okay, now, the error, that's x hat minus
x, is zero mean, and

700
00:38:10,548 --> 00:38:15,610
has a co-variance matrix, which is
actually just this thing here.

701
00:38:15,610 --> 00:38:18,473
So that, that's the co-variance matrix,
okay?

702
00:38:18,473 --> 00:38:21,535
And, and notice that this kind of makes
sense, right?

703
00:38:21,535 --> 00:38:24,841
Because look at this, if, that's, now
that's a co-variance matrix, so

704
00:38:24,841 --> 00:38:28,169
it's, it's a positive semi-definite
matrix.

705
00:38:28,169 --> 00:38:30,319
what's, int, actually a positive definite
matrix,

706
00:38:30,319 --> 00:38:34,738
what's interesting about it is this, if ai
is bigger, this thing gets smaller.

707
00:38:34,738 --> 00:38:36,583
I mean if they're all bigger, right?

708
00:38:36,583 --> 00:38:38,232
Because we'll just look at it.

709
00:38:38,232 --> 00:38:41,631
This, this matrix is monotone, in the
matrix sense, in ai.

710
00:38:41,631 --> 00:38:44,627
I mean, that sounds weird, but you know
what I mean.

711
00:38:44,627 --> 00:38:46,497
It means that, if, if you were to double
a,

712
00:38:46,497 --> 00:38:51,576
you know, a seven that dyad, contribution
would be bigger in the matrix sense.

713
00:38:51,576 --> 00:38:55,395
This matrix would be bigger, and then you
take the inverse, that would switch it,

714
00:38:55,395 --> 00:38:59,533
it'd be smaller, so you do, you'd get
better estimation.

715
00:38:59,533 --> 00:39:01,312
Okay, so, it all makes sense.

716
00:39:01,312 --> 00:39:01,984
Okay, and so, for

717
00:39:01,984 --> 00:39:06,579
example, if you wanted to get a confidence
ellipsoid that would be related this way.

718
00:39:06,579 --> 00:39:08,883
It would be [COUGH] something like this,
you'd have x hat,

719
00:39:08,883 --> 00:39:11,750
that's your, that's your estimate from
here.

720
00:39:11,750 --> 00:39:14,678
And you'd select beta de, de, depending on
the confidence level you want,

721
00:39:14,678 --> 00:39:19,086
you know, depending on some chi-squared
distribution or something like that.

722
00:39:19,086 --> 00:39:21,090
That would give you the confidence level.

723
00:39:21,090 --> 00:39:23,246
And, so, experiment design, says, the
following, so

724
00:39:23,246 --> 00:39:27,482
all of this is basic, experiment design is
the following problem.

725
00:39:27,482 --> 00:39:32,747
you, can choose from a palate of
measurements that are possible.

726
00:39:32,747 --> 00:39:38,612
In other words, you have something like p,
possible measurements, here.

727
00:39:38,612 --> 00:39:42,203
They're going to be denoted v1 through vp.

728
00:39:42,203 --> 00:39:48,705
And you are allowed, to choose, like from,
for each m you choose one of those, right?

729
00:39:48,705 --> 00:39:52,467
So for example you might say something
like this.

730
00:39:52,467 --> 00:39:55,925
I want all my measurements to be v1,
that's why.

731
00:39:55,925 --> 00:39:58,451
Why?
Because it's the best sensor, okay,

732
00:39:58,451 --> 00:40:00,383
something like that.

733
00:40:00,383 --> 00:40:02,957
Or you might say I want to do this, I
want to, I want,

734
00:40:02,957 --> 00:40:05,993
I want to start with v1 and then v2.

735
00:40:05,993 --> 00:40:09,359
Then I'll do a v7, then I'll come back
with another v1, then a v,

736
00:40:09,359 --> 00:40:12,011
everybody following this?

737
00:40:12,011 --> 00:40:14,533
So this is what you have to choose, okay?

738
00:40:14,533 --> 00:40:17,253
Now some things are kind of obvious here.

739
00:40:17,253 --> 00:40:19,547
For example, this is a sum of dyads,
right, so

740
00:40:19,547 --> 00:40:23,920
it actually doesn't matter what order you
do them in right?

741
00:40:23,920 --> 00:40:26,740
And, in fact, the only thing you really
have to, say here,

742
00:40:26,740 --> 00:40:32,084
to specify an experiment design is to say,
I want to do 22 of experiment one.

743
00:40:32,084 --> 00:40:34,829
12 of experiment two, zero of experiment
three, and so

744
00:40:34,829 --> 00:40:38,093
on, and those integers have to add up to
m.

745
00:40:38,093 --> 00:40:41,600
Which is the total number of experiments
you're going to do.

746
00:40:41,600 --> 00:40:42,856
Everybody got that?

747
00:40:42,856 --> 00:40:45,570
So, and, you can imagine also it's a
variations to where you add cost for

748
00:40:45,570 --> 00:40:49,047
the experiments, and time or money or
something like that.

749
00:40:49,047 --> 00:40:50,565
You can imagine all sorts of things, but

750
00:40:50,565 --> 00:40:53,187
we're going to stick with the simple one
at first.

751
00:40:53,187 --> 00:40:55,533
Okay, so we end up with a problem that
looks like this, and, and

752
00:40:55,533 --> 00:40:59,212
this'll, this looks like a good time for
me to say something.

753
00:40:59,212 --> 00:41:02,287
It's a general thing, that we can talk
about, and it's fun, okay.

754
00:41:02,287 --> 00:41:05,012
So, you end up with a problem that looks,
looks like this.

755
00:41:05,012 --> 00:41:07,912
Choose a bunch of integers, m1 through mp,
they add up to m.

756
00:41:07,912 --> 00:41:10,615
In fact, what you're really doing is
you're allocating a set of m

757
00:41:10,615 --> 00:41:13,589
experiments across, p possible
experiments.

758
00:41:13,589 --> 00:41:16,033
So you're just, that's, it's an allocation
problem, but

759
00:41:16,033 --> 00:41:19,098
notice it's an integer allocation problem,
okay?

760
00:41:19,098 --> 00:41:22,223
And you want to do that, to minimize, this
covariance matrix.

761
00:41:22,223 --> 00:41:25,425
Now, of course, that's a vector
optimization problem, right?

762
00:41:25,425 --> 00:41:26,343
Because it doesn't,

763
00:41:26,343 --> 00:41:30,614
it makes absolutely no sense to stay
minimize a covariance matrix, right?

764
00:41:30,614 --> 00:41:33,820
So, it's pretty optimal and many ways to
do that.

765
00:41:33,820 --> 00:41:37,434
You could scalarize, you could do all
sorts of things, right?

766
00:41:37,434 --> 00:41:41,777
There's many ways to talk about a,
minimizing a covariance matrix, right?

767
00:41:41,777 --> 00:41:43,724
So, or if you like, this has been geomet,

768
00:41:43,724 --> 00:41:49,331
if you like thinking geometrically, buy me
the smallest confidence ellipsoid, right?

769
00:41:49,331 --> 00:41:53,526
By the way, there is no such thing, so you
want to say something like this.

770
00:41:53,526 --> 00:41:55,486
Find me a small confidence ellipsoid, and

771
00:41:55,486 --> 00:41:59,209
let's translate it to mean find the[
pareto optimal one.

772
00:41:59,209 --> 00:42:02,014
So pareto optimal, in that sense means,
find me a confidence,

773
00:42:02,014 --> 00:42:06,252
a choice of these m's, so the, the
confidence ellipsoid you get.

774
00:42:06,252 --> 00:42:09,444
There's no smaller confidence ellipsoid,
that you can get all,

775
00:42:09,444 --> 00:42:12,596
by another allocation of experiments,
right?

776
00:42:12,596 --> 00:42:13,922
That's what that means.

777
00:42:13,922 --> 00:42:14,483
Okay, so, but

778
00:42:14,483 --> 00:42:18,227
we all know that because that's what
multi-criterion optimization is.

779
00:42:18,227 --> 00:42:21,944
Okay, now the variables are a bunch of
integers, that should be lighting up,

780
00:42:21,944 --> 00:42:23,916
a big warning sign.

781
00:42:23,916 --> 00:42:26,874
So the amber light, in your convex
optimization conceptual,

782
00:42:26,874 --> 00:42:30,967
center should be blinking when you hear
integers.

783
00:42:30,967 --> 00:42:34,372
And in fact, this problem is difficult in
general, right?

784
00:42:34,372 --> 00:42:38,207
I mean that's the, right thing to say
whenever you see integers.

785
00:42:38,207 --> 00:42:41,627
Actually, in some cases it's false, of
course there are plenty of

786
00:42:41,627 --> 00:42:47,255
integer problems which, actually, are
solvable, but In, in general they're not.

787
00:42:47,255 --> 00:42:51,367
Okay, so instead, what we're going to do
is relaxed experiment design.

788
00:42:51,367 --> 00:42:53,373
So relaxed experiment design says this,

789
00:42:53,373 --> 00:42:57,245
hm, what I'm going to do is I'm going to
rewrite it this way.

790
00:42:57,245 --> 00:43:00,962
I'm going to work with the fractions of
the experiments of each type I do, and so

791
00:43:00,962 --> 00:43:01,670
I'm going to get,

792
00:43:01,670 --> 00:43:07,528
I'm going to work with a lambda, and, I'll
have one transposed lambda as zero.

793
00:43:07,528 --> 00:43:12,658
And, if I added one more constraint, if I
added simply this, m lambda is in z.

794
00:43:12,658 --> 00:43:17,247
Then, this problem would be identical to
that problem, right?

795
00:43:17,247 --> 00:43:21,672
This says that the Lambdas are integer
multiples of one over m.

796
00:43:21,672 --> 00:43:25,488
So if I added this constraint, this is
identical to that one, and

797
00:43:25,488 --> 00:43:31,252
now, you want to see what, want to see how
convex relaxation is done?

798
00:43:31,252 --> 00:43:35,307
Everybody know this now, you know what
convex relaxation is, right?

799
00:43:35,307 --> 00:43:38,132
Watch very carefully, convex relaxation is
this.

800
00:43:38,132 --> 00:43:40,517
You write down an optimization problem.

801
00:43:40,517 --> 00:43:45,232
There are a few constraints that you don't
know how to handle, comment them out.

802
00:43:45,232 --> 00:43:46,707
That's what it is, okay?

803
00:43:46,707 --> 00:43:48,772
What are some common scalarizations,

804
00:43:48,772 --> 00:43:52,928
you can minimize the log of the
determinate that's a convex.

805
00:43:52,928 --> 00:43:55,094
Well sorry concave, increasing function
and

806
00:43:55,094 --> 00:43:58,279
therefore will produce a pareto optimal
point.

807
00:43:58,279 --> 00:44:02,766
You can take the traits the maximum
eigenvalue, all sorts of crazy stuff.

808
00:44:02,766 --> 00:44:07,022
You know, you can add all sorts of
constraints now to it [COUGH], sorry.

809
00:44:07,022 --> 00:44:10,476
for, for this thing, okay, so, and these
have names, right?

810
00:44:10,476 --> 00:44:13,501
So, [COUGH] when you minimize the log
determinant of the inverse,

811
00:44:13,501 --> 00:44:15,715
it's beautiful interpretation.

812
00:44:15,715 --> 00:44:20,150
That's exactly, you are minimizing the
volume of the confidence ellipsoid, right?

813
00:44:20,150 --> 00:44:23,085
And you end up with this problem that is
convex, right?

814
00:44:23,085 --> 00:44:24,518
So you solve that.

815
00:44:24,518 --> 00:44:28,858
And then, you know, depending on what, if
you were successful in convincing someone

816
00:44:28,858 --> 00:44:34,356
that was more sophisticated to solve the
relaxed problem, you're done.

817
00:44:34,356 --> 00:44:36,036
If you haven't then you round, and

818
00:44:36,036 --> 00:44:40,813
you hold up the optimal value, of the
relaxed problem, as a lower map.

819
00:44:40,813 --> 00:44:42,915
Okay, let's take a look at that.

820
00:44:42,915 --> 00:44:44,739
I'm not going to go into the details here,

821
00:44:44,739 --> 00:44:50,018
I'm not going to argue everything over the
dual but it's actually quite interesting.

822
00:44:50,018 --> 00:44:54,544
So the dual problem of this can be
messaged, I should say a dual problem.

823
00:44:54,544 --> 00:44:58,108
And that means that you have to allow me
to do a few little manipulations before,

824
00:44:58,108 --> 00:45:02,833
and a few after but you get it into a real
form that looks like this.

825
00:45:02,833 --> 00:45:06,963
Maximizes log det W, plus a constant that
is totally irrelevant.

826
00:45:06,963 --> 00:45:09,710
Subject to [COUGH], vk transpose W vk less
than or

827
00:45:09,710 --> 00:45:15,417
equal to one and this has a beautiful
interpretation geometrically.

828
00:45:15,417 --> 00:45:19,701
It turns out this says, find me the
minimum volume ellipsoid centered at

829
00:45:19,701 --> 00:45:23,912
the origin, that includes all the test
vectors.

830
00:45:23,912 --> 00:45:28,073
Um, [COUGH], and it turns out, that, the
Lagrange multipliers on these constraints

831
00:45:28,073 --> 00:45:33,564
are precisely the fractions the optimal
fractions that you should use, right?

832
00:45:33,564 --> 00:45:37,048
So, this is beautiful, so you get this
connection between, an experiment design

833
00:45:37,048 --> 00:45:41,601
problem and a geometric problem, which is
going to be our next topic, right?

834
00:45:41,601 --> 00:45:45,101
And so the picture is something like this.

835
00:45:45,101 --> 00:45:47,009
well, this is the picture, right?

836
00:45:47,009 --> 00:45:48,359
The picture is this.

837
00:45:48,359 --> 00:45:49,814
We have 20 test vectors.

838
00:45:49,814 --> 00:45:52,782
That's the origin and here's a bunch of
things, tests you can do,

839
00:45:52,782 --> 00:45:55,870
potential tests and here's a bunch, okay?

840
00:45:55,870 --> 00:45:58,894
And I'm going to, before you even look at
what the optimal design is,

841
00:45:58,894 --> 00:46:01,988
I'm going to ask you some questions about
it.

842
00:46:01,988 --> 00:46:03,582
Okay, so let me ask you.

843
00:46:03,582 --> 00:46:08,297
Please explain, intuitively, why you would
never choose that experiment?

844
00:46:08,297 --> 00:46:10,921
Well let's suppose that experiment's
number 11,

845
00:46:10,921 --> 00:46:13,897
why would you not choose experiment 11?

846
00:46:13,897 --> 00:46:15,472
[COUGH] You can say.

847
00:46:15,472 --> 00:46:19,184
And don't say anything about convex
optimization or log [UNKNOWN] or

848
00:46:19,184 --> 00:46:20,792
anything else.

849
00:46:20,792 --> 00:46:24,652
Just, you tell me why would you not choose
that experiment.

850
00:46:24,652 --> 00:46:28,175
Well what, what is this experiment here?

851
00:46:28,175 --> 00:46:30,695
It's basically, it gives the, it,

852
00:46:30,695 --> 00:46:37,286
it's the negative of that experiment, but
it's about twice as big.

853
00:46:37,286 --> 00:46:38,576
So in other words,

854
00:46:38,576 --> 00:46:44,662
it's a completely equivalent measurement
with one half the noise.

855
00:46:44,662 --> 00:46:47,032
Twice the signal to noise ratio, right?

856
00:46:47,032 --> 00:46:50,680
So, if I give you a set of, a palette, of
20 experiments you can carry out, one is

857
00:46:50,680 --> 00:46:56,337
basically a copy of another, but twice as
bad, then why would you ever use it?

858
00:46:56,337 --> 00:46:57,543
You would not, okay?

859
00:46:57,543 --> 00:47:01,837
So, I'm just, I'm just saying, all of this
is kind of intuitive, right?

860
00:47:01,837 --> 00:47:04,893
You want to use experiments that are
spread apart.

861
00:47:04,893 --> 00:47:08,028
Now, roughly speaking, you don't choose
any of these.

862
00:47:08,028 --> 00:47:11,049
Because these, well not quite because they
don't go up here, but

863
00:47:11,049 --> 00:47:14,462
these measurements here are kind of like
those.

864
00:47:14,462 --> 00:47:17,320
I mean there's got some minus signs but
who cares.

865
00:47:17,320 --> 00:47:20,808
It actually goes away immediately when you
form that dyad.

866
00:47:20,808 --> 00:47:23,730
The vk, vk transpose goes away instantly.

867
00:47:23,730 --> 00:47:26,810
So what happens is, you're, you're not
going to choose, any of these,

868
00:47:26,810 --> 00:47:31,742
because these sort of stand in for them
and have higher signal-noise ratio.

869
00:47:31,742 --> 00:47:33,366
And sure enough, look what it does,

870
00:47:33,366 --> 00:47:36,998
it picks the two [COUGH] which have kind
of highest angle.

871
00:47:36,998 --> 00:47:40,408
The two measurements which are most
independent, if you want to say that, or

872
00:47:40,408 --> 00:47:43,692
something like that, have the highest
angle.

873
00:47:43,692 --> 00:47:45,922
And it concentrates at 50 50 on them,
okay?

874
00:47:45,922 --> 00:47:49,659
So, by the way, stupid thing in R too, you
don't need this, right?

875
00:47:49,659 --> 00:47:51,873
But, if you're making, if you're
estimating 10 or

876
00:47:51,873 --> 00:47:55,633
100 parameters, these things are not
obvious at all, right?

877
00:47:55,633 --> 00:47:56,632
Not at all, right?

878
00:47:56,632 --> 00:47:59,585
You have a thousand possible experiments
to do, and so on.
