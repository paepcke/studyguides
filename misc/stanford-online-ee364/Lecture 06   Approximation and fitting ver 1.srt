1
00:00:00,920 --> 00:00:03,368
Part one of the course was basically,
well,

2
00:00:03,368 --> 00:00:09,300
all the analysis and the mathematics
behind convex optimization right?

3
00:00:09,300 --> 00:00:14,052
So, convex analysis, the basics there the
most important thing there you needed to

4
00:00:14,052 --> 00:00:19,319
know are things like convexity of a whole
bunch of atoms.

5
00:00:19,319 --> 00:00:21,383
And a lot of, and a lot of these
composition rules and

6
00:00:21,383 --> 00:00:24,643
other rules, these are good to know about,
right?

7
00:00:24,643 --> 00:00:27,337
then, there was a section on the anatomy.

8
00:00:27,337 --> 00:00:30,371
Of the anatomy of optimization problems of
the anatomy of optimization problems and

9
00:00:30,371 --> 00:00:32,236
some of them had names, right?

10
00:00:32,236 --> 00:00:35,932
Like you have linear programs, and second
order comp programs, and

11
00:00:35,932 --> 00:00:39,964
geometric programs and things like that,
right?

12
00:00:39,964 --> 00:00:43,387
In some sense, those names matter less and
less.

13
00:00:43,387 --> 00:00:47,320
You should know that already because with
things like CBX a lot of

14
00:00:47,320 --> 00:00:54,580
the transformation of a problem that has
no name into some problem that has a name.

15
00:00:54,580 --> 00:00:58,960
It can be automated or is part, at least
partially automated.

16
00:00:58,960 --> 00:01:02,243
Still you need to know the name, so that I
don't know, you can talk to other people,

17
00:01:02,243 --> 00:01:06,250
but that's the idea, and then of course,
there's duality.

18
00:01:06,250 --> 00:01:09,154
So in duality is sort of a big field by
itself.

19
00:01:09,154 --> 00:01:11,058
And it's, it's really nothing but

20
00:01:11,058 --> 00:01:16,160
an organized way to come up with lower
bounds for problems, period.

21
00:01:16,160 --> 00:01:17,440
That, that's what it is.

22
00:01:17,440 --> 00:01:21,072
In fact, it's a quite a trivial way, in
some sense, to come up with them.

23
00:01:21,072 --> 00:01:22,260
But it's an organized way.

24
00:01:22,260 --> 00:01:24,532
And what is shocking is that in, for

25
00:01:24,532 --> 00:01:29,360
convex problems, it's generally speaking,
not always, but that these are little

26
00:01:29,360 --> 00:01:36,320
minor details without with a constraint
qualification, it's sharp, right?

27
00:01:36,320 --> 00:01:38,577
So that these lower bounds are actually
sharp.

28
00:01:38,577 --> 00:01:41,615
But they will see lots of advantage, you
will see lots of uses, you will see,

29
00:01:41,615 --> 00:01:43,624
by the way, if you look, if you take other
courses,

30
00:01:43,624 --> 00:01:48,000
you'll see examples where people use
duality all the time.

31
00:01:48,000 --> 00:01:49,100
They won't even say it.

32
00:01:49,100 --> 00:01:50,828
Right, I mean you take a wireless course
and

33
00:01:50,828 --> 00:01:54,236
there you're trying to figure out how to
do, you know, assign powers to channels or

34
00:01:54,236 --> 00:01:59,425
whatever and they'll call it something
else like water filling or something.

35
00:01:59,425 --> 00:02:02,827
But now that you know the secret, you'll
just look at it and say oh,

36
00:02:02,827 --> 00:02:06,710
that's that, that's called duality, right?

37
00:02:06,710 --> 00:02:09,671
Or, you take an information theory course,
they're telling you about how to

38
00:02:09,671 --> 00:02:13,820
allocate bits to something, and you look
at it and you go, that's duality.

39
00:02:13,820 --> 00:02:16,060
So, okay.

40
00:02:16,060 --> 00:02:19,964
Duality also has, there's a lot of
aesthetics in duality where it's useful to

41
00:02:19,964 --> 00:02:22,770
recognize that problems that can at first
look very

42
00:02:22,770 --> 00:02:27,570
different are actually intimately related,
right?

43
00:02:27,570 --> 00:02:30,840
So and this will come up when we look at
applications a lot.

44
00:02:30,840 --> 00:02:33,740
So for example, we read things like,
again, if you're in statistics or

45
00:02:33,740 --> 00:02:37,140
if you have taken some advanced machine
courses, you would know things like this,

46
00:02:37,140 --> 00:02:39,340
but you get weird things where, you know,
duality of,

47
00:02:39,340 --> 00:02:44,030
sort of you get maximum likelihood
estimation problems.

48
00:02:44,030 --> 00:02:46,830
The duals are these max, or things that
look like maximum entropy or

49
00:02:46,830 --> 00:02:49,900
Kullbackâ€“Leibler divergence type things,
right?

50
00:02:49,900 --> 00:02:53,316
And so, I, by the way, if you don't know
what I'm talking about, that's fine.

51
00:02:53,316 --> 00:02:57,336
I'm being very obscure now, but I just, I
want to just point out that this is

52
00:02:57,336 --> 00:03:01,461
going to permeate lots of things you've
seen.

53
00:03:01,461 --> 00:03:04,290
And actually, in various areas, it's
completely well-known.

54
00:03:04,290 --> 00:03:06,373
I mean people know these things.

55
00:03:06,373 --> 00:03:06,942
okay.

56
00:03:06,942 --> 00:03:10,480
So, now, we're going to start another
section of the course.

57
00:03:10,480 --> 00:03:13,165
It's the middle section and honestly this
is the pay-off.

58
00:03:13,165 --> 00:03:16,970
So, I mean, the truth is you've already
seen enough.

59
00:03:16,970 --> 00:03:19,763
You're already being paid right now, I
mean, the homework is now fun,

60
00:03:19,763 --> 00:03:21,544
it's useful and so on.

61
00:03:21,544 --> 00:03:24,624
Well, [LAUGH] that's my opinion, you're
the one doing it, but, but

62
00:03:24,624 --> 00:03:28,599
I still say it's fun and its useful and
stuff like that.

63
00:03:28,599 --> 00:03:31,514
This is really the pay-off of the course,
what we're going to do is for

64
00:03:31,514 --> 00:03:34,200
the next couple of weeks.

65
00:03:34,200 --> 00:03:37,521
We're just going to look at various
application areas, you know, one by one.

66
00:03:37,521 --> 00:03:39,196
And we'll look at some topics,

67
00:03:39,196 --> 00:03:45,005
some will be obvious our first topic here
is going to be totally obvious.

68
00:03:45,005 --> 00:03:46,338
Actually, some will be less obvious,

69
00:03:46,338 --> 00:03:48,187
I mean we'll look at some weird
geometrical ones and

70
00:03:48,187 --> 00:03:50,337
some ones in statistics and stuff like
that, and they'll be,

71
00:03:50,337 --> 00:03:53,151
they won't be totally obvious.

72
00:03:53,151 --> 00:03:54,285
actually, watch out,

73
00:03:54,285 --> 00:03:58,317
because some of the most useful material
is in the totally obvious stuff like

74
00:03:58,317 --> 00:04:04,690
the stuff we're about to do is shockingly
elementary and unbelievably useful.

75
00:04:04,690 --> 00:04:05,762
So so this is it.

76
00:04:05,762 --> 00:04:07,027
Oh, this is also, by the way,

77
00:04:07,027 --> 00:04:11,584
the material that's not in any traditional
course on this material, right?

78
00:04:11,584 --> 00:04:15,366
because the traditional course segue
immediately from convex analysis to

79
00:04:15,366 --> 00:04:20,815
basically deadly boring, long complicated
proofs of complexity of algorithms.

80
00:04:20,815 --> 00:04:23,220
All right.
So this is the part that's missing.

81
00:04:23,220 --> 00:04:24,120
So, okay.

82
00:04:24,120 --> 00:04:25,611
So we're going to start in.

83
00:04:25,611 --> 00:04:30,719
By the way, the, these problems have been
categorized into gross areas.

84
00:04:30,719 --> 00:04:33,797
This is approximation and fitting.

85
00:04:33,797 --> 00:04:35,953
But we're going to see that this'll
overlap, of course,

86
00:04:35,953 --> 00:04:37,860
with things like statistics.

87
00:04:37,860 --> 00:04:41,310
I mean duh, most, a lot of statistics is
basically fitting models, which is,

88
00:04:41,310 --> 00:04:43,890
in some sense, approximating data.

89
00:04:45,230 --> 00:04:50,180
So we'll see connections between these
things, right, you shouldn't be surprised.

90
00:04:50,180 --> 00:04:52,437
Also, I should add that in the lectures
here,

91
00:04:52,437 --> 00:04:56,240
we're just going to cover some of the
high-level stuff.

92
00:04:56,240 --> 00:04:59,526
There's a lot more material in the book,
and you should absolutely read it,

93
00:04:59,526 --> 00:05:02,865
well, let me put it another way: we will
hold you absolutely responsible for

94
00:05:02,865 --> 00:05:05,651
everything in the book.

95
00:05:05,651 --> 00:05:09,135
So we'll exercise a reasonable fraction of
it through the homework, not all,

96
00:05:09,135 --> 00:05:11,357
because it's just too much.

97
00:05:11,357 --> 00:05:13,220
But we're, we will work with the,

98
00:05:13,220 --> 00:05:18,588
our working assumption is that you will
have read all of it, so okay.

99
00:05:18,588 --> 00:05:24,779
So we'll start with just this very general
idea of approximation and, and fitting.

100
00:05:24,779 --> 00:05:28,142
And we'll start with norm approximation
and then something which is

101
00:05:28,142 --> 00:05:32,154
a vague dual of it is least norm problems,
and then something that combines kind of

102
00:05:32,154 --> 00:05:38,290
both regularized approximation and then
we'll look at robust approximation.

103
00:05:38,290 --> 00:05:42,098
And this is sort of this is relatively
new, this is stuff from the last couple of

104
00:05:42,098 --> 00:05:45,650
years or last decade or something like
that.

105
00:05:45,650 --> 00:05:46,488
So, okay.

106
00:05:46,488 --> 00:05:50,870
So norm approximation.

107
00:05:50,870 --> 00:05:53,710
Well I mean the simplest case is just
this.

108
00:05:53,710 --> 00:05:56,200
We minimize the norm of Ax minus b, right?

109
00:05:56,200 --> 00:06:01,273
And you have your data is a matrix A
usually with m bigger than n, so

110
00:06:01,273 --> 00:06:04,299
it's taller than, it's a tall matrix,

111
00:06:04,299 --> 00:06:11,940
a skinny matrix and this is just any old
norm on Rm.

112
00:06:11,940 --> 00:06:15,784
So when we instantiate or specify the
norm, it becomes a specific problem,

113
00:06:15,784 --> 00:06:20,310
it becomes regression or least-squares,
that kind of thing.

114
00:06:20,310 --> 00:06:24,214
Okay, and now, when we're doing this if x
star is the minimum of this oh, and

115
00:06:24,214 --> 00:06:28,423
I should warn you right now the minimum
doesn't have to be unique, you know that,

116
00:06:28,423 --> 00:06:31,469
of course.

117
00:06:31,469 --> 00:06:36,170
For the two norm if A is full rank, it's
unique, right?

118
00:06:36,170 --> 00:06:39,140
But, in fact, for other norms it's
absolutely false.

119
00:06:39,140 --> 00:06:41,678
So if you do one norm minimization,
infinity norm minimization,

120
00:06:41,678 --> 00:06:44,950
it's incredibly common that there are
multiple optimizers.

121
00:06:44,950 --> 00:06:48,400
So here, this just, this notation, it, I
don't mean,

122
00:06:48,400 --> 00:06:54,727
I don't mean to suggest that there's a
unique one because there need not be.

123
00:06:54,727 --> 00:06:55,278
Okay.

124
00:06:55,278 --> 00:06:56,460
So, what's an interpretation?

125
00:06:56,460 --> 00:06:58,540
Well, there's lot's of interpretation, the
first one is geometric.

126
00:06:58,540 --> 00:07:02,720
Basically, what it says is vectors of the
form Ax were x varies,

127
00:07:02,720 --> 00:07:05,930
sweeps out the range of it.

128
00:07:05,930 --> 00:07:07,750
All right, so that's a subspace.

129
00:07:07,750 --> 00:07:11,288
And it says, then it says you have a fixed
vector b, and so the question, and

130
00:07:11,288 --> 00:07:16,530
then you have the, the distance as
measured by the norm between b and Ax.

131
00:07:16,530 --> 00:07:19,154
And so, basically, what you're saying is
please,

132
00:07:19,154 --> 00:07:22,674
this basically says the optical value of
this thing is literally,

133
00:07:22,674 --> 00:07:27,390
it is literal the distance of B to the
range of eight.

134
00:07:28,900 --> 00:07:31,609
So solving this problem is the same as
saying find me

135
00:07:31,609 --> 00:07:38,357
the distance under the norm, norm, here b
to range of A so that, that's what you do.

136
00:07:38,357 --> 00:07:40,880
So that's what it is.

137
00:07:40,880 --> 00:07:45,065
And and you say find a point closest to
this, right?

138
00:07:45,065 --> 00:07:51,080
And here, I didn't put v, so it's not
technically wrong.

139
00:07:51,080 --> 00:07:53,660
If I had put any article in here, it would
have been A,

140
00:07:53,660 --> 00:07:56,758
and then, it would have made it right.

141
00:07:56,758 --> 00:07:57,623
So, okay.

142
00:07:57,623 --> 00:08:01,740
Now another fit another, another way, this
comes up is in estimation.

143
00:08:01,740 --> 00:08:03,230
So in estimation it goes like this.

144
00:08:03,230 --> 00:08:05,796
It says, I, I, I want to estimate some
parameters x, okay?

145
00:08:05,796 --> 00:08:08,042
And what I have is I have linear
measurements of x.

146
00:08:08,042 --> 00:08:16,100
That's y equals a x, but they're corrupted
with noise.

147
00:08:16,100 --> 00:08:18,920
And that's the, so I add v, right?

148
00:08:18,920 --> 00:08:19,550
And so now,

149
00:08:19,550 --> 00:08:24,990
if I ask you to guess x interestingly,
implicitly, you are guessing v.

150
00:08:24,990 --> 00:08:30,086
That's really what you're doing, because
when you guess x,

151
00:08:30,086 --> 00:08:34,100
then y minus Ax is v, right?

152
00:08:34,100 --> 00:08:38,220
And so, the problem of guessing x is the
same as guessing v.

153
00:08:38,220 --> 00:08:42,290
And we'll see that this is exactly the
statistical interpretation as well, right?

154
00:08:42,290 --> 00:08:44,590
So you're basically guessing that.

155
00:08:44,590 --> 00:08:48,250
And so, what we're doing is something like
this, we're intuitively here,

156
00:08:48,250 --> 00:08:51,070
we'll do it quite differently when we do
statistics, but

157
00:08:51,070 --> 00:08:57,690
here, we're saying intuitively that among
possible v's smaller is more plausible.

158
00:08:57,690 --> 00:09:00,790
Smaller v in norm is more plausible than
larger V, right?

159
00:09:00,790 --> 00:09:02,320
So that, that's the implicit assumption
here.

160
00:09:02,320 --> 00:09:07,472
And in that case, you would say that
solving this problem would give

161
00:09:07,472 --> 00:09:13,288
you something like the most plausible
value of x.

162
00:09:13,288 --> 00:09:16,968
Right, because it's the one where the
implicit,

163
00:09:16,968 --> 00:09:24,720
when you are implicitly guessing v, it is
smallest as measured by the norm.

164
00:09:24,720 --> 00:09:27,552
Right, so that, tells you about, plaus,
you know, plausibility, it's,

165
00:09:27,552 --> 00:09:30,588
it's the least implausible, or something,
okay.

166
00:09:30,588 --> 00:09:32,640
Another one is optimal design.

167
00:09:32,640 --> 00:09:35,630
So here, x is a set of design parameters.

168
00:09:36,660 --> 00:09:38,250
That you could fiddle with.

169
00:09:38,250 --> 00:09:40,512
These could be a force profile, for

170
00:09:40,512 --> 00:09:45,000
a vehicle, it could be all sorts of
things, right?

171
00:09:45,000 --> 00:09:48,834
Then, you have a linear process, so "X"
causes the results, "Ax",

172
00:09:48,834 --> 00:09:53,910
then you have a target, which is "B" that
you want to hit.

173
00:09:53,910 --> 00:09:57,260
If you can hit it, fantastic.

174
00:09:57,260 --> 00:10:01,680
But if you can't you have to get, you have
to compromise and simply get close.

175
00:10:01,680 --> 00:10:04,542
And then, how do you determine, what your
happiness level is, or

176
00:10:04,542 --> 00:10:08,010
really, we should say your irritation
level, right.

177
00:10:08,010 --> 00:10:10,710
So, how irritated will you be, if you want
b,

178
00:10:10,710 --> 00:10:15,520
little b and get Ax, and we'll measure
that by a norm.

179
00:10:15,520 --> 00:10:17,710
So the norm encodes your irritation level.

180
00:10:17,710 --> 00:10:23,380
And then it says the best design is, is
obtained by solving this problem here.

181
00:10:23,380 --> 00:10:26,708
Because it gets you closest to what you
want, measured by your irritation and

182
00:10:26,708 --> 00:10:29,455
we're measuring irritation by a norm.

183
00:10:29,455 --> 00:10:30,520
Everybody got this?

184
00:10:30,520 --> 00:10:35,070
So these are, I mean, don't overinterpret
what I'm saying because what I'm saying is

185
00:10:35,070 --> 00:10:37,370
completely trivial.

186
00:10:37,370 --> 00:10:40,670
So, if you think for a second, I better go
think about this for

187
00:10:40,670 --> 00:10:44,110
a second, you're overinterpreting it.

188
00:10:44,110 --> 00:10:44,970
I'm saying nothing deep.

189
00:10:44,970 --> 00:10:46,160
And that's going to continue, by the way,
for

190
00:10:46,160 --> 00:10:48,070
a few more things I'm going to say about
this.

191
00:10:48,070 --> 00:10:51,566
So it really is this simple.

192
00:10:51,566 --> 00:10:55,244
Okay okay.

193
00:10:55,244 --> 00:10:58,450
Now, let's substantiate some specific
examples.

194
00:10:58,450 --> 00:11:01,490
If you take the two norm, you get least
squares, right?

195
00:11:01,490 --> 00:11:03,120
And that's the analytical solution of it.

196
00:11:03,120 --> 00:11:04,716
I mean, you know, if A is full rank,

197
00:11:04,716 --> 00:11:09,320
it's just the least-squares, it's a
transpose a and a transpose b.

198
00:11:09,320 --> 00:11:11,510
Right, so that's your least-squares
solution.

199
00:11:11,510 --> 00:11:12,210
oh, in that case,

200
00:11:12,210 --> 00:11:16,150
I should say that the problem has lot's of
names, it's called regression.

201
00:11:16,150 --> 00:11:17,716
In statistics least-squares,

202
00:11:17,716 --> 00:11:21,510
I guess it's called least-squares fitting
everywhere else.

203
00:11:21,510 --> 00:11:22,465
Okay?
So, that's the,

204
00:11:22,465 --> 00:11:25,963
the so there's probably some other name
specific fields for this, but I,

205
00:11:25,963 --> 00:11:29,126
I can't remember what they are right now.

206
00:11:29,126 --> 00:11:29,943
Okay.

207
00:11:29,943 --> 00:11:33,780
But interestingly, we can do other things.

208
00:11:33,780 --> 00:11:35,508
If you take an infinity norm,

209
00:11:35,508 --> 00:11:40,713
then it's called, it's actually got lots
of names actually.

210
00:11:40,713 --> 00:11:43,092
If you minimize the norm of Ax minus b
infinity,

211
00:11:43,092 --> 00:11:46,990
it's called Chebyshev approximation is one
name.

212
00:11:46,990 --> 00:11:51,245
Another name is minimax fitting, right?

213
00:11:51,245 --> 00:11:54,295
And, it's actually quite interesting what
minimax fitting is and

214
00:11:54,295 --> 00:11:56,580
it's it has its uses, right?

215
00:11:56,580 --> 00:12:00,925
It basically says I, I want to fit a
model, but what I care about is

216
00:12:00,925 --> 00:12:08,170
the absolute maximum error, like not the,
well, not the sum of the squares.

217
00:12:08,170 --> 00:12:10,830
I care about the absolute maximum, right?

218
00:12:10,830 --> 00:12:12,460
And that would be minimax fitting.

219
00:12:12,460 --> 00:12:15,366
It's got lots of applications.

220
00:12:15,366 --> 00:12:23,690
interestingly, least-squares of course is
used by zillions of applications, right?

221
00:12:23,690 --> 00:12:26,925
It's the basis of, it's the workhorse of,
you know, giant fields.

222
00:12:26,925 --> 00:12:30,580
It's pretty much it's the workhorse of
statistics, right?

223
00:12:30,580 --> 00:12:31,690
I mean, pretty much, right?

224
00:12:31,690 --> 00:12:34,640
All the advanced stuff people do that's
coming on, online a bit, but

225
00:12:34,640 --> 00:12:38,384
bottom line is most people are doing
regressions, right?

226
00:12:38,384 --> 00:12:41,980
And if you're snickering, making fun of
how unsophisticated people are in

227
00:12:41,980 --> 00:12:46,450
statistics, don't, because for example,
control is the same way.

228
00:12:46,450 --> 00:12:47,218
All of control, and

229
00:12:47,218 --> 00:12:50,620
I mean look, there are some advanced
control methods, that's fine.

230
00:12:50,620 --> 00:12:51,800
Are they working?

231
00:12:51,800 --> 00:12:53,030
Yes, they're working.

232
00:12:53,030 --> 00:12:55,626
But the bottom line is that most control
is kind of least is variations on

233
00:12:55,626 --> 00:12:56,880
least squares.

234
00:12:56,880 --> 00:13:00,128
Signal processing, image processing,
again, don't make fun of them,

235
00:13:00,128 --> 00:13:02,160
it's mostly least-squares.

236
00:13:02,160 --> 00:13:04,085
Are there more advanced methods coming
online?

237
00:13:04,085 --> 00:13:05,840
Absolutely, right?

238
00:13:05,840 --> 00:13:11,408
But mostly, it's done by least-squares, so
interestingly, what you find is,

239
00:13:11,408 --> 00:13:18,010
this is discussed almost not at all in any
standard curriculum.

240
00:13:18,010 --> 00:13:22,235
Bottom line is that the modern viewpoint,
in my opinion is, it turns out, we're

241
00:13:22,235 --> 00:13:27,710
going to find out later, computationally,
this is no harder than that.

242
00:13:28,930 --> 00:13:30,080
Shocking, isn't it?

243
00:13:30,080 --> 00:13:33,370
It's absolutely no harder, right?

244
00:13:33,370 --> 00:13:36,718
But no one knows it, because everyone in
this room has been exposed to this,

245
00:13:36,718 --> 00:13:39,590
probably in multiple contexts, right?

246
00:13:39,590 --> 00:13:43,661
In some class on Fourier series, on some
class on linear algebra, probably in other

247
00:13:43,661 --> 00:13:49,258
classes, you've seen this and probably no
one has seen this I'm guessing.

248
00:13:49,258 --> 00:13:55,276
So okay so anyway, back to the main thread
here.

249
00:13:55,276 --> 00:13:59,565
That this, you can solve this problem as a
linear program.

250
00:13:59,565 --> 00:14:02,035
Now that may or may not be relevant but,
and

251
00:14:02,035 --> 00:14:07,260
because many times, an automatic system
will do this for you, right?

252
00:14:07,260 --> 00:14:10,020
But nevertheless, it can be written as an
LP.

253
00:14:10,020 --> 00:14:12,504
What there is not is there's not a formula
like that, and

254
00:14:12,504 --> 00:14:17,106
that's why we don't teach it in some of
the more traditional courses.

255
00:14:17,106 --> 00:14:18,158
All right?

256
00:14:18,158 --> 00:14:20,790
So, okay, here's another very interesting
one.

257
00:14:20,790 --> 00:14:22,515
It also has a very long history.

258
00:14:22,515 --> 00:14:26,870
It's the sum of absolute values sum of
absolute residuals approximation use

259
00:14:26,870 --> 00:14:30,305
norm one that says minimize, this.

260
00:14:30,305 --> 00:14:32,880
That's extremely interesting.

261
00:14:32,880 --> 00:14:36,270
Again, no formula, you can solve it as an
LP.

262
00:14:36,270 --> 00:14:39,460
We'll find out later, the computational
effort, of solving this, and

263
00:14:39,460 --> 00:14:41,780
this, it's basically the same.

264
00:14:41,780 --> 00:14:43,870
It's the same, there's no difference,
right?

265
00:14:45,230 --> 00:14:48,086
I mean, there's an aesthetic difference,
if you're locked in a 19th

266
00:14:48,086 --> 00:14:52,870
century aesthetic, there's a big aesthetic
difference, but computationally, none.

267
00:14:52,870 --> 00:14:55,070
This is really interesting, this one.

268
00:14:55,070 --> 00:14:59,455
It's an example of what we're going to see
later is a robust estimator.

269
00:14:59,455 --> 00:15:01,915
The properties are shocking, and it's
actually,

270
00:15:01,915 --> 00:15:05,935
things like that are form the basis of,
actually, a lot of modern statistics, and

271
00:15:05,935 --> 00:15:10,130
machine learning, and a lot of other
things.

272
00:15:10,130 --> 00:15:11,525
So we'll, we'll, we'll see that.

273
00:15:11,525 --> 00:15:15,211
So it's quite different and, you know,
these are not new.

274
00:15:15,211 --> 00:15:19,144
The fact, well, this is Chebyshev, and I'm
not sure, I'm sure there's a Russian name

275
00:15:19,144 --> 00:15:23,940
we can attach to this, and it would be
traced to the 30s and 40s.

276
00:15:23,940 --> 00:15:26,713
And in fact, at Moscow State University,
I'll check with my friends and

277
00:15:26,713 --> 00:15:30,338
find out what the correct name is, but
there is surely one.

278
00:15:30,338 --> 00:15:32,582
And you know, so these are not new things,

279
00:15:32,582 --> 00:15:37,730
they've just kind of languished in
obscurity until the last decade.

280
00:15:37,730 --> 00:15:40,221
Now they're the height of fashion, of
course, and depending on,

281
00:15:40,221 --> 00:15:42,512
you know, different, various fields.

282
00:15:42,512 --> 00:15:43,093
Okay, so

283
00:15:43,093 --> 00:15:50,255
now we're going to look at sort of a
generalization of, of norm minimization.

284
00:15:50,255 --> 00:15:52,730
What we'll do is this we're going to just
to make the notation easier,

285
00:15:52,730 --> 00:15:54,870
we're going to introduce Ax minus b.

286
00:15:54,870 --> 00:15:59,110
We're going to call that the residual,
assign a name to it, r, okay?

287
00:15:59,110 --> 00:16:00,286
And then, what you'll want to do is,

288
00:16:00,286 --> 00:16:03,130
essentially, you want all the entries of r
small.

289
00:16:03,130 --> 00:16:06,530
In fact, if you can get all the entries of
r zero, that would be ideal, right?

290
00:16:06,530 --> 00:16:08,720
Because you'd have, I dunno, I mean,

291
00:16:08,720 --> 00:16:13,052
you got exactly what you wanted, Ax equals
b, right?

292
00:16:13,052 --> 00:16:14,532
So, but let's, assuming that's not the
case,

293
00:16:14,532 --> 00:16:18,130
you're going to have to compromise, and
you can't have all the residuals 0, right?

294
00:16:18,130 --> 00:16:23,170
So what we'll do is we'll penalize values
of R by function phi.

295
00:16:23,170 --> 00:16:25,360
That's just a function from R to R, okay?

296
00:16:25,360 --> 00:16:28,114
That's just, that's it, and so we're
going to minimize the sum of

297
00:16:28,114 --> 00:16:32,950
the penalties, you know, you can divide by
m and get the average penalty.

298
00:16:32,950 --> 00:16:34,786
I mean, it wouldn't make, it wouldn't
change the problem, but

299
00:16:34,786 --> 00:16:37,070
give you a better interpretation or
something, okay?

300
00:16:37,070 --> 00:16:39,054
So, now you can choose phi and, and

301
00:16:39,054 --> 00:16:43,470
please don't overinterpret what I'm going
to say, because you're going to want to,

302
00:16:43,470 --> 00:16:48,266
but actually it really is as trivial as
I'm saying.

303
00:16:48,266 --> 00:16:50,250
So here it is.

304
00:16:50,250 --> 00:16:58,434
You use phi to shape how irritated you are
with the residual of a certain size.

305
00:16:58,434 --> 00:16:59,954
That's it, okay?

306
00:16:59,954 --> 00:17:02,744
So for example, if you take phi is the
square function,

307
00:17:02,744 --> 00:17:05,844
then that tells you something like this,
if a residual is my,

308
00:17:05,844 --> 00:17:11,970
I'll ask you, if phi is the square
function, this is least squares.

309
00:17:11,970 --> 00:17:17,590
And, and that implicitly says that if
residuals is small, how irritated are you?

310
00:17:17,590 --> 00:17:18,980
Yeah, very little.

311
00:17:18,980 --> 00:17:19,680
Everyone got that?

312
00:17:19,680 --> 00:17:21,450
So that very means it's small.

313
00:17:21,450 --> 00:17:22,152
It's small squared.

314
00:17:22,152 --> 00:17:24,616
Okay, now, in least-squares, if the
residual is big,

315
00:17:24,616 --> 00:17:26,835
how much does it irritate you?

316
00:17:26,835 --> 00:17:30,580
Very much, thank you very much [LAUGH].

317
00:17:30,580 --> 00:17:32,410
And the very is not casual, right?

318
00:17:32,410 --> 00:17:37,560
It's not okay, right, so, so that's kind
of the idea behind least-squares, right?

319
00:17:37,560 --> 00:17:40,374
That, that, you know, you have a residual
small and

320
00:17:40,374 --> 00:17:44,720
you're like fine if it's big, not cool,
right?

321
00:17:44,720 --> 00:17:47,933
So, all right, and, and just don't
overinterpret this, right we'll,

322
00:17:47,933 --> 00:17:51,146
we'll, by, we'll have multiple
overinterpretations when we do statistics

323
00:17:51,146 --> 00:17:54,410
about densities and all sorts of crazy
stuff and maximum likelihood, but for

324
00:17:54,410 --> 00:18:00,720
now, take everything sophisticated out of
your mind and we're just doing this way.

325
00:18:00,720 --> 00:18:03,570
Now, if I take phi as the absolute value,
I get L1.

326
00:18:03,570 --> 00:18:04,380
Let's talk about that.

327
00:18:04,380 --> 00:18:08,628
If I take in L1, an absolute value
penalty, then if a residiual is small,

328
00:18:08,628 --> 00:18:11,870
how much does it irritate you?

329
00:18:11,870 --> 00:18:15,542
Let's compare, how, if you're, if you
compare that to the square,

330
00:18:15,542 --> 00:18:19,990
how much does it irritate you more, much
more, right?

331
00:18:19,990 --> 00:18:25,170
So in least-squares, a small residual
irritates you very little.

332
00:18:25,170 --> 00:18:26,495
I'm going to try to say it correctly.

333
00:18:26,495 --> 00:18:30,428
And, but in L1 norm, right, irritate, in
the absolute value, it says,

334
00:18:30,428 --> 00:18:36,200
you're irritated a little bit, but
relatively speaking, that's a lot.

335
00:18:36,200 --> 00:18:37,870
I mean compared to other squares, right?

336
00:18:37,870 --> 00:18:40,658
Okay how about this with absolute value if

337
00:18:40,658 --> 00:18:45,900
you've got a big residual how irritating
is that in least squares?

338
00:18:46,900 --> 00:18:50,292
Very.
And in absolute value?

339
00:18:50,292 --> 00:18:55,640
It's irritating without the variable.

340
00:18:55,640 --> 00:18:56,355
Everybody got this?

341
00:18:56,355 --> 00:18:57,410
Okay.

342
00:18:57,410 --> 00:19:00,434
So I mean, look this, I know this, this
sound, it really is this dumb,

343
00:19:00,434 --> 00:19:05,620
you'll, you'll, it really is, trust, trust
me, but this is unbelievably useful.

344
00:19:05,620 --> 00:19:06,545
Right?

345
00:19:06,545 --> 00:19:10,120
Because I think what has not been
recognized, recently, you know,

346
00:19:10,120 --> 00:19:11,810
until relatively recently and

347
00:19:11,810 --> 00:19:17,300
it's not by any means fully recognized
now, is the following fact.

348
00:19:17,300 --> 00:19:20,215
That least-squares is the one case where
you get some silly 19th

349
00:19:20,215 --> 00:19:22,490
century formula, nineteenth?

350
00:19:22,490 --> 00:19:24,010
But all the other things we're talking
about,

351
00:19:24,010 --> 00:19:25,820
you can solve just as easily, right?

352
00:19:25,820 --> 00:19:27,680
And so people haven't looked at them.

353
00:19:27,680 --> 00:19:28,320
So, all right.

354
00:19:28,320 --> 00:19:29,940
So let's look at some things.

355
00:19:29,940 --> 00:19:32,405
We just talked about the quadratic in
detail.

356
00:19:32,405 --> 00:19:33,999
here, let's do something else.

357
00:19:33,999 --> 00:19:37,059
Here's dead zone linear, that's this
function right here, look at this,

358
00:19:37,059 --> 00:19:38,844
it's it's linear, and then it's got a
dead,

359
00:19:38,844 --> 00:19:43,816
what people call a deadzone in the middle,
so between plus and minus 0.25.

360
00:19:43,816 --> 00:19:46,084
Someone you should articulate,

361
00:19:46,084 --> 00:19:51,125
what does that mean about how you care
about residuals?

362
00:19:51,125 --> 00:19:53,440
Well, in this case, it's very specific.

363
00:19:53,440 --> 00:19:55,870
If it's less than some threshold, it
doesn't irritate you at all.

364
00:19:55,870 --> 00:19:58,660
And, and in particular, that goes two
ways.

365
00:19:58,660 --> 00:20:00,648
It means that a residual of point,

366
00:20:00,648 --> 00:20:05,490
of .001 is absolutely no better than a
residual of 0.25 here.

367
00:20:05,490 --> 00:20:07,260
None, right?

368
00:20:07,260 --> 00:20:09,730
By the way, there's plenty of applications
of this and it doesn't, it doesn't,

369
00:20:09,730 --> 00:20:13,250
you know, you can think, they're going to
come to you, like, pretty quickly.

370
00:20:13,250 --> 00:20:17,390
you, if, if you have, like a an analog
digital converter and you, you make

371
00:20:17,390 --> 00:20:21,650
a measurement, then, you know, being off
by something less than you're your, your

372
00:20:21,650 --> 00:20:28,380
lsb or least significant bit, it's not
better, I mean it's, you're not better.

373
00:20:28,380 --> 00:20:30,684
All you know is that this signal value is
in some interval,

374
00:20:30,684 --> 00:20:33,270
you don't know anything else.

375
00:20:33,270 --> 00:20:36,975
Okay, and the fact that it's linear once
you go outside the deadzone,

376
00:20:36,975 --> 00:20:42,360
let's talk about how much you care about
larger residuals.

377
00:20:42,360 --> 00:20:44,990
So you'd say, look, I don't, you know, who
likes large residuals?

378
00:20:44,990 --> 00:20:45,540
No one.

379
00:20:45,540 --> 00:20:47,108
I don't like large residuals, but

380
00:20:47,108 --> 00:20:50,270
you don't go nuts about it as with the
quadratic.

381
00:20:50,270 --> 00:20:51,310
That would be the explanation.

382
00:20:51,310 --> 00:20:52,030
Everybody got this?

383
00:20:52,030 --> 00:20:54,882
And by the way, we're anthropomorphizing
all this, but

384
00:20:54,882 --> 00:21:00,260
what we're going to find out is things,
this does exactly what you want, right?

385
00:21:00,260 --> 00:21:01,190
So, okay.

386
00:21:01,190 --> 00:21:03,020
So here's another weird one.

387
00:21:03,020 --> 00:21:06,857
Here's log barrier with a limit.

388
00:21:06,857 --> 00:21:09,250
So this is just an expression, it doesn't
matter what it is.

389
00:21:09,250 --> 00:21:12,770
I can tell you what it is, it matches the
quadratic almost perfectly over plus

390
00:21:12,770 --> 00:21:16,390
minus you know, over, over a pretty big
range.

391
00:21:16,390 --> 00:21:20,668
It matches the quadratic very closely, but
then what happens is it accelerates and

392
00:21:20,668 --> 00:21:23,140
goes up to plus infinity.

393
00:21:23,140 --> 00:21:25,050
Someone explain that to me.

394
00:21:25,050 --> 00:21:28,300
It means that I do not even want to
discuss the possibility of

395
00:21:28,300 --> 00:21:33,360
residual more than plus or minus one I
won't allow it, right?

396
00:21:33,360 --> 00:21:36,960
But for smaller residuals it's just like
lee squares okay so

397
00:21:36,960 --> 00:21:39,840
these are the idea now, the whole point of
this is

398
00:21:39,840 --> 00:21:47,870
that you would craft your own your own
penalty function, and let's just try one.

399
00:21:47,870 --> 00:21:50,678
Suppose I'm doing least-squares fitting or
regression, so I,

400
00:21:50,678 --> 00:21:53,540
I just want to minimize, I want Ax to
equal b.

401
00:21:53,540 --> 00:21:58,963
But suppose I said that I care about, take
the residual r.

402
00:21:58,963 --> 00:22:05,700
And then overfit, let's say that being
above is okay.

403
00:22:05,700 --> 00:22:06,550
I don't want to be below.

404
00:22:06,550 --> 00:22:10,600
I, I, really I, I, I'm, I care four times
as much about being below.

405
00:22:10,600 --> 00:22:13,080
So, you can interview me and you can say,
tell me about your residuals.

406
00:22:13,080 --> 00:22:14,780
And the model and the data that you
want to fit.

407
00:22:14,780 --> 00:22:19,350
And I'd say, well I want it to fit.

408
00:22:19,350 --> 00:22:20,838
And some, then you'd, some would ask,

409
00:22:20,838 --> 00:22:23,990
well, what about, how do you care about
big residuals?

410
00:22:23,990 --> 00:22:25,280
And you'd say, I don't like them.

411
00:22:25,280 --> 00:22:26,584
I don't.
And of course, I don't like them,

412
00:22:26,584 --> 00:22:27,256
you say, but I mean,

413
00:22:27,256 --> 00:22:31,090
will you go way out of your way to not
have a big [INAUDIBLE], you don't know.

414
00:22:31,090 --> 00:22:32,800
If we have to have some big ones, well
have some big ones.

415
00:22:32,800 --> 00:22:33,887
Okay?
You should get,

416
00:22:33,887 --> 00:22:37,073
you should, we just nailed, by the way,
the asymptotics of the, of

417
00:22:37,073 --> 00:22:42,980
the penalty function, and then you'd say,
well tell me about under and over fitting.

418
00:22:42,980 --> 00:22:44,390
Is it symmetric?

419
00:22:44,390 --> 00:22:46,994
You know what under and overfitting is, it
means you look at an entry of b and

420
00:22:46,994 --> 00:22:50,500
you look at an entry of Ax, and you
compare the numbers, right?

421
00:22:50,500 --> 00:22:53,210
If you say I don't care, I mean, above,
below, it doesn't matter to me.

422
00:22:53,210 --> 00:22:57,406
And you, but suppose you said, no actually
there's an asymmetry there.

423
00:22:57,406 --> 00:23:01,070
It's much worse to underestimate.

424
00:23:01,070 --> 00:23:03,435
So if Ax is less than b, that's bad.

425
00:23:03,435 --> 00:23:06,755
Right, how bad?

426
00:23:06,755 --> 00:23:09,980
[INAUDIBLE], got reasonable factor for
something, it's five times worse.

427
00:23:09,980 --> 00:23:14,372
By the way, there's a bigger picture here,
which actually fits with all of

428
00:23:14,372 --> 00:23:20,593
observatio that the idea is not to focus
on the L grid that produces x.

429
00:23:20,593 --> 00:23:26,380
What you do is, your tools are, is the
problem.

430
00:23:26,380 --> 00:23:27,622
You formulate phi and

431
00:23:27,622 --> 00:23:33,160
the optimization will carry out your
wishes to he extent that it can, right?

432
00:23:33,160 --> 00:23:33,950
So, that's the idea.

433
00:23:33,950 --> 00:23:38,570
So, you should express your wishes in
fitting, by setting a penalty function or

434
00:23:38,570 --> 00:23:44,070
something like that, then let optimization
sort out the details.

435
00:23:44,070 --> 00:23:44,750
Everybody see what I am saying?

436
00:23:44,750 --> 00:23:47,429
I mean they're totally obvious things,
but, believe me,

437
00:23:47,429 --> 00:23:50,165
they have not been, they have not been
internalized by

438
00:23:50,165 --> 00:23:55,950
the vast majority people doing kind of
mathematically sophisticated things.

439
00:23:55,950 --> 00:23:58,030
So, let's look at a, here's an example,
very simple.

440
00:23:58,030 --> 00:24:01,132
Let me just generate a random data problem
with a hundred

441
00:24:01,132 --> 00:24:05,932
measurements thirty parameters that we
want to estimate.

442
00:24:05,932 --> 00:24:09,370
These are and then we, we just solve
various problems.

443
00:24:09,370 --> 00:24:14,867
We'll do one, we'll do one norm, we'll do
least-squares regression.

444
00:24:14,867 --> 00:24:21,170
We'll do a deadzone-linear, and we'll do
this analytic center, what did I call it?

445
00:24:21,170 --> 00:24:22,810
I forget what I just called it log
barrier.

446
00:24:22,810 --> 00:24:23,760
Okay, yeah, log barrier.

447
00:24:23,760 --> 00:24:24,720
So we'll do log barrier.

448
00:24:25,860 --> 00:24:28,850
By the way, the name of the point is the
analytic center, we'll get to that later.

449
00:24:28,850 --> 00:24:32,930
But, okay, so what's plotted here, let me
explain what's plotted.

450
00:24:32,930 --> 00:24:34,241
This function here, so

451
00:24:34,241 --> 00:24:40,560
each of these, that's one norm, two norm,
well, square, deadzone-linear log barrier.

452
00:24:40,560 --> 00:24:44,110
And you see the actual function plotted on
some scale here.

453
00:24:44,110 --> 00:24:46,210
So this function tells you, kind of,

454
00:24:46,210 --> 00:24:50,806
your irritation at having a residual of
that level, 'kay?

455
00:24:50,806 --> 00:24:53,848
And then, what's shown here is the
histogram of

456
00:24:53,848 --> 00:24:56,910
residuals that you get, right?

457
00:24:56,910 --> 00:24:59,280
So you get different Xs in each case,
right?

458
00:24:59,280 --> 00:25:01,175
And you get different histograms.

459
00:25:01,175 --> 00:25:03,065
We'll start with least-squares, because
this would be the most,

460
00:25:03,065 --> 00:25:05,242
this is kind of the starting point for
everything.

461
00:25:05,242 --> 00:25:06,194
Here they are.

462
00:25:06,194 --> 00:25:07,693
Well, it's kind of nice.

463
00:25:07,693 --> 00:25:11,242
You get a lot, bunch of, get, a bunch that
are out here.

464
00:25:11,242 --> 00:25:14,386
You have a few that are big, not so big,
you know.

465
00:25:14,386 --> 00:25:16,478
But they're all bunched in here.

466
00:25:16,478 --> 00:25:17,072
Okay.

467
00:25:17,072 --> 00:25:19,013
Let's look at the L1 case.

468
00:25:19,013 --> 00:25:21,177
You get something, bizarre.

469
00:25:21,177 --> 00:25:25,973
And it is completely explained,
intuitively, by what we talked about.

470
00:25:25,973 --> 00:25:30,849
The first thing you see that catches your
eye is, that's a different scale.

471
00:25:30,849 --> 00:25:35,606
Is, you've got this giant pile of Xs, that
sorry, of residuals, that are flat 0,

472
00:25:35,606 --> 00:25:41,832
and by the way, it's not that they're
small, they are actually zero, 'kay?

473
00:25:41,832 --> 00:25:47,700
So and okay, so I'll give a very simple
intuitive explanation for this.

474
00:25:47,700 --> 00:25:53,113
The incentive to drive a residual to 0
holds up until it's 0.

475
00:25:53,113 --> 00:25:55,925
So, if you're, if a residual is at this
value,

476
00:25:55,925 --> 00:26:00,679
there's still a strong incentive to reduce
it, okay?

477
00:26:00,679 --> 00:26:02,779
Now, that's completely false for

478
00:26:02,779 --> 00:26:08,708
quadratic, because the smaller a residual
becomes, the less incentive theory.

479
00:26:08,708 --> 00:26:11,599
And incentive is just the slope of this
thing, it tells you,

480
00:26:11,599 --> 00:26:15,198
it says, you know how much benefit would I
get in reducing the objective to

481
00:26:15,198 --> 00:26:19,030
make that residual slightly smaller?

482
00:26:19,030 --> 00:26:22,446
And the answer in least-squares is you
have a diminishing residual

483
00:26:22,446 --> 00:26:24,053
benefit, right?

484
00:26:24,053 --> 00:26:26,182
Here it holds up to the very end.

485
00:26:26,182 --> 00:26:29,322
Okay, and the result is a whole bunch of
them become 0.

486
00:26:29,322 --> 00:26:32,400
By the way, for those of you who've taken
machine learning class,

487
00:26:32,400 --> 00:26:36,879
a statistics class, whole bunch of
classes, you'll recognize this.

488
00:26:36,879 --> 00:26:40,299
You should have already have heard about
things like L1 and sparsity and

489
00:26:40,299 --> 00:26:43,092
things like that, and we're going to see,
you're going to have in

490
00:26:43,092 --> 00:26:49,516
your head burned a connection between L1
and sparsity where things are actually 0.

491
00:26:49,516 --> 00:26:53,468
[COUGH] And in fact, it turns out it's not
L1, it's just the point.

492
00:26:53,468 --> 00:26:56,884
It, it turns out the point is here, so
our, the one we looked at before, our,

493
00:26:56,884 --> 00:27:00,551
our asymmetric one would have the same
property.

494
00:27:00,551 --> 00:27:03,363
All right, but the other interesting thing
is this.

495
00:27:03,363 --> 00:27:07,545
Look at that, there's a residual out here
which is like one point eight.

496
00:27:07,545 --> 00:27:14,100
And you didn't get that in the L2 one,
right?

497
00:27:14,100 --> 00:27:18,415
Because 1.8 is a big residual, and in
these squares you care about that a lot.

498
00:27:18,415 --> 00:27:20,000
1.8 squared, right?

499
00:27:20,000 --> 00:27:23,640
And so, again, I'm anthropomorphizing, but
in these squares,

500
00:27:23,640 --> 00:27:27,700
pushing these down here was a good
trade-off in terms of these things,

501
00:27:27,700 --> 00:27:34,010
split and moved all around here, and so on
and that sort of stuff.

502
00:27:34,010 --> 00:27:36,714
I mean, this is all just
anthropomorphization of all this, but

503
00:27:36,714 --> 00:27:40,690
this is the idea, okay here's
deadzone-linear.

504
00:27:40,690 --> 00:27:42,438
And you see something very very
interesting,

505
00:27:42,438 --> 00:27:46,370
that if your residual is between plus and
minus 1 half, we don't care at all.

506
00:27:47,790 --> 00:27:50,274
One half, a residual of 1 half is,

507
00:27:50,274 --> 00:27:55,780
in terms of irritation, no better, no
worse than 0.

508
00:27:55,780 --> 00:28:02,900
That deadzone, deadzone-linear says
there's an incentive to reduce a residual.

509
00:28:02,900 --> 00:28:06,230
The incentive entirely disappears once a
residual has hit one half.

510
00:28:06,230 --> 00:28:07,030
Right?
And in fact,

511
00:28:07,030 --> 00:28:09,410
that's what's happened, right?

512
00:28:09,410 --> 00:28:13,440
So, these are sort of residuals, that,
well, you know, why not?

513
00:28:13,440 --> 00:28:17,410
But they weren't forced to be between
there, they just happened this way.

514
00:28:17,410 --> 00:28:19,412
And here is the log barrier, and

515
00:28:19,412 --> 00:28:24,700
sure enough, as required, they've all been
crushed down here.

516
00:28:25,840 --> 00:28:28,744
So now let me introduce some really cool
stuff you can

517
00:28:28,744 --> 00:28:31,420
make cool, Frankenstein-like.

518
00:28:31,420 --> 00:28:35,763
We can cut up penalties and re-sew them
together and

519
00:28:35,763 --> 00:28:42,860
make, you know, things that I guess don't
come up obviously.

520
00:28:42,860 --> 00:28:46,717
Here's a The very famous one is the Huber
penalty function.

521
00:28:46,717 --> 00:28:48,510
The Huber penalty function is this.

522
00:28:48,510 --> 00:28:54,780
It's, quadratic, up to a, a threshold,
where upon it, it transitions to linear.

523
00:28:54,780 --> 00:28:59,776
And so, this is the Huber function here
with threshold M.

524
00:28:59,776 --> 00:29:03,108
actually, it's a very important function
in, you know, for

525
00:29:03,108 --> 00:29:06,508
I would say 80% of where people use
least-squares right now,

526
00:29:06,508 --> 00:29:13,520
my guess is that in 80% of those cases,
the application would be improved.

527
00:29:13,520 --> 00:29:16,142
The performance in whatever the
application would be improved if it

528
00:29:16,142 --> 00:29:19,271
was replaced with a Huber, and no one
knows that, right?

529
00:29:19,271 --> 00:29:20,171
I mean people know that,

530
00:29:20,171 --> 00:29:23,630
people do advance statistics, they always
do this kind of stuff, right?

531
00:29:23,630 --> 00:29:28,114
But in lots and lots of other areas they
don't know that.

532
00:29:28,114 --> 00:29:29,250
So that's the Huber function.

533
00:29:29,250 --> 00:29:33,183
First, let's just talk through what it
means so the Huber function so something

534
00:29:33,183 --> 00:29:36,945
like this if you are below the threshold
of the residual is below the threshold,

535
00:29:36,945 --> 00:29:42,695
it is identical to least-squares,
basically says, it's least-squares.

536
00:29:42,695 --> 00:29:46,246
But if you go over the threshold, it's
radically different from least-squares,

537
00:29:46,246 --> 00:29:50,980
and what, what can you say about a Huber
penalty as opposed to least-squares?

538
00:29:54,090 --> 00:29:59,600
It looked like an L1, and it is a penalty
that it, it's more relaxed.

539
00:29:59,600 --> 00:30:01,985
So you would say something, you could just
guess this,

540
00:30:01,985 --> 00:30:04,635
you would say that if you do Huber
Approximation, let's say,

541
00:30:04,635 --> 00:30:09,268
you'll end up with something like this,
compared to least squares.

542
00:30:09,268 --> 00:30:10,381
It will be, it's going to be,

543
00:30:10,381 --> 00:30:14,038
if everything, if all the residuals are in
the threshold, below the threshold, it,

544
00:30:14,038 --> 00:30:16,953
it's just exactly the same as
least-squares, but what it will do is

545
00:30:16,953 --> 00:30:23,320
it's going to be much more relaxed about
having a couple of outliers.

546
00:30:23,320 --> 00:30:25,210
So it will actually allow a couple of
outliers.

547
00:30:25,210 --> 00:30:26,354
It's not going to like it, but

548
00:30:26,354 --> 00:30:29,730
it's not going to go insane, like least
squares will.

549
00:30:29,730 --> 00:30:31,990
And it won't, it won't actually ruin
everything else.

550
00:30:31,990 --> 00:30:35,530
It won't increase all those better things
to try, to, to improve one, okay?

551
00:30:35,530 --> 00:30:37,200
So it turns out this is a robust
estimator.

552
00:30:37,200 --> 00:30:38,976
We'll see lots of, we'll see lots of this
in the future, but

553
00:30:38,976 --> 00:30:40,462
it's a robust estimator.

554
00:30:40,462 --> 00:30:44,285
It works unbelievably well.

555
00:30:44,285 --> 00:30:47,320
And so let me show you, the kind of thing
it works on.

556
00:30:47,320 --> 00:30:51,720
So, here's a, here's a picture [COUGH]
with, you know, 42 points, okay.

557
00:30:51,720 --> 00:30:55,860
So, 40 points are obviously scattered on
that line, you know, on a line.

558
00:30:55,860 --> 00:31:01,800
And, these are two, you know, obvious
outliers, okay.

559
00:31:01,800 --> 00:31:04,610
So, and we just made the data, just, just
plot it so you can see it.

560
00:31:04,610 --> 00:31:07,759
You know, obviously for fitting data at
two dimensions you don't need any fancy,

561
00:31:07,759 --> 00:31:09,930
you use your eyeball to do it.

562
00:31:09,930 --> 00:31:12,045
So, that's not what this is for, okay.

563
00:31:12,045 --> 00:31:14,530
This is just to illustrate what happens,
okay.

564
00:31:14,530 --> 00:31:19,938
So there's your data If you do the least
squares fit, you get this dash line here.

565
00:31:19,938 --> 00:31:21,970
Everybody see that?

566
00:31:21,970 --> 00:31:23,890
And the discussion is extremely simple.

567
00:31:23,890 --> 00:31:28,900
It's basically look that is so big, that
when you square it, it's gigantic.

568
00:31:28,900 --> 00:31:32,186
It's extremely irritating and least
squares will actually bend the fit to

569
00:31:32,186 --> 00:31:34,359
reduce that big number which you're
squaring and

570
00:31:34,359 --> 00:31:37,900
it will take a hit on all those other
numbers.

571
00:31:37,900 --> 00:31:39,226
Everybody Got that?

572
00:31:39,226 --> 00:31:40,393
Okay.
Now, and by the way,

573
00:31:40,393 --> 00:31:44,120
in your head, you can take these two
things, and you can move them this way.

574
00:31:44,120 --> 00:31:45,610
You can make them much worse.

575
00:31:45,610 --> 00:31:48,571
I can make the least, I can make the least
square thing actually flip over and

576
00:31:48,571 --> 00:31:50,414
have the wrong slope.

577
00:31:50,414 --> 00:31:51,370
Okay?

578
00:31:51,370 --> 00:31:53,150
Now by the way, if you think.

579
00:31:53,150 --> 00:31:56,686
I mean, of course, in our plotting a silly
data with like, you know, one varia-, it,

580
00:31:56,686 --> 00:31:58,558
it's goofy, right?

581
00:31:58,558 --> 00:32:02,914
But the point is, you got the same thing
when you are fitting things like,

582
00:32:02,914 --> 00:32:08,690
things in dimension, in, in very high
dimensions, like dimension 100.

583
00:32:08,690 --> 00:32:11,990
At which point, by the way, your eyeball
is no good, just for the record.

584
00:32:11,990 --> 00:32:19,842
[LAUGH] so, and it's, so, so the Huber fit
is that Is the dark line here.

585
00:32:19,842 --> 00:32:21,029
Right?

586
00:32:21,029 --> 00:32:23,990
And it's false to say that it's
unaffected.

587
00:32:23,990 --> 00:32:25,220
That is actually false.

588
00:32:25,220 --> 00:32:28,325
Because I could also show you a fit where
I remove these two.

589
00:32:28,325 --> 00:32:29,820
Right?

590
00:32:29,820 --> 00:32:32,912
And then, and that black line would move
very slightly.

591
00:32:32,912 --> 00:32:34,092
Okay?

592
00:32:34,092 --> 00:32:37,420
So, and, and, and, would, would then fit
those things perfectly.

593
00:32:37,420 --> 00:32:40,310
By the way, that's also a standard
two-step procedure in outlier.

594
00:32:40,310 --> 00:32:42,454
You first use something like a Huber or

595
00:32:42,454 --> 00:32:46,675
robust estimator to guess which are the
outliers, remove them and refit, so

596
00:32:46,675 --> 00:32:52,612
that's, that is the whole, I mean a whole
lot of stuff on this.

597
00:32:52,612 --> 00:32:56,029
So this is a, this is this, by the way
these, this thing works so

598
00:32:56,029 --> 00:33:00,183
well for a lot of serve practical data,
you know, if you blabber on and on and

599
00:33:00,183 --> 00:33:05,865
on about things like big data and stuff
like that.

600
00:33:05,865 --> 00:33:10,823
So things like, things like these, these
robust estimators are absolutely critical

601
00:33:10,823 --> 00:33:14,180
in, in a situation like that, right?

602
00:33:14,180 --> 00:33:16,406
Because when you have a billion pieces of
data, and

603
00:33:16,406 --> 00:33:20,130
you know a thousand of them are complete
and utter nonsense.

604
00:33:20,130 --> 00:33:24,690
There's out liars, that can render lease
squares utterly useless.

605
00:33:24,690 --> 00:33:26,751
And these things will just power through
it.

606
00:33:26,751 --> 00:33:29,373
Actually in Shocking ways where you can
take a prob,

607
00:33:29,373 --> 00:33:33,800
we'll have some homework problems on this,
but it'll be shocking.

608
00:33:33,800 --> 00:33:38,356
You take something, you add in outliers at
the 10, 10% of the data will be outliers,

609
00:33:38,356 --> 00:33:43,600
20, 30, and that's usually a threshold
point where it fails.

610
00:33:43,600 --> 00:33:47,280
But you can get just absolutely sh, and
it.

611
00:33:47,280 --> 00:33:50,130
A lot of times you have to double-check
that what you're looking at is

612
00:33:50,130 --> 00:33:51,670
the right thing.

613
00:33:51,670 --> 00:33:55,400
You think maybe you've used the true
parameter, not the estimated one.

614
00:33:55,400 --> 00:33:57,060
These things work so well.

615
00:33:57,060 --> 00:34:01,156
So, and this is something that, you know,
it's in, it's implicit in a lot of

616
00:34:01,156 --> 00:34:05,828
modern statistics, it's implicit in a lot
of machine learning and things like that,

617
00:34:05,828 --> 00:34:09,988
so it's a, but it's a very good thing for
ev, in my opinion, everyone should know

618
00:34:09,988 --> 00:34:14,916
about this, these, these kinds of things,
So we'll look now at something it's like

619
00:34:14,916 --> 00:34:24,840
the dual of the approximation problems or
norm approximation problems.

620
00:34:24,840 --> 00:34:29,523
And that's a least-norm problem, and the
basic idea is, is this.

621
00:34:29,523 --> 00:34:31,795
Here you have a quality constraint, so

622
00:34:31,795 --> 00:34:36,795
in this case A is not generically tall,
it's generically wide.

623
00:34:36,795 --> 00:34:39,320
So, A is a wide matrix.

624
00:34:39,320 --> 00:34:44,065
And this says that I have something like m
equality constraints on x, presumably not

625
00:34:44,065 --> 00:34:47,640
enough to complete constrain x, I mean
then it's a silly problem as

626
00:34:47,640 --> 00:34:52,060
one feasible point, and then among the xs
that satisfy ax equals b, we're going to

627
00:34:52,060 --> 00:35:00,560
choose the one that has minimum norm, and
you've probably seen this in some class.

628
00:35:00,560 --> 00:35:04,421
If you took 263 this was a least norm
problem, it was the two norm there.

629
00:35:07,030 --> 00:35:10,849
So what's the interpretation of this, and
of course I should say something here,

630
00:35:10,849 --> 00:35:13,756
when I say equals argmin, if this norm is
strictly convex, and

631
00:35:13,756 --> 00:35:19,200
that would be the case for a two norm,
then it's unique, of course.

632
00:35:19,200 --> 00:35:22,270
But it can easily be not unique, right?

633
00:35:22,270 --> 00:35:24,990
So we should say something like, well,

634
00:35:24,990 --> 00:35:29,155
one notation would be x star is in Argmin,
because one, one,

635
00:35:29,155 --> 00:35:37,141
one notational convention is that argmin
returns the set of minimizers.

636
00:35:37,141 --> 00:35:41,160
So by x star equals argmin, we just mean
it's a minimizer.

637
00:35:41,160 --> 00:35:43,997
So the geometric interpretation is this.

638
00:35:43,997 --> 00:35:47,447
We have an affine set that's generically
given by the set of x

639
00:35:47,447 --> 00:35:50,158
such that Ax equals b.

640
00:35:50,158 --> 00:35:55,170
So we have this affine set, and what we
want to do is find the point.

641
00:35:55,170 --> 00:35:58,190
Closest to zero in that set, right?

642
00:35:58,190 --> 00:36:01,258
And a simple completely elementary
variation on this is

643
00:36:01,258 --> 00:36:06,040
where we project a point in, a non, in a
non-zero point on, onto set.

644
00:36:06,040 --> 00:36:09,940
So that's the idea it's a, it's a
projection of zero.

645
00:36:09,940 --> 00:36:11,253
Onto the affine set.

646
00:36:11,253 --> 00:36:12,987
Estimation is this.

647
00:36:12,987 --> 00:36:18,420
The idea in the estimation interpretation
is something like this.

648
00:36:18,420 --> 00:36:24,372
We interpret b equals Ax to mean that we
have m perfect linear measurements.

649
00:36:24,372 --> 00:36:26,788
So there are, there's no noise.

650
00:36:26,788 --> 00:36:28,730
Nothing, they're perfect.

651
00:36:28,730 --> 00:36:32,030
And then, but we have n parameters to
estimate.

652
00:36:32,030 --> 00:36:34,550
And we have more parameters than we have
measurements.

653
00:36:34,550 --> 00:36:36,780
Right, so I have 150 parameters to
estimate.

654
00:36:36,780 --> 00:36:40,090
I have 50 measurements or 100
measurements, something like that.

655
00:36:40,090 --> 00:36:43,990
And that leaves me basically 50 unknown
dimensions or something like that.

656
00:36:43,990 --> 00:36:46,170
So Ax = b in that case.

657
00:36:46,170 --> 00:36:48,926
We interpret as the set of parameter
values are consistent with

658
00:36:48,926 --> 00:36:51,250
the perfect measurements, right?

659
00:36:51,250 --> 00:36:52,848
So this, if you were to put a comment
here,

660
00:36:52,848 --> 00:36:57,080
you'd say something like "consistent with
measurements", like, like this, right?

661
00:36:57,080 --> 00:36:59,085
And the, the assumption is the
measurements are perfect.

662
00:36:59,085 --> 00:37:00,840
Right?

663
00:37:00,840 --> 00:37:02,100
So, that would be that.

664
00:37:02,100 --> 00:37:05,458
And then the interpretation of minimizing
the norm is,

665
00:37:05,458 --> 00:37:08,920
you then choose most plausible, right?

666
00:37:08,920 --> 00:37:12,050
Choose the most plausible point.

667
00:37:12,050 --> 00:37:12,780
Right?

668
00:37:12,780 --> 00:37:16,330
And here implicit is the outfollowing
idea.

669
00:37:16,330 --> 00:37:20,870
That the larger x is, the less plausible
it is, right?

670
00:37:20,870 --> 00:37:22,020
So that's, that's the idea.

671
00:37:22,020 --> 00:37:24,924
In other words, you're in some situation
where there are many parameter values

672
00:37:24,924 --> 00:37:27,800
consistent with the measurements.

673
00:37:27,800 --> 00:37:30,816
So, nobody, any, anybody who chooses a x
with a x equals be cannot say to

674
00:37:30,816 --> 00:37:35,935
another person who chooses another x that
satisfy x would be that they're wrong.

675
00:37:35,935 --> 00:37:38,826
What, what one can do then is just talk
about which is more plausible, or

676
00:37:38,826 --> 00:37:40,570
something like that.

677
00:37:40,570 --> 00:37:43,375
And so the idea is that the norm is here
used as a surrogate for,

678
00:37:43,375 --> 00:37:49,760
I should say implausibility, because the
larger the norm the less plausible it is.

679
00:37:49,760 --> 00:37:53,060
And, by the way, we'll connect this
possibly even later today,

680
00:37:53,060 --> 00:37:57,552
we'll connect this to a statistical
interpretation.

681
00:37:57,552 --> 00:37:59,040
Another one is design.

682
00:37:59,040 --> 00:38:03,200
So here you think of x as a bunch of
Design variables.

683
00:38:03,200 --> 00:38:05,906
These are inputs for example, this could
be forces that you apply to a vehicle or

684
00:38:05,906 --> 00:38:07,680
something like that right?

685
00:38:07,680 --> 00:38:11,145
It could be voltages or current drives
that you're going to send to a motor, or,

686
00:38:11,145 --> 00:38:15,048
or current drives that you're going to
send to an antenna.

687
00:38:15,048 --> 00:38:18,560
Right, something like that.

688
00:38:18,560 --> 00:38:21,830
Then a x gives you something like the
result.

689
00:38:21,830 --> 00:38:23,810
Right, so this, this gives you the result.

690
00:38:23,810 --> 00:38:25,394
So, and it might be something like this,

691
00:38:25,394 --> 00:38:30,394
if you choose 150 forces to apply, maybe
at different times, different actuators.

692
00:38:30,394 --> 00:38:33,370
But actually all you care about are maybe
six or eight things at the end, for

693
00:38:33,370 --> 00:38:38,090
example the position and momentum at some
final target time, something like that.

694
00:38:38,090 --> 00:38:38,590
Right.

695
00:38:38,590 --> 00:38:43,530
In which case Ax gives you the result Of
the action X.

696
00:38:43,530 --> 00:38:50,070
Then b is actually your desired action, so
any x that satisfies a equals b is valid.

697
00:38:50,070 --> 00:38:53,020
That's basically, find me a set of forces
that will take my spacecraft,

698
00:38:53,020 --> 00:38:54,920
move it over to here twelve seconds later,
and

699
00:38:54,920 --> 00:38:58,730
arrive here you know, with this position
and momentum.

700
00:38:58,730 --> 00:39:01,857
Right, so that would be, lots of x's do
that And here what you're doing is

701
00:39:01,857 --> 00:39:06,720
you're saying please find me an efficient
one, one that minimizes a norm.

702
00:39:06,720 --> 00:39:09,460
Could be two norms, could be one norm.

703
00:39:09,460 --> 00:39:14,370
One norm would be closer to fuel usage in
a lot of cases, something like that.

704
00:39:14,370 --> 00:39:19,192
It could be infinity, it could be anything
like that.

705
00:39:19,192 --> 00:39:23,644
Okay, so this, this just interpretations
of the problem

706
00:39:23,644 --> 00:39:30,972
minimizing the norm of of x subject to ax
equals b equality constraints.

707
00:39:30,972 --> 00:39:32,254
Right.

708
00:39:32,254 --> 00:39:35,830
Oh, I should say that one nice thing
about.

709
00:39:35,830 --> 00:39:38,227
Our approach to these, which is simply to
look at that and

710
00:39:38,227 --> 00:39:41,120
simply say, that's a convex problem,
period.

711
00:39:41,120 --> 00:39:41,620
Right?

712
00:39:41,620 --> 00:39:43,820
One nice thing is, you can mix and match
if all of a sudden comes,

713
00:39:43,820 --> 00:39:47,820
somebody comes back and says oh dear, you
know, I need to add some constraints here.

714
00:39:47,820 --> 00:39:50,660
Like for example the xs have to be between
plus and minus one.

715
00:39:50,660 --> 00:39:52,990
It's not a big deal for us, we just add
those constraints.

716
00:39:52,990 --> 00:39:53,685
Right?
So.

717
00:39:53,685 --> 00:39:55,240
Okay.

718
00:39:55,240 --> 00:39:56,995
I'm not showing them here just for
simplicity.

719
00:39:56,995 --> 00:40:01,460
Okay, let's look at some specific examples
of least norm problems.

720
00:40:01,460 --> 00:40:04,057
Here's one, the most famous one, is you
take the two norm and

721
00:40:04,057 --> 00:40:06,283
in fact what you really minimize is not
the norm but

722
00:40:06,283 --> 00:40:11,120
the two norm squared, not the two norm but
the two norm squared.

723
00:40:11,120 --> 00:40:12,975
And this is the same because minimizing
the two norm squared and

724
00:40:12,975 --> 00:40:14,210
two norm is the same.

725
00:40:14,210 --> 00:40:15,870
it has the same solution.

726
00:40:15,870 --> 00:40:18,895
And this, the optimality conditions you
work them out That the KKT

727
00:40:18,895 --> 00:40:22,270
conditions are linear, a set of linear
equations.

728
00:40:22,270 --> 00:40:25,278
So you can solve them exactly, and there's
a formula for it if A is full rank and

729
00:40:25,278 --> 00:40:27,299
all that kind of stuff, and you've seen
that in 263 or

730
00:40:27,299 --> 00:40:30,370
some other linear algebra course.

731
00:40:30,370 --> 00:40:32,524
Right?
So, and that's why this is, this is,

732
00:40:32,524 --> 00:40:35,280
of course, extremely widely used.

733
00:40:35,280 --> 00:40:36,686
Right?
Because it Fits that, fits,

734
00:40:36,686 --> 00:40:37,970
it fits everything.

735
00:40:37,970 --> 00:40:39,300
Right?
Looks like it's simple,

736
00:40:39,300 --> 00:40:43,530
fits the 19th century, you know, formula
model and all that kind of stuff.

737
00:40:43,530 --> 00:40:45,024
Okay.
Now you can do other things,

738
00:40:45,024 --> 00:40:47,160
which is actually quite interesting.

739
00:40:47,160 --> 00:40:49,698
You can do things like this: you can
actually take the L1 norm, and

740
00:40:49,698 --> 00:40:52,029
this is a very interesting problem.

741
00:40:52,029 --> 00:40:58,252
This says minimize the norm of X one,
subject to Ax equals B.

742
00:40:58,252 --> 00:40:59,938
OK.

743
00:40:59,938 --> 00:41:04,346
And this is something, actually, that's
quite in vogue right now, and

744
00:41:04,346 --> 00:41:08,715
maybe has for five, maybe ten years now.

745
00:41:08,715 --> 00:41:12,190
It has no analytical solution, in general.

746
00:41:12,190 --> 00:41:16,840
Well, I mean, except in very silly cases,
has no analytical solution, So

747
00:41:16,840 --> 00:41:19,660
it's got various names.

748
00:41:19,660 --> 00:41:22,480
This one's basis pursuit, and let me ask
you.

749
00:41:22,480 --> 00:41:24,960
In view of the discussion we had last
time.

750
00:41:24,960 --> 00:41:29,306
Last time we talked about a penalty
function and how the shape of

751
00:41:29,306 --> 00:41:36,310
the penalty function has an influence on
the distribution of residuals.

752
00:41:36,310 --> 00:41:40,050
That discussion transposes perfectly to
this situation.

753
00:41:40,050 --> 00:41:40,585
Right?
So here,

754
00:41:40,585 --> 00:41:42,340
the idea is that if you have a penalty
function,

755
00:41:42,340 --> 00:41:45,920
here the penalty is embarassingly simple,
it's the absolute value.

756
00:41:45,920 --> 00:41:49,570
But what it does is that's going to shape
the distribution of X's.

757
00:41:49,570 --> 00:41:51,730
And what I'd like to know from you is,

758
00:41:51,730 --> 00:41:58,170
what do you imagine the solutions of the
basis pursuit problem look like.

759
00:41:58,170 --> 00:42:01,272
What you would expect, and turns out, in
fact, to be true,

760
00:42:01,272 --> 00:42:06,168
is that when you solve this problem, most
of the xs are zero.

761
00:42:06,168 --> 00:42:07,350
M'kay?

762
00:42:07,350 --> 00:42:10,159
And I mean, there are various ways, you
will observe this is as a,

763
00:42:10,159 --> 00:42:12,860
as, as an empirical observation.

764
00:42:12,860 --> 00:42:16,967
And in fact, you can prove various things
about that and so on, and so forth.

765
00:42:16,967 --> 00:42:24,395
so, in fact, this is a heuristic for
getting a sparse solution of ax = b.

766
00:42:24,395 --> 00:42:25,390
Right?

767
00:42:25,390 --> 00:42:27,260
So, okay.

768
00:42:27,260 --> 00:42:28,534
And this is going to be a theme,

769
00:42:28,534 --> 00:42:32,370
it's going to kind of follow us through
the rest of the course, and so on.

770
00:42:32,370 --> 00:42:32,870
So.

771
00:42:32,870 --> 00:42:33,770
Okay.

772
00:42:35,103 --> 00:42:37,040
We'll see a lot about this.

773
00:42:37,040 --> 00:42:38,600
okay.

774
00:42:38,600 --> 00:42:41,064
So, well, we've already been talking about
this, but

775
00:42:41,064 --> 00:42:44,816
the least penalty problem, says that you,
you choose a penalty function phi,

776
00:42:44,816 --> 00:42:48,344
here, and then you minimize the sum of the
penalties, subject to Ax equals B,

777
00:42:48,344 --> 00:42:54,862
and then you would, you would choose To
shape to get an x that you like.

778
00:42:54,862 --> 00:42:56,793
Right?

779
00:42:56,793 --> 00:43:00,238
And by the way, this is something quite
interesting about it because it had

780
00:43:00,238 --> 00:43:03,789
something to do with how people don't talk
about it a lot, that is the question of,

781
00:43:03,789 --> 00:43:07,633
sort of, the entire design flow.

782
00:43:07,633 --> 00:43:12,310
How do you use like convex optimization,
this kind of thing.

783
00:43:12,310 --> 00:43:14,840
And in fact, there's a big trend across
many fields,

784
00:43:14,840 --> 00:43:18,305
to move away from something that's a
direct solution that says "oh you know,

785
00:43:18,305 --> 00:43:22,155
I want a sparse solution, so here's what
you should do with the data," towards one

786
00:43:22,155 --> 00:43:27,310
where you put one level of indirection in
between.

787
00:43:27,310 --> 00:43:29,650
And what you do is you form an
optimization problem.

788
00:43:29,650 --> 00:43:33,613
And what the user does is mess with the
optimization problem.

789
00:43:33,613 --> 00:43:38,675
And then you let some numerical method
work out the actual solution.

790
00:43:38,675 --> 00:43:39,580
Right?

791
00:43:39,580 --> 00:43:43,010
So, I mean this is a big trend across
many, many fields.

792
00:43:43,010 --> 00:43:43,530
Right?

793
00:43:43,530 --> 00:43:48,636
That, for example, in areas like control,
signal processing, even statistics, you,

794
00:43:48,636 --> 00:43:52,293
you move away from this idea of Here's
what you do to the data, and

795
00:43:52,293 --> 00:43:55,467
instead, what the designer of a method is
really doing,

796
00:43:55,467 --> 00:44:01,490
is actually designing an optimization
problem.

797
00:44:01,490 --> 00:44:05,537
You change parameters here and there, if
something's too smooth, or something's not

798
00:44:05,537 --> 00:44:11,080
smooth enough, you add a little
regularization, and you crank that up.

799
00:44:11,080 --> 00:44:13,720
But you're not, you as a user are not
actually figuring out exactly how to

800
00:44:13,720 --> 00:44:16,760
transform your data into your estimate or
something.

801
00:44:16,760 --> 00:44:17,500
Everybody see what I'm saying?

802
00:44:17,500 --> 00:44:20,864
So this is, I mean this is a very big,
it's a big picture observation but

803
00:44:20,864 --> 00:44:23,270
it's it's happening.

804
00:44:23,270 --> 00:44:24,150
Okay.

805
00:44:28,150 --> 00:44:32,428
Well, the parent of these two problems,
least norm and then norm approximation is,

806
00:44:32,428 --> 00:44:35,460
is just regularized approximation.

807
00:44:35,460 --> 00:44:38,460
And in fact, there's even a more general
parent.

808
00:44:38,460 --> 00:44:43,509
The basic idea, it's a regularizaiton, is,
it's ubiquitous, across lots of fields.

809
00:44:43,509 --> 00:44:45,470
and, it, it's basic.

810
00:44:45,470 --> 00:44:48,325
The correct way I think to think of it is
it's a bicriterion problem.

811
00:44:48,325 --> 00:44:53,410
And it basically says, I care about two
norms, each of which I would like small.

812
00:44:53,410 --> 00:44:53,928
Right?
So

813
00:44:53,928 --> 00:44:57,090
the norms are, this is the most
traditional setting.

814
00:44:57,090 --> 00:45:01,550
The first one is something like Ax minus
B.

815
00:45:01,550 --> 00:45:03,398
That can have lots of interpretations,

816
00:45:03,398 --> 00:45:08,040
depending on if you're doing a statistical
model or, or data fitting.

817
00:45:08,040 --> 00:45:09,010
This is your misfit.

818
00:45:10,030 --> 00:45:11,120
X is your model.

819
00:45:11,120 --> 00:45:14,600
Ax minus b norm is a measure of how well
your model agrees with the data you

820
00:45:14,600 --> 00:45:16,460
have observed.

821
00:45:16,460 --> 00:45:20,836
That's, that's, that's, that's something
like misfit or fit.

822
00:45:20,836 --> 00:45:27,140
Your x is basically how big are your model
parameters.

823
00:45:27,140 --> 00:45:27,840
And we'll talk in

824
00:45:27,840 --> 00:45:31,636
a minute about why would it be that you
would want small parameters.

825
00:45:31,636 --> 00:45:34,964
After this some very good reasons why you
would want small parameters, well,

826
00:45:34,964 --> 00:45:37,824
like many good ideas it can be justified
from like five different,

827
00:45:37,824 --> 00:45:41,155
completely different points of view.

828
00:45:41,155 --> 00:45:43,850
So, let's look at some of them.

829
00:45:43,850 --> 00:45:45,880
So the idea here, essentially, is you're
saying, "You know what?

830
00:45:45,880 --> 00:45:49,054
What I want is a good fit." I want "AX" to
be about "B", but

831
00:45:49,054 --> 00:45:52,030
I want to do it efficiently.

832
00:45:52,030 --> 00:45:55,032
I want "X" to be small, so this is the
essential idea.

833
00:45:55,032 --> 00:45:58,752
Oh, and again, these ideas are all very
simple, so if you're not following -

834
00:45:58,752 --> 00:46:01,950
don't over-interpret what I'm saying.

835
00:46:01,950 --> 00:46:04,688
What I'm saying is so simple that there
are no subtleties here.

836
00:46:04,688 --> 00:46:05,872
None.

837
00:46:05,872 --> 00:46:07,660
So, okay.

838
00:46:07,660 --> 00:46:08,690
So, when would this come up?

839
00:46:09,940 --> 00:46:13,020
Well an estimation it might be something
like this.

840
00:46:13,020 --> 00:46:18,110
You'd say, well I happen to know, I mean
what I'm observing is y equals Ax.

841
00:46:18,110 --> 00:46:19,582
So, Ax minus y or Ax minus v or

842
00:46:19,582 --> 00:46:24,360
something like Ax minus b if b is
something that I measure.

843
00:46:24,360 --> 00:46:25,080
Is, is V.

844
00:46:25,080 --> 00:46:30,580
And so this is something like the norm of
that this thing.

845
00:46:30,580 --> 00:46:31,700
And you know that that's small.

846
00:46:31,700 --> 00:46:33,810
But I had prior knowledge that x is small.

847
00:46:33,810 --> 00:46:34,498
Right?
So, and

848
00:46:34,498 --> 00:46:38,120
that's, that's what this term does for me.

849
00:46:38,120 --> 00:46:40,522
Right?
So that, that's what that says,.

850
00:46:40,522 --> 00:46:43,170
You can have optimal design.

851
00:46:43,170 --> 00:46:47,510
You could say, actually, I don't insist
that AX equals B.

852
00:46:47,510 --> 00:46:49,020
I will give up on AX equals B.

853
00:46:49,020 --> 00:46:52,930
I will simply get close enough, close
enough?

854
00:46:52,930 --> 00:46:55,476
Well this is a bicriterion problem, so you
have a whole paretooptimal curve, and

855
00:46:55,476 --> 00:46:59,290
you will determine where to operate on
that curve once you see the curve.

856
00:46:59,290 --> 00:47:02,889
Then you'll look at it and say okay, fine,
I don't need to dot, I mean, if I'm

857
00:47:02,889 --> 00:47:07,019
within three millimeters that's actually
fine, especially if that allows me to use

858
00:47:07,019 --> 00:47:13,652
one quarter of the fuel I might have used
otherwise, right, something like that.

859
00:47:13,652 --> 00:47:17,474
So there the idea is you're willing you
don't even, you,

860
00:47:17,474 --> 00:47:24,799
you're willing to not have Ax equals b,
you'll get up a little bit on that.

861
00:47:24,799 --> 00:47:29,398
and, hopefully you'll take, as a benefit
for not doing that as exactly, small and

862
00:47:29,398 --> 00:47:33,597
efficient small likes, so that's the idea.

863
00:47:33,597 --> 00:47:40,220
Another interpretation, is this, is,
robust approximation.

864
00:47:40,220 --> 00:47:42,980
And this is something we're going to talk
about later,

865
00:47:42,980 --> 00:47:46,640
this is quite a modern interpretation and
it's extremely important, and

866
00:47:46,640 --> 00:47:49,900
the idea is something like this.

867
00:47:49,900 --> 00:47:51,946
You really just want to get a model, so

868
00:47:51,946 --> 00:47:55,590
you really want norm ax minus b to be
smaller.

869
00:47:55,590 --> 00:47:56,650
That's really what you want.

870
00:47:56,650 --> 00:47:57,178
Problem is you don't quite know A, right?

871
00:47:57,178 --> 00:47:58,528
This is extremely typical, right, that the
As are measured or

872
00:47:58,528 --> 00:48:00,270
something like that, you don't quite know
what they are.

873
00:48:00,270 --> 00:48:04,688
And by the way, it's, this can happen in a
design setting,

874
00:48:04,688 --> 00:48:09,536
it can happen in an estimation setting.

875
00:48:09,536 --> 00:48:10,719
Right?

876
00:48:10,719 --> 00:48:14,639
In a design setting Someone, you say, "you
know what, I,

877
00:48:14,639 --> 00:48:21,340
I do know, I know the moment of, you know
I know the moment of inertia.

878
00:48:21,340 --> 00:48:25,110
I know the mass of the vehicle, the CG, I
know all these things pretty well." But

879
00:48:25,110 --> 00:48:28,690
actually only about 1%, no better than
that.

880
00:48:28,690 --> 00:48:35,320
And what that says is if I apply a bunch
of control surface deviations.

881
00:48:35,320 --> 00:48:38,296
To an aircraft and I ask where will it be
in 22 seconds the answer is actually

882
00:48:38,296 --> 00:48:40,807
technically you don't know, right?

883
00:48:40,807 --> 00:48:43,768
because I don't know there is plenty of
air I mean hopefully you have a pretty

884
00:48:43,768 --> 00:48:46,412
good idea of what it's going to be right?

885
00:48:46,412 --> 00:48:49,548
because otherwise you're going to be in
pretty deep trouble but the point is

886
00:48:49,548 --> 00:48:52,782
it's really like a cloud of where it might
be depending on what it might be doing,

887
00:48:52,782 --> 00:48:56,280
everybody see what I'm saying?

888
00:48:56,280 --> 00:48:59,540
It should be a tight cloud, one hopes,
right?

889
00:48:59,540 --> 00:49:01,421
But the fact of the matter is, you don't,

890
00:49:01,421 --> 00:49:06,390
you don't know exactly what's going to
happen, so this is -- that's the idea.

891
00:49:08,650 --> 00:49:10,234
So then you say, well,

892
00:49:10,234 --> 00:49:17,560
what's interesting about this is how much
is your x affected by changes in A?

893
00:49:17,560 --> 00:49:20,970
And that we can even work out, I mean this
is going to be an intuitive one.

894
00:49:20,970 --> 00:49:22,881
We'll look at this more carefully later.

895
00:49:22,881 --> 00:49:27,555
But, so what you do is you think of A as
being sort of the nominal one and

896
00:49:27,555 --> 00:49:32,570
delta is a, is a matrix, presumably small.

897
00:49:32,570 --> 00:49:34,820
Which is basically your error in A.

898
00:49:34,820 --> 00:49:35,600
Right?

899
00:49:35,600 --> 00:49:37,260
So, this is, this is what you have.

900
00:49:37,260 --> 00:49:38,825
Right?
So if, for an aircraft this is,

901
00:49:38,825 --> 00:49:40,990
this is the nominal model.

902
00:49:40,990 --> 00:49:44,528
That assumes that your estimate of the
mass, moment of inertia, you know,

903
00:49:44,528 --> 00:49:47,940
blah blah blah, all these things are
perfect.

904
00:49:47,940 --> 00:49:50,085
Right?
They're, they're double precision perfect.

905
00:49:50,085 --> 00:49:51,150
Right?

906
00:49:51,150 --> 00:49:54,920
This is basically a var, these are
variations due to the fact that you know,

907
00:49:54,920 --> 00:49:57,356
these things are manufactured, things vary
by 1%,

908
00:49:57,356 --> 00:50:00,900
you know, all sorts of things happen.

909
00:50:00,900 --> 00:50:02,215
Right?
That's what the delta is here.

910
00:50:02,215 --> 00:50:05,907
And if you look at this equation we're
going to work this out but

911
00:50:05,907 --> 00:50:10,451
it is very simple what you get is ax minus
b and then plus delta x and what you see

912
00:50:10,451 --> 00:50:14,711
very clearly is the following is that the
errors in A multiply x and so for

913
00:50:14,711 --> 00:50:24,320
example suppose I chose X to be 0 How much
would that be affected by model errors?

914
00:50:24,320 --> 00:50:25,500
By the delta?

915
00:50:25,500 --> 00:50:26,024
Not at all.
And

916
00:50:26,024 --> 00:50:29,000
you can see immediately, I'm just very
intuitive, but the basic idea is oh,

917
00:50:29,000 --> 00:50:32,760
by the way, these are sometimes called
multiplicative errors.

918
00:50:32,760 --> 00:50:36,579
That's a name you will hear, and it makes
perfect sense because it's an error in A,

919
00:50:36,579 --> 00:50:39,660
which then multiples your choice x.

920
00:50:39,660 --> 00:50:41,148
So, it's a multiplicative error, and

921
00:50:41,148 --> 00:50:44,364
a multiplicative error it's, it's, it is
in your, you want to be less sensitive to

922
00:50:44,364 --> 00:50:48,390
a multiplicative error Then here's what
you want.

923
00:50:48,390 --> 00:50:49,610
You want X to be small.

924
00:50:49,610 --> 00:50:51,050
The thing.
Everybody see this?

925
00:50:51,050 --> 00:50:54,450
And so this is why a small X would be
preferable.

926
00:50:54,450 --> 00:50:56,225
Right.
That, it would, it would be that,

927
00:50:56,225 --> 00:50:59,090
it would be less sensitive to errors in A.

928
00:50:59,090 --> 00:51:00,995
That was a long description.

929
00:51:00,995 --> 00:51:04,722
I, that's not supposed to be fancier than
what I just said, but that's the idea.

930
00:51:04,722 --> 00:51:07,584
And I can give you yet another
interpretation now, but I mean in,

931
00:51:07,584 --> 00:51:12,160
in some sense all these interpretations
come around to the same thing.

932
00:51:12,160 --> 00:51:18,520
Another one is this, you have y equals you
have a model like this estimation.

933
00:51:18,520 --> 00:51:20,590
You have y equals Ax plus v.

934
00:51:20,590 --> 00:51:24,402
But the truth is, you really have
something like this.

935
00:51:24,402 --> 00:51:24,994
Right?

936
00:51:24,994 --> 00:51:28,900
You really have a non-linear model/g.

937
00:51:28,900 --> 00:51:32,710
However, near, you do, near where you're
looking right now?

938
00:51:32,710 --> 00:51:35,570
Y equals ax plus v works.

939
00:51:35,570 --> 00:51:37,662
That's provided x is small.

940
00:51:37,662 --> 00:51:39,142
Okay?

941
00:51:39,142 --> 00:51:40,759
So the idea there is this,

942
00:51:40,759 --> 00:51:47,310
this sort of tells you your nominal error
based on your say, linearized model.

943
00:51:49,160 --> 00:51:52,638
This says, please have X small, because
the smaller X is,

944
00:51:52,638 --> 00:51:58,120
the more accurate, the more I trust My
linearized model, right.

945
00:51:58,120 --> 00:52:02,200
And in fact, in that case, the term that
constrains x either via regularization or

946
00:52:02,200 --> 00:52:06,648
as an additive term in regularization or
as a constraint.

947
00:52:06,648 --> 00:52:09,748
If it's a constraint it's called a trust
region constraint,

948
00:52:09,748 --> 00:52:13,530
which is a beautiful term, because it
basically says, please estimate x,

949
00:52:13,530 --> 00:52:18,270
I want norm Ax minus b small, but I want
norm x small.

950
00:52:18,270 --> 00:52:19,900
And someone says, why do you want norm x
small?

951
00:52:19,900 --> 00:52:20,490
Do you care?

952
00:52:20,490 --> 00:52:23,770
Then you go actually no, I don't care at
all how big the parameter is.

953
00:52:23,770 --> 00:52:26,786
Problem is, I have to add that there
because if "x" gets bigger,

954
00:52:26,786 --> 00:52:30,320
my model "Ax" is no longer accurate.

955
00:52:30,320 --> 00:52:30,990
Doubting this?

956
00:52:30,990 --> 00:52:33,830
I, I'm, tha, got, got, got the idea, it's
very, it's pretty straightforward.

957
00:52:33,830 --> 00:52:34,575
Okay.
All right.

958
00:52:34,575 --> 00:52:39,460
Let's look at some simple cases.

959
00:52:39,460 --> 00:52:42,528
I mean, so, how do you, how do you solve a
bicriterion problem?

960
00:52:42,528 --> 00:52:43,480
You scalarize, right?

961
00:52:43,480 --> 00:52:44,610
This is the simplest method.

962
00:52:44,610 --> 00:52:48,510
And so you'd take the norm of Ax - b plus
Gamma norm-x.

963
00:52:48,510 --> 00:52:50,815
For us, that's a convex problem, no big
deal, we solve it.

964
00:52:50,815 --> 00:52:51,580
Okay?

965
00:52:51,580 --> 00:52:52,750
So that's fine.

966
00:52:52,750 --> 00:52:55,040
And, and Gamma is a positive parameter.

967
00:52:55,040 --> 00:52:58,658
You sweep it from 0 to infinity and at the
two extremes you would get you can get

968
00:52:58,658 --> 00:53:03,708
the extreme points by solving a constrain
problem or something like that.

969
00:53:03,708 --> 00:53:04,316
Okay.
And, so

970
00:53:04,316 --> 00:53:07,132
this traces out the, the trade-off curve.

971
00:53:07,132 --> 00:53:08,510
a, a very.

972
00:53:08,510 --> 00:53:10,690
A, a very traditional method is to square
the two.

973
00:53:10,690 --> 00:53:12,151
You get the same trade-off curv.

974
00:53:12,151 --> 00:53:14,270
And by the way that's something you would
want to check.

975
00:53:14,270 --> 00:53:15,690
Right?
That in fact, the,

976
00:53:15,690 --> 00:53:20,046
the curve of solutions here parameterized
by delta is identical to the curve of

977
00:53:20,046 --> 00:53:24,142
solutions parameterized by gamma here.

978
00:53:24,142 --> 00:53:25,224
Right?

979
00:53:25,224 --> 00:53:29,299
So, in fact, you can even how delta and
gamma are related, right?

980
00:53:29,299 --> 00:53:34,405
And it's not totally straightforward and
it has to do with the particular problem.

981
00:53:34,405 --> 00:53:36,699
But these are two parameterizations.

982
00:53:36,699 --> 00:53:38,373
Okay.
So if these are 2-norms, and

983
00:53:38,373 --> 00:53:42,630
the reason generally one squares a norm, I
mean there's several reasons.

984
00:53:42,630 --> 00:53:45,870
But the most traditional is when you
square a 2-norm, you get a quadratic

985
00:53:45,870 --> 00:53:49,380
function, which is nice and smooth and you
know, the derivative is linear, and

986
00:53:49,380 --> 00:53:55,400
you, you know, and then Then all your you
have a formula for the answer, right?

987
00:53:55,400 --> 00:53:57,119
OK.
So, most famous there is,

988
00:53:57,119 --> 00:53:59,280
is Tikhonov regularization.

989
00:53:59,280 --> 00:54:02,635
Oh, and I should say in statistics it's
called ridge regression is

990
00:54:02,635 --> 00:54:04,820
the name in statistics.

991
00:54:04,820 --> 00:54:06,565
Maybe another one, but it's called ridge
regression.

992
00:54:06,565 --> 00:54:10,570
And this just turns into a single
least-squares.

993
00:54:10,570 --> 00:54:12,670
I mean, you can solve this analytically of
course.

994
00:54:12,670 --> 00:54:13,170
Two norms.

995
00:54:13,170 --> 00:54:13,846
Right?

996
00:54:13,846 --> 00:54:15,676
so.
And, the solution you can either make it,

997
00:54:15,676 --> 00:54:17,480
stack it, and make a big least squares
problem, or

998
00:54:17,480 --> 00:54:20,831
you just get, sort of an analytical
solution like that.

999
00:54:20,831 --> 00:54:21,412
Okay?
And, and that's the thing.

1000
00:54:21,412 --> 00:54:22,086
Now, the, horrible, reason, to use this.

1001
00:54:22,086 --> 00:54:24,761
The, the, the, the one that, that I don't
like at all,.

1002
00:54:24,761 --> 00:54:30,316
And is in fact probably, the most Common
use of regularization is,

1003
00:54:30,316 --> 00:54:40,180
someone says, what are you doing, you say,
I'm doing, I'm, I'm adding regularization.

1004
00:54:40,180 --> 00:54:41,934
Why?
And you'd say, because without it,

1005
00:54:41,934 --> 00:54:45,160
I was getting numerical, my solver was
sending complaints to me.

1006
00:54:46,240 --> 00:54:48,120
Right?
So everybody see that?

1007
00:54:48,120 --> 00:54:50,047
If you add delta here, this just, this is
just now,

1008
00:54:50,047 --> 00:54:52,370
works perf-,it has to work perfectly.

1009
00:54:52,370 --> 00:54:55,394
You can never get a complaint that says,
you know, inverse condition number big or,

1010
00:54:55,394 --> 00:54:58,270
you know, numerical error, or something
like that.

1011
00:54:58,270 --> 00:54:59,090
It's not going to happen.

1012
00:54:59,090 --> 00:55:00,880
Can't get something singular working
position.

1013
00:55:02,300 --> 00:55:03,410
So let's look in the example.

1014
00:55:03,410 --> 00:55:07,040
And this example actually it's, it, it's
very simple.

1015
00:55:07,040 --> 00:55:10,150
But it's, it's going to be an example,
actually, of.

1016
00:55:10,150 --> 00:55:13,749
It's sup, meant to illustrate exactly what
I talked about earlier,

1017
00:55:13,749 --> 00:55:16,200
this idea of a design flow.

1018
00:55:16,200 --> 00:55:17,190
Right?

1019
00:55:17,190 --> 00:55:19,980
How, how do you use these things, right?

1020
00:55:19,980 --> 00:55:22,560
And this'll be one from, you know,
control, if you like.

1021
00:55:22,560 --> 00:55:26,242
This could just as well be statistics it
could just as well be anything else.

1022
00:55:26,242 --> 00:55:30,110
Image-processing, video-processing, it
could be anything.

1023
00:55:30,110 --> 00:55:31,720
And you could construct a similar story.

1024
00:55:31,720 --> 00:55:33,765
Could be finance, right?

1025
00:55:33,765 --> 00:55:35,570
So let's look at this.

1026
00:55:35,570 --> 00:55:40,300
Here's the idea, I have a convolution
system, so I apply an input u,

1027
00:55:40,300 --> 00:55:44,940
scalar input at various time intervals.

1028
00:55:44,940 --> 00:55:48,550
It could be a force I apply to something,
it doesn't matter what it is, right?

1029
00:55:48,550 --> 00:55:52,798
And, something I call the output is a
convolution of the input with some

1030
00:55:52,798 --> 00:55:59,420
convolution kernel, or in EE dialect
that's called an impulse response, okay?

1031
00:55:59,420 --> 00:56:01,300
But that's dialect.

1032
00:56:01,300 --> 00:56:02,986
Standard term is convolution kernel.

1033
00:56:02,986 --> 00:56:04,017
'Kay?

1034
00:56:04,017 --> 00:56:08,690
So, and in mixed company you should always
say convolution curve [UNKNOWN] Okay,.

1035
00:56:08,690 --> 00:56:12,904
So, the input design problem, is choose
this input u, so

1036
00:56:12,904 --> 00:56:16,690
that y does something you want.

1037
00:56:16,690 --> 00:56:17,206
And, you know,

1038
00:56:17,206 --> 00:56:19,915
this is just going to be a very simple
example to show what a typical design flow

1039
00:56:19,915 --> 00:56:21,270
looks like.

1040
00:56:21,270 --> 00:56:22,390
So Here it is.

1041
00:56:22,390 --> 00:56:26,830
What I want to do is I'm given a desired
trajectory, and I want to track it.

1042
00:56:26,830 --> 00:56:27,470
That's it.

1043
00:56:27,470 --> 00:56:32,334
So, and, and I, I want to make, I, I have
to give a measure for mistracking, and so

1044
00:56:32,334 --> 00:56:34,462
I'm going to call that J track and if,

1045
00:56:34,462 --> 00:56:41,734
if you can't think of anything if nothing
immediately comes to mind.

1046
00:56:41,734 --> 00:56:44,090
Then you should just use the squares,

1047
00:56:44,090 --> 00:56:50,788
just historical, to follow in historical
tradition, it's not a bad thing, right?

1048
00:56:50,788 --> 00:56:55,428
I mean, by the way, if I were to make this
the sum of the absolute values,

1049
00:56:55,428 --> 00:56:57,508
it would change the result, and

1050
00:56:57,508 --> 00:57:06,462
you could even kind of guess what would
happen when you, when you do that, right?

1051
00:57:06,462 --> 00:57:10,100
If I were to make this the infinity norm,
right?

1052
00:57:10,100 --> 00:57:12,810
That you would call it something like mini
max tracking error.

1053
00:57:12,810 --> 00:57:14,330
You might give a, a name like that.

1054
00:57:14,330 --> 00:57:16,570
And you would get different results as
well.

1055
00:57:16,570 --> 00:57:20,065
Right, so but here we just take the
squares and make it simple.

1056
00:57:20,065 --> 00:57:25,030
Okay, Now at the same time I want you not
so big, right.

1057
00:57:25,030 --> 00:57:30,114
So, I am going to introduce an objective
called the magnitude, Jmag.

1058
00:57:30,114 --> 00:57:31,492
And these are, well, by the way,

1059
00:57:31,492 --> 00:57:34,729
if you look at these, they are quadratic
forms, right.

1060
00:57:34,729 --> 00:57:38,563
So, so far, right, because they're square,
they are squares, their sums are squares,

1061
00:57:38,563 --> 00:57:42,583
and I will take an input very this is,
tells me how.

1062
00:57:42,583 --> 00:57:45,283
well, I shouldn't say something about how
wiggly the signal is or

1063
00:57:45,283 --> 00:57:47,083
something like that, and it's going to be,

1064
00:57:47,083 --> 00:57:51,588
the sum of the difference, of the first
differences here, right?

1065
00:57:51,588 --> 00:57:54,720
and, and now I can ask some questions,
like when would J track be 0,

1066
00:57:54,720 --> 00:57:58,400
which is its minimum possible value.

1067
00:57:58,400 --> 00:58:01,600
It means you have -- your Y is equal to Y
desired.

1068
00:58:01,600 --> 00:58:02,130
And you -- yes.

1069
00:58:02,130 --> 00:58:04,866
And a good name for that is you might, you
might say that you

1070
00:58:04,866 --> 00:58:09,141
interpolate The desired thing or you might
say you achieved perfect tracking meaning

1071
00:58:09,141 --> 00:58:14,720
that the output absolutely tracks what it
is that you required, okay?

1072
00:58:14,720 --> 00:58:21,186
So that would be that when would the input
magnitude objective term be 0?

1073
00:58:21,186 --> 00:58:23,961
When u=0.

1074
00:58:23,961 --> 00:58:25,195
Okay?

1075
00:58:25,195 --> 00:58:25,960
So, in which case,

1076
00:58:25,960 --> 00:58:30,230
by the way your tracking error would
simply be some wide desired square.

1077
00:58:30,230 --> 00:58:31,510
Right?
Because your output would be 0

1078
00:58:31,510 --> 00:58:32,470
with that point.

1079
00:58:32,470 --> 00:58:33,950
Okay, and how about input variation?

1080
00:58:33,950 --> 00:58:36,140
When would that be 0?

1081
00:58:36,140 --> 00:58:37,460
Constant, exactly.

1082
00:58:37,460 --> 00:58:44,970
So, if you were to crank up The
coefficient on input variation, very high.

1083
00:58:44,970 --> 00:58:48,906
you would expect to see inputs that were
constant.

1084
00:58:48,906 --> 00:58:49,493
Right?

1085
00:58:49,493 --> 00:58:52,700
And at the same time they're attempting to
track things like that.

1086
00:58:52,700 --> 00:58:54,547
Okay?
And you could go on and on.

1087
00:58:54,547 --> 00:58:57,721
For example you could have a second finite
difference,

1088
00:58:57,721 --> 00:59:00,895
which would be something like a smoothness
measure and

1089
00:59:00,895 --> 00:59:06,790
that would be something like ut plus 1
minus 2ut Plus UT minus 1.

1090
00:59:06,790 --> 00:59:09,122
That's I, I don't know if you recognize
that but it's the all,

1091
00:59:09,122 --> 00:59:13,370
it's the famous, you know, minus 1 2 1,
and the tri-diagonal matrix or something.

1092
00:59:13,370 --> 00:59:15,740
It's a second difference, right?

1093
00:59:15,740 --> 00:59:17,440
And then, and that would give you
something smooth.

1094
00:59:17,440 --> 00:59:18,745
And when would that be zero, by the way?

1095
00:59:18,745 --> 00:59:19,420
Linear.

1096
00:59:19,420 --> 00:59:22,470
Exactly, so if U, U looks like that or
that, that would be zero.

1097
00:59:22,470 --> 00:59:23,260
Right?

1098
00:59:23,260 --> 00:59:25,510
So Okay great.

1099
00:59:25,510 --> 00:59:29,100
So what we'll do is we'll just make this a
regularized least-squares problem.

1100
00:59:29,100 --> 00:59:32,976
We'll take the tracking error plus delta
times the derivative error plus aeta

1101
00:59:32,976 --> 00:59:35,420
times the magnitude error.

1102
00:59:35,420 --> 00:59:37,535
And, I mean this is a least-squares
problem, right, so

1103
00:59:37,535 --> 00:59:39,860
we just solve it, right, this is idea.

1104
00:59:39,860 --> 00:59:41,460
We get a least-squares problem.

1105
00:59:41,460 --> 00:59:44,126
And then the idea now is that delta And

1106
00:59:44,126 --> 00:59:51,160
aita are knobs that you will fiddle with,
to get something that you like.

1107
00:59:51,160 --> 00:59:51,964
Right.
And you could,

1108
00:59:51,964 --> 00:59:55,940
you could do this in a formal way, and
actually do 10 values of each.

1109
00:59:55,940 --> 00:59:58,740
I mean it's a trivial problem, you could
do 10 values of each.

1110
00:59:58,740 --> 01:00:02,220
And then just make a, a big 10 by 10
matrix and actually plot all these things.

1111
01:00:02,220 --> 01:00:03,777
[COUGH] Look at them whatever you like.

1112
01:00:03,777 --> 01:00:06,864
You could have two big knobs in front of
you that labeled, you know, Delta and

1113
01:00:06,864 --> 01:00:09,363
Eta or two sliders or whatever on some
user interface and, and

1114
01:00:09,363 --> 01:00:13,230
you can fiddle with them and see what you
like.

1115
01:00:13,230 --> 01:00:14,252
Alright, so that's the idea.

1116
01:00:14,252 --> 01:00:16,940
Okay, so.

1117
01:00:16,940 --> 01:00:17,840
Quick example here.

1118
01:00:17,840 --> 01:00:23,566
Here is some, here are three pareta
optimal solutions.

1119
01:00:23,566 --> 01:00:24,460
Right?

1120
01:00:24,460 --> 01:00:27,092
So the first one has delta equals 0 and
that means that we,

1121
01:00:27,092 --> 01:00:31,640
we're putting 0 penalty on the variation
in you.

1122
01:00:31,640 --> 01:00:33,565
Right?
So, fine.

1123
01:00:33,565 --> 01:00:38,345
we're, and we're putting a small weight on
the sides of you, that's eight small.

1124
01:00:38,345 --> 01:00:41,080
And, here's the input, and the output.

1125
01:00:41,080 --> 01:00:44,530
And, two things are plotted in the output,
you can't see there's a dashed curve,

1126
01:00:44,530 --> 01:00:47,240
here, which shows you the desired one.

1127
01:00:47,240 --> 01:00:49,180
The desired one is just this kind of
square wave thing.

1128
01:00:49,180 --> 01:00:50,380
That was dialect.

1129
01:00:50,380 --> 01:00:52,930
It goes like, it jumps up, and then jumps
down.

1130
01:00:52,930 --> 01:00:53,570
Right?

1131
01:00:53,570 --> 01:00:55,090
Like that.
So that, that's the desired one.

1132
01:00:55,090 --> 01:00:58,421
And you can see if you look at this, that
we're tracking quite well.

1133
01:00:58,421 --> 01:01:01,437
Well you can see there's little errors
right at the transitions and so

1134
01:01:01,437 --> 01:01:05,630
on, and this is the input that does the
trick, over here.

1135
01:01:05,630 --> 01:01:07,160
Right?
So that's the input, and

1136
01:01:07,160 --> 01:01:10,814
you can see the input gets this high as,
you know, almost 5 positive and

1137
01:01:10,814 --> 01:01:16,220
it jerks down to like minus oh, I don't
know, 7.5 or 8 there.

1138
01:01:16,220 --> 01:01:17,685
Okay?
So that's, that's it, and

1139
01:01:17,685 --> 01:01:20,625
you can see that also the input Is quite
wiggly.

1140
01:01:20,625 --> 01:01:21,410
Right?

1141
01:01:21,410 --> 01:01:25,976
So, fine /g, that's just one point down
the tradeoff curve, right?

1142
01:01:25,976 --> 01:01:29,945
Then you'd say, well, in the second, in
the middle, what we're going to do is

1143
01:01:29,945 --> 01:01:33,851
we're still going to have no derivative
here /g, but we're going to crank up eta,

1144
01:01:33,851 --> 01:01:40,370
and we expect, so that means we're
going to add more penalty To the size.

1145
01:01:40,370 --> 01:01:42,260
And so what we expect is the size of you
to come down, and

1146
01:01:42,260 --> 01:01:45,670
by the way we're going to pay for that in
tracking error.

1147
01:01:45,670 --> 01:01:48,502
Right, so, and in data if you look on the
right you can see here, you know,

1148
01:01:48,502 --> 01:01:51,670
the tracking error probably is mostly
accumulated around these end points and

1149
01:01:51,670 --> 01:01:54,669
it's pretty small.

1150
01:01:54,669 --> 01:01:55,257
Here you can see,

1151
01:01:55,257 --> 01:01:58,875
there's some pretty, you can actually see
some pretty substantial tracking error.

1152
01:01:58,875 --> 01:02:00,105
Here.

1153
01:02:00,105 --> 01:02:03,150
Everybody got this?

1154
01:02:03,150 --> 01:02:07,910
The differences, look at this, instead,
we've basically halved th input, right, so

1155
01:02:07,910 --> 01:02:09,746
with half the input, [NOISE], so

1156
01:02:09,746 --> 01:02:16,410
if this is good enough for you, great, I
mean this is kind of the idea, right?

1157
01:02:16,410 --> 01:02:21,189
And finally in the, in the last one what
we're going to do is we are going to,

1158
01:02:21,189 --> 01:02:26,373
and we're now going to turn on a, a bunch
of smoothing regularization, and

1159
01:02:26,373 --> 01:02:32,510
again, you pay for it in terms of
tracking.

1160
01:02:32,510 --> 01:02:35,360
Eh, maybe this is good enough for you,
maybe not, and now, but

1161
01:02:35,360 --> 01:02:38,210
you can see immediately what happens
things like these,

1162
01:02:38,210 --> 01:02:42,520
which rack a big bill in the derivative.

1163
01:02:42,520 --> 01:02:48,220
Cost function, now we're smoothed out and
you get something like this, okay?

1164
01:02:48,220 --> 01:02:50,460
So this is not supposed to be complicated.

1165
01:02:50,460 --> 01:02:53,950
It is not complicated, it's simple, but
the idea is this is what you'd do.

1166
01:02:53,950 --> 01:02:58,000
And you would typically sit there and turn
knobs and see what you like.

1167
01:02:58,000 --> 01:03:00,012
And this could be any applicationary.

1168
01:03:00,012 --> 01:03:02,588
This could image-processing, where you
turn a knob and

1169
01:03:02,588 --> 01:03:06,900
you go no no no too smooth, and you crank
it down the other way.

1170
01:03:06,900 --> 01:03:08,750
And now some of the noise comes back.

1171
01:03:08,750 --> 01:03:11,520
And then, you turn another knob, and this
is how this goes.

1172
01:03:11,520 --> 01:03:13,060
So let's look at another area.

1173
01:03:13,060 --> 01:03:15,400
It's, it's also very simple, bicriterion
problem.

1174
01:03:15,400 --> 01:03:16,460
It's signal reconstructions.

1175
01:03:16,460 --> 01:03:17,440
Quite straightforward.

1176
01:03:17,440 --> 01:03:18,136
It's this.

1177
01:03:18,136 --> 01:03:23,610
What I have is I have, I'm given a
corrupted signal.

1178
01:03:23,610 --> 01:03:25,750
That's x, you know, corrupted.

1179
01:03:25,750 --> 01:03:27,415
Okay?
And what I would like to do is to

1180
01:03:27,415 --> 01:03:31,250
come up with x hat, which is supposed to
be an estimate of this corrupt well,

1181
01:03:31,250 --> 01:03:36,070
it's an estimate of the signal before it
was corrupted.

1182
01:03:36,070 --> 01:03:37,350
Okay?
And that's what I'm going to do.

1183
01:03:37,350 --> 01:03:41,132
And so at I want "x" to be close to "x"
corrupted, but I don't want it to be

1184
01:03:41,132 --> 01:03:44,356
equal to it because its been corrupted,
and what I'm going to do is

1185
01:03:44,356 --> 01:03:51,600
I'm going to have a second function here,
which is known as, it's got lots of names.

1186
01:03:51,600 --> 01:03:53,370
It's a regularization function, or

1187
01:03:53,370 --> 01:03:56,625
sometimes people call it a smoothing
regularizer.

1188
01:03:56,625 --> 01:03:57,910
Okay?

1189
01:03:57,910 --> 01:04:00,110
And so that's this function phi of X.

1190
01:04:00,110 --> 01:04:00,950
X hat.

1191
01:04:00,950 --> 01:04:05,297
And the idea's by looking at a trade-off
between these, I'm going to do some,

1192
01:04:05,297 --> 01:04:10,870
this is a principled way to do smoothing
or something like that.

1193
01:04:10,870 --> 01:04:11,752
To smooth a signal.

1194
01:04:11,752 --> 01:04:13,800
All right.

1195
01:04:13,800 --> 01:04:15,410
And the model is something like this.

1196
01:04:15,410 --> 01:04:16,860
You have some unknown signal.

1197
01:04:16,860 --> 01:04:21,396
You absorb a corrupted version and then
you're going to solve this problem and

1198
01:04:21,396 --> 01:04:24,636
examples would be something like this, you
know so

1199
01:04:24,636 --> 01:04:29,748
simple quadratic smoothing would be you
simply take the differences [UNKNOWN] like

1200
01:04:29,748 --> 01:04:33,492
we did in the previous example Some of the
absolute values,

1201
01:04:33,492 --> 01:04:40,250
that's called a total variation
regularizer.

1202
01:04:40,250 --> 01:04:44,340
So that's completely standard, so, that's
actually a standard term.

1203
01:04:44,340 --> 01:04:47,187
Let's look at an example.

1204
01:04:47,187 --> 01:04:49,210
I use simple example, right?

1205
01:04:49,210 --> 01:04:51,050
So, here's some signal it's 4000 time
samples or

1206
01:04:51,050 --> 01:04:55,270
whatever you like but this is the original
signal and this is the corrupted one.

1207
01:04:55,270 --> 01:04:58,610
And this one's kind of goofy because No.

1208
01:04:58,610 --> 01:05:01,442
Taken the signal, you've added some, and
you can see that,

1209
01:05:01,442 --> 01:05:06,633
that the difference between the signal
and, and, and the, the corruption.

1210
01:05:06,633 --> 01:05:09,927
You can see that the, the corruptions very
like high frequency or

1211
01:05:09,927 --> 01:05:15,145
something like that, okay, so, that's, so
it's kind of obvious, right.

1212
01:05:15,145 --> 01:05:19,147
So here what you do now is we simply use a
quadratic smoother and

1213
01:05:19,147 --> 01:05:22,183
what would you get would be things like
this so

1214
01:05:22,183 --> 01:05:26,806
this is with a the little big of smoothing
this is what you would reconstruct this

1215
01:05:26,806 --> 01:05:34,090
would be substantially more and even more
still Right?

1216
01:05:34,090 --> 01:05:35,650
So, that's the idea.

1217
01:05:35,650 --> 01:05:41,690
And you might say, that's a little bit too
much, and then that's just about right.

1218
01:05:41,690 --> 01:05:43,896
Right.
Now, now unfortunately, for

1219
01:05:43,896 --> 01:05:49,002
smoothing problems like this there really
aren't any particularly good ways,

1220
01:05:49,002 --> 01:05:55,229
to choose The level of smoothing you do,
except maybe aesthetically.

1221
01:05:55,229 --> 01:05:58,320
I mean you have to have some side
information, right?

1222
01:05:58,320 --> 01:06:01,296
Or, you could have some cases where you
actually knew what the exact signal

1223
01:06:01,296 --> 01:06:02,510
was, right?

1224
01:06:02,510 --> 01:06:06,770
So then you could actually do reasonable
things like cross-validation.

1225
01:06:06,770 --> 01:06:09,278
Here's one, and this is much more modern
-- by the way,

1226
01:06:09,278 --> 01:06:14,110
total variation reconstruction, this was
introduced maybe, only 1990s.

1227
01:06:14,110 --> 01:06:16,456
So this is pretty relatively recent.

1228
01:06:16,456 --> 01:06:20,395
And it's, it's actually quite interesting,
and quite stunning.

1229
01:06:20,395 --> 01:06:23,845
In fact, I've tried to get some audio
recordings of this.

1230
01:06:23,845 --> 01:06:25,870
I mean, I haven't tried that hard.

1231
01:06:25,870 --> 01:06:29,587
I should, because they're amazing, and I'd
put them on the course website or

1232
01:06:29,587 --> 01:06:31,680
something like that.

1233
01:06:31,680 --> 01:06:33,606
Anyway, I'll explain that in a minute.

1234
01:06:33,606 --> 01:06:34,720
So here's a picture.

1235
01:06:34,720 --> 01:06:37,040
What I have is, here's the original
signal.

1236
01:06:37,040 --> 01:06:40,240
It's got the smooth component, but these,
these kind of jumps every so often.

1237
01:06:40,240 --> 01:06:41,630
I mean, this is all just made up.

1238
01:06:41,630 --> 01:06:44,580
Right so, this is just to illustrate, what
happens.

1239
01:06:44,580 --> 01:06:46,310
Right?
So, here we have this signal, and, so

1240
01:06:46,310 --> 01:06:50,320
you, and then we add this high frequency
noise to it, [COUGH] like that.

1241
01:06:50,320 --> 01:06:51,900
So that's the corrupted state.

1242
01:06:51,900 --> 01:06:52,720
Right?

1243
01:06:52,720 --> 01:06:55,526
Now, one of the problems here is that you
don't have this

1244
01:06:55,526 --> 01:06:58,270
frequency scale, separation.

1245
01:06:58,270 --> 01:07:00,516
Right?
Because, the original signal,

1246
01:07:00,516 --> 01:07:02,108
has these jumps.

1247
01:07:02,108 --> 01:07:05,820
And a jump, again if you know about forier
analysis and all that kind of stuff,

1248
01:07:05,820 --> 01:07:08,546
roughly it is going to contain a lot of
high frequency, so

1249
01:07:08,546 --> 01:07:15,419
you don't have the spectral separation of
the underlying signal and the noise here.

1250
01:07:15,419 --> 01:07:18,947
And the result is if you do quadratic
smoothing, which in fact is

1251
01:07:18,947 --> 01:07:22,601
a linear operation It's just a low pass
filter frankly is what it is and

1252
01:07:22,601 --> 01:07:25,814
what you get is this if you do some low
pass filtering well,

1253
01:07:25,814 --> 01:07:30,161
sure this gets smoothed out, that gets
attenuated but you can see as you crank it

1254
01:07:30,161 --> 01:07:38,470
up to smooth out more, you can see exactly
what is happening here.

1255
01:07:38,470 --> 01:07:42,004
That a transition that was in fact a
single value Has now widened, and

1256
01:07:42,004 --> 01:07:47,540
now it's taking place over, ooh, that's a
lot, you know, 50 or something.

1257
01:07:47,540 --> 01:07:49,484
Right.
So, a transition that was,

1258
01:07:49,484 --> 01:07:54,990
that, that went in like 1 time sample, is
now happening over 50, right.

1259
01:07:54,990 --> 01:08:00,260
So, by the way if this where audio This
would make something sound very muffled.

1260
01:08:00,260 --> 01:08:05,200
If this was the attack caused by a drum,
right, someone hits a snare drum.

1261
01:08:05,200 --> 01:08:06,702
You would get something that looks like
that.

1262
01:08:06,702 --> 01:08:11,363
And if you smooth it like this, and
instead of the attack rising in 10

1263
01:08:11,363 --> 01:08:19,330
samples, 48 khz or something like that,
instead of 10 samples it goes at 1000.

1264
01:08:19,330 --> 01:08:23,840
It just sounds like a thud, it doesn't
sound like a drum anymore, okay?

1265
01:08:23,840 --> 01:08:24,660
So, all right.

1266
01:08:24,660 --> 01:08:29,610
But let's do total variation de-noising.

1267
01:08:29,610 --> 01:08:31,510
So total variation de-noising does this,
and

1268
01:08:31,510 --> 01:08:35,160
actually, we can see a lot of things here
that you would predict.

1269
01:08:36,760 --> 01:08:39,660
So the first is this: this is where you
would put a lot of total

1270
01:08:39,660 --> 01:08:41,754
variation de-noising.

1271
01:08:42,990 --> 01:08:45,130
[INAUDIBLE] and you're beginning to see
something pretty cool.

1272
01:08:45,130 --> 01:08:49,830
When you crank up something which is
basically the L1 norm of

1273
01:08:49,830 --> 01:08:55,407
the difference of x hat t plus 1 and x hat
t.

1274
01:08:55,407 --> 01:08:57,339
Again You should start now and

1275
01:08:57,339 --> 01:09:03,822
by the end of the course it will, should
be completely ingrained in second nature.

1276
01:09:03,822 --> 01:09:06,738
But you, you want to make connections
between things like L one and

1277
01:09:06,738 --> 01:09:08,328
sparcity, right?

1278
01:09:08,328 --> 01:09:09,422
And that's the very simplified model.

1279
01:09:09,422 --> 01:09:10,860
But it should really be kinxed and
Sparsity, right?

1280
01:09:10,860 --> 01:09:13,638
So lets look at L one and Sparsity.

1281
01:09:13,638 --> 01:09:21,360
When someone says please minimize you
know.

1282
01:09:21,360 --> 01:09:22,580
Well it's part of it right?

1283
01:09:22,580 --> 01:09:24,490
If they say please minimize this, right?

1284
01:09:24,490 --> 01:09:28,285
What you would expect is that if the
coefficient in front of this is

1285
01:09:28,285 --> 01:09:34,020
high enough, right, that a whole lot of
these numbers would be zero.

1286
01:09:34,020 --> 01:09:36,352
And that says that if you do total
variation.

1287
01:09:36,352 --> 01:09:40,930
>> Denoising, you should expect a
piece-wise constant signal.

1288
01:09:40,930 --> 01:09:43,890
Because a piece-wise constant signal is,
that's what it means.

1289
01:09:43,890 --> 01:09:45,835
This is like a derivative, this first
difference.

1290
01:09:45,835 --> 01:09:50,128
And if you have sparse derivative, it
means you're piece-wise constant.

1291
01:09:50,128 --> 01:09:51,491
Everybody got that?

1292
01:09:51,491 --> 01:09:55,551
By the way, its just were the second
difference, and I put an L1 norm And

1293
01:09:55,551 --> 01:09:57,540
I minimized.

1294
01:09:58,630 --> 01:10:00,640
What would you expect to see.

1295
01:10:00,640 --> 01:10:05,300
Piece was linear, exactly, right.

1296
01:10:05,300 --> 01:10:09,168
And if I took the third difference, what
would you get.

1297
01:10:09,168 --> 01:10:11,300
V y quadratic.

1298
01:10:11,300 --> 01:10:13,580
And by the way those would be splines just
for the record.

1299
01:10:13,580 --> 01:10:16,710
You get splines.

1300
01:10:16,710 --> 01:10:18,280
Okay?
So this is sort of the idea.

1301
01:10:18,280 --> 01:10:21,730
Now here what's happened is you are
tracking this, right.

1302
01:10:21,730 --> 01:10:23,432
But what's happened is regularization is
so

1303
01:10:23,432 --> 01:10:26,180
high that things like that just got
constant, right?

1304
01:10:26,180 --> 01:10:29,550
You've, you've lost the fact that this is
varying here, right.

1305
01:10:29,550 --> 01:10:31,180
Oh I should add also kind of a cool thing.

1306
01:10:31,180 --> 01:10:33,356
If you do this on images, Is a grayscale
or

1307
01:10:33,356 --> 01:10:38,740
color stuff, you get very cool stuff and
things start looking cartoonish.

1308
01:10:38,740 --> 01:10:40,504
Right?
Because you have a whole bunch of,

1309
01:10:40,504 --> 01:10:43,432
you have a, you have whole regions where
it was one color or one shade or, or

1310
01:10:43,432 --> 01:10:45,770
there was some gradient.

1311
01:10:45,770 --> 01:10:47,740
It's just like now we're placed flat.

1312
01:10:47,740 --> 01:10:50,800
So I think you can even imagine what this
looks like.

1313
01:10:50,800 --> 01:10:51,320
Right?

1314
01:10:51,320 --> 01:10:51,870
So.
Right?

1315
01:10:51,870 --> 01:10:55,596
So this is, so that if you do total
variation Regularization,

1316
01:10:55,596 --> 01:11:01,200
on an image -- what happens is, at first,
you have a million pixels.

1317
01:11:01,200 --> 01:11:04,144
You probably can have a million gray scale
values right, I mean, no reason for

1318
01:11:04,144 --> 01:11:06,190
any of them to repeat, right?

1319
01:11:06,190 --> 01:11:07,660
If they do, it's an accident.

1320
01:11:07,660 --> 01:11:09,963
You turn up total variation regularization
on an image and

1321
01:11:09,963 --> 01:11:12,160
they'll jump down dramatically.

1322
01:11:12,160 --> 01:11:14,440
And after a while you'll have only a
thousand, and you keep turning it and

1323
01:11:14,440 --> 01:11:17,095
then at some point you'll have like ten
grey levels.

1324
01:11:17,095 --> 01:11:18,580
Right?

1325
01:11:18,580 --> 01:11:21,880
But it'll still be completely recognizable
as the image you wanted, right?

1326
01:11:21,880 --> 01:11:25,357
But anyway, so, I'm just saying these
things apply to everything, right,

1327
01:11:25,357 --> 01:11:28,190
you'll see these all over the place.

1328
01:11:28,190 --> 01:11:32,155
So here you can see this is where we might
have maybe not quite enough Total

1329
01:11:32,155 --> 01:11:37,030
variation regularization because you can
see that there is some squigglies there

1330
01:11:37,030 --> 01:11:44,080
and this is kind of the proverbial just
enough, right, something like that.

1331
01:11:44,080 --> 01:11:48,548
Finally it's a big thing we'll look at the
idea of robust approximation.

1332
01:11:48,548 --> 01:11:52,188
So here we've already seen hints at it,
right, so the idea You want to minimize,

1333
01:11:52,188 --> 01:11:56,700
let's say, (?) x minus b, but the problem
is you don't know a.

1334
01:11:56,700 --> 01:11:57,705
Now, I should add,

1335
01:11:57,705 --> 01:12:02,660
this issue is universal, in actually all
optimization problems right.

1336
01:12:02,660 --> 01:12:04,592
That you have data in an optimization
problem, and

1337
01:12:04,592 --> 01:12:06,650
you know, we can talk a little bit.

1338
01:12:06,650 --> 01:12:10,081
Say some generic things about data, you
know, when data,

1339
01:12:10,081 --> 01:12:15,540
when values that are zero, one, minus one,
Sometimes 2, -2, 1/2.

1340
01:12:15,540 --> 01:12:16,240
Things like that.

1341
01:12:16,240 --> 01:12:17,970
Those are probably really those numbers,
right?

1342
01:12:17,970 --> 01:12:19,906
Because it may be some coefficient that
makes sense,

1343
01:12:19,906 --> 01:12:23,330
like you're basically saying, you know,
this thing's equal to that.

1344
01:12:23,330 --> 01:12:24,780
And that would reveal itself as a 1 and a
-1.

1345
01:12:24,780 --> 01:12:28,820
And that's really, really, probably are
zer, 1 and -1.

1346
01:12:28,820 --> 01:12:32,720
Any other constant other than the ones I
just named and maybe a handful of others.

1347
01:12:32,720 --> 01:12:35,570
Directly speaking they have a providence
and

1348
01:12:35,570 --> 01:12:39,920
you can trace it back to models of things
or measurements or experts or

1349
01:12:39,920 --> 01:12:47,660
econometric models or eh, mechnaics or
psychics or something like that right?

1350
01:12:47,660 --> 01:12:50,527
and in fact if you trace them all the way
back you'd say oh,

1351
01:12:50,527 --> 01:12:52,906
that came from a finite element
calculation or

1352
01:12:52,906 --> 01:12:58,690
blah, blah and by the way That means they,
those numbers are suspects.

1353
01:12:58,690 --> 01:12:59,231
Right?
I

1354
01:12:59,231 --> 01:13:02,250
mean they could be accurate two three
significant figures.

1355
01:13:02,250 --> 01:13:04,430
Maybe five, right?

1356
01:13:04,430 --> 01:13:06,662
Maybe one, right?

1357
01:13:06,662 --> 01:13:10,310
Or in Economics, the sinus suspect right
[LAUGH] or something like,

1358
01:13:10,310 --> 01:13:14,470
I mean you just don't You know, so there's
places where it's dominated, right,

1359
01:13:14,470 --> 01:13:19,515
I mean, you know, there's extremes here
right?

1360
01:13:19,515 --> 01:13:21,752
so, all right.

1361
01:13:21,752 --> 01:13:24,834
so, the point is though that in all these
cases you have,

1362
01:13:24,834 --> 01:13:30,340
there, there is error in the data, I mean
that's just, period right?

1363
01:13:30,340 --> 01:13:34,750
And there are ways to handle it simply,.

1364
01:13:34,750 --> 01:13:36,755
Simple methods that, we've seen one
already.

1365
01:13:36,755 --> 01:13:40,595
Regularization is essentially a method to
chose x's that are not

1366
01:13:40,595 --> 01:13:43,836
extremely vulnerable to changes in a.

1367
01:13:43,836 --> 01:13:49,751
For example, that's a really simple
example of a robust optimization problem.

1368
01:13:49,751 --> 01:13:52,679
But there are plenty of others, right?

1369
01:13:52,679 --> 01:13:54,941
By the way, how is parameter.

1370
01:13:54,941 --> 01:13:57,741
>> Variation handled in real life.

1371
01:13:57,741 --> 01:14:00,765
So what do you think is by far the most
prevalent method for

1372
01:14:00,765 --> 01:14:05,520
handling the fact that your parameters are
not known?

1373
01:14:05,520 --> 01:14:08,953
In real, in the, in real life, I'm talking
about when people actually solve things.

1374
01:14:08,953 --> 01:14:09,522
>> [INAUDIBLE] What?

1375
01:14:09,522 --> 01:14:10,651
>> Ignore it.

1376
01:14:10,651 --> 01:14:11,440
>> Ignore it.
Thank you.

1377
01:14:11,440 --> 01:14:12,440
That is absolutely correct.

1378
01:14:12,440 --> 01:14:13,880
That is by far the prevalent method.

1379
01:14:13,880 --> 01:14:19,290
Now, That's okay, in my opinion, provided
you do one thing.

1380
01:14:19,290 --> 01:14:22,050
Which is, which, which then take, makes
this okay.

1381
01:14:22,050 --> 01:14:24,490
What would that one thing, be to do?

1382
01:14:27,060 --> 01:14:28,220
It's the least you could do.

1383
01:14:28,220 --> 01:14:30,260
So you've just solved a problem.

1384
01:14:30,260 --> 01:14:32,180
You assumed that all your data was
accurate.

1385
01:14:32,180 --> 01:14:34,330
They're double precision numbers.

1386
01:14:34,330 --> 01:14:35,491
You got them from, you know Bob or

1387
01:14:35,491 --> 01:14:37,770
whatever, some other intern, doesn't
matter.

1388
01:14:37,770 --> 01:14:39,925
You got the model and you did it.

1389
01:14:39,925 --> 01:14:43,500
What should you do now.

1390
01:14:43,500 --> 01:14:44,675
Generate a new A.

1391
01:14:44,675 --> 01:14:48,470
With the entries changed by a plausible
amount.

1392
01:14:48,470 --> 01:14:51,100
Everybody got this, right.

1393
01:14:51,100 --> 01:14:55,790
And you just simply test the, the x you
found before.

1394
01:14:55,790 --> 01:14:58,358
With the new A.
And if things are way off now,

1395
01:14:58,358 --> 01:15:03,970
then you know that you cannot ignore,
safely, robustus/g.

1396
01:15:03,970 --> 01:15:04,910
Everybody got this.

1397
01:15:04,910 --> 01:15:07,740
I mean this is incredibly simple, but it's
unbelievably important.

1398
01:15:07,740 --> 01:15:10,180
It's actually shocking to me, how many
people don't do this.

1399
01:15:10,180 --> 01:15:12,750
Right, so, this is just completely
standard.

1400
01:15:12,750 --> 01:15:17,365
Oh, wow, I've got a fantastic, a fantastic
force sequence to land an airplane,

1401
01:15:17,365 --> 01:15:19,760
you gotta see this.

1402
01:15:19,760 --> 01:15:21,680
It's just unbelievable, yeah.

1403
01:15:21,680 --> 01:15:27,160
People don't even notice it when they're,
you say okay, fine, here it is.

1404
01:15:27,160 --> 01:15:30,230
You absolutely have to go back.

1405
01:15:30,230 --> 01:15:32,970
And change the model, change the total
mass.

1406
01:15:32,970 --> 01:15:34,395
Move the center of gravity and

1407
01:15:34,395 --> 01:15:38,040
re-simulate the same thing with these
changed parameters.

1408
01:15:38,040 --> 01:15:39,802
And you have to do 10 hundred of these.

1409
01:15:39,802 --> 01:15:40,936
You know, after which by the way,

1410
01:15:40,936 --> 01:15:43,828
you know absolutely nothing, I might add,
technically.

1411
01:15:43,828 --> 01:15:47,890
But I mean from a Strict point of view,
right?

1412
01:15:47,890 --> 01:15:49,570
But at least you've done the common sense
check.

1413
01:15:49,570 --> 01:15:51,150
Everybody got this, right?

1414
01:15:51,150 --> 01:15:56,510
If you, if you come and say, I've got a
trading strategy, it's unbeleviable.

1415
01:15:56,510 --> 01:15:57,550
And they go, really, and you go,

1416
01:15:57,550 --> 01:16:00,860
oh my god, you should see what it did last
year, unbeleviable.

1417
01:16:00,860 --> 01:16:03,525
And then you'd say well did you try it on
the year before, yeah, no.

1418
01:16:03,525 --> 01:16:05,901
The main method, traditional method is for

1419
01:16:05,901 --> 01:16:09,380
handling uncertainty in data is to ignore
it.

1420
01:16:09,380 --> 01:16:12,470
And I would say actually that's a
perfectly respectable option.

1421
01:16:12,470 --> 01:16:16,570
Provided you do a posterior analysis of
the effected variation.

1422
01:16:16,570 --> 01:16:18,610
Then it's, then it's totally legit.

1423
01:16:18,610 --> 01:16:21,755
I mean, assuming the posterior analysis
reveals that it works fine.

1424
01:16:21,755 --> 01:16:24,998
Okay, now the next step is to have a
heuristic that kind of,

1425
01:16:24,998 --> 01:16:26,378
kind of is a heuristic for

1426
01:16:26,378 --> 01:16:32,530
making sure that things don't get too
wacky when the parameters change.

1427
01:16:32,530 --> 01:16:35,040
That'd be like regularization, absolutely
fine.

1428
01:16:35,040 --> 01:16:38,610
Again, you do a posterior analysis to
understand if it works.

1429
01:16:38,610 --> 01:16:41,630
And the new thing, this has been coming
up.

1430
01:16:41,630 --> 01:16:43,610
Maybe in the last, could be 20 years, but

1431
01:16:43,610 --> 01:16:48,900
then it was done by a handful of esoteric,
you know, work by academics.

1432
01:16:48,900 --> 01:16:52,660
It's now coming on mainline, main,
mainstream, it's this.

1433
01:16:52,660 --> 01:16:55,780
You simply take into account the
uncertainty directly in

1434
01:16:55,780 --> 01:16:57,858
the problem statement.

1435
01:16:57,858 --> 01:16:58,810
Okay?

1436
01:16:58,810 --> 01:17:01,831
And so, that, these are called rob, that's
robust optimization, and

1437
01:17:01,831 --> 01:17:03,690
this is a special case.

1438
01:17:03,690 --> 01:17:04,250
Okay.

1439
01:17:04,250 --> 01:17:06,110
Now we're jumping back to the specific.

1440
01:17:06,110 --> 01:17:07,055
That was the background,

1441
01:17:07,055 --> 01:17:10,166
we're jumping back to the specific problem
we're looking at.

1442
01:17:10,166 --> 01:17:14,030
So you have to have a model for how A
varies, right?

1443
01:17:14,030 --> 01:17:16,860
And in one, it's stock-, it's stochastic.

1444
01:17:16,860 --> 01:17:20,051
You would say A is random and you would
describe its distribution.

1445
01:17:20,051 --> 01:17:21,870
And the distribution could even be finite.

1446
01:17:21,870 --> 01:17:25,550
Like you just say it's got 25 values with
these probabilities, right?

1447
01:17:25,550 --> 01:17:28,253
By the way, that's an extremely reasonable
thing to do.

1448
01:17:28,253 --> 01:17:30,599
Because what you do, is if you track the
provinence of a,

1449
01:17:30,599 --> 01:17:33,965
and it came from other measurements and
data, then what you do is you'd go back to

1450
01:17:33,965 --> 01:17:40,140
someone there, it's like cross validation
or something, if you know what that is.

1451
01:17:40,140 --> 01:17:43,090
But you'd go back and you say here's a
model for my chemical process, and

1452
01:17:43,090 --> 01:17:44,720
you go, great.

1453
01:17:44,720 --> 01:17:45,570
how' did you get it?

1454
01:17:45,570 --> 01:17:46,950
You go from data.

1455
01:17:46,950 --> 01:17:48,750
And you go, you know what?

1456
01:17:48,750 --> 01:17:51,740
Fit me a separate model for every day of
the week.

1457
01:17:51,740 --> 01:17:53,900
I'd like you to fit a model for you
chemical process for

1458
01:17:53,900 --> 01:17:57,610
Monday, Tuesday, Wednesday, Thursday,
Friday, Saturday, Sunday.

1459
01:17:57,610 --> 01:17:58,885
Right?
You get seven models now.

1460
01:17:58,885 --> 01:17:59,680
Right?

1461
01:17:59,680 --> 01:18:02,303
Now, by the way if you look at these a, so
now you have a Monday, a Tuesday, a,

1462
01:18:02,303 --> 01:18:05,141
by the way if they're all completely
different, you should just turn around and

1463
01:18:05,141 --> 01:18:07,790
go away.

1464
01:18:07,790 --> 01:18:10,320
Because basically you're hosed, there's
nothing you can do that,

1465
01:18:10,320 --> 01:18:14,400
there's nothing intelligent, this is not a
place for us to be doing anything.

1466
01:18:14,400 --> 01:18:17,070
Right?
because it basically means it's.

1467
01:18:17,070 --> 01:18:19,040
There's no consistency and it doesn't make
any sense.

1468
01:18:19,040 --> 01:18:20,070
Okay, fine.

1469
01:18:20,070 --> 01:18:22,338
So hopefully, all the entries, of a
Monday,

1470
01:18:22,338 --> 01:18:25,050
Tuesday, Wednesday, there're close.

1471
01:18:25,050 --> 01:18:29,046
In a worse case, in a, in a worse case
robust, fitting or

1472
01:18:29,046 --> 01:18:34,010
approximation problem, you would do this.

1473
01:18:34,010 --> 01:18:36,090
You would say I'm not going to use a
stochastic model.

1474
01:18:36,090 --> 01:18:38,895
Instead, what I'm going to do is I'm
simply going to, I'm going to have a set

1475
01:18:38,895 --> 01:18:43,330
of values of A and I'm going to, I'm
going to judge my fit by the worst case.

1476
01:18:43,330 --> 01:18:44,106
Right?
And by the way,

1477
01:18:44,106 --> 01:18:45,560
there's everything in between these two.

1478
01:18:45,560 --> 01:18:46,925
Right?
So these are just two, and

1479
01:18:46,925 --> 01:18:47,915
there's incredible,

1480
01:18:47,915 --> 01:18:52,560
you know complete nonsense silly religious
wars between people who.

1481
01:18:52,560 --> 01:18:55,590
You know, I mean, and it makes no sense
out of a context, right?

1482
01:18:55,590 --> 01:18:57,185
And in, in any particular context,

1483
01:18:57,185 --> 01:19:02,230
it makes tons of sense as to discuss what
would be a reasonable robustness model.

1484
01:19:02,230 --> 01:19:07,164
But outside of a, context-free it makes no
sense whatsoever.

1485
01:19:07,164 --> 01:19:07,930
Right?

1486
01:19:07,930 --> 01:19:12,754
Mostly what you do, oh and this, oh this
is sometimes called mini-max.

1487
01:19:12,754 --> 01:19:14,758
Mini-max fitting.

1488
01:19:14,758 --> 01:19:17,080
this, that doesn't have a name.

1489
01:19:17,080 --> 01:19:19,140
Yeah it does.
It's called stochastic optimization.

1490
01:19:19,140 --> 01:19:21,204
OK.
So, that's the idea.

1491
01:19:21,204 --> 01:19:25,335
Now, solving these problem is tractable
only in special cases.

1492
01:19:25,335 --> 01:19:28,746
I mean, in a lot of cases, it's not
tractable at all.

1493
01:19:28,746 --> 01:19:29,547
But some are.

1494
01:19:29,547 --> 01:19:32,058
Here's a super duper simple example.

1495
01:19:32,058 --> 01:19:33,111
We have a matrix.

1496
01:19:33,111 --> 01:19:35,610
It has a one parameter variation.

1497
01:19:35,610 --> 01:19:37,775
It has A0 plus uA1.

1498
01:19:37,775 --> 01:19:39,789
>> And u varies, you know, let's say
between 1 and

1499
01:19:39,789 --> 01:19:42,360
minus 1 something like that, right?

1500
01:19:42,360 --> 01:19:46,930
So basically, in matrix space, you'll have
a little line segment, right?

1501
01:19:46,930 --> 01:19:48,960
You know, hopefully it's not a giant one,
right?

1502
01:19:48,960 --> 01:19:50,256
But you know, it's a little line segment
and

1503
01:19:50,256 --> 01:19:52,900
of course, in a more realistic one it's
varying in a ball.

1504
01:19:52,900 --> 01:19:55,996
I don't know, but this has a line segment,
that's it, right?

1505
01:19:55,996 --> 01:19:57,336
So, okay.

1506
01:19:58,520 --> 01:20:01,116
And you could actually easily imagine
where the dominant variation is

1507
01:20:01,116 --> 01:20:02,330
one line, right?

1508
01:20:02,330 --> 01:20:05,080
Because it could be some process that you
run.

1509
01:20:05,080 --> 01:20:09,770
And in fact to first order it's mostly
affected by, say, ambient temperature.

1510
01:20:09,770 --> 01:20:13,845
And so the ambient temperature goes from,
you know, ten c to 35.

1511
01:20:13,845 --> 01:20:16,210
And that changes the model.

1512
01:20:16,210 --> 01:20:18,510
I mean things like this happen all the
time.

1513
01:20:18,510 --> 01:20:19,180
Right.

1514
01:20:19,180 --> 01:20:24,670
So, what you do here, in this case, you
can solve all these problems, all of them.

1515
01:20:24,670 --> 01:20:27,340
And so, here are, this shows you a couple
of the solutions.

1516
01:20:27,340 --> 01:20:31,460
So, X nom simply ignores it entirely,
minimizes this.

1517
01:20:31,460 --> 01:20:34,233
Then, what you do is take this X, and I
calculate the norm,

1518
01:20:34,233 --> 01:20:37,478
here As a function of you and that's
plotted here so this is Xnom and

1519
01:20:37,478 --> 01:20:41,372
the nominal point is right here and look
at that of course by definition it has to

1520
01:20:41,372 --> 01:20:48,470
get the lowest value at this is the
nominal value, sure enough it does.

1521
01:20:48,470 --> 01:20:49,510
It gets the lowest value.

1522
01:20:49,510 --> 01:20:54,270
It's right there but then you see as U
changes, you start paying for it right?

1523
01:20:54,270 --> 01:20:57,150
By rising costs, okay?

1524
01:20:57,150 --> 01:20:59,510
So here's sot, the X stocastoc.

1525
01:20:59,510 --> 01:21:02,670
That minimizes the average over the
interval (-1,1).

1526
01:21:02,670 --> 01:21:05,630
So over here to here, that gives you this
one here.

1527
01:21:05,630 --> 01:21:08,963
Sorry, this one here, this, this one here.

1528
01:21:08,963 --> 01:21:11,185
This is a stochastic, right?

1529
01:21:11,185 --> 01:21:16,639
And you can see that, it, you pay for it
in nominal performance, right?

1530
01:21:16,639 --> 01:21:19,690
The stochastic one dip not as well.

1531
01:21:19,690 --> 01:21:21,480
So the nominal performance is a little bit
worse.

1532
01:21:21,480 --> 01:21:27,305
But now as u varies out to be 0.7, -0.8
you're actually doing much better.

1533
01:21:27,305 --> 01:21:28,360
Okay?

1534
01:21:28,360 --> 01:21:31,030
So everybody, I mean, this is kind of
clear, but that, that's the idea.

1535
01:21:31,030 --> 01:21:34,830
And the final one shows worst case.

1536
01:21:34,830 --> 01:21:36,630
And that's this last one.

1537
01:21:36,630 --> 01:21:37,904
It's very flat.

1538
01:21:37,904 --> 01:21:39,825
You've paid for the cost.

1539
01:21:39,825 --> 01:21:40,770
In nominal cost.

1540
01:21:40,770 --> 01:21:42,752
You do much worse than if it's nominal.

1541
01:21:42,752 --> 01:21:46,640
But if you look at the mini max over minus
one, one, you do very well.

1542
01:21:46,640 --> 01:21:49,318
Right, so this is just to illustrate very
simple thing.

1543
01:21:49,318 --> 01:21:52,814
Okay so we can also handle, I mean, some,
I,

1544
01:21:52,814 --> 01:21:58,980
as an example of, in anything as an
analytical solution.

1545
01:21:58,980 --> 01:22:01,220
That would be stocastic robust lee
squares.

1546
01:22:01,220 --> 01:22:02,020
So let's work that out.

1547
01:22:02,020 --> 01:22:05,056
This is, this is absolutely traditional,
and it It's kind of cool, actually, and

1548
01:22:05,056 --> 01:22:08,145
it relates to something we were talking
about earlier.

1549
01:22:08,145 --> 01:22:08,723
It's this.

1550
01:22:08,723 --> 01:22:11,923
Suppose your Matrix A has the form A bar,
a nominal one plus U,

1551
01:22:11,923 --> 01:22:17,474
where U is random, 0 mean, and has
expected value transposed to U as P.

1552
01:22:17,474 --> 01:22:18,198
Okay?

1553
01:22:18,198 --> 01:22:22,600
So we want to minimize The expected value
of the square of the two norms of

1554
01:22:22,600 --> 01:22:29,656
this thing, of course the expectation is
over the distribution of U, here, right,?

1555
01:22:29,656 --> 01:22:33,277
So, just work that out, I mean you expand
this thing, the term,

1556
01:22:33,277 --> 01:22:36,898
the cross-terms, like here, when you
expand this quadratic,

1557
01:22:36,898 --> 01:22:43,620
there's a cross term which is x transpose,
U transpose, A bar X minus b.

1558
01:22:43,620 --> 01:22:46,390
You take the expected value over U.

1559
01:22:46,390 --> 01:22:47,240
U has 0 mean.

1560
01:22:47,240 --> 01:22:47,800
Those go away.

1561
01:22:47,800 --> 01:22:50,360
So the cross-terms drop out in the
expectation here.

1562
01:22:50,360 --> 01:22:51,220
Right?

1563
01:22:51,220 --> 01:22:55,744
Leaving you with the nominal cost plus
this thing and you get that, and

1564
01:22:55,744 --> 01:22:58,120
it's super cool.

1565
01:22:59,140 --> 01:23:00,430
You recognize that immediately.

1566
01:23:00,430 --> 01:23:02,040
That's regularization.

1567
01:23:02,040 --> 01:23:05,017
It's quadratic regularization.

1568
01:23:05,017 --> 01:23:06,640
And, in fact, it's even cooler.

1569
01:23:06,640 --> 01:23:10,544
It basically says, if you do Tikhonov
regularization like this, what you can

1570
01:23:10,544 --> 01:23:15,960
claim to be doing, is you're actually
minimizing this, which is super cool.

1571
01:23:15,960 --> 01:23:20,985
And you say, your minimizing This thing
where, instead of thinking of the matrix

1572
01:23:20,985 --> 01:23:27,400
A as fixed you're actually taking into
account that every entry varies.

1573
01:23:27,400 --> 01:23:30,550
They are all independent and have a
variance which is I don't know something

1574
01:23:30,550 --> 01:23:34,382
like Delta over N I mean you can figure
out what it is, right?

1575
01:23:34,382 --> 01:23:35,630
Everybody got this?

1576
01:23:35,630 --> 01:23:36,620
So, it's actually super cool.

1577
01:23:36,620 --> 01:23:42,490
So if you do taken off regularization or,
or Ridge regression.

1578
01:23:42,490 --> 01:23:43,516
I guess if you're in statistics,

1579
01:23:43,516 --> 01:23:46,275
you do ridge regression, you don't have to
justify it to anybody.

1580
01:23:46,275 --> 01:23:49,083
But if you're in a field where they don't
have a special name for

1581
01:23:49,083 --> 01:23:53,240
taking off regularization, and they ask
what are you doing?

1582
01:23:53,240 --> 01:23:56,350
You can say oh I'm doing robustly squares.

1583
01:23:56,350 --> 01:23:58,912
I'm taking into account minor variations
in the As.

1584
01:23:58,912 --> 01:24:00,234
Everybody got it?

1585
01:24:00,234 --> 01:24:03,124
Now, we can do some worst-case ones.

1586
01:24:03,124 --> 01:24:05,570
I'm not going to go into the details here
because they're kind of hairy.

1587
01:24:05,570 --> 01:24:06,580
This, this is quite new.

1588
01:24:06,580 --> 01:24:09,208
This is something that's like 15, 20 years
old.

1589
01:24:09,208 --> 01:24:12,400
So let's do the case where the matrix A
actually is,

1590
01:24:12,400 --> 01:24:15,860
lies in an ellipsoid of matrices.

1591
01:24:15,860 --> 01:24:17,378
Again, completely reasonable.

1592
01:24:17,378 --> 01:24:23,100
and, what we really want to minimize is
the worst case.

1593
01:24:23,100 --> 01:24:26,826
So, it's going to be a weird thing, if you
like, its a game or

1594
01:24:26,826 --> 01:24:30,552
something like that, basically, you commit
to x first,

1595
01:24:30,552 --> 01:24:37,420
then your opponent will choose the worst
possible A.

1596
01:24:37,420 --> 01:24:38,920
That's what this is right?

1597
01:24:38,920 --> 01:24:41,831
So this is the, so something like that
and, and

1598
01:24:41,831 --> 01:24:46,610
there turns out, something like this can
solve exactly.

1599
01:24:46,610 --> 01:24:47,866
Right?
So if you work it out and

1600
01:24:47,866 --> 01:24:51,034
again I'm not going to go through any of
the details because they're quite hairy

1601
01:24:51,034 --> 01:24:54,210
and in some sense it doesn't really
Matter.

1602
01:24:54,210 --> 01:24:56,655
I mean you should go back over and check
it.

1603
01:24:56,655 --> 01:24:58,195
It turns out, that here when, one,

1604
01:24:58,195 --> 01:25:01,330
when the opponent is calculating what's
the worst thing I should do,

1605
01:25:01,330 --> 01:25:04,520
they've commit-ed to x, and I'm going to
find the, [INAUDIBLE] you end up

1606
01:25:04,520 --> 01:25:10,860
solving this problem, and that is a non
convex problem if I ever saw one, right?

1607
01:25:10,860 --> 01:25:12,590
Because the objective.

1608
01:25:12,590 --> 01:25:15,890
Is you're maximizing, a quadratic.

1609
01:25:15,890 --> 01:25:16,773
Right?

1610
01:25:16,773 --> 01:25:19,530
Convex quadratic.

1611
01:25:19,530 --> 01:25:20,740
Right.
Now, you haven't seen this yet.

1612
01:25:20,740 --> 01:25:23,620
Maybe you did or something but it's maybe
time for you to know this.

1613
01:25:23,620 --> 01:25:25,070
So, I'm going to say it.

1614
01:25:25,070 --> 01:25:26,875
I'm going to use this as an excuse to let
you know.

1615
01:25:26,875 --> 01:25:31,247
There are some generic, non-convex
problems that can be solved.

1616
01:25:31,247 --> 01:25:32,917
'Kay?
And, the simplest one,

1617
01:25:32,917 --> 01:25:36,355
has a very simple and short description
length.

1618
01:25:36,355 --> 01:25:43,955
Any optimization problem involving two
quadratics, can be solved globally, 'kay?

1619
01:25:43,955 --> 01:25:46,100
Everybody got that?

1620
01:25:46,100 --> 01:25:47,470
That's in the appendix of the book.

1621
01:25:47,470 --> 01:25:48,849
You should read it.

1622
01:25:48,849 --> 01:25:50,470
Or remember what I just said.

1623
01:25:50,470 --> 01:25:53,308
Okay, because it actually comes up in a
whole bunch of different applications,

1624
01:25:53,308 --> 01:25:54,150
right?

1625
01:25:54,150 --> 01:25:56,665
So, and, and that's convexity not
convexity.

1626
01:25:56,665 --> 01:26:00,050
And you know a lot, and by the way you
know this already.

1627
01:26:00,050 --> 01:26:01,190
If I walked up to you and

1628
01:26:01,190 --> 01:26:05,410
I said, oh you know please help me solve
you know this problem.

1629
01:26:05,410 --> 01:26:07,800
A is symmetric you know.

1630
01:26:07,800 --> 01:26:08,500
Right?

1631
01:26:08,500 --> 01:26:12,489
Something like that I said please maximize
this thing.

1632
01:26:12,489 --> 01:26:13,820
Right?

1633
01:26:13,820 --> 01:26:16,270
How do you maximize that subject to that?

1634
01:26:16,270 --> 01:26:19,190
Well that, that sure is no convex
optimization problem.

1635
01:26:19,190 --> 01:26:21,360
Right?
Because This thing was not concave,

1636
01:26:21,360 --> 01:26:25,820
unless A is un, un, un, unless A is, is
negative definite.

1637
01:26:25,820 --> 01:26:26,780
Okay, negative semi-definite.

1638
01:26:26,780 --> 01:26:27,430
Right?

1639
01:26:27,430 --> 01:26:30,070
There's no way that's never a convex
constraint.

1640
01:26:30,070 --> 01:26:31,540
Right?
That, that's a sphere, right?

1641
01:26:31,540 --> 01:26:35,940
So, that's a hideously non-convex problem.

1642
01:26:35,940 --> 01:26:38,240
But everyone here knows the answer, knows
this can be solved.

1643
01:26:38,240 --> 01:26:40,280
I mean, it's basically an eigenvalue
problem, right?

1644
01:26:40,280 --> 01:26:41,792
The answer is the largest, eigen,

1645
01:26:41,792 --> 01:26:46,490
the eigenvector, corresponding, it's the
largest eigenvalue, and blah blah blah.

1646
01:26:46,490 --> 01:26:47,178
Everybody got that?

1647
01:26:47,178 --> 01:26:49,720
Right, so alright.

1648
01:26:49,720 --> 01:26:51,990
This fits the thing I'm saying here.

1649
01:26:51,990 --> 01:26:55,090
That is an optimization problem and it
involves exactly two

1650
01:26:55,090 --> 01:26:58,190
quadratic functions and the general
statement is this- any

1651
01:26:58,190 --> 01:27:04,532
optimization problem with exactly two
quadratic functions can be solved exactly.

1652
01:27:04,532 --> 01:27:07,840
And so what that says is, you know if
someone says, what are you doing?

1653
01:27:07,840 --> 01:27:10,430
I'm taking a class in [INAUDIBLE], oh, is
it interesting?

1654
01:27:10,430 --> 01:27:11,210
I don't know.

1655
01:27:11,210 --> 01:27:13,780
And they, they say well are all problems
convexed?

1656
01:27:13,780 --> 01:27:15,272
You go oh no, no, no.

1657
01:27:15,272 --> 01:27:21,340
And then someone says name a widely used,
useful problem that's not convex?

1658
01:27:21,340 --> 01:27:24,196
The first thing you should mention I would
hope would be things like

1659
01:27:24,196 --> 01:27:27,620
singular value decomposition, PCA Eigen
vaulues, okay?

1660
01:27:27,620 --> 01:27:29,420
We know what this says, this says those
are convex, sorry.

1661
01:27:29,420 --> 01:27:30,740
So I mean it's kind of weird and obscure.

1662
01:27:30,740 --> 01:27:36,860
And the reason, by the way, is that the
duals

1663
01:27:36,860 --> 01:27:44,892
of these problems have 0 duality gap.

1664
01:27:44,892 --> 01:27:46,050
Anyway, alright.

1665
01:27:46,050 --> 01:27:47,850
So thi, that was just my weird little
aside.

1666
01:27:47,850 --> 01:27:49,230
But this is an advanced thing.

1667
01:27:49,230 --> 01:27:50,220
This is not a simple thing.

1668
01:27:50,220 --> 01:27:53,221
But it's an extremely good thing to know,
right?

1669
01:27:53,221 --> 01:27:57,836
So, and, if you include these, you'd be
very hard pressed to find problems,

1670
01:27:57,836 --> 01:28:03,630
any problem you can solve that is not
convex if you include these.

1671
01:28:03,630 --> 01:28:05,300
So that, and that is an open challenge.

1672
01:28:05,300 --> 01:28:08,776
So So, okay.

1673
01:28:08,776 --> 01:28:13,200
So, this one actually can be solved by
solving its dual, which is this SDP.

1674
01:28:13,200 --> 01:28:17,235
Doesn't matter.
The, the fact is zero duality gap, right?

1675
01:28:17,235 --> 01:28:18,360
so, okay, now it's fine.

1676
01:28:21,820 --> 01:28:24,850
Because you want to minimize over choice
of x.

1677
01:28:24,850 --> 01:28:28,968
This thing, but this thing has just been
expressed as a minimization function.

1678
01:28:28,968 --> 01:28:32,820
Right, and therefor you minimize both at
the same time.

1679
01:28:32,820 --> 01:28:34,036
You solve this SDP, and

1680
01:28:34,036 --> 01:28:39,750
you solve, you actually solve exactly this
robust least squares problem, right?

1681
01:28:39,750 --> 01:28:42,110
Here, what this is the following is, we
have the,

1682
01:28:42,110 --> 01:28:47,749
I mean this is a very simple thing, we
actually have, it's a two dimensional.

1683
01:28:47,749 --> 01:28:50,134
ellipsoid, I guess you might even call it
an ellipse or

1684
01:28:50,134 --> 01:28:52,700
something like that, of A matrices.

1685
01:28:52,700 --> 01:28:55,682
Now, you commit to X and then we find the,
we'll find

1686
01:28:55,682 --> 01:29:02,106
like the worst [INAUDIBLE] plot here is
the histogram of the residuals, right?

1687
01:29:02,106 --> 01:29:05,602
Now here we'll take U, uniformly
distributed on the unit disk.

1688
01:29:05,602 --> 01:29:07,450
Okay?
And we'll take several values of x.

1689
01:29:07,450 --> 01:29:10,362
The first is, suppose you just completely
ignore uncertainty, and

1690
01:29:10,362 --> 01:29:14,120
you solve the least squares problem
minimize A zero X minus B.

1691
01:29:14,120 --> 01:29:16,430
That's the nominal problem.

1692
01:29:16,430 --> 01:29:20,000
You minimize that, and you get this
distribution of residuals.

1693
01:29:20,000 --> 01:29:20,760
Right?

1694
01:29:20,760 --> 01:29:22,830
So, they're all over the place.

1695
01:29:22,830 --> 01:29:25,758
And You know, some of the residuals are
four and five,, and

1696
01:29:25,758 --> 01:29:31,420
by the way, how would you describe what
happened in this situation on the left?

1697
01:29:31,420 --> 01:29:32,020
Dumb luck.

1698
01:29:32,020 --> 01:29:33,720
That's exactly what that is, right?

1699
01:29:33,720 --> 01:29:37,606
You committed to an x, in fact there are
as that make the residual smaller than you

1700
01:29:37,606 --> 01:29:40,260
thought it was going to be.

1701
01:29:42,010 --> 01:29:43,055
But you paid for it over here.

1702
01:29:43,055 --> 01:29:46,142
Okay, now you turn on Tickenoff
regularization.

1703
01:29:46,142 --> 01:29:49,750
And you can even imagine, I mean, one
could even make a cartoon of this, right?

1704
01:29:49,750 --> 01:29:52,495
You can imagine turning off the Tikhonov
regularization knob, when at 0,

1705
01:29:52,495 --> 01:29:54,550
you get this distribution, 'kay?

1706
01:29:54,550 --> 01:29:55,474
As I turn it up,

1707
01:29:55,474 --> 01:30:01,250
what happens is that distribution will,
will end up looking like this.

1708
01:30:01,250 --> 01:30:03,792
This is sort of the best looking one, the
Tikhonov regularization way, and

1709
01:30:03,792 --> 01:30:06,598
you can see it did exactly what it was
supposed to do.

1710
01:30:06,598 --> 01:30:09,366
Right.
Tikhonov regularization is something like

1711
01:30:09,366 --> 01:30:13,071
a heuristic for it's a heuristic for
giving you a robust solution and

1712
01:30:13,071 --> 01:30:18,230
you can see and here robustness we're
going to make it very big.

1713
01:30:18,230 --> 01:30:21,710
You get a tighter distribution of
residuals, okay.

1714
01:30:21,710 --> 01:30:25,420
And in fact the expected value of this
distribution is smaller than this one.

1715
01:30:25,420 --> 01:30:27,715
I mean, I'm not going to work it out
because it's too simple here.

1716
01:30:27,715 --> 01:30:28,404
But you can see,

1717
01:30:28,404 --> 01:30:32,045
it's simply, from a robustness point of
view, a better solution.

1718
01:30:32,045 --> 01:30:33,210
OK?

1719
01:30:33,210 --> 01:30:34,970
So this shows you exactly what people have
known for,

1720
01:30:34,970 --> 01:30:37,280
I don't know, fifteen hundred years.

1721
01:30:37,280 --> 01:30:41,398
That you had, you had regularization and
one interpretation of regularization is to

1722
01:30:41,398 --> 01:30:45,125
make your solution more robust to
variation here.

1723
01:30:45,125 --> 01:30:46,470
OK.

1724
01:30:46,470 --> 01:30:50,970
Now the exact Robustly squirts the one
that minimizes the worst case

1725
01:30:50,970 --> 01:30:54,345
residual over this, over this ellipsoid of
matrices,

1726
01:30:54,345 --> 01:31:01,160
is the one you obtain by solving this this
sdp here, right?

1727
01:31:01,160 --> 01:31:02,408
So you solve that sdp there, and

1728
01:31:02,408 --> 01:31:05,580
you get the following distribution of
residuals right here.

1729
01:31:05,580 --> 01:31:09,130
And this is, right there, that is, that's
the number.

1730
01:31:09,130 --> 01:31:10,436
Right.
That, that is the optimal,

1731
01:31:10,436 --> 01:31:13,665
that is the globally optimal number for
the best you can do.

1732
01:31:13,665 --> 01:31:14,590
Right?

1733
01:31:14,590 --> 01:31:15,740
And so this is kind of the idea.

1734
01:31:15,740 --> 01:31:18,930
And I think these distributions sort of
explain everything.

1735
01:31:18,930 --> 01:31:20,010
Right?
About what you're,

1736
01:31:20,010 --> 01:31:23,410
what you're trying to do here and what
robust optimization does.

1737
01:31:23,410 --> 01:31:25,055
Right?
So this is the kind of, this is,

1738
01:31:25,055 --> 01:31:26,573
this is the idea.

1739
01:31:28,140 --> 01:31:32,410
Notice that when you, I mean we can
anthropomorphize this a little bit but

1740
01:31:32,410 --> 01:31:36,540
one thing interesting to notice is all of
these, everything over here,

1741
01:31:36,540 --> 01:31:42,659
can either be taken off regularized or
denominal solution.

1742
01:31:42,659 --> 01:31:45,599
All of those are cases where by dumb luck
you did better than

1743
01:31:45,599 --> 01:31:47,922
the robust lead-squares.

1744
01:31:47,922 --> 01:31:51,612
Right so and what's interesting though is
when you

1745
01:31:51,612 --> 01:31:56,286
typically push the worst case residual
down usually an effect is

1746
01:31:56,286 --> 01:32:04,170
that actually the best case actually
[LAUGH] goes the other way, right?

1747
01:32:04,170 --> 01:32:05,844
I mean this is sort of natural but, and

1748
01:32:05,844 --> 01:32:08,873
again what I'm saying now is extremely
vague.

1749
01:32:08,873 --> 01:32:11,421
These are not the arguments you would be
allowed to make.

1750
01:32:11,421 --> 01:32:15,429
In public but it's just something to point
out.
