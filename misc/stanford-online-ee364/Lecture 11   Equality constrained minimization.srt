1
00:00:00,120 --> 00:00:04,960
What we're to do now is look at equality
constrained minimization, and you know,

2
00:00:04,960 --> 00:00:09,320
we're stepping slowly towards solving
general context [COUGH]

3
00:00:09,320 --> 00:00:10,190
optimization problems.

4
00:00:10,190 --> 00:00:11,560
Right?
[COUGH] So, so,

5
00:00:11,560 --> 00:00:14,680
we're going to add constraints.

6
00:00:14,680 --> 00:00:17,195
But, the easiest constraints they handle
are equality constraints which are linear.

7
00:00:17,195 --> 00:00:19,960
Okay, so, and it's going to turn

8
00:00:19,960 --> 00:00:24,560
out actually there's a few interesting
things here, but some of it is actually.

9
00:00:24,560 --> 00:00:26,220
It turns out to be quite simple.

10
00:00:26,220 --> 00:00:27,730
We just reduce it to the other one.

11
00:00:27,730 --> 00:00:31,490
Okay so we want to minimize f at the
smooth.

12
00:00:31,490 --> 00:00:33,184
Subject Ax equals b.

13
00:00:33,184 --> 00:00:36,600
And we-, we're going to assume that A s
full rank.

14
00:00:36,600 --> 00:00:37,820
It's wide and full rank.

15
00:00:39,220 --> 00:00:42,260
And we'll assume the optimum is attained.

16
00:00:42,260 --> 00:00:42,840
Right?

17
00:00:42,840 --> 00:00:44,760
And so basically it says you simply want,

18
00:00:44,760 --> 00:00:47,346
these are the necessary insufficient
optimality conditions right.

19
00:00:47,346 --> 00:00:50,020
The, it's the kk key conditions.

20
00:00:50,020 --> 00:00:53,970
Nu star is an optimal dual variable here.

21
00:00:55,410 --> 00:00:57,520
That's the, you know, or Lagrange
multiplier.

22
00:00:57,520 --> 00:00:59,080
And so you want to solve these two
equations, and

23
00:00:59,080 --> 00:01:00,630
let's stare at the two equations for a
minute.

24
00:01:02,150 --> 00:01:05,290
interestingly, these equations are affine
in nu star.

25
00:01:06,480 --> 00:01:07,590
Right?
They do not,

26
00:01:07,590 --> 00:01:09,240
they're not nonlinear in nu star, they're
affine.

27
00:01:10,400 --> 00:01:12,940
Now what are they in x?

28
00:01:12,940 --> 00:01:15,310
Well, that's linear, right?

29
00:01:15,310 --> 00:01:16,690
That depends on that.

30
00:01:16,690 --> 00:01:18,670
Oh, except for one case.

31
00:01:18,670 --> 00:01:21,470
When is the gradient of a function an
affine function?

32
00:01:22,740 --> 00:01:24,010
When it's quadratic, right?

33
00:01:24,010 --> 00:01:29,150
So if the function is quadratic these are
a set of linear equations, okay, and

34
00:01:29,150 --> 00:01:33,370
that's, so that says that solving linearly
constrained quadratic

35
00:01:33,370 --> 00:01:36,410
Optimization problems, is pure linear
algebra.

36
00:01:36,410 --> 00:01:36,950
Right?
That's it.

37
00:01:36,950 --> 00:01:39,430
You just set up a linear algebra and you
do it.

38
00:01:39,430 --> 00:01:40,520
OK?

39
00:01:40,520 --> 00:01:45,440
So, but in general, this thing, if f is
non-quadratic, this is non-affine, and

40
00:01:45,440 --> 00:01:47,240
that's a set of nonlinear equations.

41
00:01:48,410 --> 00:01:50,670
People have names for the residuals,
right?

42
00:01:50,670 --> 00:01:54,195
The name for the residual in this equation
is called the primal residual.

43
00:01:54,195 --> 00:01:55,890
>> And sometimes denoted RP.

44
00:01:57,890 --> 00:02:01,300
and, and the condition, of course, the
optimality conditions are P equals 0.

45
00:02:01,300 --> 00:02:05,620
This one, that expression on the left is
called the Dual Residual and

46
00:02:05,620 --> 00:02:07,140
it denoted RD.

47
00:02:07,140 --> 00:02:09,960
And, of course, the optimality condition
is that RD equals 0.

48
00:02:09,960 --> 00:02:15,850
So, you're looking to find X and Nu that
make RP and RD 0.

49
00:02:15,850 --> 00:02:16,570
Okay?
And by the way,

50
00:02:16,570 --> 00:02:18,110
the number of equations is about right.

51
00:02:18,110 --> 00:02:23,570
You have m plus p-, you have m plus p
variables, right?

52
00:02:23,570 --> 00:02:24,880
N plus p variables.

53
00:02:24,880 --> 00:02:27,010
And you have n plus p equations.

54
00:02:27,010 --> 00:02:28,840
So, but they're possibly non linear.

55
00:02:28,840 --> 00:02:29,560
Okay.

56
00:02:29,560 --> 00:02:31,610
So,.

57
00:02:31,610 --> 00:02:33,090
Well, we already talked about this, right?

58
00:02:33,090 --> 00:02:36,070
If, if you, if you want to do equality
constrained quadratic minimization,

59
00:02:36,070 --> 00:02:39,130
it's linear algebra, until you get
something like this.

60
00:02:39,130 --> 00:02:39,630
Right.

61
00:02:41,080 --> 00:02:43,950
The right, the optimality conditions this
way, and

62
00:02:43,950 --> 00:02:48,130
you're actually computing both x star,
and, nu star,

63
00:02:48,130 --> 00:02:51,660
which is an optimal Lagrange multiplier,
you compute both of them.

64
00:02:51,660 --> 00:02:54,830
By the way, there's some structure here,
there's a block of zeros, and

65
00:02:54,830 --> 00:02:58,560
your eyes should be looking at it or
something, already, without my saying.

66
00:02:58,560 --> 00:02:59,170
I mean that's not a lot,

67
00:02:59,170 --> 00:03:01,650
you know, you could have more structure
but that, that, that's that.

68
00:03:03,050 --> 00:03:06,380
And it turns out, I mean you can work out
a lot about this KKT matrix, for

69
00:03:06,380 --> 00:03:13,020
example It's non singular if and only if
if you're in the null space of A,

70
00:03:13,020 --> 00:03:15,640
P is positive definite or something like
that.

71
00:03:15,640 --> 00:03:20,090
So another way to say that roughly is P is
positive definite on the null space of

72
00:03:20,090 --> 00:03:21,020
A, right.

73
00:03:21,020 --> 00:03:25,660
So P has to have positive curvature kind
of on the feasible set,

74
00:03:25,660 --> 00:03:27,430
that's the right way to say it, okay.

75
00:03:28,750 --> 00:03:31,580
That wasn't quite right, but that's the
spirit.

76
00:03:31,580 --> 00:03:33,260
Okay, just set up linear equations.

77
00:03:35,390 --> 00:03:36,620
There's a very general method for

78
00:03:36,620 --> 00:03:38,880
handling equality constraints if you just
eliminate them.

79
00:03:38,880 --> 00:03:41,660
Right?
So, you would do the following.

80
00:03:41,660 --> 00:03:45,810
I would, this is my constrained
representation of an affine set, but

81
00:03:45,810 --> 00:03:50,320
I will write it instead as a free
parameter representation.

82
00:03:50,320 --> 00:03:55,360
And so what I'll do is, you compute an x
hat that satisfies Ax equals b.

83
00:03:55,360 --> 00:03:56,450
That's this one.

84
00:03:56,450 --> 00:03:57,700
Lot's of names for this.

85
00:03:57,700 --> 00:04:02,460
By the way, a traditional name for this is
x sub p for a particular solution.

86
00:04:02,460 --> 00:04:05,840
That's the 19th-century name is, you have
a particular solution.

87
00:04:05,840 --> 00:04:08,170
I think the other one's, like, a
homogeneous, I don't know, anyway.

88
00:04:08,170 --> 00:04:11,940
So, that's a partic-, a par-, part-, a
particular solution, sorry.

89
00:04:11,940 --> 00:04:16,430
And then, f is a matrix whose range is
exactly the null space of a.

90
00:04:16,430 --> 00:04:17,030
Right?

91
00:04:17,030 --> 00:04:20,570
Lots of ways to compute that, that's just
linear algebra.

92
00:04:20,570 --> 00:04:23,540
So, tons of ways to use, a QR
factorization,

93
00:04:23,540 --> 00:04:25,980
all sorts of things, all sorts of ways to
do this.

94
00:04:25,980 --> 00:04:26,480
Okay.

95
00:04:28,360 --> 00:04:32,670
so, what you do now is, we'll change
variables in our optimization problem.

96
00:04:32,670 --> 00:04:36,160
Instead of x, we'll go with z, and you end
up with this.

97
00:04:36,160 --> 00:04:38,100
And now it's totally unconstrained.

98
00:04:38,100 --> 00:04:40,220
Still a convex problem, because, you know,

99
00:04:40,220 --> 00:04:46,070
f is simply applied to a applied to an
affine transformation.

100
00:04:46,070 --> 00:04:50,120
Here by the way if the original f was self
concordant it still is right?

101
00:04:51,720 --> 00:04:54,370
So that's it and we know how to do that.

102
00:04:54,370 --> 00:04:56,740
I mean we can apply Newton's method
gradients, deepest ascent,

103
00:04:56,740 --> 00:04:57,790
whatever you like.

104
00:04:57,790 --> 00:05:00,425
Okay so that's, that's the idea.

105
00:05:00,425 --> 00:05:04,870
Okay, and if you want, after solving this
problem, you can reconstruct x.

106
00:05:04,870 --> 00:05:08,820
I mean that's x but you can also
reconstruct the optimal Lagrange

107
00:05:08,820 --> 00:05:12,690
multiplier, and it's got just a particular
formula like that.

108
00:05:12,690 --> 00:05:14,595
That's if you wanted it, in the original
one.

109
00:05:14,595 --> 00:05:15,890
Okay?

110
00:05:15,890 --> 00:05:20,510
So, oh, I should mention something about
the interpretation, sometimes it's useful,

111
00:05:20,510 --> 00:05:22,390
sometime you want to know this.

112
00:05:22,390 --> 00:05:27,490
Because the interpretation of nu star is
actually something like I'll be very,

113
00:05:27,490 --> 00:05:31,710
it's something like the prices for, if
these,

114
00:05:31,710 --> 00:05:36,290
if every row of this is interpreted as
some kind of clearing constraint, right?

115
00:05:36,290 --> 00:05:39,580
That, sort of the amount produced balances
the amount consumed, or

116
00:05:39,580 --> 00:05:40,530
something like that.

117
00:05:40,530 --> 00:05:43,210
So it, that would be the economic
interpretation.

118
00:05:43,210 --> 00:05:46,990
Then in fact, nu is a vector of the
optimal prices of each of

119
00:05:46,990 --> 00:05:47,790
those commodities.

120
00:05:47,790 --> 00:05:49,800
It said, basically as you know,

121
00:05:49,800 --> 00:05:52,750
the interpretation of nu star is, you know
what?

122
00:05:52,750 --> 00:05:54,930
What if you allowed me to change b a
little bit?

123
00:05:54,930 --> 00:05:55,585
What would happen?

124
00:05:55,585 --> 00:05:59,490
Nu Start tells you how the optimal value
would change, okay?

125
00:05:59,490 --> 00:06:00,830
So, I mean I just mentioned this.

126
00:06:02,240 --> 00:06:04,960
Okay, so, so that's that.

127
00:06:04,960 --> 00:06:09,050
So, what this kind of sends at this point
you don't need to know anymore.

128
00:06:09,050 --> 00:06:09,930
From some point of view,

129
00:06:09,930 --> 00:06:13,660
you don't need to know any theory about
the quality constraint problems.

130
00:06:13,660 --> 00:06:16,200
Because you just reduce it to an
unconstrained problem, it's done.

131
00:06:16,200 --> 00:06:17,100
Okay.

132
00:06:17,100 --> 00:06:19,820
So quick example of that.

133
00:06:19,820 --> 00:06:23,330
Let's solve here's an optimal allocation
with a resource constraint.

134
00:06:23,330 --> 00:06:29,130
So I minimize a bunch of individual costs
subject to all these things add up to b.

135
00:06:29,130 --> 00:06:29,990
Right?
And so, so

136
00:06:29,990 --> 00:06:31,560
this is an allocation problem, right?

137
00:06:31,560 --> 00:06:34,390
I have, I have b as sort of an amount of
resource I have,

138
00:06:34,390 --> 00:06:39,250
and I'm, I'm dividing it up among a bunch
of agents, right?

139
00:06:39,250 --> 00:06:40,090
And agents.

140
00:06:40,090 --> 00:06:40,820
And I want to do that,

141
00:06:40,820 --> 00:06:44,890
sometimes this is called the social cost
in that context, right?

142
00:06:44,890 --> 00:06:48,880
And here, by the way, The optimal Lagrange
multiply, which is scalar,

143
00:06:48,880 --> 00:06:53,765
it actually tells you the optimal price
for the commodity that we are, allocated.

144
00:06:53,765 --> 00:06:54,380
Okay?

145
00:06:54,380 --> 00:06:57,440
So just, I mean, it's kind of obvious but,
okay.

146
00:06:57,440 --> 00:06:59,340
So, that's a constrained problem.

147
00:06:59,340 --> 00:07:00,470
How would you solve it?

148
00:07:00,470 --> 00:07:01,830
Well what we could do is just is.

149
00:07:01,830 --> 00:07:03,790
We'll do the following obvious
elimination.

150
00:07:06,290 --> 00:07:10,640
I could take X hat to be BEN, here's
another X hat.

151
00:07:10,640 --> 00:07:11,530
Right?

152
00:07:11,530 --> 00:07:14,610
It's B over N times the vector of ones.

153
00:07:14,610 --> 00:07:15,920
Right?
That's a uniform allocation.

154
00:07:15,920 --> 00:07:16,560
But, doesn't matter.

155
00:07:16,560 --> 00:07:18,760
So, I'll take it to be BEN.

156
00:07:18,760 --> 00:07:20,805
That means I give everything to agent N.

157
00:07:20,805 --> 00:07:28,460
Okay, and here's a matrix whose whose
range is exactly the null space of A.

158
00:07:28,460 --> 00:07:30,470
A is the, is the vector.

159
00:07:30,470 --> 00:07:32,260
It is the row vector one, one, one, one,
one,

160
00:07:32,260 --> 00:07:35,170
one and the null space of that is all
things which sum to zero.

161
00:07:35,170 --> 00:07:37,020
Right?
So, that's this.

162
00:07:37,020 --> 00:07:38,420
And you know, look.

163
00:07:38,420 --> 00:07:42,850
It just says that this transforms to that,
and that's unconstrained.

164
00:07:42,850 --> 00:07:44,820
By the way, so that says.

165
00:07:44,820 --> 00:07:49,880
Whatever you can say about this problem,
you can solve it super fast, okay.

166
00:07:49,880 --> 00:07:53,210
Actually we'll get to that, because we're
going to see that there's a way to,

167
00:07:53,210 --> 00:07:55,615
to get the structure to exploit the
structure directly.

168
00:07:55,615 --> 00:08:02,120
Newton's step for a equality constrained
problem and it looks like this.

169
00:08:04,450 --> 00:08:09,380
It says, you formed this 2 by 2 matrix,
right, 2 by 2 block matrix, here.

170
00:08:09,380 --> 00:08:11,280
You have v and w.

171
00:08:11,280 --> 00:08:15,370
And you solve, and on the right-hand side
you have the gradient and 0.

172
00:08:15,370 --> 00:08:16,180
Right?
And, look,

173
00:08:16,180 --> 00:08:18,470
if you read the first one that says Av
equals 0,

174
00:08:18,470 --> 00:08:20,310
that means v's in the null space.

175
00:08:20,310 --> 00:08:21,690
If you're in a null space, by the way,

176
00:08:21,690 --> 00:08:25,680
that says that you can freely add it to a
feasible point x.

177
00:08:25,680 --> 00:08:31,270
Because if a x equals b, and a v equals 0,
can add any multiple of v to x,

178
00:08:31,270 --> 00:08:36,550
and it still satisfies a x plus, you know,
h v equals b, right.

179
00:08:36,550 --> 00:08:38,420
So, that's the second.

180
00:08:38,420 --> 00:08:41,770
And the top one, basically tells you
something, it,

181
00:08:41,770 --> 00:08:43,510
it actually has to do with that dual.

182
00:08:43,510 --> 00:08:45,770
Residual which we'll talk about in a
minute.

183
00:08:45,770 --> 00:08:48,180
So couple of interpretations of this.

184
00:08:48,180 --> 00:08:51,610
Oh notice that it reverts to the Newton
step when there are no

185
00:08:51,610 --> 00:08:54,380
equality constraints because when there
are no equality constraints that's just

186
00:08:54,380 --> 00:08:55,480
a one by one and it's,

187
00:08:55,480 --> 00:09:01,550
it's Hessian times v equals minus gradient
that gives you the Newton step.

188
00:09:01,550 --> 00:09:04,910
So where does this come from, lots of
interpretations.

189
00:09:04,910 --> 00:09:05,410
Here's one.

190
00:09:06,790 --> 00:09:11,210
Form, the quadratic approximation of f at
x.

191
00:09:11,210 --> 00:09:12,870
That's this thing here.

192
00:09:12,870 --> 00:09:16,766
And notice v is like a step or a
perturbation, okay?

193
00:09:16,766 --> 00:09:21,740
Then, this, the equality are, they're
equality constraints,

194
00:09:21,740 --> 00:09:23,020
you don't need to approximate anything.

195
00:09:23,020 --> 00:09:24,080
You don't need to linearize.

196
00:09:24,080 --> 00:09:25,250
Or, you could, even, right?

197
00:09:25,250 --> 00:09:27,060
You could implement the method.

198
00:09:27,060 --> 00:09:30,040
Which is the, the linearize method, and

199
00:09:30,040 --> 00:09:33,190
you would call it on an affine function,
and it would simply return itself.

200
00:09:34,230 --> 00:09:35,800
Right?
So this would be just fine.

201
00:09:35,800 --> 00:09:38,010
And so, how do you linearize the quality
constraints?

202
00:09:38,010 --> 00:09:39,190
They're just the same.

203
00:09:39,190 --> 00:09:39,720
'Kay?

204
00:09:39,720 --> 00:09:41,900
Now, this we know how to do.

205
00:09:41,900 --> 00:09:42,660
Right?
Because this is

206
00:09:42,660 --> 00:09:44,800
minimizing a convex quadratic in v.

207
00:09:44,800 --> 00:09:46,050
Subject to equality constraints.

208
00:09:46,050 --> 00:09:47,350
We know how to do that.

209
00:09:47,350 --> 00:09:48,380
And guess what?

210
00:09:48,380 --> 00:09:50,820
All you do is, you solve this equation
here.

211
00:09:50,820 --> 00:09:53,300
And, that gives you v and w.

212
00:09:53,300 --> 00:09:54,260
So that's, that's another way to say.

213
00:09:54,260 --> 00:09:59,150
And notice that that preserves the spirit
of Newton's method for,

214
00:09:59,150 --> 00:10:00,310
without equality constraints.

215
00:10:00,310 --> 00:10:01,710
I mean, so.

216
00:10:01,710 --> 00:10:03,960
Instead of being explicit, you should say
Newton's method,

217
00:10:03,960 --> 00:10:09,430
really the idea behind it is this, is you
implement a method that says,

218
00:10:09,430 --> 00:10:13,690
you know, get quadratic, get quadratic
approximation or something like that.

219
00:10:13,690 --> 00:10:18,650
And, so, then you call it on, you do
f.quadapprox...

220
00:10:18,650 --> 00:10:23,080
At x, and it returns a quadratic model
valid near that point.

221
00:10:23,080 --> 00:10:24,320
And then you minimize it.

222
00:10:24,320 --> 00:10:26,030
That's Newton's method, right?

223
00:10:26,030 --> 00:10:27,310
This is the same thing.

224
00:10:27,310 --> 00:10:30,530
It says, you call the same method on f,
and now you minimize it,

225
00:10:30,530 --> 00:10:31,810
but subject to the equality constraints.

226
00:10:31,810 --> 00:10:34,420
But you know we know how to minimize
quadratics subject to

227
00:10:34,420 --> 00:10:36,490
equality constraints, because that's
linear algebra.

228
00:10:36,490 --> 00:10:39,015
OK?
So, so that, I think it's natural.

229
00:10:39,015 --> 00:10:42,580
>> Another way to say it is this.

230
00:10:42,580 --> 00:10:44,020
Here's what you really want.

231
00:10:44,020 --> 00:10:49,470
What you really want, when you take a step
here you want,

232
00:10:49,470 --> 00:10:51,470
you would like this thing to be 0.

233
00:10:51,470 --> 00:10:53,500
That's what you'd like.

234
00:10:53,500 --> 00:10:54,000
Right?

235
00:10:55,170 --> 00:10:56,410
that, that's what you really want.

236
00:10:56,410 --> 00:11:01,490
That, that's driving our, our d, the dual
residual to zero and

237
00:11:01,490 --> 00:11:04,860
so what we do is we simply take an
approximation of this.

238
00:11:04,860 --> 00:11:06,800
That's a that's in general non affine.

239
00:11:06,800 --> 00:11:10,830
If f is quadratic its affine but if f is
non quadratic that's affine and

240
00:11:10,830 --> 00:11:14,810
an approximation of the gradient at x plus
v is very simple.

241
00:11:14,810 --> 00:11:19,090
Its the gradient at x plus the [UNKNOWN]
which is after all the Derivative of

242
00:11:19,090 --> 00:11:24,170
the gradient multiplied by the step, and
if you solve these two equations?

243
00:11:24,170 --> 00:11:25,580
That's this.

244
00:11:25,580 --> 00:11:26,320
Okay?

245
00:11:26,320 --> 00:11:31,130
So, so you can either call the linearize
method on the optimality conditions and

246
00:11:31,130 --> 00:11:32,530
solve linear equations.

247
00:11:32,530 --> 00:11:33,970
You can solve the quad, you can, or

248
00:11:33,970 --> 00:11:39,120
you can call the quadratic method on the
original problem and then solve that.

249
00:11:39,120 --> 00:11:40,700
Right?
Either way, and they're the same thing.

250
00:11:41,810 --> 00:11:48,650
Okay, so the Newton decrement is it can be
written several ways.

251
00:11:48,650 --> 00:11:51,220
This is one of them and it's that.

252
00:11:51,220 --> 00:11:52,690
By the way there are some other formulas
for

253
00:11:52,690 --> 00:11:57,110
the Newton decrement which are completely
false in the quality constraint case.

254
00:11:57,110 --> 00:12:03,050
One is I think, Delta x Newton transpose
times the inverse of this times that.

255
00:12:03,050 --> 00:12:03,780
Right?

256
00:12:03,780 --> 00:12:06,670
There's a, there's, there's one of the
formulas looks like that and its wrong.

257
00:12:06,670 --> 00:12:08,680
OK but so you have to be careful.

258
00:12:08,680 --> 00:12:10,270
A couple of the formulas you know for

259
00:12:10,270 --> 00:12:14,470
Newton decrement are correct others are
are are wrong.

260
00:12:14,470 --> 00:12:15,960
OK and its the same thing.

261
00:12:15,960 --> 00:12:18,100
It simply gives you an estimate.

262
00:12:18,100 --> 00:12:22,430
Its the difference between It says that if
you form the quadratic model,

263
00:12:22,430 --> 00:12:25,450
solve the constraint problem, how much
would the objective go down.

264
00:12:25,450 --> 00:12:27,130
And that's, that's lambda squared over 2.

265
00:12:27,130 --> 00:12:30,220
Is, is what that is, so, makes perfect,
perfect sense.

266
00:12:30,220 --> 00:12:32,880
It's also the directional derivative in
the Newton direction, so

267
00:12:32,880 --> 00:12:34,060
that also is preserved.

268
00:12:36,220 --> 00:12:37,980
But it is not equal to that.

269
00:12:37,980 --> 00:12:41,090
For, that's an example of one of the
formulas that's just wrong.

270
00:12:41,090 --> 00:12:42,900
For equality constraint.

271
00:12:42,900 --> 00:12:43,880
Okay.

272
00:12:43,880 --> 00:12:46,060
Now here's Newton's method with equality
constraints.

273
00:12:46,060 --> 00:12:48,240
So here's the algorithm.

274
00:12:48,240 --> 00:12:50,450
You know you compute the Newton step and
the decrement.

275
00:12:50,450 --> 00:12:52,540
You quit if, if the decrement squared over
two,

276
00:12:52,540 --> 00:12:54,530
which is your expected decrease, is less
than epsilon.

277
00:12:55,730 --> 00:12:57,130
The you align search and you update.

278
00:12:57,130 --> 00:12:57,810
So guess what?

279
00:12:57,810 --> 00:13:00,680
That algorithm is identical to Newton's
method for unconstrained.

280
00:13:00,680 --> 00:13:01,790
It's, it's exactly the same.

281
00:13:02,850 --> 00:13:06,370
The difference is we have now kind of
overloaded a few things here.

282
00:13:06,370 --> 00:13:09,450
Newton's step is been overloaded to handle
equality constraints.

283
00:13:09,450 --> 00:13:13,260
It doesn't mean Hessian inverse minus
Hessian inverse gradient.

284
00:13:13,260 --> 00:13:15,360
It means solve this big k k key system.

285
00:13:15,360 --> 00:13:15,940
Right?

286
00:13:15,940 --> 00:13:17,790
And we also overloaded,

287
00:13:17,790 --> 00:13:23,310
the, Newton decrement to mean something
appropriate in that case.

288
00:13:23,310 --> 00:13:26,180
Okay, otherwise, it's identical, the
algorithm.

289
00:13:26,180 --> 00:13:31,690
And it's a feasible descent method,
meaning, every step x is feasible,

290
00:13:31,690 --> 00:13:34,800
and your objective goes down, unless
you're at the optimum point.

291
00:13:34,800 --> 00:13:36,030
And it's affine invariant.

292
00:13:36,030 --> 00:13:39,540
If you change coordinates, it's doesn't
matter.

293
00:13:39,540 --> 00:13:42,120
Okay, I mean you get the commutative
diagram.

294
00:13:42,120 --> 00:13:43,420
You get exactly the same caliber.

295
00:13:44,900 --> 00:13:46,480
Okay, by the way,

296
00:13:46,480 --> 00:13:53,550
what that means is that scaling is sort of
a second-order issue for Newton methods,

297
00:13:53,550 --> 00:13:59,690
right, meaning, yeah if you scale things
by 10 to the 38 and 10 to the -38,

298
00:13:59,690 --> 00:14:02,670
you can have underflow and overflow and
numerical issues and things like that.

299
00:14:02,670 --> 00:14:03,210
Sure.

300
00:14:03,210 --> 00:14:03,880
Okay?
But,

301
00:14:03,880 --> 00:14:07,900
you can scale things very happily by
thousand, ten thousand, and

302
00:14:07,900 --> 00:14:09,660
it will work fine.

303
00:14:09,660 --> 00:14:14,710
And the reason for that is to a numerical
analyst a condition number of a system of

304
00:14:14,710 --> 00:14:16,590
linear equations of 10 to the 3 or 10 to
the 4.

305
00:14:16,590 --> 00:14:17,990
It's not that big a deal.

306
00:14:17,990 --> 00:14:18,730
Right?

307
00:14:18,730 --> 00:14:23,700
Whereas for any first order method,
condition number is a first order effect.

308
00:14:23,700 --> 00:14:28,270
If the first, if the condition number is
like ten, it's going to work super well.

309
00:14:28,270 --> 00:14:31,590
If it's a hundred thousand, just forget it
for

310
00:14:31,590 --> 00:14:32,640
all practical purposes it doesn't work.

311
00:14:32,640 --> 00:14:34,200
If it's a thousand it may not work.

312
00:14:36,280 --> 00:14:38,320
Another way to say it is.

313
00:14:38,320 --> 00:14:42,220
If your method is working with a gradient
method, I can break it by scaling.

314
00:14:42,220 --> 00:14:44,750
Scaling even modest, by modest scaling.

315
00:14:44,750 --> 00:14:46,480
Newton's method I won't break.

316
00:14:46,480 --> 00:14:48,020
It'll worth perfectly.

317
00:14:48,020 --> 00:14:48,540
OK?

318
00:14:48,540 --> 00:14:49,140
So.

319
00:14:49,140 --> 00:14:49,640
OK.

320
00:14:51,220 --> 00:14:55,240
Now it turns out there's a, we don't have
to do any analysis of this and

321
00:14:55,240 --> 00:14:56,670
the reason is the following.

322
00:14:56,670 --> 00:14:59,540
There's a communative you get a perfect
communative diagram here.

323
00:14:59,540 --> 00:15:02,750
If you take a problem.

324
00:15:02,750 --> 00:15:04,810
You take an original problem with equality
constraints,

325
00:15:04,810 --> 00:15:06,730
then you eliminate the variables.

326
00:15:06,730 --> 00:15:10,030
You do variable elimination, then you
apply Newton's method.

327
00:15:10,030 --> 00:15:12,650
You get a commutative diagram with
applying Newton's method with

328
00:15:12,650 --> 00:15:14,930
equality constraints, it's identical.

329
00:15:14,930 --> 00:15:16,570
And that means we don't have to do any
analysis at

330
00:15:16,570 --> 00:15:17,660
all because we already know it all.

331
00:15:17,660 --> 00:15:18,360
We know everything.

332
00:15:18,360 --> 00:15:21,620
Including for self importance I might add,
we know everything.

333
00:15:21,620 --> 00:15:22,790
Nothing to do.

334
00:15:22,790 --> 00:15:25,160
So you don't need any convergence
analysis.

335
00:15:25,160 --> 00:15:25,660
That's good.

336
00:15:27,830 --> 00:15:30,020
Now we're talking about something
different.

337
00:15:30,020 --> 00:15:32,780
And honestly, this is maybe the more
useful one.

338
00:15:32,780 --> 00:15:34,890
And this is actually the more modern one.

339
00:15:34,890 --> 00:15:39,100
And it's actually probably more useful,
and so here it is.

340
00:15:40,410 --> 00:15:43,680
If the idea of a Newton step at an
infeasible point.

341
00:15:43,680 --> 00:15:50,290
And so the basic idea is that you really
say that what you want to do is you,

342
00:15:50,290 --> 00:15:55,680
you, you write down the residual, right,
and that's in n plus rn plus p.

343
00:15:55,680 --> 00:15:56,800
And it's the following.

344
00:15:56,800 --> 00:16:00,170
It's, it's the dual residual and then the
primal residual.

345
00:16:00,170 --> 00:16:06,900
And what you want to solve is a set of
nonlinear equations in n plus p variables.

346
00:16:06,900 --> 00:16:08,270
Right?

347
00:16:08,270 --> 00:16:11,530
Of which, of wh, and the number of
equations you have is n plus p.

348
00:16:11,530 --> 00:16:14,320
Okay, so that's, that's what you want to
do.

349
00:16:14,320 --> 00:16:15,600
So, that's one view of it.

350
00:16:15,600 --> 00:16:20,170
Now, by the way, equa, solving non linear
equations in rn plus b,

351
00:16:20,170 --> 00:16:24,240
with n plus b equations It's super
complicated,

352
00:16:24,240 --> 00:16:27,090
like you don't even know that there exists
a solution.

353
00:16:27,090 --> 00:16:29,090
Right, I mean here you know that there's a
solution,

354
00:16:29,090 --> 00:16:32,440
because well it's a convex problem and so,
and so on and so forth.

355
00:16:32,440 --> 00:16:36,450
So that's the nice part, is that there's
some regularity to this.

356
00:16:36,450 --> 00:16:38,350
But Newton's method simply says,

357
00:16:38,350 --> 00:16:41,400
I want to solve that equation here's what
we're going to do.

358
00:16:41,400 --> 00:16:45,200
I'm going to, y is going to be, actually,
a primal dual variable.

359
00:16:45,200 --> 00:16:49,730
It's going to be a pair consisting of a
primal variable and a dual variable.

360
00:16:49,730 --> 00:16:52,740
And what we're going to do is this, I want
r of y as zero, so

361
00:16:52,740 --> 00:16:57,840
I'll imagine that I'm going to go y plus
delta y, that's going to be my step.

362
00:16:57,840 --> 00:17:00,250
And what I would like, is I want that to
be zero!

363
00:17:00,250 --> 00:17:01,710
But
>> That's equal to R.

364
00:17:01,710 --> 00:17:06,650
I mean, the, the linear approximation of R
is, half fine, but people call it linear,

365
00:17:06,650 --> 00:17:12,670
is R of Y plus DR, that's the derivative,
or Jacobian, multiplied by Delta 1.

366
00:17:12,670 --> 00:17:13,900
DR is exactly that.

367
00:17:13,900 --> 00:17:16,530
Now, you get a similar set of equations,
here's what you get.

368
00:17:16,530 --> 00:17:18,920
You get this.
Actually the right hand side is is, is,

369
00:17:18,920 --> 00:17:21,780
is a little bit different here.

370
00:17:21,780 --> 00:17:24,630
actually, it turns out these are
completely the same, right?

371
00:17:24,630 --> 00:17:29,450
If, if, here, we're actually calculating
the, that's the full Newton thing.

372
00:17:29,450 --> 00:17:30,830
sorry, the full dual variable.

373
00:17:30,830 --> 00:17:35,440
And here we're calculating an update to
the, the, a dual variable, okay?

374
00:17:35,440 --> 00:17:39,130
So, this is a, a, very nice form of
Newton's method.

375
00:17:39,130 --> 00:17:41,090
And it is to be interpreted this way.

376
00:17:41,090 --> 00:17:44,010
It says, and you should think of it as a
primal dual method.

377
00:17:44,010 --> 00:17:48,670
Because every step, you're updating a
primal and a dual variable, right?

378
00:17:48,670 --> 00:17:50,250
And the idea is to drive both the primal
and

379
00:17:50,250 --> 00:17:53,230
dual residuals to zero and, so, you solve
this.

380
00:17:53,230 --> 00:17:54,600
This is the same KKT system, and

381
00:17:54,600 --> 00:17:57,970
on the right hand side are the residuals
of both, right?

382
00:17:57,970 --> 00:18:02,500
So, by the way, it generalizes the Newton
step for a feasible point,

383
00:18:02,500 --> 00:18:04,860
because if x is feasible, that's 0.

384
00:18:04,860 --> 00:18:06,370
And guess what?

385
00:18:06,370 --> 00:18:09,090
You get the Newton step, that we had
before, for feasible points.

386
00:18:09,090 --> 00:18:10,980
So it's a generalization.

387
00:18:10,980 --> 00:18:12,000
But you have to be kind of careful.

388
00:18:13,510 --> 00:18:15,280
Let me tell you why you have to kind of be
careful.

389
00:18:15,280 --> 00:18:18,510
What's kind of cool about this is, this
defines a Newton step,

390
00:18:18,510 --> 00:18:19,560
even when you're infeasible.

391
00:18:19,560 --> 00:18:21,890
In other words, you don't satisfy a x
equals b.

392
00:18:21,890 --> 00:18:23,340
It says, as a direction to go in.

393
00:18:23,340 --> 00:18:25,360
Now you have to be very careful.

394
00:18:25,360 --> 00:18:28,710
Because that direction to go in is a
combination of two things.

395
00:18:28,710 --> 00:18:32,800
There's one is you want to make f small so

396
00:18:32,800 --> 00:18:35,370
you're kind of be going in the direction
minus gradiant f.

397
00:18:35,370 --> 00:18:37,800
I mean that is after all what we're trying
to do.

398
00:18:37,800 --> 00:18:40,670
But if you don't satisfy ax equals b
you're also going to

399
00:18:40,670 --> 00:18:44,920
be going in a direction towards which ax
equals b.

400
00:18:44,920 --> 00:18:48,990
Right, so you going to have kind of these
two directions and you also have to be

401
00:18:48,990 --> 00:18:52,570
very careful because in a problem like
this you could end up,its,

402
00:18:52,570 --> 00:18:57,720
its not a dissent method, I mean for
obvious reasons right, if that's my

403
00:18:57,720 --> 00:19:03,000
equality restraints, right and here's my,
my function I'm minimizing.

404
00:19:03,000 --> 00:19:03,650
Right?

405
00:19:03,650 --> 00:19:04,170
You know, I don't know.

406
00:19:04,170 --> 00:19:06,050
Maybe the, it's the solution would be
right there.

407
00:19:06,050 --> 00:19:07,883
It's, it's whenever the tangent touches
this line

408
00:19:07,883 --> 00:19:09,362
>> [COUGH]
>> That's the solution right?

409
00:19:09,362 --> 00:19:11,590
But, suppose I started right here.

410
00:19:11,590 --> 00:19:12,610
That, that's my initial point.

411
00:19:13,650 --> 00:19:15,340
Okay?
So, at that point,

412
00:19:15,340 --> 00:19:18,610
what's the gradient of F?

413
00:19:18,610 --> 00:19:19,390
It's 0.

414
00:19:19,390 --> 00:19:20,080
Right?

415
00:19:20,080 --> 00:19:20,580
So.

416
00:19:21,600 --> 00:19:23,700
The only minor problem is you're not
feesible.

417
00:19:23,700 --> 00:19:26,750
Right?
So, the Newton step, in this case,

418
00:19:26,750 --> 00:19:29,780
is going to point you to somewhere on the
line here.

419
00:19:29,780 --> 00:19:30,860
Right?
because, if you take one,

420
00:19:30,860 --> 00:19:34,220
if you take a full Newton step, the
equality constraint becomes feasible.

421
00:19:34,220 --> 00:19:35,510
Okay?
So that's, that's, that's the idea.

422
00:19:35,510 --> 00:19:38,270
And by the way what will happen to f from
your first iteration?

423
00:19:40,620 --> 00:19:41,170
It will go up.

424
00:19:42,290 --> 00:19:45,170
Has to go up because you started at the
minimum of F right?

425
00:19:45,170 --> 00:19:46,890
So, it's clearly not a descend method.

426
00:19:46,890 --> 00:19:50,670
Now when you see F go up someone says, hey
that's a crappy job you

427
00:19:50,670 --> 00:19:54,290
did on the optimization I gave you an
initial point and the first thing you

428
00:19:54,290 --> 00:19:58,120
do is your function values goes up and
what do you say to that?

429
00:20:00,160 --> 00:20:02,170
What would be your, what's your comeback?

430
00:20:03,860 --> 00:20:06,930
Yeah, it's like, well dude, your point
wasn't feasible.

431
00:20:06,930 --> 00:20:11,900
So, it had excellent objective, but, it
didn't, satisfy the constraints.

432
00:20:11,900 --> 00:20:12,590
So, that's all.

433
00:20:12,590 --> 00:20:13,130
Okay.

434
00:20:13,130 --> 00:20:15,750
I just mention this because, things like
this come up,

435
00:20:15,750 --> 00:20:18,890
and, you'd be surprised, actually.

436
00:20:18,890 --> 00:20:21,250
So, okay, so, it says the line search is
different.

437
00:20:22,380 --> 00:20:26,880
So you don't, you don't measure progress
by the function value obviously although,

438
00:20:26,880 --> 00:20:29,330
you should measure values, you should
measure two things.

439
00:20:29,330 --> 00:20:30,560
The primal and

440
00:20:30,560 --> 00:20:34,810
dual residual norms or some people lump in
together into one and either, either way.

441
00:20:34,810 --> 00:20:39,562
By the way this will tell you something if
you look at things like Sedumi or

442
00:20:39,562 --> 00:20:41,050
STPT3 output.

443
00:20:41,050 --> 00:20:42,130
I don't know if you do, but

444
00:20:42,130 --> 00:20:47,310
when you run CVX a few of the columns are
now going to be demystified.

445
00:20:47,310 --> 00:20:49,730
Now they'll be fully demystified by next
week.

446
00:20:49,730 --> 00:20:52,660
But, you will see things that say RP and
RD.

447
00:20:52,660 --> 00:20:53,700
Or, it'll be obscure.

448
00:20:53,700 --> 00:20:55,500
It'll say d norm and p norm.

449
00:20:55,500 --> 00:20:58,480
And these, well these literally mean the
primal residual and

450
00:20:58,480 --> 00:20:59,510
the dual residual norm.

451
00:20:59,510 --> 00:21:00,810
So, OK.

452
00:21:00,810 --> 00:21:01,690
OK.
So here it is.

453
00:21:03,910 --> 00:21:08,450
What you do is you compute these Newton,
the, the Newt-, the primal dual Newton

454
00:21:08,450 --> 00:21:13,700
steps, and then you do a backtracking line
search, on the, on the norm of R, okay?

455
00:21:13,700 --> 00:21:17,330
So, on the total residual, that's easy
enough.

456
00:21:17,330 --> 00:21:21,810
And it turns out the directional
derivative is simply minus the residual.

457
00:21:21,810 --> 00:21:23,290
That's a, a quick calculation.

458
00:21:23,290 --> 00:21:26,240
And so you get a very simple code that
looks like this, then you update x and

459
00:21:26,240 --> 00:21:27,600
you keep going.

460
00:21:27,600 --> 00:21:28,860
It's not a descent method, but

461
00:21:28,860 --> 00:21:31,370
I'll tell you what does go down is the
norm of the residual.

462
00:21:31,370 --> 00:21:34,495
So whenever you have an algorithm and
people analyze it,

463
00:21:34,495 --> 00:21:39,300
there's the basis of a proof or something
is of some function that goes down, right?

464
00:21:39,300 --> 00:21:42,950
In the Newton method the gradient descent
of things we focus on the function value

465
00:21:42,950 --> 00:21:46,060
going down I mean what could be more
natural you want to minimize that.

466
00:21:46,060 --> 00:21:50,440
So its kind of cool that the thing you
want to happen happens actually locally at

467
00:21:50,440 --> 00:21:51,040
every step.

468
00:21:51,040 --> 00:21:52,370
Greedily it happens right?

469
00:21:53,720 --> 00:21:58,210
Here it turns out that the by the way its
got all sorts of names.

470
00:21:58,210 --> 00:22:02,390
I think one name, one method is a
[UNKNOWN] function and

471
00:22:02,390 --> 00:22:03,580
there's another one.

472
00:22:03,580 --> 00:22:04,860
I forget there's all sorts of names and

473
00:22:04,860 --> 00:22:07,860
optimization too for that I forgot the
name.

474
00:22:07,860 --> 00:22:09,670
And in computer science you got another
name,

475
00:22:09,670 --> 00:22:12,350
which I also can't remember right now but
anyway.

476
00:22:12,350 --> 00:22:14,200
But the concept is very simple.

477
00:22:14,200 --> 00:22:16,350
There's a, you have an internet method and

478
00:22:16,350 --> 00:22:18,590
you have a function that goes down every
step.

479
00:22:18,590 --> 00:22:22,740
And that's base, the basis of your proof
that it works.

480
00:22:22,740 --> 00:22:23,650
Right?
So, by the way,

481
00:22:23,650 --> 00:22:30,430
there's a huge practical, advantage of
having such a function, if it's explicit.

482
00:22:30,430 --> 00:22:33,550
Because if it basically says you can do
anything you want in that algorithm.

483
00:22:33,550 --> 00:22:35,580
Any modification that you want at all,

484
00:22:35,580 --> 00:22:38,903
as long as you make sure that function
does go down.

485
00:22:38,903 --> 00:22:42,610
Right, so for example, you can insert
weird steps in your algorithm that does

486
00:22:42,610 --> 00:22:44,110
some kind of local optimization.

487
00:22:45,430 --> 00:22:49,150
But, you better make sure that local
optimization better ensure the Lyapunov

488
00:22:49,150 --> 00:22:50,930
function goes down.

489
00:22:50,930 --> 00:22:53,260
I just remembered one of the names, merit
function.

490
00:22:53,260 --> 00:22:56,993
But there's like five of them, you'll,
the, the concept is clear.

491
00:22:56,993 --> 00:23:02,660
So here it turns out that the merit
function is the norm of the residual.

492
00:23:02,660 --> 00:23:06,130
So, let's talk about solving KKT systems.

493
00:23:06,130 --> 00:23:07,870
How do you solve something like that?

494
00:23:07,870 --> 00:23:09,030
Well, there's lots of ways.

495
00:23:10,200 --> 00:23:11,260
so, one is,

496
00:23:11,260 --> 00:23:15,200
you just use LDL transpose, because that's
a symmetric indefinite system, right?

497
00:23:15,200 --> 00:23:17,310
It's indefinite because you look at the 0s
down there.

498
00:23:17,310 --> 00:23:19,250
And in fact, it has exactly n.

499
00:23:19,250 --> 00:23:21,490
Well, if H is positive definite, has
exactly N.

500
00:23:22,570 --> 00:23:26,770
actually, if it's solvable, it has N
positive item values and exactly M.

501
00:23:26,770 --> 00:23:28,710
Or P?
I think its P is the size of

502
00:23:28,710 --> 00:23:29,530
the equality constraints.

503
00:23:29,530 --> 00:23:30,920
P negative was.

504
00:23:30,920 --> 00:23:32,320
Okay?

505
00:23:32,320 --> 00:23:34,450
so, that's very simple.

506
00:23:34,450 --> 00:23:38,400
You just plug in this stuff and, and, and
use LDL transpose factorization.

507
00:23:40,510 --> 00:23:42,410
You can also do elimination.

508
00:23:42,410 --> 00:23:43,230
What you do is, you know,

509
00:23:43,230 --> 00:23:48,860
in many applications, H will be diagonal,
block diagonal, something simple.

510
00:23:48,860 --> 00:23:49,650
There we go.

511
00:23:49,650 --> 00:23:51,090
And so, H will be, and

512
00:23:51,090 --> 00:23:54,550
then, and then H would then be a natural
target for elimination.

513
00:23:54,550 --> 00:23:58,280
I mean, for sure, if you saw that system,
and H was diagonal.

514
00:23:58,280 --> 00:24:01,750
I'm hoping every single one of you would
have

515
00:24:01,750 --> 00:24:05,080
an overwhelming urge to eliminate the one
one block.

516
00:24:05,080 --> 00:24:08,550
You should and, and we're going to train
you until that's the case.

517
00:24:08,550 --> 00:24:10,320
Okay?
So that, if,

518
00:24:10,320 --> 00:24:13,820
if, but doesn't, I mean the math doesn't
matter either way.

519
00:24:13,820 --> 00:24:16,950
So if you eliminate the, the v.

520
00:24:16,950 --> 00:24:20,850
Here you end up with a system that looks
like this.

521
00:24:20,850 --> 00:24:24,730
By the way that is the negative sure
compliment of that matrix.

522
00:24:24,730 --> 00:24:28,800
Right, because the sure complement is zero
minus a h inverse transpose,

523
00:24:28,800 --> 00:24:30,320
that's the sure complement.

524
00:24:30,320 --> 00:24:31,460
Well, with a minus sign.

525
00:24:31,460 --> 00:24:32,690
Okay?

526
00:24:32,690 --> 00:24:34,410
so, that's a sure complement.

527
00:24:34,410 --> 00:24:34,930
And, by the way,

528
00:24:34,930 --> 00:24:39,440
I think this is called, people refer to
this system as the reduced system.

529
00:24:39,440 --> 00:24:41,030
That's standard notation.

530
00:24:41,030 --> 00:24:43,780
So people would say, you, you'd say, oh
what are you doing, Newton,

531
00:24:43,780 --> 00:24:46,310
blah blah blah, and say oh yeah, how are
you solving it?

532
00:24:46,310 --> 00:24:47,910
And they'd say reduce system.

533
00:24:47,910 --> 00:24:50,700
By the way, they then call this the
augmented system.

534
00:24:50,700 --> 00:24:54,350
Don't ask me why, the KKT system, that's
augmented and reduced.

535
00:24:54,350 --> 00:24:55,950
Okay, so you could do this.

536
00:24:57,360 --> 00:25:01,660
Now if H is singular, there's lots of
interesting things you can do.

537
00:25:01,660 --> 00:25:08,210
One is that in fact any solution of this
is identical to a solution to that.

538
00:25:08,210 --> 00:25:11,190
You can just check, but the point is that
what this allows you to

539
00:25:11,190 --> 00:25:16,200
do is to make sure that this 1 1 block is
non-singular.

540
00:25:16,200 --> 00:25:17,990
Right?
Then you can apply elimination.

541
00:25:17,990 --> 00:25:18,940
So, okay.

542
00:25:20,640 --> 00:25:22,750
Now we're going to, we're going to finish
with an example.

543
00:25:22,750 --> 00:25:25,590
And it's going to tie a whole bunch of
stuff together.

544
00:25:25,590 --> 00:25:29,880
And actually, it's going to, you're
going to see something really interesting.

545
00:25:29,880 --> 00:25:34,000
Which is to say, that all these, there's
lots of methods to solve a single problem.

546
00:25:34,000 --> 00:25:36,480
They relate, they go through duality and
stuff like that.

547
00:25:38,230 --> 00:25:43,256
And what, you'll, you'll see, but I'll get
to the punchline right now.

548
00:25:43,256 --> 00:25:46,820
That you, you're going to look at a
problem, and you can solve it many ways.

549
00:25:46,820 --> 00:25:52,030
You can solve the problem, you could solve
the dual, you could do this and that, and

550
00:25:52,030 --> 00:25:56,280
in each case, if you use Newton's method,
this is going to be the critical part.

551
00:25:56,280 --> 00:25:59,530
If you use Newton's method, The systems of
equations you're going to

552
00:25:59,530 --> 00:26:04,010
come down to solving in each step of each
of those is not exactly the same but

553
00:26:04,010 --> 00:26:08,310
has exactly the same structure and that
structure will be such that if you

554
00:26:08,310 --> 00:26:10,770
know what you're doing you can solve it
efficiently.

555
00:26:10,770 --> 00:26:11,290
Right?

556
00:26:11,290 --> 00:26:13,880
And so, what's what's kind of interesting
about that is a lot of

557
00:26:13,880 --> 00:26:17,495
times you will actually hear people who
don't know any better.

558
00:26:17,495 --> 00:26:19,860
>> Say things like, Well, I'm solving the
dual.

559
00:26:19,860 --> 00:26:20,370
Right?

560
00:26:20,370 --> 00:26:22,100
And they might say that because it sounds
cool.

561
00:26:22,100 --> 00:26:22,850
Right?
To solve them.

562
00:26:22,850 --> 00:26:24,450
Doesn't it?
I mean, you kind of,

563
00:26:24,450 --> 00:26:26,500
people don't know, you say, Well, I'm, I'm
solving the dual [UNKNOWN] .

564
00:26:26,500 --> 00:26:28,440
Anyway, sounds sophisticated.

565
00:26:28,440 --> 00:26:29,340
And you say, why is that?

566
00:26:29,340 --> 00:26:30,990
You go, Well, I get efficiency or
something.

567
00:26:30,990 --> 00:26:32,400
I mean, there are good reason to solve
duals.

568
00:26:32,400 --> 00:26:35,700
Like, you can distribute stuff in parallel
thing.

569
00:26:35,700 --> 00:26:36,740
That's another story.

570
00:26:36,740 --> 00:26:38,720
But, if you're doing like a Newton method,

571
00:26:38,720 --> 00:26:42,770
there's absolutely no reason to do it if
you know linear algebra.

572
00:26:42,770 --> 00:26:43,890
If you know numerical linear algebra.

573
00:26:43,890 --> 00:26:45,000
There's absolutely no reason to do that.

574
00:26:45,000 --> 00:26:45,500
We'll see that.

575
00:26:46,690 --> 00:26:48,570
What is different, actually the
initializations.

576
00:26:48,570 --> 00:26:49,270
So, all right.

577
00:26:49,270 --> 00:26:50,930
Let's, let's jump into it.

578
00:26:50,930 --> 00:27:00,020
We want to calculate the analytic center
of AX equals B and X nonnegative.

579
00:27:00,020 --> 00:27:03,250
And that's the feasible set for a classic
linear program.

580
00:27:03,250 --> 00:27:04,520
Right?
Minimizing transpose X.

581
00:27:04,520 --> 00:27:06,980
Subject AX is BX, X bigger than or equal
to 0.

582
00:27:06,980 --> 00:27:07,830
That's a feasible set.

583
00:27:07,830 --> 00:27:12,800
It's the intersection if you like of an
affine set with a non-negative orphan.

584
00:27:12,800 --> 00:27:13,860
That's what it is.

585
00:27:13,860 --> 00:27:18,850
And we want to write down, I mean so the
analytics center is this, right,

586
00:27:18,850 --> 00:27:23,590
that we want to minimize some of the logs
that 's

587
00:27:23,590 --> 00:27:27,090
the barrier associated with that subject
to Ax equals b.

588
00:27:27,090 --> 00:27:28,040
Okay.

589
00:27:28,040 --> 00:27:31,800
So, now the dual of that problem, you can
work it out, is this!

590
00:27:31,800 --> 00:27:33,130
You maximize this.

591
00:27:33,130 --> 00:27:37,170
And, here, it turns out, this thing is
strictly,

592
00:27:37,170 --> 00:27:41,660
the original function is strictly convex,
and that says that if we know the,

593
00:27:41,660 --> 00:27:44,950
if we solve the dual, we can recover the
primal solution.

594
00:27:44,950 --> 00:27:48,330
And there's a silly, simple formula (but
it's a really dumb formula) it's like,

595
00:27:48,330 --> 00:27:51,280
it doesn't even matter what it is, let's
just It's there.

596
00:27:51,280 --> 00:27:52,120
Right?
And you can,

597
00:27:52,120 --> 00:27:54,210
by solving the dual, you get the primal.

598
00:27:54,210 --> 00:27:55,380
Okay.

599
00:27:55,380 --> 00:27:56,480
And they're kind of interesting.

600
00:27:56,480 --> 00:27:57,330
Let's take a look at them.

601
00:27:57,330 --> 00:27:59,510
The dual is unconstrained.

602
00:27:59,510 --> 00:28:01,030
Well, not quite.

603
00:28:01,030 --> 00:28:04,920
There's an implicit constraint that A
transpose new is positive.

604
00:28:04,920 --> 00:28:05,460
Okay?
So,

605
00:28:05,460 --> 00:28:07,410
the domain of the dual is a bit
complicated.

606
00:28:07,410 --> 00:28:11,090
It's a set of vectors new for which A
transpose is positive.

607
00:28:11,090 --> 00:28:15,080
That's actually a open polyhedron, right?

608
00:28:15,080 --> 00:28:18,640
So, the domain of the primal is simple.

609
00:28:18,640 --> 00:28:20,620
It's r plus to the end.

610
00:28:20,620 --> 00:28:22,220
R plus plus to the end, right?

611
00:28:22,220 --> 00:28:24,860
It's all positive vectors, right?

612
00:28:24,860 --> 00:28:26,210
Sorry.

613
00:28:26,210 --> 00:28:29,410
Intersects with a x equals b, my mistake,
so, okay.

614
00:28:29,410 --> 00:28:30,730
So, whatever.

615
00:28:30,730 --> 00:28:31,570
So, let's look at some things.

616
00:28:31,570 --> 00:28:32,100
Here's what we can do.

617
00:28:33,390 --> 00:28:35,470
Let's just take Newton method with the
quality constraints.

618
00:28:35,470 --> 00:28:38,950
So we have a problem with 500 variables,
100 equality constraints, and

619
00:28:38,950 --> 00:28:42,020
if you run Newton's method, you get
something like this.

620
00:28:42,020 --> 00:28:43,220
And what we're doing,

621
00:28:43,220 --> 00:28:48,390
here, is these four traces show you just
four different starting points.

622
00:28:48,390 --> 00:28:50,390
Right?
And, you know, that looks very Newton-ish.

623
00:28:50,390 --> 00:28:55,360
This what you should, this is what you
should see, and you will see, presumably.

624
00:28:55,360 --> 00:28:56,090
Later.
This week?

625
00:28:56,090 --> 00:28:57,105
And then, they're doing it this week.

626
00:28:57,105 --> 00:28:58,750
Okay, so you'll see things that look like
this.

627
00:28:58,750 --> 00:29:00,930
We, this is what you're shooting for,
right?

628
00:29:00,930 --> 00:29:05,180
And the idea is, look that's a ridiculous,
that's a 10 to the 15 range there.

629
00:29:05,180 --> 00:29:08,660
So, this, this may be actually quite
perfectly good,

630
00:29:08,660 --> 00:29:11,040
respectable conversions, but on a plot.

631
00:29:11,040 --> 00:29:13,590
Whose range goes covers 10 to the 15.

632
00:29:13,590 --> 00:29:15,310
You're not seeing it, okay?

633
00:29:15,310 --> 00:29:17,910
But the key thing you should see is
something like that, right?

634
00:29:17,910 --> 00:29:20,670
That's, that, that's your basic Newton
behavior, right?

635
00:29:20,670 --> 00:29:24,330
By the way, that should also correspond to
the place where you do undamped steps.

636
00:29:24,330 --> 00:29:26,870
So the step size should be one in that
region.

637
00:29:26,870 --> 00:29:27,390
Fine.

638
00:29:27,390 --> 00:29:28,730
And this is what you expect.

639
00:29:28,730 --> 00:29:29,800
Okay.

640
00:29:29,800 --> 00:29:32,100
Oh, the initialization was interesting.

641
00:29:32,100 --> 00:29:36,940
We needed to know a positive vector with A
x equals b.

642
00:29:36,940 --> 00:29:37,800
Right?

643
00:29:37,800 --> 00:29:40,350
I mean if this is just a general problem,
you don't know such a thing, so

644
00:29:40,350 --> 00:29:41,440
that would be kind of a pain.

645
00:29:44,040 --> 00:29:46,440
Let's apply Newton Method to the dual
problem.

646
00:29:46,440 --> 00:29:47,670
Okay?

647
00:29:47,670 --> 00:29:51,030
So here, if you applied that means what we
do is we

648
00:29:51,030 --> 00:29:54,300
need a vector new zero that starts in the
domain.

649
00:29:54,300 --> 00:29:57,450
And we apply Newton's method, and then
here it is for four different things.

650
00:29:57,450 --> 00:30:02,230
And by the way, you might be tempted to
say, in fact I will give you the entire

651
00:30:02,230 --> 00:30:06,570
fallacious argument because I promise you
will hear this from someone you know.

652
00:30:06,570 --> 00:30:08,810
In which case you can correct their
misunderstanding.

653
00:30:09,840 --> 00:30:13,570
They would say, I have a fantastic method
for

654
00:30:13,570 --> 00:30:16,400
solving this problem, I'm going to solve
the dual.

655
00:30:16,400 --> 00:30:18,510
And someone says, well why would you do
that?

656
00:30:18,510 --> 00:30:19,790
And they would say the following.

657
00:30:19,790 --> 00:30:25,720
Number one, that original problem has 500
variables, the dual has a hundred.

658
00:30:25,720 --> 00:30:27,960
Last time I checked, 100 is smaller than
500.

659
00:30:27,960 --> 00:30:31,700
Person says, I'm solving, you're solving a
problem with five hundred variables,

660
00:30:31,700 --> 00:30:33,900
I'm solving one with a, with a, with a
hundred, 'Kay, that,

661
00:30:33,900 --> 00:30:39,390
that's fewer, that means it's a 125 times
faster, number one, number two,

662
00:30:39,390 --> 00:30:44,410
to add sort of, just a bonus, you're
taking 15, 20, steps.

663
00:30:44,410 --> 00:30:45,250
Look at this.

664
00:30:45,250 --> 00:30:48,090
This thing typically converges in eight or
nine steps.

665
00:30:48,090 --> 00:30:49,830
I'm half the number of steps.

666
00:30:49,830 --> 00:30:56,140
My advantage over you is 250x, plus you
get the added bonus of it sounds

667
00:30:56,140 --> 00:30:59,100
cool if you say, I'm solving the dual,
because it sounds more sophisticated.

668
00:30:59,100 --> 00:31:00,050
You know what I'm saying?

669
00:31:00,050 --> 00:31:02,500
Right?
Everyone following this argument?

670
00:31:02,500 --> 00:31:04,590
Okay.
We will see that it is totally and

671
00:31:04,590 --> 00:31:06,470
completely wrong.

672
00:31:06,470 --> 00:31:07,090
Okay.
But

673
00:31:07,090 --> 00:31:09,300
that's the argument you will hear it from
people.

674
00:31:09,300 --> 00:31:09,800
Okay.

675
00:31:11,690 --> 00:31:12,660
Now let's look at another one.

676
00:31:13,750 --> 00:31:16,110
Let's look at Infeasible Start Newton
Method.

677
00:31:16,110 --> 00:31:19,250
Now, this is its huge advantage.

678
00:31:19,250 --> 00:31:22,380
And this is probably why this is more
useful than, than a lot of the others.

679
00:31:22,380 --> 00:31:24,270
Although it depends on the situation.

680
00:31:24,270 --> 00:31:27,210
The initialization is very modest.

681
00:31:27,210 --> 00:31:30,670
The only thing you need, it does not have
to satisfy x equals b,

682
00:31:30,670 --> 00:31:32,020
it only has to be in the domain.

683
00:31:32,020 --> 00:31:34,350
The domain is the sum of the logs.

684
00:31:34,350 --> 00:31:35,180
Right?

685
00:31:35,180 --> 00:31:38,950
So, does someone want to suggest a
starting point?

686
00:31:41,880 --> 00:31:42,500
All 1s.

687
00:31:42,500 --> 00:31:43,020
Thank you.

688
00:31:43,020 --> 00:31:43,700
All 1s.

689
00:31:43,700 --> 00:31:47,230
That's an excellent starting point, and
you start from there.

690
00:31:47,230 --> 00:31:48,820
And then, it's actually kind of cool,
right?

691
00:31:48,820 --> 00:31:51,020
Because the first couple of, you know,

692
00:31:51,020 --> 00:31:55,900
first couple of steps, you'll see two
things happening, right?

693
00:31:55,900 --> 00:31:58,460
The function may or may not be going down.

694
00:31:58,460 --> 00:32:01,540
But, you'll be moving towards a x equals
b.

695
00:32:01,540 --> 00:32:02,920
All right, so, okay.

696
00:32:02,920 --> 00:32:05,580
And if you do that, you know, here, here
you go, you get between 10 and

697
00:32:05,580 --> 00:32:06,510
20 steps or something.

698
00:32:06,510 --> 00:32:08,020
That's the infeasible start method.

699
00:32:08,020 --> 00:32:09,800
That's the first one that actually puts.

700
00:32:09,800 --> 00:32:12,660
Approximately 0 actual burden on you.

701
00:32:12,660 --> 00:32:15,140
You know, you could write this code right
now, right?

702
00:32:15,140 --> 00:32:16,970
because it doesn't need some weird things.

703
00:32:16,970 --> 00:32:17,990
Okay.

704
00:32:17,990 --> 00:32:20,690
But now let's go back and look at these 3
methods.

705
00:32:22,240 --> 00:32:25,490
And if you look at them here's what's
going to come up.

706
00:32:25,490 --> 00:32:26,620
It's really quite cool.

707
00:32:28,170 --> 00:32:33,360
If you look at the original one you end up
solving This system

708
00:32:33,360 --> 00:32:38,380
of equations to determine to get a Newton
step and look what it is.

709
00:32:38,380 --> 00:32:41,560
Its diagonal a, a transpose.

710
00:32:41,560 --> 00:32:42,470
Right?

711
00:32:42,470 --> 00:32:46,810
That should induce in you an immediate
urge to eliminate,

712
00:32:46,810 --> 00:32:50,920
to eliminate the first top delta x block
and you will get the following.

713
00:32:50,920 --> 00:32:55,480
You will get a times you'll get this
reduced system times a transpose w

714
00:32:55,480 --> 00:32:56,210
equals b.

715
00:32:56,210 --> 00:32:57,160
Right?
So that's what you'll get and

716
00:32:57,160 --> 00:32:58,560
you'll solve that.

717
00:32:58,560 --> 00:32:59,060
Right?

718
00:33:00,370 --> 00:33:01,040
okay.

719
00:33:01,040 --> 00:33:05,100
Now, if you solve the Newton system for
the dual, well,

720
00:33:05,100 --> 00:33:09,140
you end up solving something that looks
like this, okay?

721
00:33:09,140 --> 00:33:11,190
But if you look at that carefully, you'll
realize, I mean,

722
00:33:11,190 --> 00:33:16,020
it's a totally different system from this,
right, but the structure is the same.

723
00:33:16,020 --> 00:33:19,440
It's A diagonal, A transpose.

724
00:33:19,440 --> 00:33:20,700
It's a system of that form.

725
00:33:21,850 --> 00:33:23,070
That's the coefficient equation.

726
00:33:23,070 --> 00:33:23,940
That's what you have to solve.

727
00:33:25,410 --> 00:33:25,910
Right?

728
00:33:27,070 --> 00:33:29,960
In the third one you, the Newton system
looks like that.

729
00:33:29,960 --> 00:33:33,290
I mean the only thing different here is
instead of the dual variable, you now have

730
00:33:33,290 --> 00:33:37,010
a delta dual variable, and instead of a
zero you have a residual here.

731
00:33:37,010 --> 00:33:37,690
And you get this.

732
00:33:37,690 --> 00:33:40,864
So, it's the same, actually, as the first
one and you get the same thing.

733
00:33:40,864 --> 00:33:43,120
The right hand side is different but that
doesn't matter.

734
00:33:43,120 --> 00:33:49,410
So, in each case, you end up solving A, D,
A transpose, W equals H.

735
00:33:49,410 --> 00:33:55,350
What that says is, the cost of the three
methods is identical despite

736
00:33:55,350 --> 00:34:00,710
this one appearing to have 100 variables,
whereas, this one has Well,

737
00:34:00,710 --> 00:34:05,840
this one has 500 variables, and this one,
down here, has 600.

738
00:34:05,840 --> 00:34:09,710
Because your, you would say, well I'm
updating the primal the dual, right?

739
00:34:09,710 --> 00:34:15,490
So, so all of this says don't be fooled
by, when someone just walks up to you and

740
00:34:15,490 --> 00:34:18,530
tells you about what's in Newton's method
Ask a few questions to

741
00:34:18,530 --> 00:34:20,950
prob if they know numerical linear algebra
and

742
00:34:20,950 --> 00:34:23,740
if they don't, ignore everything they have
to say about what is hard and

743
00:34:23,740 --> 00:34:26,660
what is easy because they are probably
completely wrong.

744
00:34:26,660 --> 00:34:28,240
So that's the idea.

745
00:34:28,240 --> 00:34:33,930
By the way I want to finish with just one
comment, if a has sparsity ADA transpose,

746
00:34:33,930 --> 00:34:35,420
that comes up all the time.

747
00:34:36,970 --> 00:34:41,790
And in fact, you can get the sparcity
patterns of ADA transpose.

748
00:34:41,790 --> 00:34:44,230
Right, which is what you'd have to solve,
directly.

749
00:34:44,230 --> 00:34:48,800
In this case, it's going to depend exactly
on this.

750
00:34:48,800 --> 00:34:52,970
This will have a non-zero in the I J
position, if and

751
00:34:52,970 --> 00:34:58,760
only if A, if you write, if you make a
like a, if you make a graph out of a or

752
00:34:58,760 --> 00:35:04,620
actually if you take the matrix a, only if
there is a, I guess it's a row,

753
00:35:04,620 --> 00:35:10,240
if, if i and j share a non zero in that
row, that, that's the condition.

754
00:35:10,240 --> 00:35:12,820
So, in a lot of practical applications,

755
00:35:12,820 --> 00:35:16,340
this would make it very clear what the
sparsity pattern is.

756
00:35:16,340 --> 00:35:19,750
Okay, so, anyway, so the bottom line is
for

757
00:35:19,750 --> 00:35:25,890
a lot of these problems smart that things
that would appear to be quite different

758
00:35:25,890 --> 00:35:30,390
in complexity right numbers of variables
things like that actually aren't, but

759
00:35:30,390 --> 00:35:34,050
that's assuming you are applying smart
linear algebra.

760
00:35:34,050 --> 00:35:35,850
Right.
If you just, if you just made these and

761
00:35:35,850 --> 00:35:40,560
then backslash on each one and they
weren't sparse You'd get you it would,

762
00:35:40,560 --> 00:35:42,690
it would come up with the same conclusion
you'd want.

763
00:35:42,690 --> 00:35:46,490
This would be way better because it's 100
variables way better than that and

764
00:35:46,490 --> 00:35:47,940
that's better than that.

765
00:35:47,940 --> 00:35:52,770
All right then next, the next example is
network flow optimization.

766
00:35:52,770 --> 00:35:59,000
So the idea is this we have a network we
have flows so

767
00:35:59,000 --> 00:36:03,020
Xi is the flow along arc Or edge i here.

768
00:36:04,510 --> 00:36:07,530
Fi i is a cost function for flow along
that arc.

769
00:36:07,530 --> 00:36:09,060
And ax equals b.

770
00:36:09,060 --> 00:36:12,220
A is the node-incidence matrix.

771
00:36:13,570 --> 00:36:17,070
So the node incidence matrix is the
following.

772
00:36:17,070 --> 00:36:20,560
It tells you whether there is, looks like
this.

773
00:36:20,560 --> 00:36:23,030
Right?
So you, you start with this.

774
00:36:23,030 --> 00:36:25,950
And you put a plus one and a minus one, in
each.

775
00:36:25,950 --> 00:36:27,720
So, each of these, these are the nodes.

776
00:36:28,970 --> 00:36:30,750
Here, and these are the arcs.

777
00:36:30,750 --> 00:36:36,100
And you put a plus 1 and minus 1 in each
column to tell you

778
00:36:36,100 --> 00:36:40,240
which way the, that arc, where it goes
from and where it goes to.

779
00:36:40,240 --> 00:36:43,610
Where it, it's head and tail is his
incident.

780
00:36:44,610 --> 00:36:48,980
And then this matrix here, is ranked
efficient.

781
00:36:48,980 --> 00:36:53,990
Well for example if you have one transpose
on the left times his mate, you get 0.

782
00:36:55,790 --> 00:36:57,930
That would actually be perfectly okay to
leave it here.

783
00:36:57,930 --> 00:37:03,280
But instead it's conventional to simply
remove one, row, and

784
00:37:03,280 --> 00:37:07,790
that gives you a reduced, node incidence
matrix at tilde.

785
00:37:07,790 --> 00:37:12,450
And if the graph is otherwise connected,
then that matrix is not ranked efficient.

786
00:37:12,450 --> 00:37:12,990
That's standard.

787
00:37:14,270 --> 00:37:17,510
We could say lots of things about that,
but this is the same,

788
00:37:17,510 --> 00:37:18,800
this is done in electrical engineering,

789
00:37:18,800 --> 00:37:23,635
this means you choose one node as the
ground, reference, or data node.

790
00:37:23,635 --> 00:37:27,150
Okay, so that's, that's all that matters
there.

791
00:37:27,150 --> 00:37:31,400
And Ax equals b is flow conservation.

792
00:37:31,400 --> 00:37:37,840
So that says that if I, at every point,
every node here, might,

793
00:37:37,840 --> 00:37:43,420
if I have some arcs flowing in, and some
arcs going out like this, it say, for

794
00:37:43,420 --> 00:37:50,690
example that's x3 that's x5, that's x,
x10, and that's x12.

795
00:37:50,690 --> 00:37:52,270
That says the following.

796
00:37:52,270 --> 00:37:57,020
That x3 plus x5, that's flowing in, that
has to equal.

797
00:37:57,020 --> 00:37:59,730
X10 plus x12, that's the outflow.

798
00:37:59,730 --> 00:38:07,010
So basically Ax equals b is flow
conservation, and I should say what b is.

799
00:38:07,010 --> 00:38:10,680
B most, if b on the right hand side, over
here,

800
00:38:10,680 --> 00:38:14,150
the right hand side of this equation, that
is an external source or sync.

801
00:38:14,150 --> 00:38:17,900
So that looks like that, right, so if,

802
00:38:17,900 --> 00:38:23,412
if there is nothing else connected to that
node, then that the associated

803
00:38:23,412 --> 00:38:29,870
B sub of i is zero otherwise that's a
source or sink, right, so okay.

804
00:38:29,870 --> 00:38:34,890
So, this says among all flows please find
the one that minimizes the cause,

805
00:38:34,890 --> 00:38:36,780
that's the cause.

806
00:38:36,780 --> 00:38:39,230
Okay, so we are going to take a look at
that,

807
00:38:40,740 --> 00:38:44,300
and we're going to figure out how to, how
to solve this problem.

808
00:38:44,300 --> 00:38:48,790
More interestingly, we'll see how
structure comes in.

809
00:38:48,790 --> 00:38:51,130
Well the KKT matrix looks like this.

810
00:38:51,130 --> 00:38:56,050
We minimize a sum of functions of the
individual variables.

811
00:38:56,050 --> 00:38:58,510
But I know what the Hessian of such thing
looks like.

812
00:38:58,510 --> 00:39:01,920
The Hessian is the diagonal of the second
derivatives.

813
00:39:01,920 --> 00:39:04,480
Right?
So, the Hessian, is here.

814
00:39:04,480 --> 00:39:05,580
It's diagonal.

815
00:39:05,580 --> 00:39:07,940
Just like that, and that's up here.

816
00:39:07,940 --> 00:39:10,500
Then, I put A, over here.

817
00:39:10,500 --> 00:39:13,060
Because I have AX equals B as the
constraint.

818
00:39:13,060 --> 00:39:14,040
I have, this is my Newton.

819
00:39:14,040 --> 00:39:19,620
My KKT system is, has involves A and A
transpose here, like that.

820
00:39:19,620 --> 00:39:22,620
And I have to solve that equation and, you
know, you're staring at it,

821
00:39:22,620 --> 00:39:27,750
you see a a you see a, a diagonal matrix
here and

822
00:39:27,750 --> 00:39:32,890
by now, you should have a very strong urge
to eliminate that block, right?

823
00:39:32,890 --> 00:39:34,340
And if you do eliminate that block,

824
00:39:34,340 --> 00:39:38,540
you're going to get this thing that is the
negative sure complement of this matrix

825
00:39:39,540 --> 00:39:40,780
with respect to H.

826
00:39:40,780 --> 00:39:41,820
So you get this thing.

827
00:39:41,820 --> 00:39:42,355
Now, by the way,

828
00:39:42,355 --> 00:39:46,230
H inverse shouldn't scare you, even though
if H is big, because H is diagonal.

829
00:39:46,230 --> 00:39:49,330
So, that was the whole point after all,
it's easy to invert.

830
00:39:50,410 --> 00:39:53,420
So you end up solving a system that looks
like that, okay?

831
00:39:54,740 --> 00:40:00,250
But we can actually say more, because this
is A times H inverse,

832
00:40:00,250 --> 00:40:02,100
that's diagonal times A transpose.

833
00:40:02,100 --> 00:40:03,850
And the sparsity pattern,

834
00:40:03,850 --> 00:40:07,110
well it's the same as the sparsity pattern
of AA transpose.

835
00:40:07,110 --> 00:40:14,280
Now A is here, a FAT matrix like this, so
AA transpose is the small one.

836
00:40:14,280 --> 00:40:16,424
A diagonal one doesn't affect this
sparsity pattern.

837
00:40:16,424 --> 00:40:20,110
Okay, and this is nodes here, right?

838
00:40:20,110 --> 00:40:22,230
So, this is a nodes by nodes matrix, and

839
00:40:22,230 --> 00:40:26,230
we can say exactly what its sparsity
pattern is.

840
00:40:26,230 --> 00:40:29,580
Okay, this sparsity pattern is the
following.

841
00:40:29,580 --> 00:40:34,540
An entry is non-zero here if and only if
nodes I and J are connected by an arc.

842
00:40:35,630 --> 00:40:36,250
Okay?

843
00:40:36,250 --> 00:40:40,180
So, that suggests, well, depends on the
network, of course, right?

844
00:40:40,180 --> 00:40:43,200
So that suggests something like this.

845
00:40:43,200 --> 00:40:47,820
Well I can, I don't know, I can ask you a
question about it.

846
00:40:47,820 --> 00:40:55,450
This says, that, if the degree, right, if,
if each node has a maximum degree d, say.

847
00:40:55,450 --> 00:40:56,990
That's the number of connected edges.

848
00:40:58,630 --> 00:41:03,420
Then that tells you that every row of this
matrix and column,

849
00:41:03,420 --> 00:41:08,925
by the way, has a maximum number of 3,
entries.

850
00:41:08,925 --> 00:41:10,440
Non-zeros entries in it, right?

851
00:41:10,440 --> 00:41:13,210
And then it tells you that this matrix is
probably sparse too, and

852
00:41:13,210 --> 00:41:17,970
that suggests you can do a sparse cholesky
on this matrix, right?

853
00:41:17,970 --> 00:41:21,120
And, in fact, if the graph is sparse
enough, and

854
00:41:21,120 --> 00:41:26,490
if the gods who control the heuristics use
to order,

855
00:41:27,510 --> 00:41:32,750
gelesky factorizations for sparse matrices
are smiling on you,

856
00:41:32,750 --> 00:41:37,450
then there will be very little, if you're
lucky there will be not much fill in.

857
00:41:37,450 --> 00:41:42,270
And that says basically that you can
solve, you can solve this system and

858
00:41:42,270 --> 00:41:48,860
do one Newton Step in something that's not
much more time than the number of flows or

859
00:41:48,860 --> 00:41:51,200
solve n, solve n.

860
00:41:51,200 --> 00:41:54,220
And you could, if you're lucky you could
solve a pretty big flow system pretty

861
00:41:54,220 --> 00:42:00,170
quickly in Newton steps, would only cost
you something that's really kind of on.

862
00:42:00,170 --> 00:42:03,430
I mean, it's going to have a not totally
small multiplier in front, but

863
00:42:03,430 --> 00:42:06,300
it's going to be something that would have
not many,

864
00:42:06,300 --> 00:42:09,220
it wouldn't be much more then just walking
over the graph one.

865
00:42:09,220 --> 00:42:12,050
So, and this would allow you to solve
this.

866
00:42:12,050 --> 00:42:13,330
Very, very fast.

867
00:42:13,330 --> 00:42:15,329
Again, that requires the sparsity, here.
