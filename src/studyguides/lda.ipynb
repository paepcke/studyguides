{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "\n",
    "from gensim import corpora, models\n",
    "from ipywidgets import IntProgress\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def srt_to_strs(f):\n",
    "    '''Convert a .srt file to a string'''\n",
    "    text_lines = []\n",
    "    while True:\n",
    "        _seq_no = f.readline().strip()\n",
    "        if _seq_no == '': break\n",
    "        _time_str = f.readline()\n",
    "        \n",
    "        while True:\n",
    "            text_line = f.readline().strip()\n",
    "            if text_line == '': break\n",
    "            text_line = text_line.replace('&#39;', \"'\")\n",
    "            text_line = text_line.replace('&gt;', '')\n",
    "            text_line = text_line.replace('[inaudible]', '')\n",
    "            text_line = text_line.replace('gonna', 'going to')\n",
    "            text_lines.append(text_line)\n",
    "            \n",
    "    return ' '.join(text_lines)\n",
    "\n",
    "DATA_DIR = '../../data/'\n",
    "CAPTIONS_DIR = DATA_DIR + 'compilers_captions/'\n",
    "SRT_FILE_NAMES = [CAPTIONS_DIR + file_name for file_name in os.listdir(CAPTIONS_DIR) if file_name.endswith('.srt')]\n",
    "\n",
    "docs = []\n",
    "tokenized_docs = []\n",
    "\n",
    "for name in SRT_FILE_NAMES:\n",
    "    with open(name) as f:\n",
    "        doc = unicode(srt_to_strs(f), 'utf-8').lower()\n",
    "        docs.append(doc)\n",
    "        doc = doc.replace(\"'\", '')\n",
    "        tokenized_docs.append([nltk.word_tokenize(sent) for sent in nltk.sent_tokenize(doc)])\n",
    "        \n",
    "\n",
    "\n",
    "# for doc in documents:\n",
    "#     sents = [nltk.word_tokenize(sent) for sent in nltk.sent_tokenize(doc)]\n",
    "#     tags.extend(tagger.tag_sents(sents))\n",
    "#     prog.value += 1\n",
    "        \n",
    "        \n",
    "# # Remove stopwords\n",
    "# stopwords = set()\n",
    "# with open('/Users/andrewlamb/Google_Drive/Stanford/CS199/RAKE-tutorial/SmartStoplistAdditional.txt') as f:\n",
    "#     for line in f:\n",
    "#         stopwords.add(line.strip())\n",
    "        \n",
    "#     stopwords = stopwords.union({',', '.', '?', '!'})\n",
    "        \n",
    "# stopped_tokens = [[tok for tok in toks if tok not in stopwords] for toks in tokens]\n",
    "\n",
    "# stemmer = nltk.stem.PorterStemmer()\n",
    "# stemmed_docs = [[stemmer.stem(token) for token in doc] for doc in stopped_tokens]\n",
    "\n",
    "# dictionary = corpora.Dictionary(stemmed_docs)\n",
    "# corpus = [dictionary.doc2bow(text) for text in stemmed_docs]\n",
    "\n",
    "# ldamodel = models.ldamodel.LdaModel(corpus, num_topics=20, id2word=dictionary, passes=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# POS Tag\n",
    "pos_docs = []\n",
    "prog = IntProgress(min=0, max=len(docs))\n",
    "display(prog)\n",
    "\n",
    "os.environ['CLASSPATH'] = '/Users/andrewlamb/Downloads/stanford-postagger-2015-12-09'\n",
    "os.environ['STANFORD_MODELS'] = '/Users/andrewlamb/Downloads/stanford-postagger-2015-12-09'\n",
    "tagger = nltk.tag.StanfordPOSTagger('models/english-bidirectional-distsim.tagger')\n",
    "\n",
    "for doc in tokenized_docs:\n",
    "    pos_docs.append(tagger.tag_sents(doc))\n",
    "    prog.value += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def extract_tags(tree):\n",
    "    results = []\n",
    "    \n",
    "    if type(tree) == nltk.tree.Tree:\n",
    "        if tree.label() == 'NP':\n",
    "            phrase = ' '.join([word for word, _pos in tree])\n",
    "            phrase = phrase.lower()\n",
    "            results.append(phrase)\n",
    "\n",
    "        for child in tree:\n",
    "            results.extend(extract_tags(child))\n",
    "    \n",
    "    return results\n",
    "\n",
    "grammar = '''\n",
    "NP: {(<JJ>|<JJS>|<JJR>)*(<NN>|<NNS>|<NNP>|<NNPS>)+} \n",
    "'''\n",
    "\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "candidates = set()\n",
    "\n",
    "chunked_docs = []\n",
    "\n",
    "for doc in pos_docs:\n",
    "    chunked_doc = []\n",
    "    for sent in doc:\n",
    "        result = cp.parse(sent)\n",
    "        chunked_doc.extend(extract_tags(result))\n",
    "    chunked_docs.append(chunked_doc)\n",
    "    \n",
    "# Remove stopwords\n",
    "stopwords = set()\n",
    "with open('/Users/andrewlamb/Google_Drive/Stanford/CS199/RAKE-tutorial/SmartStoplistAdditional.txt') as f:\n",
    "    for line in f:\n",
    "        stopwords.add(line.strip())\n",
    "stopped_docs = []\n",
    "for doc in chunked_docs:\n",
    "    stopped_docs.append([token for token in doc if len(token) > 2 and token not in stopwords])\n",
    "\n",
    "stemmer = nltk.stem.PorterStemmer()\n",
    "stemmed_docs = []\n",
    "for doc in stopped_docs:\n",
    "    stemmed_doc = []\n",
    "    for chunk in doc:\n",
    "        stemmed_doc.append(' '.join([stemmer.stem(word) for word in nltk.word_tokenize(chunk)]))\n",
    "\n",
    "    stemmed_docs.append(stemmed_doc)\n",
    "\n",
    "    \n",
    "dictionary = corpora.Dictionary(stemmed_docs)\n",
    "corpus = [dictionary.doc2bow(text) for text in stemmed_docs]\n",
    "\n",
    "ldamodel = models.ldamodel.LdaModel(corpus, num_topics=20, id2word=dictionary, passes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'pre-process step'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  u'0.078*state + 0.044*input + 0.033*machin + 0.023*start state + 0.022*final state + 0.022*set + 0.021*transit + 0.019*languag + 0.019*nfa + 0.014*dfa + 0.012*string + 0.012*regular express + 0.009*automaton + 0.008*java + 0.008*end + 0.007*epsilon closur + 0.007*case + 0.006*time + 0.006*fact + 0.006*finit automaton'),\n",
      " (1,\n",
      "  u'0.073*object + 0.045*pointer + 0.024*program + 0.016*point + 0.012*old space + 0.012*loop + 0.012*new space + 0.011*memori + 0.010*reachabl object + 0.009*assign + 0.009*statement + 0.009*space + 0.007*string + 0.007*time + 0.007*thing + 0.007*garbag collect + 0.006*copi + 0.006*keyword + 0.006*forward pointer + 0.006*futur'),\n",
      " (2,\n",
      "  u'0.047*code + 0.026*string + 0.018*list + 0.016*program + 0.014*express + 0.011*time + 0.011*instruct + 0.009*case + 0.008*assign + 0.008*thing + 0.008*optim + 0.007*result + 0.007*languag + 0.006*compil + 0.006*item + 0.006*constant + 0.006*accumul + 0.005*regist + 0.005*number + 0.005*predic'),\n",
      " (3,\n",
      "  u'0.027*optim + 0.025*regular express + 0.018*digit + 0.012*basic block + 0.012*letter + 0.010*string + 0.010*compil + 0.010*program + 0.010*block + 0.008*statement + 0.007*charact + 0.007*machin + 0.006*sequenc + 0.006*kind + 0.006*definit + 0.006*number + 0.006*thing + 0.006*control flow graph + 0.006*union + 0.006*e-mail address'),\n",
      " (4,\n",
      "  u'0.074*type + 0.024*express + 0.019*rule + 0.018*store + 0.017*environ + 0.017*variabl + 0.016*class + 0.014*object + 0.014*method + 0.011*type check + 0.010*argument + 0.010*case + 0.009*self type + 0.008*result + 0.008*sub type + 0.008*context + 0.007*self-typ + 0.007*initi + 0.007*evalu + 0.007*self object'),\n",
      " (5,\n",
      "  u'0.024*program + 0.015*bottom + 0.015*constant + 0.015*interpret + 0.014*compil + 0.011*top + 0.010*fortran + 0.008*rule + 0.008*abstract valu + 0.008*number + 0.008*thing + 0.007*valu + 0.007*inform + 0.007*machin + 0.007*program languag + 0.006*percent + 0.006*speed code + 0.006*today + 0.006*pictur + 0.006*time'),\n",
      " (6,\n",
      "  u'0.038*except + 0.026*program + 0.020*array + 0.019*compil + 0.017*object + 0.014*code + 0.010*thing + 0.010*type + 0.010*java + 0.009*kind + 0.009*memori + 0.008*data + 0.008*express + 0.008*jack + 0.008*point + 0.007*sentenc + 0.007*method + 0.007*problem + 0.007*word + 0.006*lot'),\n",
      " (7,\n",
      "  u'0.032*prefix + 0.025*product + 0.019*open paren + 0.018*input + 0.016*right hand side + 0.011*pars tree + 0.011*part + 0.010*viabl prefix + 0.010*left + 0.009*dot + 0.008*number + 0.007*grammar + 0.007*order + 0.007*termin + 0.007*free grammar + 0.006*item + 0.006*close paren + 0.006*omega + 0.005*input string + 0.005*case'),\n",
      " (8,\n",
      "  u'0.026*point + 0.026*pointer + 0.019*inform + 0.016*assign + 0.016*predecessor + 0.011*loop + 0.011*analysi + 0.011*= zero + 0.009*edg + 0.009*control flow graph + 0.007*predic + 0.007*cycl + 0.007*path + 0.007*c++ + 0.006*word + 0.005*branch + 0.005*bottom + 0.005*entri point + 0.005*control + 0.005*statement'),\n",
      " (9,\n",
      "  u'0.035*languag + 0.022*string + 0.013*program + 0.012*time + 0.012*thing + 0.011*lot + 0.010*peopl + 0.010*type + 0.009*program languag + 0.008*integ + 0.008*tabl + 0.008*set + 0.008*oper + 0.008*question + 0.008*java + 0.007*regular express + 0.007*number + 0.007*compil + 0.006*kind + 0.006*fact'),\n",
      " (10,\n",
      "  u'0.030*cach + 0.022*set + 0.017*languag + 0.017*express + 0.017*program + 0.015*compil + 0.014*string + 0.011*syntax + 0.010*number + 0.010*instruct + 0.009*mean + 0.008*loop + 0.008*regular express + 0.008*formal languag + 0.008*alphabet + 0.007*element + 0.007*notat + 0.007*semant + 0.007*optim + 0.006*order'),\n",
      " (11,\n",
      "  u'0.019*activ + 0.015*time + 0.015*procedur + 0.014*life time + 0.012*program + 0.012*assumpt + 0.012*main + 0.010*lifetim + 0.009*execut + 0.009*argument + 0.009*statement + 0.009*call + 0.009*except + 0.007*problem + 0.007*point + 0.007*step + 0.005*moment + 0.005*code gener + 0.005*variabl + 0.005*code'),\n",
      " (12,\n",
      "  u'0.038*class + 0.023*accumul + 0.019*result + 0.016*argument + 0.015*machin + 0.015*top + 0.013*regist + 0.011*featur + 0.010*java + 0.009*instruct + 0.009*scope + 0.008*program + 0.008*initi + 0.008*oper + 0.007*error + 0.007*express + 0.007*memori + 0.006*thing + 0.006*content + 0.006*pointer'),\n",
      " (13,\n",
      "  u'0.026*refer count + 0.024*temporari + 0.024*number + 0.023*languag + 0.023*string + 0.014*regular express + 0.013*token class + 0.013*keyword + 0.013*input + 0.012*identifi + 0.011*white space + 0.009*lexic specif + 0.008*thing + 0.008*class + 0.008*result + 0.007*predic + 0.007*mani temporari + 0.007*space + 0.007*prefix + 0.007*activ record'),\n",
      " (14,\n",
      "  u'0.032*error + 0.030*program + 0.020*data + 0.016*compil + 0.013*word + 0.011*word boundari + 0.011*express + 0.010*machin + 0.010*byte + 0.009*string + 0.009*mani error + 0.008*peopl + 0.008*lot + 0.007*thing + 0.007*memori + 0.007*correct + 0.007*parser + 0.006*align + 0.006*programm + 0.006*program languag'),\n",
      " (15,\n",
      "  u'0.051*product + 0.043*state + 0.036*grammar + 0.027*input + 0.024*alpha + 0.021*item + 0.020*epsilon + 0.019*move + 0.015*set + 0.014*right hand side + 0.013*symbol + 0.013*start symbol + 0.012*bottom + 0.011*termin + 0.011*time + 0.011*automaton + 0.010*int + 0.009*reduct + 0.009*dfa + 0.009*dot'),\n",
      " (16,\n",
      "  u'0.040*pars tree + 0.022*deriv + 0.019*tree + 0.015*string + 0.013*mark bit + 0.013*express + 0.009*type ent + 0.009*program + 0.009*product + 0.008*start symbol + 0.008*type int + 0.008*conclus + 0.008*free list + 0.008*rightmost deriv + 0.008*leav + 0.008*compil + 0.007*same thing + 0.007*root + 0.007*abstract syntax tree + 0.007*mark phase'),\n",
      " (17,\n",
      "  u'0.047*attribut + 0.043*object + 0.032*class + 0.025*method + 0.022*program + 0.012*graph + 0.012*number + 0.010*code + 0.010*color + 0.010*compil + 0.009*cool + 0.009*case + 0.009*memori + 0.007*store + 0.007*languag + 0.006*neighbor + 0.006*layout + 0.006*order + 0.005*heap + 0.005*node'),\n",
      " (18,\n",
      "  u'0.033*statement + 0.018*product + 0.018*point + 0.014*input + 0.013*rule + 0.013*program point + 0.012*inform + 0.012*activ record + 0.010*function + 0.009*assign + 0.009*program + 0.008*path + 0.008*execut + 0.008*fact + 0.008*case + 0.008*predecessor + 0.008*constant + 0.007*result + 0.007*code + 0.006*video'),\n",
      " (19,\n",
      "  u'0.034*thread + 0.020*method + 0.015*interfac + 0.015*frame pointer + 0.015*function + 0.015*code + 0.013*activ record + 0.013*pointer + 0.012*return address + 0.011*class + 0.011*argument + 0.009*function call + 0.008*instruct + 0.008*entri + 0.008*object + 0.008*intermedi languag + 0.007*call + 0.007*intermedi code + 0.007*languag + 0.007*frame')]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "pprint(ldamodel.print_topics(num_topics=20, num_words=20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(stemmed_docs)\n",
    "corpus = [dictionary.doc2bow(text) for text in stemmed_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "157"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
