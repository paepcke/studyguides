{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk, os, itertools, re\n",
    "\n",
    "from tfidf_extractor import TfidfExtractor\n",
    "\n",
    "from ipywidgets import IntProgress\n",
    "from IPython.display import display\n",
    "from tabulate import tabulate\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def srt_to_strs(f):\n",
    "    '''Convert a .srt file to a string'''\n",
    "    text_lines = []\n",
    "    while True:\n",
    "        _seq_no = f.readline().strip()\n",
    "        if _seq_no == '': break\n",
    "        _time_str = f.readline()\n",
    "        \n",
    "        while True:\n",
    "            text_line = f.readline().strip()\n",
    "            if text_line == '': break\n",
    "            text_line = text_line.replace('&#39;', \"'\")\n",
    "            text_line = text_line.replace('&gt;', '')\n",
    "            text_line = text_line.replace('[inaudible]', '')\n",
    "            text_line = text_line.replace('gonna', 'going to')\n",
    "            text_lines.append(text_line)\n",
    "            \n",
    "    return ' '.join(text_lines)\n",
    "\n",
    "DATA_DIR = '../../data/'\n",
    "CAPTIONS_DIR = DATA_DIR + 'compilers_captions/'\n",
    "SRT_FILE_NAMES = [CAPTIONS_DIR + file_name for file_name in os.listdir(CAPTIONS_DIR) if file_name.endswith('.srt')]\n",
    "\n",
    "documents = []\n",
    "\n",
    "for name in SRT_FILE_NAMES:\n",
    "    with open(name) as f:\n",
    "        document = srt_to_strs(f)\n",
    "        documents.append(unicode(document, 'utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "POS tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tags = []\n",
    "prog = IntProgress(min=0, max=len(documents))\n",
    "display(prog)\n",
    "\n",
    "os.environ['CLASSPATH'] = '/Users/andrewlamb/Downloads/stanford-postagger-2015-12-09'\n",
    "os.environ['STANFORD_MODELS'] = '/Users/andrewlamb/Downloads/stanford-postagger-2015-12-09'\n",
    "tagger = nltk.tag.StanfordPOSTagger('models/english-bidirectional-distsim.tagger')\n",
    "\n",
    "for doc in documents:\n",
    "    sents = [nltk.word_tokenize(sent) for sent in nltk.sent_tokenize(doc)]\n",
    "    tags.extend(tagger.tag_sents(sents))\n",
    "    prog.value += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Form a set of candidate keywords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def extract_tags(tree):\n",
    "    results = []\n",
    "    \n",
    "    if type(tree) == nltk.tree.Tree:\n",
    "        if tree.label() == 'NP':\n",
    "            phrase = ' '.join([word for word, _pos in tree])\n",
    "            phrase = phrase.lower()\n",
    "            phrase = re.sub(\"[().,']\", '', phrase)\n",
    "            phrase = re.sub('-', ' ', phrase)\n",
    "            results.append(phrase)\n",
    "\n",
    "        for child in tree:\n",
    "            results.extend(extract_tags(child))\n",
    "    \n",
    "    return results\n",
    "\n",
    "grammar = '''\n",
    "NP: {(<JJ>|<JJS>|<JJR>)*(<NN>|<NNS>|<NNP>|<NNPS>)+} \n",
    "'''\n",
    "\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "candidates = set()\n",
    "\n",
    "prog = IntProgress(min=0, max=len(tags))\n",
    "display(prog)\n",
    "\n",
    "for sent in tags:\n",
    "    result = cp.parse(sent)\n",
    "        \n",
    "    candidates = candidates.union(set(extract_tags(result)))\n",
    "    prog.value += 1\n",
    "    \n",
    "    assert 'weve' not in candidates\n",
    "    \n",
    "# Remove stopwords\n",
    "stopwords = set()\n",
    "with open('/Users/andrewlamb/Google_Drive/Stanford/CS199/RAKE-tutorial/SmartStoplistAdditional.txt') as f:\n",
    "    for line in f:\n",
    "        stopwords.add(line.strip())\n",
    "        \n",
    "candidates.difference_update(stopwords)\n",
    "\n",
    "# Remove words less than 2 letters long\n",
    "candidates = set([tag for tag in candidates if len(tag) > 2]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stem the candidate keywords and documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stemmer = nltk.stem.WordNetLemmatizer()\n",
    "candidates = set([' '.join([stemmer.lemmatize(word) for word in nltk.word_tokenize(candidate)]) for candidate in candidates])\n",
    "\n",
    "# for i, document in enumerate(documents):\n",
    "#     documents[i] = ' '.join([stemmer.lemmatize(word) for word in nltk.word_tokenize(document)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "index_size = 100\n",
    "\n",
    "tfidf = TfidfExtractor(ngram_range=range(1,5), vocabulary=candidates)\n",
    "words, scores = zip(*tfidf.extract_documents(documents)[:index_size])\n",
    "tfs = [tfidf._tfs[word] for word in words]\n",
    "dfs = [tfidf._dfs[word] for word in words]\n",
    "idfs = [tfidf._idfs[word] for word in words]\n",
    "\n",
    "print(tabulate(\n",
    "        zip(range(len(words)), words, scores, tfs, dfs, idfs), \n",
    "        headers=['rank', 'word', 'tfidf', 'tf', 'df', 'idf']\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "split_documents = []\n",
    "split_len = 15\n",
    "\n",
    "for document in documents:\n",
    "    sents = nltk.sent_tokenize(document)\n",
    "    for i in range(0, len(sents), split_len):\n",
    "        split_documents.append(' '.join(sents[i:i + split_len]))\n",
    "\n",
    "index_size = 100\n",
    "\n",
    "tfidf = TfidfExtractor(ngram_range=range(1,5), vocabulary=candidates)\n",
    "words, scores = zip(*tfidf.extract_documents(split_documents)[:index_size])\n",
    "tfs = [tfidf._tfs[word] for word in words]\n",
    "dfs = [tfidf._dfs[word] for word in words]\n",
    "idfs = [tfidf._idfs[word] for word in words]\n",
    "\n",
    "print(tabulate(\n",
    "        zip(range(len(words)), words, scores, tfs, dfs, idfs), \n",
    "        headers=['rank', 'word', 'tfidf', 'tf', 'df', 'idf']\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(ngram_range=(2, max(len(word.split()) for word in candidates)))\n",
    "X = tfidf.fit_transform(documents)\n",
    "sums = X.sum(axis=0)\n",
    "sorted_inds = sums.argsort()\n",
    "inverse_voc = {v: k for k, v in tfidf.vocabulary_.items()}\n",
    "words = [inverse_voc[ind] for ind in sorted_inds.tolist()[0]]\n",
    "words = [word for word in words if word in candidates]\n",
    "words = list(reversed(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  rank  word\n",
      "------  -----------------------\n",
      "     0  self type\n",
      "     1  hand side\n",
      "     2  parse tree\n",
      "     3  activation record\n",
      "     4  right hand\n",
      "     5  non terminal\n",
      "     6  start symbol\n",
      "     7  right hand side\n",
      "     8  regular expression\n",
      "     9  type check\n",
      "    10  open paren\n",
      "    11  type checking\n",
      "    12  sub type\n",
      "    13  start state\n",
      "    14  final state\n",
      "    15  frame pointer\n",
      "    16  no type\n",
      "    17  little bit\n",
      "    18  dynamic type\n",
      "    19  run time\n",
      "    20  control flow\n",
      "    21  abstract syntax\n",
      "    22  first thing\n",
      "    23  programming language\n",
      "    24  abstract syntax tree\n",
      "    25  lexical analysis\n",
      "    26  code generation\n",
      "    27  next input\n",
      "    28  garbage collection\n",
      "    29  white space\n",
      "    30  control flow graph\n",
      "    31  reference counting\n",
      "    32  recursive descent\n",
      "    33  look ahead\n",
      "    34  intermediate code\n",
      "    35  least upper\n",
      "    36  type system\n",
      "    37  reference count\n",
      "    38  left hand side\n",
      "    39  basic block\n",
      "    40  semantic analysis\n",
      "    41  id id\n",
      "    42  static type\n",
      "    43  type self type\n",
      "    44  return address\n",
      "    45  self object\n",
      "    46  input string\n",
      "    47  mark bit\n",
      "    48  intermediate language\n",
      "    49  bottom up parsing\n",
      "    50  old space\n",
      "    51  program point\n",
      "    52  input pointer\n",
      "    53  vertical bar\n",
      "    54  sub typing\n",
      "    55  same thing\n",
      "    56  token class\n",
      "    57  first position\n",
      "    58  left to right\n",
      "    59  sub expression\n",
      "    60  deterministic automaton\n",
      "    61  return type\n",
      "    62  empty string\n",
      "    63  new space\n",
      "    64  next thing\n",
      "    65  increment method\n",
      "    66  type int\n",
      "    67  last time\n",
      "    68  new object\n",
      "    69  close paren\n",
      "    70  first production\n",
      "    71  finite automaton\n",
      "    72  lexical specification\n",
      "    73  new store\n",
      "    74  left recursive\n",
      "    75  assembly language\n",
      "    76  whole thing\n",
      "    77  interference graph\n",
      "    78  e1 e2\n",
      "    79  type t1\n",
      "    80  type checker\n",
      "    81  function call\n",
      "    82  context free grammar\n",
      "    83  free grammar\n",
      "    84  next pointer\n",
      "    85  type object\n",
      "    86  if then else\n",
      "    87  input symbol\n",
      "    88  point interface\n",
      "    89  type in\n",
      "    90  register allocation\n",
      "    91  operational semantics\n",
      "    92  memory location\n",
      "    93  entire expression\n",
      "    94  assembly code\n",
      "    95  word boundary\n",
      "    96  class name\n",
      "    97  main method\n",
      "    98  entry point\n",
      "    99  parsing algorithm\n"
     ]
    }
   ],
   "source": [
    "print(tabulate(\n",
    "        zip(range(len(words)), words)[:100], \n",
    "        headers=['rank', 'word']\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'little bit', u'programming language', u'programming languages']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(set([lemmatizer.lemmatize(word) for word in words[:50] if word in documents[2]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "final = set()\n",
    "for doc in documents:\n",
    "    final = final.union(set([word for word in words if word in doc][:20]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(len(word.split()) for word in candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
